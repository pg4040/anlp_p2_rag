Faculty Name: yulia tsvetkov
Metadata:
Paperid: 94be9ee72de86a909fd29a739f203e1aa6bc165e
Title: On the Zero-Shot Generalization of Machine-Generated Text Detectors
Year: 2023
Abstract: The rampant proliferation of large language models, fluent enough to generate text indistinguishable from human-written language, gives unprecedented importance to the detection of machine-generated text. This work is motivated by an important research question: How will the detectors of machine-generated text perform on outputs of a new generator, that the detectors were not trained on? We begin by collecting generation data from a wide range of LLMs, and train neural detectors on data from each generator and test its performance on held-out generators. While none of the detectors can generalize to all generators, we observe a consistent and interesting pattern that the detectors trained on data from a medium-size LLM can zero-shot generalize to the larger version. As a concrete application, we demonstrate that robust detectors can be built on an ensemble of training data from medium-sized models.
Authors: Xiao Pu, Jingyu Zhang, Xiaochuang Han, Yulia Tsvetkov, Tianxing He
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'It is demonstrated that robust detectors can be built on an ensemble of training data from medium-sized models, and a consistent and interesting pattern is observed that the detectors trained on data from a medium-size LLM can zero-shot generalize to the larger version.'}
