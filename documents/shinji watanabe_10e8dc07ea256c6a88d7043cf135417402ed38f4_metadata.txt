Faculty Name: shinji watanabe
Metadata:
Paperid: 10e8dc07ea256c6a88d7043cf135417402ed38f4
Title: Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization
Year: 2023
Abstract: We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper
Authors: Puyuan Peng, Brian Yan, Shinji Watanabe, David F. Harwath
Venue: Interspeech
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work investigates the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering, and designs task-specific prompts that improve performance on the three zero-shot tasks and even outperform SotA supervised models on some datasets.'}
