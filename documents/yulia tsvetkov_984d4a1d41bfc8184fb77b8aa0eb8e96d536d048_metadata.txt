Faculty Name: yulia tsvetkov
Metadata:
Paperid: 984d4a1d41bfc8184fb77b8aa0eb8e96d536d048
Title: Trusting Your Evidence: Hallucinate Less with Context-aware Decoding
Year: 2023
Abstract: Language models (LMs) often struggle to pay enough attention to the input context, and generate texts that are unfaithful or contain hallucinations. To mitigate this issue, we present context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context. Our experiments show that CAD, without additional training, significantly improves the faithfulness of different LM families, including OPT, GPT, LLaMA and FLAN-T5 for summarization tasks (e.g., 14.3% gain for LLaMA in factuality metrics). Furthermore, CAD is particularly effective in overriding a model's prior knowledge when it contradicts the provided context, leading to substantial improvements in tasks where resolving the knowledge conflict is essential.
Authors: Weijia Shi, Xiaochuang Han, M. Lewis, Yulia Tsvetkov, Luke Zettlemoyer, S. Yih
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'Context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context, improves the faithfulness of different LM families.'}
