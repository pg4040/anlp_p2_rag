Faculty Name: jeffrey bigham
Metadata:
Paperid: 8ab27849799286459465d2262f926354093b20a9
Title: USB: A Unified Summarization Benchmark Across Tasks and Domains
Year: 2023
Abstract: While the NLP community has produced numerous summarization benchmarks, none provide the rich annotations required to simultaneously address many important problems related to control and reliability. We introduce a Wikipedia-derived benchmark, complemented by a rich set of crowd-sourced annotations, that supports $8$ interrelated tasks: (i) extractive summarization; (ii) abstractive summarization; (iii) topic-based summarization; (iv) compressing selected sentences into a one-line summary; (v) surfacing evidence for a summary sentence; (vi) predicting the factual accuracy of a summary sentence; (vii) identifying unsubstantiated spans in a summary sentence; (viii) correcting factual errors in summaries. We compare various methods on this benchmark and discover that on multiple tasks, moderately-sized fine-tuned models consistently outperform much larger few-shot prompted language models. For factuality-related tasks, we also evaluate existing heuristics to create training data and find that training on them results in worse performance than training on $20\times$ less human-labeled data. Our articles draw from $6$ domains, facilitating cross-domain analysis. On some tasks, the amount of training data matters more than the domain where it comes from, while for other tasks training specifically on data from the target domain, even if limited, is more beneficial.
Authors: Kundan Krishna, Prakhar Gupta, S. Ramprasad, Byron C. Wallace, Jeffrey P. Bigham, Zachary Chase Lipton
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A Wikipedia-derived benchmark is introduced, complemented by a rich set of crowd-sourced annotations, that supports interrelated tasks and finds that on multiple tasks, moderately-sized fine-tuned models consistently outperform much larger few-shot prompted language models.'}
