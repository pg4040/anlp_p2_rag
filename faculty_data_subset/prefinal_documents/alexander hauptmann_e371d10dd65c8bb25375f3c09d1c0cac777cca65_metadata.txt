Faculty Name: alexander hauptmann
Paperid: e371d10dd65c8bb25375f3c09d1c0cac777cca65
Title: Hyperbolic vs Euclidean Embeddings in Few-Shot Learning: Two Sides of the Same Coin
Year: 2023
Abstract: Recent research in representation learning has shown that hierarchical data lends itself to low-dimensional and highly informative representations in hyperbolic space. However, even if hyperbolic embeddings have gathered attention in image recognition, their optimization is prone to numerical hurdles. Further, it remains unclear which applications stand to benefit the most from the implicit bias imposed by hyperbolicity, when compared to traditional Euclidean features. In this paper, we focus on prototypical hyperbolic neural networks. In particular, the tendency of hyperbolic embeddings to converge to the boundary of the Poincar\'e ball in high dimensions and the effect this has on few-shot classification. We show that the best few-shot results are attained for hyperbolic embeddings at a common hyperbolic radius. In contrast to prior benchmark results, we demonstrate that better performance can be achieved by a fixed-radius encoder equipped with the Euclidean metric, regardless of the embedding dimension.
Authors: Gabriel Moreira, Manuel Marques, J. Costeira, Alexander Hauptmann
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'It is demonstrated that better performance can be achieved by a fixed-radius encoder equipped with the Euclidean metric, regardless of the embedding dimension, and that the best few-shot results are attained for hyperbolic embeddings at a commonhyperbolic radius.'}
Url: https://arxiv.org/pdf/2309.10013
