Faculty Name: shinji watanabe
Paperid: f53b6f5a85f2d74deb32022795b5dab0aa753cf4
Title: Deep Speech Synthesis from MRI-Based Articulatory Representations
Year: 2023
Abstract: In this paper, we study articulatory synthesis, a speech synthesis method using human vocal tract information that offers a way to develop efficient, generalizable and interpretable synthesizers. While recent advances have enabled intelligible articulatory synthesis using electromagnetic articulography (EMA), these methods lack critical articulatory information like excitation and nasality, limiting generalization capabilities. To bridge this gap, we propose an alternative MRI-based feature set that covers a much more extensive articulatory space than EMA. We also introduce normalization and denoising procedures to enhance the generalizability of deep learning methods trained on MRI data. Moreover, we propose an MRI-to-speech model that improves both computational efficiency and speech fidelity. Finally, through a series of ablations, we show that the proposed MRI representation is more comprehensive than EMA and identify the most suitable MRI feature subset for articulatory synthesis.
Authors: Peter Wu, Tingle Li, Yijingxiu Lu, Yubin Zhang, Jiachen Lian, A. Black, L. Goldstein, Shinji Watanabe, G. Anumanchipalli
Venue: Interspeech
Tldr: {'model': 'tldr@v2.0.0', 'text': 'An MRI-to-speech model that improves both computational efficiency and speech fidelity is proposed and the proposed MRI representation is more comprehensive than EMA and the most suitable MRI feature subset for articulatory synthesis is identified.'}
Url: https://arxiv.org/pdf/2307.02471
