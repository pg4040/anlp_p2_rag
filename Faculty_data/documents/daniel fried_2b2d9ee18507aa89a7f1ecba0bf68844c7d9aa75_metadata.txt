Faculty Name: daniel fried
Metadata:
Paperid: 2b2d9ee18507aa89a7f1ecba0bf68844c7d9aa75
Title: Grounding Language Models to Images for Multimodal Generation
Year: 2023
Abstract: We propose an efﬁcient method to ground pre-trained text-only language models to the visual domain, enabling them to process and generate arbitrarily interleaved image-and-text data. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and ﬁnetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text inter-leaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.
Authors: Jing Yu Koh, R. Salakhutdinov, Daniel Fried
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': None}
