Faculty Name: berg kirkpatrick taylor
Metadata:
Paperid: 3a899fbda8071af3fce011ae2c1f6c00264c070f
Title: Exploring the Relationship Between Model Architecture and In-Context Learning Ability
Year: 2023
Abstract: What is the relationship between model architecture and the ability to perform in-context learning? In this empirical study, we take the first steps toward answering this question. We evaluate twelve model architectures capable of causal language modeling across a suite of synthetic in-context learning tasks. These selected architectures represent a broad range of paradigms, including recurrent and convolution-based neural networks, transformers, state-space model inspired, and other emerging attention alternatives. We discover that all the considered architectures can perform in-context learning under a wider range of conditions than previously documented. Additionally, we observe stark differences in statistical efficiency and consistency by varying context length and task difficulty. We also measure each architecture's predisposition towards in-context learning when presented with alternative routes for task resolution. Finally, and somewhat surprisingly, we find that several attention alternatives are more robust in-context learners than transformers. Given that such approaches have constant-sized memory footprints at inference time, this result opens the possibility of scaling up in-context learning to accommodate vastly larger numbers of in-context examples.
Authors: Ivan Lee, Nan Jiang, Taylor Berg-Kirkpatrick
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This empirical study evaluates twelve model architectures capable of causal language modeling across a suite of synthetic in- context learning tasks and finds that several attention alternatives are more robust in-context learners than transformers.'}
