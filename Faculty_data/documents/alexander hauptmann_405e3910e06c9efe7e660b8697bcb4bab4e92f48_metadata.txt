Faculty Name: alexander hauptmann
Metadata:
Paperid: 405e3910e06c9efe7e660b8697bcb4bab4e92f48
Title: STMT: A Spatial-Temporal Mesh Transformer for MoCap-Based Action Recognition
Year: 2023
Abstract: We study the problem of human action recognition using motion capture (MoCap) sequences. Unlike existing techniques that take multiple manual steps to derive standard-ized skeleton representations as model input, we propose a novel Spatial-Temporal Mesh Transformer (STMT) to directly model the mesh sequences. The model uses a hierarchical transformer with intra-frame off-set attention and inter-frame self-attention. The attention mechanism allows the model to freely attend between any two vertex patches to learn nonlocal relationships in the spatial-temporal domain. Masked vertex modeling and future frame prediction are used as two self-supervised tasks to fully activate the bi-directional and auto-regressive attention in our hierarchical transformer. The proposed method achieves state-of-the-art performance compared to skeleton-based and point-cloud-based models on common MoCap benchmarks. Code is available at https://github.com/zgzxy001/STMT.
Authors: Xiaoyu Zhu, Po-Yao (Bernie) Huang, Junwei Liang, Celso M. de Melo, A. Hauptmann
Venue: Computer Vision and Pattern Recognition
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A novel Spatial-Temporal Mesh Transformer (STMT) is proposed to directly model the mesh sequences using motion capture sequences to achieve state-of-the-art performance compared to skeleton-based and point-cloud-based models on common MoCap benchmarks.'}
