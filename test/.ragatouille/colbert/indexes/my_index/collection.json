[
  "Faculty Name: alexander hauptmann\nMetadata:\nPaperid: 2107b867cb8f8afa30a9a940288d7c8b657f8aa5\nTitle: Zero-Shot and Few-Shot Stance Detection on Varied Topics via Conditional Generation\nYear: 2023\nAbstract: Zero-shot and few-shot stance detection identify the polarity of text with regard to a certain target when we have only limited or no training resources for the target. Previous work generally formulates the problem into a classification setting, ignoring the potential use of label text. In this paper, we instead utilize a conditional generation framework and formulate the problem as denoising from partially-filled templates, which can better utilize the semantics among input, label, and target texts. We further propose to jointly train an auxiliary task, target prediction, and to incorporate manually constructed incorrect samples with unlikelihood training to improve the representations for both target and label texts. We also verify the effectiveness of target-related Wikipedia knowledge with the generation framework. Experiments show that our proposed method significantly outperforms several strong baselines on VAST, and achieves new state-of-the-art performance.",
  "We also verify the effectiveness of target-related Wikipedia knowledge with the generation framework. Experiments show that our proposed method significantly outperforms several strong baselines on VAST, and achieves new state-of-the-art performance.\nAuthors: Haoyang Wen, A. Hauptmann\nVenue: Annual Meeting of the Association for Computational Linguistics\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This paper utilizes a conditional generation framework and formulates the zero-shot and few-shot stance detection problem as denoising from partially-filled templates, which can better utilize the semantics among input, label, and target texts.'}",
  "Faculty Name: alexander hauptmann\nMetadata:\nPaperid: 376f494126d1ea4f571ea0263c43ac2b6331800a\nTitle: SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs\nYear: 2023\nAbstract: In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.",
  "Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.\nAuthors: Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A. Ross, Irfan Essa, Yonatan Bisk, Ming Yang, K. Murphy, A. Hauptmann, Lu Jiang\nVenue: Neural Information Processing Systems\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.'}",
  "Faculty Name: alexander hauptmann\nMetadata:\nPaperid: 405e3910e06c9efe7e660b8697bcb4bab4e92f48\nTitle: STMT: A Spatial-Temporal Mesh Transformer for MoCap-Based Action Recognition\nYear: 2023\nAbstract: We study the problem of human action recognition using motion capture (MoCap) sequences. Unlike existing techniques that take multiple manual steps to derive standard-ized skeleton representations as model input, we propose a novel Spatial-Temporal Mesh Transformer (STMT) to directly model the mesh sequences. The model uses a hierarchical transformer with intra-frame off-set attention and inter-frame self-attention. The attention mechanism allows the model to freely attend between any two vertex patches to learn nonlocal relationships in the spatial-temporal domain. Masked vertex modeling and future frame prediction are used as two self-supervised tasks to fully activate the bi-directional and auto-regressive attention in our hierarchical transformer. The proposed method achieves state-of-the-art performance compared to skeleton-based and point-cloud-based models on common MoCap benchmarks. Code is available at https://github.com/zgzxy001/STMT.",
  "The proposed method achieves state-of-the-art performance compared to skeleton-based and point-cloud-based models on common MoCap benchmarks. Code is available at https://github.com/zgzxy001/STMT.\nAuthors: Xiaoyu Zhu, Po-Yao (Bernie) Huang, Junwei Liang, Celso M. de Melo, A. Hauptmann\nVenue: Computer Vision and Pattern Recognition\nTldr: {'model': 'tldr@v2.0.0', 'text': 'A novel Spatial-Temporal Mesh Transformer (STMT) is proposed to directly model the mesh sequences using motion capture sequences to achieve state-of-the-art performance compared to skeleton-based and point-cloud-based models on common MoCap benchmarks.'}",
  "Faculty Name: alexander hauptmann\nMetadata:\nPaperid: 72cce47fd053bf916314d89a8174726c58c05e02\nTitle: Towards Open-Domain Twitter User Profile Inference\nYear: 2023\nAbstract: ,\nAuthors: Haoyang Wen, Zhenxin Xiao, E. Hovy, Alexander Hauptmann\nVenue: Annual Meeting of the Association for Computational Linguistics\nTldr: {'model': 'tldr@v2.0.0', 'text': None}",
  "Faculty Name: alexander hauptmann\nMetadata:\nPaperid: 8ccda6de0223bcd897d5dc0efc8f33222a899d0d\nTitle: DocumentNet: Bridging the Data Gap in Document Pre-training\nYear: 2023\nAbstract: Document understanding tasks, in particular, Visually-rich Document Entity Retrieval (VDER), have gained significant attention in recent years thanks to their broad applications in enterprise AI. However, publicly available data have been scarce for these tasks due to strict privacy constraints and high annotation costs. To make things worse, the non-overlapping entity spaces from different datasets hinder the knowledge transfer between document types. In this paper, we propose a method to collect massive-scale and weakly labeled data from the web to benefit the training of VDER models. The collected dataset, named DocumentNet, does not depend on specific document types or entity sets, making it universally applicable to all VDER tasks. The current DocumentNet consists of 30M documents spanning nearly 400 document types organized in a four-level ontology.",
  "The collected dataset, named DocumentNet, does not depend on specific document types or entity sets, making it universally applicable to all VDER tasks. The current DocumentNet consists of 30M documents spanning nearly 400 document types organized in a four-level ontology. Experiments on a set of broadly adopted VDER tasks show significant improvements when DocumentNet is incorporated into the pre-training for both classic and few-shot learning settings. With the recent emergence of large language models (LLMs), DocumentNet provides a large data source to extend their multi-modal capabilities for VDER.\nAuthors: Lijun Yu, Jin Miao, Xiaoyu Sun, Jiayi Chen, A. Hauptmann, H. Dai, Wei Wei\nVenue: Conference on Empirical Methods in Natural Language Processing\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes a method to collect massive-scale and weakly labeled data from the web to benefit the training of VDER models, and provides a large data source to extend their multi-modal capabilities for VDER.'}",
  "Faculty Name: alexander hauptmann\nMetadata:\nPaperid: 985f0c89c5a607742ec43c1fdc2cbfe54541cbad\nTitle: Language Model Beats Diffusion - Tokenizer is Key to Visual Generation\nYear: 2023\nAbstract: While Large Language Models (LLMs) are the dominant models for generative tasks in language, they do not perform as well as diffusion models on image and video generation. To effectively use LLMs for visual generation, one crucial component is the visual tokenizer that maps pixel-space inputs to discrete tokens appropriate for LLM learning. In this paper, we introduce MAGVIT-v2, a video tokenizer designed to generate concise and expressive tokens for both videos and images using a common token vocabulary. Equipped with this new tokenizer, we show that LLMs outperform diffusion models on standard image and video generation benchmarks including ImageNet and Kinetics. In addition, we demonstrate that our tokenizer surpasses the previously top-performing video tokenizer on two more tasks: (1) video compression comparable to the next-generation video codec (VCC) according to human evaluations, and (2) learning effective representations for action recognition tasks.",
  "In addition, we demonstrate that our tokenizer surpasses the previously top-performing video tokenizer on two more tasks: (1) video compression comparable to the next-generation video codec (VCC) according to human evaluations, and (2) learning effective representations for action recognition tasks.\nAuthors: Lijun Yu, Jos'e Lezama, Nitesh B. Gundavarapu, Luca Versari, Kihyuk Sohn, David C. Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander G. Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David A. Ross, Lu Jiang\nVenue: arXiv.org\nTldr: {'model': 'tldr@v2.0.0', 'text': None}",
  "Faculty Name: alexander hauptmann\nMetadata:\nPaperid: e371d10dd65c8bb25375f3c09d1c0cac777cca65\nTitle: Hyperbolic vs Euclidean Embeddings in Few-Shot Learning: Two Sides of the Same Coin\nYear: 2023\nAbstract: Recent research in representation learning has shown that hierarchical data lends itself to low-dimensional and highly informative representations in hyperbolic space. However, even if hyperbolic embeddings have gathered attention in image recognition, their optimization is prone to numerical hurdles. Further, it remains unclear which applications stand to benefit the most from the implicit bias imposed by hyperbolicity, when compared to traditional Euclidean features. In this paper, we focus on prototypical hyperbolic neural networks. In particular, the tendency of hyperbolic embeddings to converge to the boundary of the Poincar\\'e ball in high dimensions and the effect this has on few-shot classification. We show that the best few-shot results are attained for hyperbolic embeddings at a common hyperbolic radius.",
  "In particular, the tendency of hyperbolic embeddings to converge to the boundary of the Poincar\\'e ball in high dimensions and the effect this has on few-shot classification. We show that the best few-shot results are attained for hyperbolic embeddings at a common hyperbolic radius. In contrast to prior benchmark results, we demonstrate that better performance can be achieved by a fixed-radius encoder equipped with the Euclidean metric, regardless of the embedding dimension.\nAuthors: Gabriel Moreira, Manuel Marques, J. Costeira, Alexander Hauptmann\nVenue: arXiv.org\nTldr: {'model': 'tldr@v2.0.0', 'text': 'It is demonstrated that better performance can be achieved by a fixed-radius encoder equipped with the Euclidean metric, regardless of the embedding dimension, and that the best few-shot results are attained for hyperbolic embeddings at a commonhyperbolic radius.'}",
  "List of 2023 Open Access papers by alexander hauptmann are:\nTowards Open-Domain Twitter User Profile Inference\nZero-Shot and Few-Shot Stance Detection on Varied Topics via Conditional Generation\nSPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs\nSTMT: A Spatial-Temporal Mesh Transformer for MoCap-Based Action Recognition\nDocumentNet: Bridging the Data Gap in Document Pre-training\nLanguage Model Beats Diffusion - Tokenizer is Key to Visual Generation\nHyperbolic vs Euclidean Embeddings in Few-Shot Learning: Two Sides of the Same Coin",
  "Faculty Name: alexander rudnicky\nMetadata:\nPaperid: 06a8f2e3c4266196b008851f1ec7ef9f340809da\nTitle: Advancing Regular Language Reasoning in Linear Recurrent Neural Networks\nYear: 2023\nAbstract: In recent studies, linear recurrent neural networks (LRNNs) have achieved Transformer-level performance in natural language modeling and long-range modeling while offering rapid parallel training and constant inference costs. With the resurged interest in LRNNs, we study whether they can learn the hidden rules in training sequences, such as the grammatical structures of regular language. We theoretically analyze some existing LRNNs and discover their limitations on regular language. Motivated by the analysis, we propose a new LRNN equipped with a block-diagonal and input-dependent transition matrix. Experiments suggest that the proposed model is the only LRNN that can perform length extrapolation on regular language tasks such as Sum, Even Pair, and Modular Arithmetic.",
  "Motivated by the analysis, we propose a new LRNN equipped with a block-diagonal and input-dependent transition matrix. Experiments suggest that the proposed model is the only LRNN that can perform length extrapolation on regular language tasks such as Sum, Even Pair, and Modular Arithmetic.\nAuthors: Ting-Han Fan, Ta-Chung Chi, Alexander I. Rudnicky\nVenue: arXiv.org\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This work theoretically analyze some existing LRNNs and proposes a new LRNN equipped with a block-diagonal and input-dependent transition matrix that is the only LRNN that can perform length extrapolation on regular language tasks such as Sum, Even Pair, and Modular Arithmetic.'}",
  "Faculty Name: alexander rudnicky\nMetadata:\nPaperid: 161bf3f0705ef8e088f53b383363338daac9af44\nTitle: Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings\nYear: 2023\nAbstract: The use of positional embeddings in transformer language models is widely accepted. However, recent research has called into question the necessity of such embeddings. We further extend this inquiry by demonstrating that a randomly initialized and frozen transformer language model, devoid of positional embeddings, inherently encodes strong positional information through the shrinkage of self-attention variance. To quantify this variance, we derive the underlying distribution of each step within a transformer layer. Through empirical validation using a fully pretrained model, we show that the variance shrinkage effect still persists after extensive gradient updates. Our findings serve to justify the decision to discard positional embeddings and thus facilitate more efficient pretraining of transformer language models.",
  "Through empirical validation using a fully pretrained model, we show that the variance shrinkage effect still persists after extensive gradient updates. Our findings serve to justify the decision to discard positional embeddings and thus facilitate more efficient pretraining of transformer language models.\nAuthors: Ta-Chung Chi, Ting-Han Fan, Li-Wei Chen, A. Rudnicky, P. Ramadge\nVenue: Annual Meeting of the Association for Computational Linguistics\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This work demonstrates that a randomly initialized and frozen transformer language model, devoid of positional embeddings, inherently encodes strong positional information through the shrinkage of self-attention variance, and derives the underlying distribution of each step within a transformer layer.'}",
  "Faculty Name: alexander rudnicky\nMetadata:\nPaperid: 2670612b5e11297cd9b98f4d7ff796725f77fe35\nTitle: Structured Dialogue Discourse Parsing\nYear: 2023\nAbstract: Dialogue discourse parsing aims to uncover the internal structure of a multi-participant conversation by finding all the discourse links and corresponding relations. Previous work either treats this task as a series of independent multiple-choice problems, in which the link existence and relations are decoded separately, or the encoding is restricted to only local interaction, ignoring the holistic structural information. In contrast, we propose a principled method that improves upon previous work from two perspectives: encoding and decoding. From the encoding side, we perform structured encoding on the adjacency matrix followed by the matrix-tree learning algorithm, where all discourse links and relations in the dialogue are jointly optimized based on latent tree-level distribution. From the decoding side, we perform structured inference using the modified Chiu-Liu-Edmonds algorithm, which explicitly generates the labeled multi-root non-projective spanning tree that best captures the discourse structure.",
  "From the decoding side, we perform structured inference using the modified Chiu-Liu-Edmonds algorithm, which explicitly generates the labeled multi-root non-projective spanning tree that best captures the discourse structure. In addition, unlike in previous work, we do not rely on hand-crafted features; this improves the model\u2019s robustness. Experiments show that our method achieves new state-of-the-art, surpassing the previous model by 2.3 on STAC and 1.5 on Molweni (F1 scores).\nAuthors: Ta-Chung Chi, Alexander I. Rudnicky\nVenue: SIGDIAL Conferences\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This work proposes a principled method that improves upon previous work from two perspectives: encoding and decoding and achieves new state-of-the-art results, surpassing the previous model by 2.3 on STAC and 1.5 on Molweni (F1 scores).'}",
  "Faculty Name: alexander rudnicky\nMetadata:\nPaperid: 465ec2212d865e875e64638b3dd1ecaac21c5ddd\nTitle: Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation\nYear: 2023\nAbstract: Unlike recurrent models, conventional wisdom has it that Transformers cannot perfectly model regular languages. Inspired by the notion of working memory, we propose a new Transformer variant named RegularGPT. With its novel combination of Weight-Sharing, Adaptive-Depth, and Sliding-Dilated-Attention, RegularGPT constructs working memory along the depth dimension, thereby enabling efficient and successful modeling of regular languages such as PARITY. We further test RegularGPT on the task of natural language length extrapolation and surprisingly find that it rediscovers the local windowed attention effect deemed necessary in prior work for length extrapolation.",
  "We further test RegularGPT on the task of natural language length extrapolation and surprisingly find that it rediscovers the local windowed attention effect deemed necessary in prior work for length extrapolation.\nAuthors: Ta-Chung Chi, Ting-Han Fan, A. Rudnicky, P. Ramadge\nVenue: Conference on Empirical Methods in Natural Language Processing\nTldr: {'model': 'tldr@v2.0.0', 'text': 'Inspired by the notion of working memory, a new Transformer variant named RegularGPT is proposed, which constructs working memory along the depth dimension, thereby enabling efficient and successful modeling of regular languages such as PARITY.'}",
  "Faculty Name: alexander rudnicky\nMetadata:\nPaperid: 4b8d3ede673ddeab9dfb5184da6b748d7a526754\nTitle: A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech\nYear: 2023\nAbstract: Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human-level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregressive models, leading to unintelligible synthesis, and demonstrate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality.",
  "We introduce our MQTTS system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ablation analyses to identify the efficacy of our methods. We show that MQTTS outperforms existing TTS systems in several objective and subjective measures.\nAuthors: Li-Wei Chen, Shinji Watanabe, Alexander I. Rudnicky\nVenue: AAAI Conference on Artificial Intelligence\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This work introduces the MQTTS system, a Text-to-Speech system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality, and shows that MqTTS outperforms existing TTS systems in several objective and subjective measures.'}",
  "Faculty Name: alexander rudnicky\nMetadata:\nPaperid: 9799c17fd287bb9e8d231fe032c6dbf9c0c9d675\nTitle: Overview of Robust and Multilingual Automatic Evaluation Metrics\n\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4\nYear: 2023\nAbstract: The advent and fast development of neural networks have revolutionized the research on dialogue systems and subsequently have triggered various challenges regarding their automatic evaluation. Automatic evaluation of open-domain dialogue systems as an open challenge has been the center of the attention of many researchers. Despite the consistent efforts to improve automatic metrics\u2019 correlations with human evaluation, there have been very few attempts to assess their robustness over multiple domains and dimensions. Also, their focus is mainly on the English language. All of these challenges prompt the development of automatic evaluation metrics that are reliable in various domains, dimensions, and languages. This track in the 11th Dialogue System Technology Challenge (DSTC11) is part of the ongoing effort to promote robust and multilingual automatic evaluation metrics. This article describes the datasets and baselines provided to participants and discusses the submission and result details of the two proposed subtasks.",
  "This track in the 11th Dialogue System Technology Challenge (DSTC11) is part of the ongoing effort to promote robust and multilingual automatic evaluation metrics. This article describes the datasets and baselines provided to participants and discusses the submission and result details of the two proposed subtasks.\nAuthors: Mario Rodr'iguez-Cantelar, Chen Zhang, Chengguang Tang, Ke Shi, Sarik Ghazarian, Jo\u00e3o Sedoc, L. F. D\u2019Haro, Alexander I. Rudnicky\nVenue: DSTC\nTldr: {'model': 'tldr@v2.0.0', 'text': 'The datasets and baselines provided to participants are described and the submission and result details of the two proposed subtasks are discussed, which promote robust and multilingual automatic evaluation metrics.'}",
  "Faculty Name: alexander rudnicky\nMetadata:\nPaperid: f743324682d5d50db9b114fa60b908f09c10c9a0\nTitle: Learning to Ask Questions for Zero-shot Dialogue State Tracking\nYear: 2023\nAbstract: We present a method for performing zero-shot Dialogue State Tracking (DST) by casting the task as a learning-to-ask-questions framework. The framework learns to pair the best question generation (QG) strategy with in-domain question answering (QA) methods to extract slot values from a dialogue without any human intervention. A novel self-supervised QA pretraining step using in-domain data is essential to learn the structure without requiring any slot-filling annotations. Moreover, we show that QG methods need to be aligned with the same grammatical person used in the dialogue. Empirical evaluation on the MultiWOZ 2.1 dataset demonstrates that our approach, when used alongside robust QA models, outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation-given a comparable amount of domain knowledge during data creation.",
  "Empirical evaluation on the MultiWOZ 2.1 dataset demonstrates that our approach, when used alongside robust QA models, outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation-given a comparable amount of domain knowledge during data creation. Finally, we analyze the impact of the types of questions used, and demonstrate that the algorithmic approach outperforms template-based question generation.\nAuthors: Diogo Tavares, David Semedo, Alexander I. Rudnicky, Jo\u00e3o Magalh\u00e3es\nVenue: Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This work presents a method for performing zero-shot Dialogue State Tracking by casting the task as a learning-to-ask-questions framework that outperforms template-based question generation and shows that QG methods need to be aligned with the same grammatical person used in the dialogue.'}",
  "List of 2023 Open Access papers by alexander rudnicky are:\nAdvancing Regular Language Reasoning in Linear Recurrent Neural Networks\nStructured Dialogue Discourse Parsing\nA Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech\nOverview of Robust and Multilingual Automatic Evaluation Metrics\n\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4\nLearning to Ask Questions for Zero-shot Dialogue State Tracking\nLatent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings\nTransformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation",
  "Faculty Name: alexander waibel\nMetadata:\nPaperid: 100eb82862a66e264686d015934c97c54bdadb4f\nTitle: SYNTACC : Synthesizing Multi-Accent Speech By Weight Factorization\nYear: 2023\nAbstract: Conventional multi-speaker text-to-speech synthesis (TTS) is known to be capable of synthesizing speech for multiple voices, yet it cannot generate speech in different accents. This limitation has motivated us to develop SYNTACC (Synthesizing speech with accents) which adapts conventional multi-speaker TTS to produce multi-accent speech. Our method uses the YourTTS model and involves a novel multi-accent training mechanism. The method works by decomposing each weight matrix into a shared component and an accent-dependent component, with the former being initialized by the pretrained multi-speaker TTS model and the latter being factorized into vectors using rank-1 matrices to reduce the number of training parameters per accent. This weight factorization method proves to be effective in fine-tuning the SYNTACC on multi-accent data sets in a low-resource condition.",
  "This weight factorization method proves to be effective in fine-tuning the SYNTACC on multi-accent data sets in a low-resource condition. Our SYNTACC model eventually allows speech synthesis in not only different voices but also in different accents.\nAuthors: Tuan-Nam Nguyen, Ngoc-Quan Pham, A. Waibel\nVenue: IEEE International Conference on Acoustics, Speech, and Signal Processing\nTldr: {'model': 'tldr@v2.0.0', 'text': 'The SYNTACC model eventually allows speech synthesis in not only different voices but also in different accents, and the weight factorization method proves to be effective in fine-tuning the SYNT ACC on multi-accent data sets in a low-resource condition.'}",
  "Faculty Name: alexander waibel\nMetadata:\nPaperid: 610d9958390ab83515d0d81e19f8e5264faf8e9b\nTitle: KIT\u2019s Multilingual Speech Translation System for IWSLT 2023\nYear: 2023\nAbstract: Many existing speech translation benchmarks focus on native-English speech in high-quality recording conditions, which often do not match the conditions in real-life use-cases. In this paper, we describe our speech translation system for the multilingual track of IWSLT 2023, which focuses on the translation of scientific conference talks. The test condition features accented input speech and terminology-dense contents. The tasks requires translation into 10 languages of varying amounts of resources. In absence of training data from the target domain, we use a retrieval-based approach (kNN-MT) for effective adaptation (+0.8 BLEU for speech translation). We also use adapters to easily integrate incremental training data from data augmentation, and show that it matches the performance of re-training. We observe that cascaded systems are more easily adaptable towards specific target domains, due to their separate modules.",
  "We also use adapters to easily integrate incremental training data from data augmentation, and show that it matches the performance of re-training. We observe that cascaded systems are more easily adaptable towards specific target domains, due to their separate modules. Our cascaded speech system outperforms its end-to-end counterpart on scientific talk translation, although their performance remains similar on TED talks.\nAuthors: Danni Liu, T. Nguyen, Sai Koneru, Enes Yavuz Ugan, Ngoc-Quan Pham, Tuan-Nam Nguyen, Tu Anh Dinh, Carlos Mullov, A. Waibel, J. Niehues\nVenue: International Workshop on Spoken Language Translation\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This paper describes the speech translation system for the multilingual track of IWSLT 2023, which focuses on the translation of scientific conference talks, and observes that cascaded systems are more easily adaptable towards specific target domains, due to their separate modules.'}",
  "Faculty Name: alexander waibel\nMetadata:\nPaperid: 7e13fcb7b7bae202fb9087e87abaa71a4b19a3e3\nTitle: Convoifilter: A case study of doing cocktail party speech recognition\nYear: 2023\nAbstract: This paper presents an end-to-end model designed to improve automatic speech recognition (ASR) for a particular speaker in a crowded, noisy environment. The model utilizes a single-channel speech enhancement module that isolates the speaker's voice from background noise (ConVoiFilter) and an ASR module. The model can decrease ASR's word error rate (WER) from 80% to 26.4% through this approach. Typically, these two components are adjusted independently due to variations in data requirements. However, speech enhancement can create anomalies that decrease ASR efficiency. By implementing a joint fine-tuning strategy, the model can reduce the WER from 26.4% in separate tuning to 14.5% in joint tuning. We openly share our pre-trained model to foster further research hf.co/nguyenvulebinh/voice-filter.",
  "By implementing a joint fine-tuning strategy, the model can reduce the WER from 26.4% in separate tuning to 14.5% in joint tuning. We openly share our pre-trained model to foster further research hf.co/nguyenvulebinh/voice-filter.\nAuthors: T. Nguyen, A. Waibel\nVenue: arXiv.org\nTldr: {'model': 'tldr@v2.0.0', 'text': \"An end-to-end model designed to improve automatic speech recognition for a particular speaker in a crowded, noisy environment that utilizes a single-channel speech enhancement module that isolates the speaker's voice from background noise and an ASR module.\"}",
  "Faculty Name: alexander waibel\nMetadata:\nPaperid: 807abb9c185ce233e2c8a2fcee49be851a1c968f\nTitle: Incremental Learning of Humanoid Robot Behavior from Natural Interaction and Large Language Models\nYear: 2023\nAbstract: Natural-language dialog is key for intuitive human-robot interaction. It can be used not only to express humans' intents, but also to communicate instructions for improvement if a robot does not understand a command correctly. Of great importance is to endow robots with the ability to learn from such interaction experience in an incremental way to allow them to improve their behaviors or avoid mistakes in the future. In this paper, we propose a system to achieve incremental learning of complex behavior from natural interaction, and demonstrate its implementation on a humanoid robot. Building on recent advances, we present a system that deploys Large Language Models (LLMs) for high-level orchestration of the robot's behavior, based on the idea of enabling the LLM to generate Python statements in an interactive console to invoke both robot perception and action.",
  "Building on recent advances, we present a system that deploys Large Language Models (LLMs) for high-level orchestration of the robot's behavior, based on the idea of enabling the LLM to generate Python statements in an interactive console to invoke both robot perception and action. The interaction loop is closed by feeding back human instructions, environment observations, and execution results to the LLM, thus informing the generation of the next statement. Specifically, we introduce incremental prompt learning, which enables the system to interactively learn from its mistakes. For that purpose, the LLM can call another LLM responsible for code-level improvements of the current interaction based on human feedback. The improved interaction is then saved in the robot's memory, and thus retrieved on similar requests. We integrate the system in the robot cognitive architecture of the humanoid robot ARMAR-6 and evaluate our methods both quantitatively (in simulation) and qualitatively (in simulation and real-world) by demonstrating generalized incrementally-learned knowledge.",
  "We integrate the system in the robot cognitive architecture of the humanoid robot ARMAR-6 and evaluate our methods both quantitatively (in simulation) and qualitatively (in simulation and real-world) by demonstrating generalized incrementally-learned knowledge.\nAuthors: Leonard B\u00e4rmann, Rainer Kartmann, Fabian Peller-Konrad, Alexander H. Waibel, T. Asfour\nVenue: arXiv.org\nTldr: {'model': 'tldr@v2.0.0', 'text': \"A system that deploys Large Language Models for high-level orchestration of the robot's behavior based on the idea of enabling the LLM to generate Python statements in an interactive console to invoke both robot perception and action is presented.\"}",
  "Faculty Name: alexander waibel\nMetadata:\nPaperid: 954ca9ab894df43e2cac18bc3813e9f9bc1bd488\nTitle: Continually learning new languages\nYear: 2023\nAbstract: Multilingual speech recognition with neural networks is often implemented with batch-learning, when all of the languages are available before training. An ability to add new languages after the prior training sessions can be economically bene-\ufb01cial, but the main challenge is catastrophic forgetting. In this work, we combine the qualities of weight factorization, transfer learning and Elastic Weight Consolidation in order to counter catastrophic forgetting and facilitate learning new languages quickly. Such combination allowed us to eliminate catastrophic forgetting while still achieving performance for the new languages comparable with having all languages at once, in experiments of learning from an initial 10 languages to achieve 27 languages.",
  "Such combination allowed us to eliminate catastrophic forgetting while still achieving performance for the new languages comparable with having all languages at once, in experiments of learning from an initial 10 languages to achieve 27 languages.\nAuthors: Ngoc-Quan Pham, J. Niehues, A. Waibel\nVenue: arXiv.org\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This work combines the qualities of weight factorization, transfer learning and Elastic Weight Consolidation in order to counter catastrophic forgetting and facilitate learning new languages quickly.'}",
  "Faculty Name: alexander waibel\nMetadata:\nPaperid: aab2ed83bc3739a20e90ae1d97dcf45f3bc8e508\nTitle: AdapITN: A Fast, Reliable, and Dynamic Adaptive Inverse Text Normalization\nYear: 2023\nAbstract: Inverse text normalization (ITN) is the task that transforms text in spoken-form into written-form. While automatic speech recognition (ASR) produces text in spoken-form, human and natural language understanding systems prefer to consume text in written-form. ITN generally deals with semiotic phrases (e.g., numbers, date, time). However, lack of studies to deal with phonetization phrases, which is ASR\u2019s output when it handles unseen data (e.g., foreign-named entities, domain names), although these exist in the same form in the spoken-form text. The reason is that phonetization phrases are infinite patterns and language-dependent. In this study, we introduce a novel end2end model that can handle both semiotic phrases (SEP) and phonetization phrases (PHP), named AdapITN.",
  "The reason is that phonetization phrases are infinite patterns and language-dependent. In this study, we introduce a novel end2end model that can handle both semiotic phrases (SEP) and phonetization phrases (PHP), named AdapITN. We call it \"Adap\" because it allows for handling unseen PHP. The model performs only when necessary by providing a mechanism to narrow normalized regions and external query knowledge, reducing the runtime significantly.\nAuthors: T. Nguyen, Le Duc Minh Nhat, Quang Minh Nguyen, Quoc Truong Do, C. Luong, A. Waibel\nVenue: IEEE International Conference on Acoustics, Speech, and Signal Processing\nTldr: {'model': 'tldr@v2.0.0', 'text': 'A novel end2end model that can handle both semiotic phrases (SEP) and phonetization phrases (PHP), named AdapITN is introduced, named \"Adap\" because it allows for handling unseen PHP.'}",
  "Faculty Name: alexander waibel\nMetadata:\nPaperid: d24d60719e90e69749a75c160cb760d1d9fca44a\nTitle: Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff\nYear: 2023\nAbstract: Blockwise self-attentional encoder models have recently emerged as one promising end-to-end approach to simultaneous speech translation. These models employ a blockwise beam search with hypothesis reliability scoring to determine when to wait for more input speech before translating further. However, this method maintains multiple hypotheses until the entire speech input is consumed -- this scheme cannot directly show a single \\textit{incremental} translation to users. Further, this method lacks mechanisms for \\textit{controlling} the quality vs. latency tradeoff. We propose a modified incremental blockwise beam search incorporating local agreement or hold-$n$ policies for quality-latency control. We apply our framework to models trained for online or offline translation and demonstrate that both types can be effectively used in online mode.",
  "latency tradeoff. We propose a modified incremental blockwise beam search incorporating local agreement or hold-$n$ policies for quality-latency control. We apply our framework to models trained for online or offline translation and demonstrate that both types can be effectively used in online mode. Experimental results on MuST-C show 0.6-3.6 BLEU improvement without changing latency or 0.8-1.4 s latency improvement without changing quality.\nAuthors: Peter Pol\u00e1k, Brian Yan, Shinji Watanabe, A. Waibel, Ondrej Bojar\nVenue: Interspeech\nTldr: {'model': 'tldr@v2.0.0', 'text': 'A modified incremental blockwise beam search incorporating local agreement or hold-$n$ policies for quality-latency control is proposed and applied to models trained for online or offline translation and it is demonstrated that both types can be effectively used in online mode.'}",
  "Faculty Name: alexander waibel\nMetadata:\nPaperid: f3e237e794bc4cd8df7f3e31d0caa2f7ee8cd06b\nTitle: Train Global, Tailor Local: Minimalist Multilingual Translation into Endangered Languages\nYear: 2023\nAbstract: In many humanitarian scenarios, translation into severely low resource languages often does not require a universal translation engine, but a dedicated text-specific translation engine. For example, healthcare records, hygienic procedures, government communication, emergency procedures and religious texts are all limited texts. While generic translation engines for all languages do not exist, translation of multilingually known limited texts into new, endangered languages may be possible and reduce human translation effort. We attempt to leverage translation resources from rich resource languages to efficiently produce best possible translation quality for well known texts, which is available in multiple languages, in a new, severely low resource language. We examine two approaches: 1.) best selection of seed sentences to jump start translations in a new language in view of best generalization to the remainder of a larger targeted text(s), and 2.)",
  "We examine two approaches: 1.) best selection of seed sentences to jump start translations in a new language in view of best generalization to the remainder of a larger targeted text(s), and 2.) we adapt large general multilingual translation engines from many other languages to focus on a specific text in a new, unknown language. We find that adapting large pretrained multilingual models to the domain/text first and then to the severely low resource language works best. If we also select a best set of seed sentences, we can improve average chrF performance on new test languages from a baseline of 21.9 to 50.7, while reducing the number of seed sentences to only \u223c1,000 in the new, unknown language.",
  "If we also select a best set of seed sentences, we can improve average chrF performance on new test languages from a baseline of 21.9 to 50.7, while reducing the number of seed sentences to only \u223c1,000 in the new, unknown language.\nAuthors: Zhong Zhou, J. Niehues, A. Waibel\nVenue: LORESMT\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This work examines two approaches to best selection of seed sentences to jump start translations in a new language in view of best generalization to the remainder of a larger targeted text(s), and it finds that adapting large pretrained multilingual models to the domain/text first and then to the severely low resource language works best.'}",
  "Faculty Name: alexander waibel\nMetadata:\nPaperid: f524f119afc13cc07ca15998c10b9509e9e9b0b5\nTitle: End-to-End Evaluation for Low-Latency Simultaneous Speech Translation\nYear: 2023\nAbstract: The challenge of low-latency speech translation has recently draw significant interest in the research community as shown by several publications and shared tasks. Therefore, it is essential to evaluate these different approaches in realistic scenarios. However, currently only specific aspects of the systems are evaluated and often it is not possible to compare different approaches. In this work, we propose the first framework to perform and evaluate the various aspects of low-latency speech translation under realistic conditions. The evaluation is carried out in an end-to-end fashion. This includes the segmentation of the audio as well as the run-time of the different components. Secondly, we compare different approaches to low-latency speech translation using this framework. We evaluate models with the option to revise the output as well as methods with fixed output. Furthermore, we directly compare state-of-the-art cascaded as well as end-to-end systems.",
  "Secondly, we compare different approaches to low-latency speech translation using this framework. We evaluate models with the option to revise the output as well as methods with fixed output. Furthermore, we directly compare state-of-the-art cascaded as well as end-to-end systems. Finally, the framework allows to automatically evaluate the translation quality as well as latency and also provides a web interface to show the low-latency model outputs to the user.\nAuthors: Christian Huber, Tu Anh Dinh, Carlos Mullov, Ngoc-Quan Pham, T. Nguyen, Fabian Retkowski, Stefan Constantin, Enes Yavuz Ugan, Danni Liu, Zhaolin Li, Sai Koneru, J. Niehues, A. Waibel\nVenue: Conference on Empirical Methods in Natural Language Processing\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This work proposes the first framework to perform and evaluate the various aspects of low-latency speech translation under realistic conditions and directly compares state-of-the-art cascaded as well as end-to-end systems.'}",
  "Faculty Name: alexander waibel\nMetadata:\nPaperid: f547c7ec86cbc0989e87f0e23f7e0b2cfc5259c3\nTitle: Towards Efficient Simultaneous Speech Translation: CUNI-KIT System for Simultaneous Track at IWSLT 2023\nYear: 2023\nAbstract: In this paper, we describe our submission to the Simultaneous Track at IWSLT 2023. This year, we continue with the successful setup from the last year, however, we adopt the latest methods that further improve the translation quality. Additionally, we propose a novel online policy for attentional encoder-decoder models. The policy prevents the model to generate translation beyond the current speech input by using an auxiliary CTC output layer. We show that the proposed simultaneous policy can be applied to both streaming blockwise models and offline encoder-decoder models. We observe significant improvements in quality (up to 1.1 BLEU) and the computational footprint (up to 45% relative RTF).",
  "We show that the proposed simultaneous policy can be applied to both streaming blockwise models and offline encoder-decoder models. We observe significant improvements in quality (up to 1.1 BLEU) and the computational footprint (up to 45% relative RTF).\nAuthors: Peter Pol\u00e1k, Danni Liu, Ngoc-Quan Pham, J. Niehues, A. Waibel, Ondrej Bojar\nVenue: International Workshop on Spoken Language Translation\nTldr: {'model': 'tldr@v2.0.0', 'text': \"This year's submission to the Simultaneous Track at IWSLT 2023 is described, and a novel online policy for attentional encoder-decoder models is proposed that prevents the model to generate translation beyond the current speech input by using an auxiliary CTC output layer.\"}",
  "Faculty Name: alexander waibel\nMetadata:\nPaperid: f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b\nTitle: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN\nYear: 2023\nAbstract: This paper reports on the shared tasks organized by the 20th IWSLT Conference. The shared tasks address 9 scientific challenges in spoken language translation: simultaneous and offline translation, automatic subtitling and dubbing, speech-to-speech translation, multilingual, dialect and low-resource speech translation, and formality control. The shared tasks attracted a total of 38 submissions by 31 teams. The growing interest towards spoken language translation is also witnessed by the constantly increasing number of shared task organizers and contributors to the overview paper, almost evenly distributed across industry and academia.",
  "The shared tasks attracted a total of 38 submissions by 31 teams. The growing interest towards spoken language translation is also witnessed by the constantly increasing number of shared task organizers and contributors to the overview paper, almost evenly distributed across industry and academia.\nAuthors: Sweta Agrawal, Antonios Anastasopoulos, L. Bentivogli, Ondrej Bojar, Claudia Borg, Marine Carpuat, R. Cattoni, Mauro Cettolo, Mingda Chen, William Chen, K. Choukri, Alexandra Chronopoulou, Anna Currey, T. Declerck, Qianqian Dong, Kevin Duh, Y. Est\u00e8ve, Marcello Federico, Souhir Gahbiche, B. Haddow, B. Hsu, Phu Mon Htut, H. Inaguma, D\u00e1vid Javorsk\u00fd, J. Judge, Yasumasa Kano, Tom Ko, Rishu Kumar, Peng Li, Xutai Ma, Prashant Mathur, E. Matusov, Paul McNamee, John P. McCrae, Kenton Murray, Maria Nadejde, Satoshi Nakamura, Matteo Negri, H. Nguyen, J. Niehues, Xing Niu, Atul Kr.",
  "Ojha, John E. Ortega, Proyag Pal, J. Pino, Lonneke van der Plas, Peter Pol\u00e1k, Elijah Matthew Rippeth, Elizabeth Salesky, Jiatong Shi, Matthias Sperber, Sebastian St\u00fcker, Katsuhito Sudoh, Yun Tang, Brian Thompson, Ke M. Tran, Marco Turchi, A. Waibel, Mingxuan Wang, Shinji Watanabe, Rodolfo Zevallos\nVenue: International Workshop on Spoken Language Translation\nTldr: {'model': 'tldr@v2.0.0', 'text': None}",
  "List of 2023 Open Access papers by alexander waibel are:\nAdapITN: A Fast, Reliable, and Dynamic Adaptive Inverse Text Normalization\nTrain Global, Tailor Local: Minimalist Multilingual Translation into Endangered Languages\nTowards Efficient Simultaneous Speech Translation: CUNI-KIT System for Simultaneous Track at IWSLT 2023\nSYNTACC : Synthesizing Multi-Accent Speech By Weight Factorization\nKIT\u2019s Multilingual Speech Translation System for IWSLT 2023\nConvoifilter: A case study of doing cocktail party speech recognition\nContinually learning new languages\nIncremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff\nEnd-to-End Evaluation for Low-Latency Simultaneous Speech Translation\nFINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN\nIncremental Learning of Humanoid Robot Behavior from Natural Interaction and Large Language Models",
  "Title: Malihe Alikhani -     Language Technologies Institute -     School of Computer Science - Carnegie Mellon University\n\nMeta Tags:\n<meta charset=\"utf-8\"/>\n<meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\"/>\n<meta content=\"width=device-width, initial-scale=1.0\" name=\"viewport\"/>\n<meta content=\"Bio of Malihe Alikhani, Adjunct Faculty at Carnegie Mellon University's Language Technologies Institute\" name=\"description\"/>\n<meta content=\"Carnegie Mellon University\" name=\"author\"/>\n<meta content=\"Adjunct Faculty\" name=\"categories-1\"/>\n<meta content=\"Faculty\" name=\"global-categories\"/>\n<meta content=\"Malihe Alikhani - Language Technologies Institute - School of Computer Science - Carnegie Mellon University\" property=\"og:title\"/>\n<meta content=\"Bio of Malihe Alikhani, Adjunct Faculty at Carnegie Mellon University's Language Technologies Institute\" property=\"og:description\"/>\n<meta content=\"profile\" property=\"og:type\"/>\n<meta content=\"Firstname\" property=\"profile:Malihe\"/>\n<meta content=\"Lastname\" property=\"profile:Alikhani\"/>\n<meta content=\"http://lti.cmu.edu//people/faculty/alikhani-malihe.",
  "cmu.edu//people/faculty/alikhani-malihe.html\" property=\"og:url\"/>\n<meta data-siteid=\"lti\" id=\"siteId\"/>\n<meta content=\"#9f0000\" name=\"msapplication-TileColor\"/>\n<meta content=\"//www.cmu.edu/favicon-144.png\" name=\"msapplication-TileImage\"/>\n\nContent:\nMalihe \n                        Alikhani\nAssistant Professor, Northeastern University\nContact\nmalihe(through)pitt.edu\nPersonal Website\n\nLinks:\nhttps://www.malihealikhani.com/",
  "Faculty Name: alon lavie\nMetadata:\nPaperid: 10127fa44054eb985ede206113b96aac3a96fd80\nTitle: The Inside Story: Towards Better Understanding of Machine Translation Neural Evaluation Metrics\nYear: 2023\nAbstract: Neural metrics for machine translation evaluation, such as COMET, exhibit significant improvements in their correlation with human judgments, as compared to traditional metrics based on lexical overlap, such as BLEU. Yet, neural metrics are, to a great extent, \u201cblack boxes\u201d returning a single sentence-level score without transparency about the decision-making process. In this work, we develop and compare several neural explainability methods and demonstrate their effectiveness for interpreting state-of-the-art fine-tuned neural metrics. Our study reveals that these metrics leverage token-level information that can be directly attributed to translation errors, as assessed through comparison of token-level neural saliency maps with Multidimensional Quality Metrics (MQM) annotations and with synthetically-generated critical translation errors.",
  "Our study reveals that these metrics leverage token-level information that can be directly attributed to translation errors, as assessed through comparison of token-level neural saliency maps with Multidimensional Quality Metrics (MQM) annotations and with synthetically-generated critical translation errors. To ease future research, we release our code at: https://github.com/Unbabel/COMET/tree/explainable-metrics\nAuthors: Ricardo Rei, Nuno M. Guerreiro, Marcos Vin\u00edcius Treviso, Lu\u00edsa Coheur, A. Lavie, Andr\u00e9 Martins\nVenue: Annual Meeting of the Association for Computational Linguistics\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This study reveals that neural explainability metrics leverage token-level information that can be directly attributed to translation errors, as assessed through comparison of token- level neural saliency maps with Multidimensional Quality Metrics annotations and with synthetically-generated critical translation errors.'}",
  "Faculty Name: alon lavie\nMetadata:\nPaperid: 176108285e0c0cacc7734a4038afb700d665f3db\nTitle: Assessing and comparing alternative certification programs: The teacher-classroom-community model\nYear: 2023\nAbstract: Alternative certification programs (ACPs) differ from traditional teacher certification programs in their target populations, duration, tools they employ, their pedagogy, and subject matter curricula. Given the acute shortage of excellent teachers, especially in STEM, significant efforts and resources are invested in ACPs so they prepare highly qualified teachers. Yet, novice teachers face difficulties during their initial integration into the school system. To better understand the state of affairs, we investigated and compared the integration into the school system of graduates of five major Israeli ACPs that are tailored for diverse student-teacher target audiences.",
  "Yet, novice teachers face difficulties during their initial integration into the school system. To better understand the state of affairs, we investigated and compared the integration into the school system of graduates of five major Israeli ACPs that are tailored for diverse student-teacher target audiences. The study goals were to (1) investigate and compare the integration of graduates of the five ACPs into the teaching profession with respect to five teacher-related aspects: (a) self-efficacy, (b) commitment to the teaching profession, (c) challenges encountered, (d) leadership roles, and (e) teamwork; (2) identify ACP characteristics that support the graduates\u2019 integration into the teaching profession. The teacher-classroom-community model we propose, holistically connects three aspects: affective \u2013 the teacher, the teaching profession \u2013 the classroom, and peer interaction and leadership \u2013 the school community. The model provides a common language for comparing how the different ACPs prepared their graduates toward the teaching profession. The model is instrumental for identifying ACP characteristics that support graduates\u2019 integration into teaching and facilitating ACP evaluation by connecting several aspects of teachers\u2019 professional lives.",
  "The model provides a common language for comparing how the different ACPs prepared their graduates toward the teaching profession. The model is instrumental for identifying ACP characteristics that support graduates\u2019 integration into teaching and facilitating ACP evaluation by connecting several aspects of teachers\u2019 professional lives. The study employed a mixed-methodology in which 506 graduates responded to a closed- and open-ended questionnaire and 71 interviews were conducted with graduates (novice teachers), ACP directors, school principals and mentor teachers. The findings depict a complex picture that reflects the different ACPs\u2019 characteristics targeted at diverse audiences. For example, graduates of STEM-oriented programs perceive the different kinds of knowledge, including content knowledge, pedagogical knowledge, and pedagogical content knowledge, as most important to their roles in schools. They undertake fewer roles, and the ones they do assume are discipline-related. Graduates of the more social-leadership-oriented programs identify developing leadership skills as most beneficial and they undertake more leadership-related roles. The research highlights key aspects that teacher education leaders should consider and use for self-evaluation of their ACPs.",
  "Graduates of the more social-leadership-oriented programs identify developing leadership skills as most beneficial and they undertake more leadership-related roles. The research highlights key aspects that teacher education leaders should consider and use for self-evaluation of their ACPs. The strength of this study stems from proposing and applying the teacher-classroom-community model for evaluating teacher certification programs in several contexts and for diverse groups along with their integration into schools.\nAuthors: Y. Dori, Daphne Goldman, Gabriella Shwartz, Nirit Lavie-Alon, Ariel Sarid, T. Tal\nVenue: Frontiers in Education\nTldr: None",
  "Faculty Name: alon lavie\nMetadata:\nPaperid: 1a74a5be47b042896a3c4b479e2acca72390ad62\nTitle: Using citizen science to protect threatened amphibian populations in urban spaces: ecology, life history, and conservation of green toads and fire salamanders in Jerusalem and Haifa\nYear: 2023\nAbstract: The rapid urbanization processes occurring worldwide are amongst the main factors driving the current biodiversity crisis. In particular, a third of known amphibian species are directly threatened by urbanization. The negation of this threat will require conservation efforts aimed at sustaining viable amphibian populations within the urban landscape, which must be informed by a deep understanding of the way amphibian populations are affected by the unique risk factors of the urban environment. To address this need for four populations of amphibians in Israel, we performed a capture-recapture analysis on two datasets. The larger of the two datasets is the result of a multi-year citizen science program focused on two Salamandra infraimmaculata populations within the city of Haifa, Israel.",
  "To address this need for four populations of amphibians in Israel, we performed a capture-recapture analysis on two datasets. The larger of the two datasets is the result of a multi-year citizen science program focused on two Salamandra infraimmaculata populations within the city of Haifa, Israel. The second dataset is the result of one year of survey following a similar protocol that we performed on two Bufotes variabilis populations within the city of Jerusalem and at a nature reserve near it. Individuals of both species have unique and recognizable dorsal spot patterns, which allowed for noninvasive recapture identification. The results of our analysis provide insights that can guide future conservation of the specific studied population, but our conclusions have wider implications, regarding both the ecology of the studied species and applied conservation science: using the salamander dataset, we developed a method of length-based age estimations for this species and found that the studied salamanders have a prolonged period of increased vulnerability throughout their first years of life, even after reaching sexual maturity. Additionally, the shared conclusions from the two case studies indicate that the creation of fish-containing artificial water bodies in Mediterranean habitats can have detrimental impacts on the resident amphibian populations.",
  "Additionally, the shared conclusions from the two case studies indicate that the creation of fish-containing artificial water bodies in Mediterranean habitats can have detrimental impacts on the resident amphibian populations. Synthesis and implications: The significance and extent of our results demonstrate the effectiveness of citizen science as a tool for research and conservation in the urban environment. Our findings call for the implementation of management practices that prioritize the protection of urban amphibians and their habitats. By identifying the vulnerability of amphibians during critical life stages and highlighting the negative impacts of fish-containing water bodies, our study contributes to the development of informed conservation policies with implications for urban planning, habitat management, and biodiversity conservation strategies.\nAuthors: Omer Darel, Olga Rybak, Asaf Ben Levy, Gabi Kolodny, Tamar Kis- Papo, Nirit Lavie Alon, Rotem Vidan, O. Kolodny\nVenue: bioRxiv\nTldr: {'model': 'tldr@v2.0.0', 'text': None}",
  "Faculty Name: alon lavie\nMetadata:\nPaperid: 5579d38636b898c6a67ad67a16a80dd83be0f8d4\nTitle: Towards Multilingual Automatic Dialogue Evaluation\nYear: 2023\nAbstract: None\nAuthors: John Mendon\u00e7a, A. Lavie, Isabel Trancoso\nVenue: arXiv.org\nTldr: None",
  "Faculty Name: alon lavie\nMetadata:\nPaperid: 5c920b2326282d93ad2ac3d1cb8f746bd7ab6f50\nTitle: Results of WMT23 Metrics Shared Task: Metrics Might Be Guilty but References Are Not Innocent\nYear: 2023\nAbstract: This paper presents the results of the WMT23 Metrics Shared Task. Participants submitting automatic MT evaluation metrics were asked to score the outputs of the translation systems competing in the WMT23 News Translation Task. All metrics were evaluated on how well they correlate with human ratings at the system and segment level. Similar to last year, we acquired our own human ratings based on expert-based human evaluation via Multidimensional Quality Metrics (MQM). Following last year\u2019s success, we also included a challenge set subtask, where participants had to create contrastive test suites for evaluating metrics\u2019 ability to capture and penalise specific types of translation errors. Furthermore, we improved our meta-evaluation procedure by considering fewer tasks and calculating a global score by weighted averaging across the various tasks.",
  "Furthermore, we improved our meta-evaluation procedure by considering fewer tasks and calculating a global score by weighted averaging across the various tasks. We present an extensive analysis on how well metrics perform on three language pairs: Chinese-English, Hebrew-English on the sentence-level and English-German on the paragraph-level. The results strongly confirm the results reported last year, that neural-based metrics are significantly better than non-neural metrics in their levels of correlation with human judgments. Further, we investigate the impact of bad reference translations on the correlations of metrics with human judgment. We present a novel approach for generating synthetic reference translations based on the collection of MT system outputs and their corresponding MQM ratings, which has the potential to mitigate bad reference issues we observed this year for some language pairs. Finally, we also study the connections between the magnitude of metric differences and their expected significance in human evaluation, which should help the community to better understand and adopt new metrics.",
  "Finally, we also study the connections between the magnitude of metric differences and their expected significance in human evaluation, which should help the community to better understand and adopt new metrics.\nAuthors: Markus Freitag, Nitika Mathur, Chi-kiu Lo, Eleftherios Avramidis, Ricardo Rei, Brian Thompson, Tom Kocmi, Frederic Blain, Daniel Deutsch, Craig Stewart, Chrysoula Zerva, Sheila Castilho, A. Lavie, George F. Foster\nVenue: Conference on Machine Translation\nTldr: {'model': 'tldr@v2.0.0', 'text': 'A novel approach for generating synthetic reference translations based on the collection of MT system outputs and their corresponding MQM ratings, which has the potential to mitigate bad reference issues the authors observed this year for some language pairs are presented.'}",
  "Faculty Name: alon lavie\nMetadata:\nPaperid: 9364720b2ab9ac67bc08e2b0b49aadded3d4e2e5\nTitle: Appropriateness is all you need!\nYear: 2023\nAbstract: The strive to make AI applications\"safe\"has led to the development of safety-measures as the main or even sole normative requirement of their permissible use. Similar can be attested to the latest version of chatbots, such as chatGPT. In this view, if they are\"safe\", they are supposed to be permissible to deploy. This approach, which we call\"safety-normativity\", is rather limited in solving the emerging issues that chatGPT and other chatbots have caused thus far. In answering this limitation, in this paper we argue for limiting chatbots in the range of topics they can chat about according to the normative concept of appropriateness. We argue that rather than looking for\"safety\"in a chatbot's utterances to determine what they may and may not say, we ought to assess those utterances according to three forms of appropriateness: technical-discursive, social, and moral.",
  "We argue that rather than looking for\"safety\"in a chatbot's utterances to determine what they may and may not say, we ought to assess those utterances according to three forms of appropriateness: technical-discursive, social, and moral. We then spell out what requirements for chatbots follow from these forms of appropriateness to avoid the limits of previous accounts: positionality, acceptability, and value alignment (PAVA). With these in mind, we may be able to determine what a chatbot may and may not say. Lastly, one initial suggestion is to use challenge sets, specifically designed for appropriateness, as a validation method.\nAuthors: Hendrik Kempt, A. Lavie, S. Nagel\nVenue: arXiv.org\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This paper argues for limiting chatbots in the range of topics they can chat about according to the normative concept of appropriateness, and spells out what requirements for chatbots follow from these forms of Appropriateness to avoid the limits of previous accounts.'}",
  "Faculty Name: alon lavie\nMetadata:\nPaperid: 9e8f125ef479af7e95ee5b8949b24e750c7df367\nTitle: Towards Multilingual Automatic Open-Domain Dialogue Evaluation\nYear: 2023\nAbstract: The main limiting factor in the development of robust multilingual open-domain dialogue evaluation metrics is the lack of multilingual data and the limited availability of open-sourced multilingual dialogue systems. In this work, we propose a workaround for this lack of data by leveraging a strong multilingual pretrained encoder-based Language Model and augmenting existing English dialogue data using Machine Translation. We empirically show that the naive approach of finetuning a pretrained multilingual encoder model with translated data is insufficient to outperform the strong baseline of finetuning a multilingual model with only source data. Instead, the best approach consists in the careful curation of translated data using MT Quality Estimation metrics, excluding low quality translations that hinder its performance.",
  "Instead, the best approach consists in the careful curation of translated data using MT Quality Estimation metrics, excluding low quality translations that hinder its performance.\nAuthors: John Mendon\u00e7a, A. Lavie, I. Trancoso\nVenue: SIGDIAL Conferences\nTldr: {'model': 'tldr@v2.0.0', 'text': 'It is empirically show that the naive approach of finetuning a pretrained multilingual encoder model with translated data is insufficient to outperform the strong baseline of finETuning a multilingual model with only source data.'}",
  "Faculty Name: alon lavie\nMetadata:\nPaperid: bcefc74b20649fd41ea05d87a3fa512d2559fc8d\nTitle: Simple LLM Prompting is State-of-the-Art for Robust and Multilingual Dialogue Evaluation\nYear: 2023\nAbstract: Despite significant research effort in the development of automatic dialogue evaluation metrics, little thought is given to evaluating dialogues other than in English. At the same time, ensuring metrics are invariant to semantically similar responses is also an overlooked topic. In order to achieve the desired properties of robustness and multilinguality for dialogue evaluation metrics, we propose a novel framework that takes advantage of the strengths of current evaluation models with the newly-established paradigm of prompting Large Language Models (LLMs). Empirical results show our framework achieves state of the art results in terms of mean Spearman correlation scores across several benchmarks and ranks first place on both the Robust and Multilingual tasks of the DSTC11 Track 4 \u201cAutomatic Evaluation Metrics for Open-Domain Dialogue Systems\u201d, proving the evaluation capabilities of prompted LLMs.",
  "Authors: J. Mendoncca, Patr\u00edcia Pereira, Joao Paulo Carvalho, A. Lavie, I. Trancoso\nVenue: DSTC\nTldr: {'model': 'tldr@v2.0.0', 'text': 'A novel framework that takes advantage of the strengths of current evaluation models with the newly-established paradigm of prompting Large Language Models (LLMs) to achieve the desired properties of robustness and multilinguality for dialogue evaluation metrics.'}",
  "List of 2023 Open Access papers by alon lavie are:\nThe Inside Story: Towards Better Understanding of Machine Translation Neural Evaluation Metrics\nTowards Multilingual Automatic Dialogue Evaluation\nResults of WMT23 Metrics Shared Task: Metrics Might Be Guilty but References Are Not Innocent\nAppropriateness is all you need!\nTowards Multilingual Automatic Open-Domain Dialogue Evaluation\nSimple LLM Prompting is State-of-the-Art for Robust and Multilingual Dialogue Evaluation\nAssessing and comparing alternative certification programs: The teacher-classroom-community model\nUsing citizen science to protect threatened amphibian populations in urban spaces: ecology, life history, and conservation of green toads and fire salamanders in Jerusalem and Haifa",
  "Faculty Name: berg kirkpatrick taylor\nMetadata:\nPaperid: 1527d3b661154ff7310fa2759b6dd0ddfd559492\nTitle: Smaller Language Models are Better Black-box Machine-Generated Text Detectors\nYear: 2023\nAbstract: With the advent of fluent generative language models that can produce convincing utterances very similar to those written by humans, distinguishing whether a piece of text is machine-generated or human-written becomes more challenging and more important, as such models could be used to spread misinformation, fake news, fake reviews and to mimic certain authors and figures. To this end, there have been a slew of methods proposed to detect machine-generated text. Most of these methods need access to the logits of the target model or need the ability to sample from the target. One such black-box detection method relies on the observation that generated text is locally optimal under the likelihood function of the generator, while human-written text is not. We find that overall, smaller and partially-trained models are better universal text detectors: they can more precisely detect text generated from both small and larger models.",
  "One such black-box detection method relies on the observation that generated text is locally optimal under the likelihood function of the generator, while human-written text is not. We find that overall, smaller and partially-trained models are better universal text detectors: they can more precisely detect text generated from both small and larger models. Interestingly, we find that whether the detector and generator were trained on the same data is not critically important to the detection success. For instance the OPT-125M model has an AUC of 0.81 in detecting ChatGPT generations, whereas a larger model from the GPT family, GPTJ-6B, has AUC of 0.45.\nAuthors: Fatemehsadat Mireshghallah, Justus Mattern, Sicun Gao, R. Shokri, Taylor Berg-Kirkpatrick\nVenue: arXiv.org\nTldr: {'model': 'tldr@v2.0.0', 'text': 'Overall, smaller and partially-trained models are better universal text detectors: they can more precisely detect text generated from both small and larger models.'}",
  "Faculty Name: berg kirkpatrick taylor\nMetadata:\nPaperid: 15b08595533bfc640f4dd470ca7a2273badec20a\nTitle: Towards Improving Harmonic Sensitivity and Prediction Stability for Singing Melody Extraction\nYear: 2023\nAbstract: In deep learning research, many melody extraction models rely on redesigning neural network architectures to improve performance. In this paper, we propose an input feature modification and a training objective modification based on two assumptions. First, harmonics in the spectrograms of audio data decay rapidly along the frequency axis. To enhance the model's sensitivity on the trailing harmonics, we modify the Combined Frequency and Periodicity (CFP) representation using discrete z-transform. Second, the vocal and non-vocal segments with extremely short duration are uncommon. To ensure a more stable melody contour, we design a differentiable loss function that prevents the model from predicting such segments. We apply these modifications to several models, including MSNet, FTANet, and a newly introduced model, PianoNet, modified from a piano transcription network. Our experimental results demonstrate that the proposed modifications are empirically effective for singing melody extraction.",
  "We apply these modifications to several models, including MSNet, FTANet, and a newly introduced model, PianoNet, modified from a piano transcription network. Our experimental results demonstrate that the proposed modifications are empirically effective for singing melody extraction.\nAuthors: Keren Shao, K. Chen, Taylor Berg-Kirkpatrick, S. Dubnov\nVenue: International Society for Music Information Retrieval Conference\nTldr: {'model': 'tldr@v2.0.0', 'text': \"This paper proposes an input feature modification and a training objective modification based on two assumptions of harmonics in the spectrograms of audio data decay rapidly along the frequency axis to enhance the model's sensitivity on the trailing harmonics.\"}",
  "Faculty Name: berg kirkpatrick taylor\nMetadata:\nPaperid: 1d7a19085d0e1c3f8593e1d9feb059a27f71a4b4\nTitle: Text Conditional Alt-Text Generation for Twitter Images\nYear: 2023\nAbstract: In this work we present an approach for generating alternative text (or alt-text) descriptions for images shared on social media, specifically Twitter. This task is more than just a special case of image captioning, as alt-text is both more literally descriptive and context-specific. Also critically, images posted to Twitter are often accompanied by user-written text that despite not necessarily describing the image may provide useful context that if properly lever-aged can be informative \u2013 e.g. the tweet may name an uncommon object in the image that the model has not previously seen. We address this with a CLIP prefix model that extracts an embedding of the image and passes it to a mapping network that outputs a short sequence in word embedding space, or a \u201cprefix\u201d, to which we also concatenate the text from the tweet itself. This lets the model condition on both visual and textual information from the post.",
  "This lets the model condition on both visual and textual information from the post. The combined multimodal prefix is then fed as a prompt to a pretrained language model which autore-gressively completes the sequence to generate the alt-text. While prior work has used similar methods for captioning, ours is the first to our knowledge that incorporates textual information from the associated social media post into the prefix as well, and we further demonstrate through ablations that utility of these two information sources stacks. We put forward a new dataset scraped from Twitter and evaluate on it across a variety of automated metrics as well as human evaluation, and show that our approach of conditioning on both tweet text and visual information significantly outperforms prior work.\nAuthors: Nikita Srivatsan, Sof\u00eda Samaniego, Omar Florez, Taylor Berg-Kirkpatrick\nVenue: arXiv.org\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This work presents an approach for generating alternative text (or alt-text) descriptions for images shared on social media, specifically Twitter, and is the first to their knowledge that incorporates textual information from the associated social media post into the prefix as well.'}",
  "Faculty Name: berg kirkpatrick taylor\nMetadata:\nPaperid: 1f1a09e59dbd178aa0988ea8f96e780e36923c8a\nTitle: Jointly modeling products and resource pages for task-oriented recommendation\nYear: 2023\nAbstract: Modeling high-level user intent in recommender systems can improve performance, although it is often difficult to obtain a ground truth measure of this intent. In this paper, we investigate a novel way to obtain such an intent signal by leveraging resource pages associated with a particular task. We jointly model product interactions and resource page interactions to create a system which can recommend both products and resource pages to users. Our experiments consider the domain of home improvement product recommendation, where resource pages are DIY (do-it-yourself) project pages from Lowes.com. Each DIY page provides a list of tools, materials, and step-by-step instructions to complete a DIY project, such as building a deck, installing cabinets, and fixing a leaking pipe. We use this data as an indicator of the intended project, which is a natural high-level intent signal for home improvement shoppers.",
  "Each DIY page provides a list of tools, materials, and step-by-step instructions to complete a DIY project, such as building a deck, installing cabinets, and fixing a leaking pipe. We use this data as an indicator of the intended project, which is a natural high-level intent signal for home improvement shoppers. We then extend a state-of-the-art system to incorporate this new intent data, and show a significant improvement in the ability of the system to recommend products. We further demonstrate that our system can be used to successfully recommend DIY project pages to users. We have taken initial steps towards deploying our method for project recommendation in production on the Lowe\u2019s website and for recommendations through marketing emails.\nAuthors: B. Duncan, Surya Kallumadi, Taylor Berg-Kirkpatrick, Julian McAuley\nVenue: The Web Conference\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This paper jointly model product interactions and resource page interactions to create a system which can recommend both products and resource pages to users, and extends a state-of-the-art system to incorporate this new intent data, and shows a significant improvement in the ability of the system to recommend products.'}",
  "Faculty Name: berg kirkpatrick taylor\nMetadata:\nPaperid: 300d5f610872b84bf8c85c2b51c421f2c7160f3d\nTitle: CuneiML: A Cuneiform Dataset for Machine Learning\nYear: 2023\nAbstract: None\nAuthors: Danlu Chen, Aditi Agarwal, Taylor Berg-Kirkpatrick, Jacobo Myerston\nVenue: Journal of Open Humanities Data\nTldr: None",
  "Faculty Name: berg kirkpatrick taylor\nMetadata:\nPaperid: 3a899fbda8071af3fce011ae2c1f6c00264c070f\nTitle: Exploring the Relationship Between Model Architecture and In-Context Learning Ability\nYear: 2023\nAbstract: What is the relationship between model architecture and the ability to perform in-context learning? In this empirical study, we take the first steps toward answering this question. We evaluate twelve model architectures capable of causal language modeling across a suite of synthetic in-context learning tasks. These selected architectures represent a broad range of paradigms, including recurrent and convolution-based neural networks, transformers, state-space model inspired, and other emerging attention alternatives. We discover that all the considered architectures can perform in-context learning under a wider range of conditions than previously documented. Additionally, we observe stark differences in statistical efficiency and consistency by varying context length and task difficulty. We also measure each architecture's predisposition towards in-context learning when presented with alternative routes for task resolution. Finally, and somewhat surprisingly, we find that several attention alternatives are more robust in-context learners than transformers.",
  "Additionally, we observe stark differences in statistical efficiency and consistency by varying context length and task difficulty. We also measure each architecture's predisposition towards in-context learning when presented with alternative routes for task resolution. Finally, and somewhat surprisingly, we find that several attention alternatives are more robust in-context learners than transformers. Given that such approaches have constant-sized memory footprints at inference time, this result opens the possibility of scaling up in-context learning to accommodate vastly larger numbers of in-context examples.\nAuthors: Ivan Lee, Nan Jiang, Taylor Berg-Kirkpatrick\nVenue: arXiv.org\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This empirical study evaluates twelve model architectures capable of causal language modeling across a suite of synthetic in- context learning tasks and finds that several attention alternatives are more robust in-context learners than transformers.'}",
  "Faculty Name: berg kirkpatrick taylor\nMetadata:\nPaperid: 464edfd902f652d3ab6a25dbb6d9fa47cc3246a9\nTitle: MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies\nYear: 2023\nAbstract: Diffusion models have shown promising results in cross-modal generation tasks, including text-to-image and text-to-audio generation. However, generating music, as a special type of audio, presents unique challenges due to limited availability of music data and sensitive issues related to copyright and plagiarism. In this paper, to tackle these challenges, we first construct a state-of-the-art text-to-music model, MusicLDM, that adapts Stable Diffusion and AudioLDM architectures to the music domain. We achieve this by retraining the contrastive language-audio pretraining model (CLAP) and the Hifi-GAN vocoder, as components of MusicLDM, on a collection of music data samples.",
  "We achieve this by retraining the contrastive language-audio pretraining model (CLAP) and the Hifi-GAN vocoder, as components of MusicLDM, on a collection of music data samples. Then, to address the limitations of training data and to avoid plagiarism, we leverage a beat tracking model and propose two different mixup strategies for data augmentation: beat-synchronous audio mixup and beat-synchronous latent mixup, which recombine training audio directly or via a latent embeddings space, respectively. Such mixup strategies encourage the model to interpolate between musical training samples and generate new music within the convex hull of the training data, making the generated music more diverse while still staying faithful to the corresponding style. In addition to popular evaluation metrics, we design several new evaluation metrics based on CLAP score to demonstrate that our proposed MusicLDM and beat-synchronous mixup strategies improve both the quality and novelty of generated music, as well as the correspondence between input text and generated music.",
  "In addition to popular evaluation metrics, we design several new evaluation metrics based on CLAP score to demonstrate that our proposed MusicLDM and beat-synchronous mixup strategies improve both the quality and novelty of generated music, as well as the correspondence between input text and generated music.\nAuthors: K. Chen, Yusong Wu, Haohe Liu, Marianna Nezhurina, Taylor Berg-Kirkpatrick, S. Dubnov\nVenue: arXiv.org\nTldr: {'model': 'tldr@v2.0.0', 'text': 'A state-of-the-art text-to-music model that adapts Stable Diffusion and AudioLDM architectures to the music domain is constructed and two different mixup strategies for data augmentation are proposed: beat-synchronous audio mixup and beat- Synchronous latent mixup, which recombine training audio directly or via a latent embeddings space, respectively.'}",
  "Faculty Name: berg kirkpatrick taylor\nMetadata:\nPaperid: 55324ec5662bbea2e226ce3d8e6258a62836e940\nTitle: Universal Source Separation with Weakly Labelled Data\nYear: 2023\nAbstract: Universal source separation (USS) is a fundamental research task for computational auditory scene analysis, which aims to separate mono recordings into individual source tracks. There are three potential challenges awaiting the solution to the audio source separation task. First, previous audio source separation systems mainly focus on separating one or a limited number of specific sources. There is a lack of research on building a unified system that can separate arbitrary sources via a single model. Second, most previous systems require clean source data to train a separator, while clean source data are scarce. Third, there is a lack of USS system that can automatically detect and separate active sound classes in a hierarchical level.",
  "Second, most previous systems require clean source data to train a separator, while clean source data are scarce. Third, there is a lack of USS system that can automatically detect and separate active sound classes in a hierarchical level. To use large-scale weakly labeled/unlabeled audio data for audio source separation, we propose a universal audio source separation framework containing: 1) an audio tagging model trained on weakly labeled data as a query net; and 2) a conditional source separation model that takes query net outputs as conditions to separate arbitrary sound sources. We investigate various query nets, source separation models, and training strategies and propose a hierarchical USS strategy to automatically detect and separate sound classes from the AudioSet ontology. By solely leveraging the weakly labelled AudioSet, our USS system is successful in separating a wide variety of sound classes, including sound event separation, music source separation, and speech enhancement.",
  "By solely leveraging the weakly labelled AudioSet, our USS system is successful in separating a wide variety of sound classes, including sound event separation, music source separation, and speech enhancement. The USS system achieves an average signal-to-distortion ratio improvement (SDRi) of 5.57 dB over 527 sound classes of AudioSet; 10.57 dB on the DCASE 2018 Task 2 dataset; 8.12 dB on the MUSDB18 dataset; an SDRi of 7.28 dB on the Slakh2100 dataset; and an SSNR of 9.00 dB on the voicebank-demand dataset.",
  "We release the source code at https://github.com/bytedance/uss\nAuthors: Qiuqiang Kong, K. Chen, Haohe Liu, Xingjian Du, Taylor Berg-Kirkpatrick, S. Dubnov, M. Plumbley\nVenue: arXiv.org\nTldr: {'model': 'tldr@v2.0.0', 'text': 'A hierarchical USS strategy to automatically detect and separate sound classes from the AudioSet ontology is proposed, which is successful in separating a wide variety of sound classes, including sound event separation, music source separation, and speech enhancement.'}",
  "Faculty Name: berg kirkpatrick taylor\nMetadata:\nPaperid: ac5b4df0e398ca48388330ac5c795b6fe708793c\nTitle: Misusing Tools in Large Language Models With Visual Adversarial Examples\nYear: 2023\nAbstract: Large Language Models (LLMs) are being enhanced with the ability to use tools and to process multiple modalities. These new capabilities bring new benefits and also new security risks. In this work, we show that an attacker can use visual adversarial examples to cause attacker-desired tool usage. For example, the attacker could cause a victim LLM to delete calendar events, leak private conversations and book hotels. Different from prior work, our attacks can affect the confidentiality and integrity of user resources connected to the LLM while being stealthy and generalizable to multiple input prompts. We construct these attacks using gradient-based adversarial training and characterize performance along multiple dimensions. We find that our adversarial images can manipulate the LLM to invoke tools following real-world syntax almost always (~98%) while maintaining high similarity to clean images (~0.9 SSIM).",
  "We construct these attacks using gradient-based adversarial training and characterize performance along multiple dimensions. We find that our adversarial images can manipulate the LLM to invoke tools following real-world syntax almost always (~98%) while maintaining high similarity to clean images (~0.9 SSIM). Furthermore, using human scoring and automated metrics, we find that the attacks do not noticeably affect the conversation (and its semantics) between the user and the LLM.\nAuthors: Xiaohan Fu, Zihan Wang, Shuheng Li, Rajesh K. Gupta, Niloofar Mireshghallah, Taylor Berg-Kirkpatrick, Earlence Fernandes\nVenue: arXiv.org\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This work shows that an attacker can use visual adversarial examples to cause attacker-desired tool usage to cause a victim LLM to delete calendar events, leak private conversations and book hotels.'}",
  "Faculty Name: berg kirkpatrick taylor\nMetadata:\nPaperid: c70c11d6de5eec2677eaa87fd3112068db6fedfe\nTitle: CLIPSonic: Text-to-Audio Synthesis with Unlabeled Videos and Pretrained Language-Vision Models\nYear: 2023\nAbstract: Recent work has studied text-to-audio synthesis using large amounts of paired text-audio data. However, audio recordings with high-quality text annotations can be difficult to acquire. In this work, we approach text-to-audio synthesis using unlabeled videos and pre-trained language-vision models. We propose to learn the desired text-audio correspondence by leveraging the visual modality as a bridge. We train a conditional diffusion model to generate the audio track of a video, given a video frame encoded by a pretrained contrastive language-image pretraining (CLIP) model. At test time, we first explore performing a zero-shot modality transfer and condition the diffusion model with a CLIP-encoded text query. However, we observe a noticeable performance drop with respect to image queries.",
  "At test time, we first explore performing a zero-shot modality transfer and condition the diffusion model with a CLIP-encoded text query. However, we observe a noticeable performance drop with respect to image queries. To close this gap, we further adopt a pretrained diffusion prior model to generate a CLIP image embedding given a CLIP text embedding. Our results show the effectiveness of the proposed method, and that the pretrained diffusion prior can reduce the modality transfer gap. While we focus on text-to-audio synthesis, the proposed model can also generate audio from image queries, and it shows competitive performance against a state-of-the-art image-to-audio synthesis model in a subjective listening test. This study offers a new direction of approaching text-to-audio synthesis that leverages the naturally-occurring audio-visual correspondence in videos and the power of pretrained language-vision models.",
  "This study offers a new direction of approaching text-to-audio synthesis that leverages the naturally-occurring audio-visual correspondence in videos and the power of pretrained language-vision models.\nAuthors: Hao-Wen Dong, Xiaoyu Liu, Jordi Pons, Gautam Bhattacharya, Santiago Pascual, J. Serr\u00e0, Taylor Berg-Kirkpatrick, Julian McAuley\nVenue: IEEE Workshop on Applications of Signal Processing to Audio and Acoustics\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This work proposes to learn the desired text-audio correspondence by leveraging the visual modality as a bridge in videos and pretrained language-vision models, and shows competitive performance against a state-of-the-art image-to-audio synthesis model in a subjective listening test.'}",
  "Faculty Name: berg kirkpatrick taylor\nMetadata:\nPaperid: c817d9901f788e40279c76068ea2622365cd9a1d\nTitle: Simple Temporal Adaptation to Changing Label Sets: Hashtag Prediction via Dense KNN\nYear: 2023\nAbstract: User-generated social media data is constantly changing as new trends influence online discussion and personal information is deleted due to privacy concerns. However, traditional NLP models rely on fixed training datasets, which means they are unable to adapt to temporal change\u2014both test distribution shift and deleted training data\u2014without frequent, costly re-training. In this paper, we study temporal adaptation through the task of longitudinal hashtag prediction and propose a non-parametric dense retrieval technique, which does not require re-training, as a simple but effective solution. In experiments on a newly collected, publicly available, year-long Twitter dataset exhibiting temporal distribution shift, our method improves by 64% over the best static parametric baseline while avoiding costly gradient-based re-training. Our approach is also particularly well-suited to dynamically deleted user data in line with data privacy laws, with negligible computational cost/performance loss.",
  "Our approach is also particularly well-suited to dynamically deleted user data in line with data privacy laws, with negligible computational cost/performance loss.\nAuthors: Niloofar Mireshghallah, Nikolai Vogler, Junxian He, Omar Florez, Ahmed El-Kishky, Taylor Berg-Kirkpatrick\nVenue: Conference on Empirical Methods in Natural Language Processing\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This paper studies temporal adaptation through the task of longitudinal hashtag prediction and proposes a non-parametric dense retrieval technique, which does not require re-training, as a simple but effective solution to temporal change.'}",
  "Faculty Name: berg kirkpatrick taylor\nMetadata:\nPaperid: cb754310302086dfbbcd098263200e2a03f65874\nTitle: Membership Inference Attacks against Language Models via Neighbourhood Comparison\nYear: 2023\nAbstract: Membership Inference attacks (MIAs) aim to predict whether a data sample was present in the training data of a machine learning model or not, and are widely used for assessing the privacy risks of language models. Most existing attacks rely on the observation that models tend to assign higher probabilities to their training samples than non-training points. However, simple thresholding of the model score in isolation tends to lead to high false-positive rates as it does not account for the intrinsic complexity of a sample. Recent work has demonstrated that reference-based attacks which compare model scores to those obtained from a reference model trained on similar data can substantially improve the performance of MIAs. However, in order to train reference models, attacks of this kind make the strong and arguably unrealistic assumption that an adversary has access to samples closely resembling the original training data. Therefore, we investigate their performance in more realistic scenarios and find that they are highly fragile in relation to the data distribution used to train reference models.",
  "Therefore, we investigate their performance in more realistic scenarios and find that they are highly fragile in relation to the data distribution used to train reference models. To investigate whether this fragility provides a layer of safety, we propose and evaluate neighbourhood attacks, which compare model scores for a given sample to scores of synthetically generated neighbour texts and therefore eliminate the need for access to the training data distribution. We show that, in addition to being competitive with reference-based attacks that have perfect knowledge about the training data distribution, our attack clearly outperforms existing reference-free attacks as well as reference-based attacks with imperfect knowledge, which demonstrates the need for a reevaluation of the threat model of adversarial attacks.",
  "We show that, in addition to being competitive with reference-based attacks that have perfect knowledge about the training data distribution, our attack clearly outperforms existing reference-free attacks as well as reference-based attacks with imperfect knowledge, which demonstrates the need for a reevaluation of the threat model of adversarial attacks.\nAuthors: Justus Mattern, Fatemehsadat Mireshghallah, Zhijing Jin, B. Sch\u00f6lkopf, Mrinmaya Sachan, Taylor Berg-Kirkpatrick\nVenue: Annual Meeting of the Association for Computational Linguistics\nTldr: {'model': 'tldr@v2.0.0', 'text': 'Neighbourhood attacks are proposed and evaluated, which compare model scores for a given sample to scores of synthetically generated neighbour texts and therefore eliminate the need for access to the training data distribution and clearly outperforms existing reference-free attacks as well as reference-based attacks with imperfect knowledge.'}",
  "Faculty Name: berg kirkpatrick taylor\nMetadata:\nPaperid: d8785264bbce47ca1ea7f97e7f3bc4ca6cbe824c\nTitle: A Block Metropolis-Hastings Sampler for Controllable Energy-based Text Generation\nYear: 2023\nAbstract: Recent work has shown that energy-based language modeling is an effective framework for controllable text generation because it enables flexible integration of arbitrary discriminators. However, because energy-based LMs are globally normalized, approximate techniques like Metropolis-Hastings (MH) are required for inference. Past work has largely explored simple proposal distributions that modify a single token at a time, like in Gibbs sampling. In this paper, we develop a novel MH sampler that, in contrast, proposes re-writes of the entire sequence in each step via iterative prompting of a large language model. Our new sampler (a) allows for more efficient and accurate sampling from a target distribution and (b) allows generation length to be determined through the sampling procedure rather than fixed in advance, as past work has required. We perform experiments on two controlled generation tasks, showing both downstream performance gains and more accurate target distribution sampling in comparison with single-token proposal techniques.",
  "We perform experiments on two controlled generation tasks, showing both downstream performance gains and more accurate target distribution sampling in comparison with single-token proposal techniques.\nAuthors: Jarad Forristal, Fatemehsadat Mireshghallah, Greg Durrett, Taylor Berg-Kirkpatrick\nVenue: Conference on Computational Natural Language Learning\nTldr: {'model': 'tldr@v2.0.0', 'text': 'A novel MH sampler is developed that proposes re-writes of the entire sequence in each step via iterative prompting of a large language model and allows generation length to be determined through the sampling procedure rather than fixed in advance, as past work has required.'}",
  "Faculty Name: berg kirkpatrick taylor\nMetadata:\nPaperid: d9dc309f719233be9f2a6b6910072e537f96eec8\nTitle: Contrastive Attention Networks for Attribution of Early Modern Print\nYear: 2023\nAbstract: In this paper, we develop machine learning techniques to identify unknown printers in early modern (c.~1500--1800) English printed books.\nSpecifically, we focus on matching uniquely damaged character type-imprints in anonymously printed books to works with known printers in order to provide evidence of their origins.\nUntil now, this work has been limited to manual investigations by analytical bibliographers.\nWe present a Contrastive Attention-based Metric Learning approach to identify similar damage across character image pairs, which is sensitive to very subtle differences in glyph shapes, yet robust to various confounding sources of noise associated with digitized historical books. \nTo overcome the scarce amount of supervised data, we design a random data synthesis procedure that aims to simulate bends, fractures, and inking variations induced by the early printing process.\nOur method successfully improves downstream damaged type-imprint matching among printed works from this period, as validated by in-domain human experts.",
  "To overcome the scarce amount of supervised data, we design a random data synthesis procedure that aims to simulate bends, fractures, and inking variations induced by the early printing process.\nOur method successfully improves downstream damaged type-imprint matching among printed works from this period, as validated by in-domain human experts. The results of our approach on two important philosophical works from the Early Modern period demonstrate potential to extend the extant historical research about the origins and content of these books.\nAuthors: Nikolai Vogler, Kartik Goyal, Kishore PV Reddy, Elizaveta Pertseva, Sam Lemley, Christopher N. Warren, M. G'Sell, Taylor Berg-Kirkpatrick\nVenue: AAAI Conference on Artificial Intelligence\nTldr: {'model': 'tldr@v2.0.0', 'text': 'The method successfully improves downstream damaged type-imprint matching among printed works from this period, as validated by in-domain human experts and demonstrates potential to extend the extant historical research about the origins and content of these books.'}",
  "List of 2023 Open Access papers by berg kirkpatrick taylor are:\nSmaller Language Models are Better Black-box Machine-Generated Text Detectors\nTowards Improving Harmonic Sensitivity and Prediction Stability for Singing Melody Extraction\nJointly modeling products and resource pages for task-oriented recommendation\nMusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies\nUniversal Source Separation with Weakly Labelled Data\nCLIPSonic: Text-to-Audio Synthesis with Unlabeled Videos and Pretrained Language-Vision Models\nMembership Inference Attacks against Language Models via Neighbourhood Comparison\nContrastive Attention Networks for Attribution of Early Modern Print\nText Conditional Alt-Text Generation for Twitter Images\nExploring the Relationship Between Model Architecture and In-Context Learning Ability\nA Block Metropolis-Hastings Sampler for Controllable Energy-based Text Generation\nMisusing Tools in Large Language Models With Visual Adversarial Examples\nSimple Temporal Adaptation to Changing Label Sets: Hashtag Prediction via Dense KNN\nCuneiML: A Cuneiform Dataset for Machine Learning",
  "Title: Taylor Berg-Kirkpatrick -     Language Technologies Institute -     School of Computer Science - Carnegie Mellon University\n\nMeta Tags:\n<meta charset=\"utf-8\"/>\n<meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\"/>\n<meta content=\"width=device-width, initial-scale=1.0\" name=\"viewport\"/>\n<meta content=\"Bio of Taylor Berg-Kirkpatrick, Adjunct Faculty at Carnegie Mellon University's Language Technologies Institute\" name=\"description\"/>\n<meta content=\"Carnegie Mellon University\" name=\"author\"/>\n<meta content=\"Adjunct Faculty\" name=\"categories-1\"/>\n<meta content=\"Machine Learning;Natural Language Processing and Computational Linguistics\" name=\"categories-2\"/>\n<meta content=\"Yes\" name=\"show-categories-2\"/>\n<meta content=\"Faculty\" name=\"global-categories\"/>\n<meta content=\"Taylor Berg-Kirkpatrick - Language Technologies Institute - School of Computer Science - Carnegie Mellon University\" property=\"og:title\"/>\n<meta content=\"Bio of Taylor Berg-Kirkpatrick,",
  "Adjunct Faculty at Carnegie Mellon University's Language Technologies Institute\" property=\"og:description\"/>\n<meta content=\"profile\" property=\"og:type\"/>\n<meta content=\"Firstname\" property=\"profile:Taylor\"/>\n<meta content=\"Lastname\" property=\"profile:Berg-Kirkpatrick\"/>\n<meta content=\"http://lti.cmu.edu//people/faculty/berg-kirkpatrick-taylor.html\" property=\"og:url\"/>\n<meta data-siteid=\"lti\" id=\"siteId\"/>\n<meta content=\"#9f0000\" name=\"msapplication-TileColor\"/>\n<meta content=\"//www.cmu.edu/favicon-144.png\" name=\"msapplication-TileImage\"/>\n\nContent:\nTaylor \n                        Berg-Kirkpatrick\nAssistant Professor, University of California, San Diego\nContact\n6403 \u2014Gates & Hillman Centers\ntberg(through)cs.cmu.edu\nResearch Area\nMachine Learning, Natural Language Processing and Computational Linguistics\nResearch\nUnsupervised Learning\nDeveloping models and methods that can learn to understand aspects of human language\u00a0without relying on hand-labeled data\nStructured Prediction\nBuilding inference procedures that can efficiently and accurately produce complex outputs,",
  "cmu.edu\nResearch Area\nMachine Learning, Natural Language Processing and Computational Linguistics\nResearch\nUnsupervised Learning\nDeveloping models and methods that can learn to understand aspects of human language\u00a0without relying on hand-labeled data\nStructured Prediction\nBuilding inference procedures that can efficiently and accurately produce complex outputs,\u00a0including both structured linguistic annotations (like parse trees) and natural language utterances\u00a0(like summaries)\nAnalysis of Human Artifacts\nUsing automated methods to analyze and decipher structured human artifacts, including text\u00a0but also sources like piano music, images of early modern books (e.g. First Folio of Shakespeare),\u00a0and cuneiform tablets\nPersonal Website\n\nLinks:\nhttps://cseweb.ucsd.edu/~tberg/",
  "Faculty Name: bhiksha raj\nMetadata:\nPaperid: 078f86c6a691806cc71bbef1e734f75690db0ffc\nTitle: FREDOM: Fairness Domain Adaptation Approach to Semantic Scene Understanding\nYear: 2023\nAbstract: Although Domain Adaptation in Semantic Scene Segmentation has shown impressive improvement in recent years, the fairness concerns in the domain adaptation have yet to be well defined and addressed. In addition, fairness is one of the most critical aspects when deploying the segmentation models into human-related real-world applications, e.g., autonomous driving, as any unfair predictions could influence human safety. In this paper, we propose a novel Fairness Domain Adaptation (FREDOM) approach to semantic scene segmentation. In particular, from the proposed formulated fairness objective, a new adaptation framework will be introduced based on the fair treatment of class distributions. Moreover, to generally model the context of structural dependency, a new conditional structural constraint is introduced to impose the consistency of predicted segmentation. Thanks to the proposed Conditional Structure Network, the self-attention mechanism has sufficiently modeled the structural information of segmentation.",
  "Moreover, to generally model the context of structural dependency, a new conditional structural constraint is introduced to impose the consistency of predicted segmentation. Thanks to the proposed Conditional Structure Network, the self-attention mechanism has sufficiently modeled the structural information of segmentation. Through the ablation studies, the proposed method has shown the performance improvement of the segmentation models and promoted fairness in the model predictions. The experimental results on the two standard benchmarks, i.e., SYNTHIA $\\rightarrow$ Cityscapes and GTA5 $\\rightarrow$ Cityscapes, have shown that our method achieved State-of-the-Art (SOTA) performance11The implementation of FREDOM is available at https://github.com/uark-cviu/FREDOM\nAuthors: Thanh-Dat Truong, Ngan T. H. Le, B. Raj, J. Cothren, Khoa Luu\nVenue: Computer Vision and Pattern Recognition\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes a novel Fairness Domain Adaptation (FREDOM) approach to semantic scene segmentation, where a new adaptation framework will be introduced based on the fair treatment of class distributions to generally model the context of structural dependency.'}",
  "Faculty Name: bhiksha raj\nMetadata:\nPaperid: 0a8d38686b18f28aae1222529e6b9e8a60cab1c2\nTitle: UTOPIA: Unconstrained Tracking Objects without Preliminary Examination via Cross-Domain Adaptation\nYear: 2023\nAbstract: Multiple Object Tracking (MOT) aims to find bounding boxes and identities of targeted objects in consecutive video frames. While fully-supervised MOT methods have achieved high accuracy on existing datasets, they cannot generalize well on a newly obtained dataset or a new unseen domain. In this work, we first address the MOT problem from the cross-domain point of view, imitating the process of new data acquisition in practice. Then, a new cross-domain MOT adaptation from existing datasets is proposed without any pre-defined human knowledge in understanding and modeling objects. It can also learn and update itself from the target data feedback. The intensive experiments are designed on four challenging settings, including MOTSynth to MOT17, MOT17 to MOT20, MOT17 to VisDrone, and MOT17 to DanceTrack. We then prove the adaptability of the proposed self-supervised learning strategy.",
  "The intensive experiments are designed on four challenging settings, including MOTSynth to MOT17, MOT17 to MOT20, MOT17 to VisDrone, and MOT17 to DanceTrack. We then prove the adaptability of the proposed self-supervised learning strategy. The experiments also show superior performance on tracking metrics MOTA and IDF1, compared to fully supervised, unsupervised, and self-supervised state-of-the-art methods.\nAuthors: Pha Nguyen, Kha Gia Quach, J. Gauch, S. Khan, B. Raj, Khoa Luu\nVenue: arXiv.org\nTldr: {'model': 'tldr@v2.0.0', 'text': 'A new cross-domain MOT adaptation from existing datasets is proposed without any pre-defined human knowledge in understanding and modeling objects, and it can also learn and update itself from the target data feedback.'}",
  "Faculty Name: bhiksha raj\nMetadata:\nPaperid: 100da279ee981960884a12dfc5a0697c24ed315a\nTitle: SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning\nYear: 2023\nAbstract: The critical challenge of Semi-Supervised Learning (SSL) is how to effectively leverage the limited labeled data and massive unlabeled data to improve the model's generalization performance. In this paper, we first revisit the popular pseudo-labeling methods via a unified sample weighting formulation and demonstrate the inherent quantity-quality trade-off problem of pseudo-labeling with thresholding, which may prohibit learning. To this end, we propose SoftMatch to overcome the trade-off by maintaining both high quantity and high quality of pseudo-labels during training, effectively exploiting the unlabeled data. We derive a truncated Gaussian function to weight samples based on their confidence, which can be viewed as a soft version of the confidence threshold. We further enhance the utilization of weakly-learned classes by proposing a uniform alignment approach. In experiments, SoftMatch shows substantial improvements across a wide variety of benchmarks, including image, text, and imbalanced classification.",
  "We further enhance the utilization of weakly-learned classes by proposing a uniform alignment approach. In experiments, SoftMatch shows substantial improvements across a wide variety of benchmarks, including image, text, and imbalanced classification.\nAuthors: Hao Chen, R. Tao, Yue Fan, Yidong Wang, Jindong Wang, B. Schiele, Xingxu Xie, B. Raj, M. Savvides\nVenue: International Conference on Learning Representations\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This paper revisits the popular pseudo-labeling methods via a unified sample weighting formulation and demonstrates the inherent quantity-quality trade-off problem of pseudo-labels with thresholding, which may prohibit learning.'}",
  "Faculty Name: bhiksha raj\nMetadata:\nPaperid: 11c50900f50036fb3247be7c83849a8774a4ba60\nTitle: Fixed Inter-Neuron Covariability Induces Adversarial Robustness\nYear: 2023\nAbstract: The vulnerability to adversarial perturbations is a major flaw of Deep Neural Networks (DNNs) that raises question about their reliability when in real-world scenarios. On the other hand, human perception, which DNNs are supposed to emulate, is highly robust to such perturbations, indicating that there may be certain features of the human perception that make it robust but are not represented in the current class of DNNs. One such feature is that the activity of biological neurons is correlated and the structure of this correlation tends to be rather rigid over long spans of times, even if it hampers performance and learning.",
  "One such feature is that the activity of biological neurons is correlated and the structure of this correlation tends to be rather rigid over long spans of times, even if it hampers performance and learning. We hypothesize that integrating such constraints on the activations of a DNN would improve its adversarial robustness, and, to test this hypothesis, we have developed the Self-Consistent Activation (SCA) layer, which comprises of neurons whose activations are consistent with each other, as they conform to a fixed, but learned, covariability pattern.",
  "When evaluated on image and sound recognition tasks, the models with a SCA layer achieved high accuracy, and exhibited significantly greater robustness than multi-layer perceptron models to state-of-the-art Auto-PGD adversarial attacks \\textit{without being trained on adversarially perturbed data\nAuthors: Muhammad A Shah, B. Raj\nVenue: arXiv.org\nTldr: {'model': 'tldr@v2.0.0', 'text': 'The SCA layer is developed, which comprises of neurons whose activations are consistent with each other, as they conform to a fixed, but learned, covariability pattern, and exhibited significantly greater robustness than multi-layer perceptron models to state-of-the-art Auto-PGD adversarial attacks.'}",
  "Faculty Name: bhiksha raj\nMetadata:\nPaperid: 1de2dcb5de694920f50f000a3795eb0ca54d57ab\nTitle: LoFT: Local Proxy Fine-tuning For Improving Transferability Of Adversarial Attacks Against Large Language Model\nYear: 2023\nAbstract: It has been shown that Large Language Model (LLM) alignments can be circumvented by appending specially crafted attack suffixes with harmful queries to elicit harmful responses. To conduct attacks against private target models whose characterization is unknown, public models can be used as proxies to fashion the attack, with successful attacks being transferred from public proxies to private target models. The success rate of attack depends on how closely the proxy model approximates the private model. We hypothesize that for attacks to be transferrable, it is sufficient if the proxy can approximate the target model in the neighborhood of the harmful query. Therefore, in this paper, we propose \\emph{Local Fine-Tuning (LoFT)}, \\textit{i.e.}, fine-tuning proxy models on similar queries that lie in the lexico-semantic neighborhood of harmful queries to decrease the divergence between the proxy and target models.",
  "Therefore, in this paper, we propose \\emph{Local Fine-Tuning (LoFT)}, \\textit{i.e.}, fine-tuning proxy models on similar queries that lie in the lexico-semantic neighborhood of harmful queries to decrease the divergence between the proxy and target models. First, we demonstrate three approaches to prompt private target models to obtain similar queries given harmful queries. Next, we obtain data for local fine-tuning by eliciting responses from target models for the generated similar queries. Then, we optimize attack suffixes to generate attack prompts and evaluate the impact of our local fine-tuning on the attack's success rate. Experiments show that local fine-tuning of proxy models improves attack transferability and increases attack success rate by $39\\%$, $7\\%$, and $0.5\\%$ (absolute) on target models ChatGPT, GPT-4, and Claude respectively.",
  "Experiments show that local fine-tuning of proxy models improves attack transferability and increases attack success rate by $39\\%$, $7\\%$, and $0.5\\%$ (absolute) on target models ChatGPT, GPT-4, and Claude respectively.\nAuthors: Muhammad Ahmed Shah, Roshan Sharma, Hira Dhamyal, R. Olivier, Ankit Shah, Dareen Alharthi, Hazim T Bukhari, Massa Baali, Soham Deshmukh, Michael Kuhlmann, Bhiksha Raj, Rita Singh\nVenue: arXiv.org\nTldr: {'model': 'tldr@v2.0.0', 'text': 'Local fine-tuning of proxy models improves attack transferability and increases attack success rate by $39, $7, and $0.5$ (absolute) on target models ChatGPT, GPT-4, and Claude respectively.'}",
  "Faculty Name: bhiksha raj\nMetadata:\nPaperid: 22c9eb4868c5cabb26d132e0a160b9a093579f08\nTitle: Understanding political polarization using language models: A dataset and method\nYear: 2023\nAbstract: Our paper aims to analyze political polarization in US political system using language models, and thereby help candidates make an informed decision. The availability of this information will help voters understand their candidates' views on the economy, healthcare, education, and other social issues. Our main contributions are a dataset extracted from Wikipedia that spans the past 120 years and a language model\u2010based method that helps analyze how polarized a candidate is. Our data are divided into two parts, background information and political information about a candidate, since our hypothesis is that the political views of a candidate should be based on reason and be independent of factors such as birthplace, alma mater, and so forth. We further split this data into four phases chronologically, to help understand if and how the polarization amongst candidates changes. This data has been cleaned to remove biases.",
  "We further split this data into four phases chronologically, to help understand if and how the polarization amongst candidates changes. This data has been cleaned to remove biases. To understand the polarization, we begin by showing results from some classical language models in Word2Vec and Doc2Vec. And then use more powerful techniques like the Longformer, a transformer\u2010based encoder, to assimilate more information and find the nearest neighbors of each candidate based on their political view and their background. The code and data for the project will be available here: \u201chttps://github.com/samirangode/Understanding_Polarization\u201d\nAuthors: Samiran Gode, Supreeth Bare, B. Raj, H. Yoo\nVenue: The AI Magazine\nTldr: {'model': 'tldr@v2.0.0', 'text': 'The main contributions are a dataset extracted from Wikipedia that spans the past 120 years and a language model\u2010based method that helps analyze how polarized a candidate is.'}",
  "Faculty Name: bhiksha raj\nMetadata:\nPaperid: 255bad49d29202e2d255926ab0983c125dcce835\nTitle: Evaluating Speech Synthesis by Training Recognizers on Synthetic Speech\nYear: 2023\nAbstract: Modern speech synthesis systems have improved significantly, with synthetic speech being indistinguishable from real speech. However, efficient and holistic evaluation of synthetic speech still remains a significant challenge. Human evaluation using Mean Opinion Score (MOS) is ideal, but inefficient due to high costs. Therefore, researchers have developed auxiliary automatic metrics like Word Error Rate (WER) to measure intelligibility. Prior works focus on evaluating synthetic speech based on pre-trained speech recognition models, however, this can be limiting since this approach primarily measures speech intelligibility. In this paper, we propose an evaluation technique involving the training of an ASR model on synthetic speech and assessing its performance on real speech. Our main assumption is that by training the ASR model on the synthetic speech, the WER on real speech reflects the similarity between distributions, a broader assessment of synthetic speech quality beyond intelligibility.",
  "Our main assumption is that by training the ASR model on the synthetic speech, the WER on real speech reflects the similarity between distributions, a broader assessment of synthetic speech quality beyond intelligibility. Our proposed metric demonstrates a strong correlation with both MOS naturalness and MOS intelligibility when compared to SpeechLMScore and MOSNet on three recent Text-to-Speech (TTS) systems: MQTTS, StyleTTS, and YourTTS.\nAuthors: Dareen Alharthi, Roshan Sharma, Hira Dhamyal, Soumi Maiti, Bhiksha Raj, Rita Singh\nVenue: arXiv.org\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes an evaluation technique involving the training of an ASR model on synthetic speech and assessing its performance on real speech, and demonstrates a strong correlation with both MOS naturalness and MOS intelligibility when compared to SpeechLMScore and MosNet on three recent Text-to-Speech systems.'}",
  "Faculty Name: bhiksha raj\nMetadata:\nPaperid: 2a8f592c31d8de9906183b081095b9842025f792\nTitle: Rethinking Audiovisual Segmentation with Semantic Quantization and Decomposition\nYear: 2023\nAbstract: Audiovisual segmentation (AVS) is a challenging task that aims to segment visual objects in videos based on their associated acoustic cues. With multiple sound sources involved, establishing robust correspondences between audio and visual contents poses unique challenges due to its (1) intricate entanglement across sound sources and (2) frequent shift among sound events. Assuming sound events occur independently, the multi-source semantic space (which encompasses all possible semantic categories) can be represented as the Cartesian product of single-source sub-spaces. This motivates us to decompose the multi-source audio semantics into single-source semantics, enabling more effective interaction with visual content. Specifically, we propose a semantic decomposition method based on product quantization, where the multi-source semantics can be decomposed and represented by several quantized single-source semantics.",
  "This motivates us to decompose the multi-source audio semantics into single-source semantics, enabling more effective interaction with visual content. Specifically, we propose a semantic decomposition method based on product quantization, where the multi-source semantics can be decomposed and represented by several quantized single-source semantics. Furthermore, we introduce a global-to-local quantization mechanism, which distills knowledge from stable global (clip-level) features into local (frame-level) ones, to handle the constant shift of audio semantics. Extensive experiments demonstrate that semantically quantized and decomposed audio representation significantly improves AVS performance, e.g., +21.2% mIoU on the most challenging AVS-Semantic benchmark.\nAuthors: Xiang Li, Jinglu Wang, Xiaohao Xu, Xiulian Peng, Rita Singh, Yan Lu, Bhiksha Raj\nVenue: arXiv.org\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This work proposes a semantic decomposition method based on product quantization, where the multi-source semantics can be decomposed and represented by several quantized single-source semantics, enabling more effective interaction with visual content.'}",
  "Faculty Name: bhiksha raj\nMetadata:\nPaperid: 35a8802facb4441787017ac5c630a8fa0f2413bd\nTitle: Prolonged school closure during the pandemic time in successive waves of COVID-19- vulnerability of children to sexual abuses \u2013 A case study in Tamil Nadu, India\nYear: 2023\nAbstract: None\nAuthors: Kandaswamy Paramasivan, B. Raj, Nandan Sudarasanam, R. Subburaj\nVenue: Heliyon\nTldr: {'model': 'tldr@v2.0.0', 'text': 'Considering that the median delay in filing CSA complaints was above 30 days in the mild and post-intervention periods, the upsurge of cases in the more relaxed phases indicates increased occurrences of CSA during strict lockdowns.'}",
  "Faculty Name: bhiksha raj\nMetadata:\nPaperid: 37e8e07d3ecfa43a1e64d48202c73f597e6f9fee\nTitle: Towards Noise-Tolerant Speech-Referring Video Object Segmentation: Bridging Speech and Text\nYear: 2023\nAbstract: None\nAuthors: Xiang Li, Jinglu Wang, Xiaohao Xu, Muqiao Yang, Fan Yang, Yizhou Zhao, Rita Singh, Bhiksha Raj\nVenue: Conference on Empirical Methods in Natural Language Processing\nTldr: None",
  "Faculty Name: bhiksha raj\nMetadata:\nPaperid: 3bd320ddb25886417ae90011b00f13f5d558097b\nTitle: BASS: Block-wise Adaptation for Speech Summarization\nYear: 2023\nAbstract: End-to-end speech summarization has been shown to improve performance over cascade baselines. However, such models are difficult to train on very large inputs (dozens of minutes or hours) owing to compute restrictions and are hence trained with truncated model inputs. Truncation leads to poorer models, and a solution to this problem rests in block-wise modeling, i.e., processing a portion of the input frames at a time. In this paper, we develop a method that allows one to train summarization models on very long sequences in an incremental manner. Speech summarization is realized as a streaming process, where hypothesis summaries are updated every block based on new acoustic information. We devise and test strategies to pass semantic context across the blocks. Experiments on the How2 dataset demonstrate that the proposed block-wise training method improves by 3 points absolute on ROUGE-L over a truncated input baseline.",
  "We devise and test strategies to pass semantic context across the blocks. Experiments on the How2 dataset demonstrate that the proposed block-wise training method improves by 3 points absolute on ROUGE-L over a truncated input baseline.\nAuthors: Roshan Sharma, Kenneth Zheng, Siddhant Arora, Shinji Watanabe, Rita Singh, B. Raj\nVenue: Interspeech\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This paper develops a method that allows one to train summarization models on very long sequences in an incremental manner and devise and test strategies to pass semantic context across the blocks.'}",
  "Faculty Name: bhiksha raj\nMetadata:\nPaperid: 4628f0c28a8ed231168d1a27a93ddb938da4102d\nTitle: Psychoacoustic Challenges Of Speech Enhancement On VoIP Platforms\nYear: 2023\nAbstract: Within the ambit of VoIP (Voice over Internet Protocol) telecommunications, the complexities introduced by acoustic transformations merit rigorous analysis. This research, rooted in the exploration of proprietary sender-side denoising effects, meticulously evaluates platforms such as Google Meets and Zoom. The study draws upon the Deep Noise Suppression (DNS) 2020 dataset, ensuring a structured examination tailored to various denoising settings and receiver interfaces. A methodological novelty is introduced via the Oaxaca decomposition, traditionally an econometric tool, repurposed herein to analyze acoustic-phonetic perturbations within VoIP systems. To further ground the implications of these transformations, psychoacoustic metrics, specifically PESQ and STOI, were harnessed to furnish a comprehensive understanding of speech alterations. Cumulatively, the insights garnered underscore the intricate landscape of VoIP-influenced acoustic dynamics.",
  "To further ground the implications of these transformations, psychoacoustic metrics, specifically PESQ and STOI, were harnessed to furnish a comprehensive understanding of speech alterations. Cumulatively, the insights garnered underscore the intricate landscape of VoIP-influenced acoustic dynamics. In addition to the primary findings, a multitude of metrics are reported, extending the research purview. Moreover, out-of-domain benchmarking for both time and time-frequency domain speech enhancement models is included, thereby enhancing the depth and applicability of this inquiry. Repository: github.com/deepology/VoIP-DNS-Challenge\nAuthors: Joseph Konan, Ojas Bhargave, Shikhar Agnihotri, Shuo Han, YUNYANG ZENG, Ankit Shah, Bhiksha Raj\nVenue: arXiv.org\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This research, rooted in the exploration of proprietary sender-side denoising effects, meticulously evaluates platforms such as Google Meets and Zoom, and draws upon the Deep Noise Suppression (DNS) 2020 dataset, ensuring a structured examination tailored to various denoised settings and receiver interfaces.'}",
  "Faculty Name: bhiksha raj\nMetadata:\nPaperid: 4e6a004e4f9a3b489004a2efb5b25b0bcd0f48b5\nTitle: Understanding Political Polarisation using Language Models: A dataset and method\nYear: 2023\nAbstract: Our paper aims to analyze political polarization in US political system using Language Models, and thereby help candidates make an informed decision. The availability of this information will help voters understand their candidates views on the economy, healthcare, education and other social issues. Our main contributions are a dataset extracted from Wikipedia that spans the past 120 years and a Language model based method that helps analyze how polarized a candidate is. Our data is divided into 2 parts, background information and political information about a candidate, since our hypothesis is that the political views of a candidate should be based on reason and be independent of factors such as birthplace, alma mater, etc. We further split this data into 4 phases chronologically, to help understand if and how the polarization amongst candidates changes. This data has been cleaned to remove biases.",
  "We further split this data into 4 phases chronologically, to help understand if and how the polarization amongst candidates changes. This data has been cleaned to remove biases. To understand the polarization we begin by showing results from some classical language models in Word2Vec and Doc2Vec. And then use more powerful techniques like the Longformer, a transformer based encoder, to assimilate more information and find the nearest neighbors of each candidate based on their political view and their background.\nAuthors: Samiran Gode, Supreeth Bare, B. Raj, H. Yoo\nVenue: arXiv.org\nTldr: {'model': 'tldr@v2.0.0', 'text': 'A dataset extracted from Wikipedia that spans the past 120 years and a Language model based method that helps analyze how polarized a candidate is are used to understand the polarization.'}",
  "Faculty Name: bhiksha raj\nMetadata:\nPaperid: 593a603354c09d151440ae044de1d80324a2ab01\nTitle: An Approach to Ontological Learning from Weak Labels\nYear: 2023\nAbstract: Ontologies encompass a formal representation of knowledge through the definition of concepts or properties of a domain, and the relationships between those concepts. In this work, we seek to investigate whether using this ontological information will improve learning from weakly labeled data, which are easier to collect since it requires only the presence or absence of an event to be known. We use the AudioSet ontology and dataset, which contains audio clips weakly labeled with the ontology concepts and the ontology providing the \"Is A\" relations between the concepts. We first re-implemented the model proposed by [1] with modifications to fit the multi-label scenario and then expand on that idea by using a Graph Convolutional Network (GCN) to model the ontology information to learn the concepts.",
  "We first re-implemented the model proposed by [1] with modifications to fit the multi-label scenario and then expand on that idea by using a Graph Convolutional Network (GCN) to model the ontology information to learn the concepts. We find that the baseline Twin Neural Network (TNN) does not perform better by incorporating ontology information in the weak and multi-label scenario, but that the GCN does capture the ontology knowledge better for weak, multi-labeled data. We also investigate how different modules can tolerate noises introduced from weak labels and better incorporate ontology information. Our best TNN-GCN model achieves mAP=0.45 and AUC=0.87 for lower-level concepts and mAP=0.72 and AUC=0.86 for higher-level concepts, which is an improvement over the baseline TNN but about the same as our models that do not use ontology information.",
  "Authors: Ankit Shah, Larry Tang, Po Hao Chou, Yilun Zheng, Ziqian Ge, B. Raj\nVenue: IEEE International Conference on Acoustics, Speech, and Signal Processing\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This work re-implements the model proposed by [1] with modifications to fit the multi-label scenario and expands on that idea by using a Graph Convolutional Network (GCN) to model the ontology information to learn the concepts.'}",
  "Faculty Name: bhiksha raj\nMetadata:\nPaperid: 5a3307b2e64bbcaff1202e261b8a83f7d03418a8\nTitle: Rethinking Voice-Face Correlation: A Geometry View\nYear: 2023\nAbstract: Previous works on voice-face matching and voice-guided face synthesis demonstrate strong correlations between voice and face, but mainly rely on coarse semantic cues such as gender, age, and emotion. In this paper, we aim to investigate the capability of reconstructing the 3D facial shape from voice from a geometry perspective without any semantic information. We propose a voice-anthropometric measurement (AM)-face paradigm, which identifies predictable facial AMs from the voice and uses them to guide 3D face reconstruction. By leveraging AMs as a proxy to link the voice and face geometry, we can eliminate the influence of unpredictable AMs and make the face geometry tractable. Our approach is evaluated on our proposed dataset with ground-truth 3D face scans and corresponding voice recordings, and we find significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium.",
  "Our approach is evaluated on our proposed dataset with ground-truth 3D face scans and corresponding voice recordings, and we find significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium. Our work offers a new perspective on voice-face correlation and can serve as a good empirical study for anthropometry science.\nAuthors: Xiang Li, Yandong Wen, Muqiao Yang, Jinglu Wang, Rita Singh, B. Raj\nVenue: ACM Multimedia\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This work proposes a voice-anthropometric measurement (AM)-face paradigm, which identifies predictable facial AMs from the voice and uses them to guide 3D face reconstruction and finds significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium.'}",
  "Faculty Name: bhiksha raj\nMetadata:\nPaperid: 611f9ee6eef0936462cd78f371798d0699951c59\nTitle: Paaploss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement\nYear: 2023\nAbstract: Despite rapid advancement in recent years, current speech enhancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics. We identify temporal acoustic parameters \u2013 such as spectral tilt, spectral flux, shimmer, etc. \u2013 that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-specific weights for each feature, as the acoustic parameters are known to show different behavior in different phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement workflows in both time-domain and time-frequency domain, as measured by standard evaluation metrics.",
  "We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement workflows in both time-domain and time-frequency domain, as measured by standard evaluation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement.\nAuthors: Muqiao Yang, Joseph Konan, David Bick, YUNYANG ZENG, Shuo Han, Anurag Kumar, Shinji Watanabe, B. Raj\nVenue: IEEE International Conference on Acoustics, Speech, and Signal Processing\nTldr: {'model': 'tldr@v2.0.0', 'text': 'A learning objective is proposed that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics, and a neural network estimator is developed that can accurately predict their time-series values across an utterance.'}",
  "Faculty Name: bhiksha raj\nMetadata:\nPaperid: 6ca2caa4edecc5f08949756266db241ef5e51fc1\nTitle: uSee: Unified Speech Enhancement and Editing with Conditional Diffusion Models\nYear: 2023\nAbstract: Speech enhancement aims to improve the quality of speech signals in terms of quality and intelligibility, and speech editing refers to the process of editing the speech according to specific user needs. In this paper, we propose a Unified Speech Enhancement and Editing (uSee) model with conditional diffusion models to handle various tasks at the same time in a generative manner. Specifically, by providing multiple types of conditions including self-supervised learning embeddings and proper text prompts to the score-based diffusion model, we can enable controllable generation of the unified speech enhancement and editing model to perform corresponding actions on the source speech. Our experiments show that our proposed uSee model can achieve superior performance in both speech denoising and dereverberation compared to other related generative speech enhancement models, and can perform speech editing given desired environmental sound text description, signal-to-noise ratios (SNR), and room impulse responses (RIR).",
  "Our experiments show that our proposed uSee model can achieve superior performance in both speech denoising and dereverberation compared to other related generative speech enhancement models, and can perform speech editing given desired environmental sound text description, signal-to-noise ratios (SNR), and room impulse responses (RIR). Demos of the generated speech are available at https://muqiaoy.github.io/usee.\nAuthors: Muqiao Yang, Chunlei Zhang, Yong Xu, Zhongweiyang Xu, Heming Wang, Bhiksha Raj, Dong Yu\nVenue: arXiv.org\nTldr: {'model': 'tldr@v2.0.0', 'text': 'The proposed uSee model can achieve superior performance in both speech denoising and dereverberation compared to other related generative speech enhancement models, and can perform speech editing given desired environmental sound text description, signal-to-noise ratios (SNR), and room impulse responses (RIR).'}",
  "Faculty Name: bhiksha raj\nMetadata:\nPaperid: 7333be530df311b3148e9857ce9f481975cf0a9b\nTitle: Improving Perceptual Quality, Intelligibility, and Acoustics on VoIP Platforms\nYear: 2023\nAbstract: In this paper, we present a method for fine-tuning models trained on the Deep Noise Suppression (DNS) 2020 Challenge to improve their performance on Voice over Internet Protocol (VoIP) applications. Our approach involves adapting the DNS 2020 models to the specific acoustic characteristics of VoIP communications, which includes distortion and artifacts caused by compression, transmission, and platform-specific processing. To this end, we propose a multi-task learning framework for VoIP-DNS that jointly optimizes noise suppression and VoIP-specific acoustics for speech enhancement. We evaluate our approach on a diverse VoIP scenarios and show that it outperforms both industry performance and state-of-the-art methods for speech enhancement on VoIP applications.",
  "We evaluate our approach on a diverse VoIP scenarios and show that it outperforms both industry performance and state-of-the-art methods for speech enhancement on VoIP applications. Our results demonstrate the potential of models trained on DNS-2020 to be improved and tailored to different VoIP platforms using VoIP-DNS, whose findings have important applications in areas such as speech recognition, voice assistants, and telecommunication.\nAuthors: Joseph Konan, Ojas Bhargave, Shikhar Agnihotri, Hojeong Lee, Ankit Shah, Shuo Han, YUNYANG ZENG, Amanda Shu, Haohui Liu, Xuankai Chang, Hamza Khalid, Minseon Gwak, Kawon Lee, Minjeong Kim, B. Raj\nVenue: arXiv.org\nTldr: {'model': 'tldr@v2.0.0', 'text': 'A multi-task learning framework forVoIP-DNS that jointly optimizes noise suppression and VoIP-specific acoustics for speech enhancement and outperforms both industry performance and state-of-the-art methods for speech Enhancement on VoIP applications is proposed.'}",
  "Faculty Name: bhiksha raj\nMetadata:\nPaperid: 740488982dee323d559f2dae70b1f4b3aa5f7171\nTitle: Approach to Learning Generalized Audio Representation Through Batch Embedding Covariance Regularization and Constant-Q Transforms\nYear: 2023\nAbstract: General-purpose embedding is highly desirable for few-shot even zero-shot learning in many application scenarios, including audio tasks. In order to understand representations better, we conducted a thorough error analysis and visualization of HEAR 2021 submission results. Inspired by the analysis, this work experiments with different front-end audio preprocessing methods, including Constant-Q Transform (CQT) and Short-time Fourier transform (STFT), and proposes a Batch Embedding Covariance Regularization (BECR) term to uncover a more holistic simulation of the frequency information received by the human auditory system. We tested the models on the suite of HEAR 2021 tasks, which encompass a broad category of tasks.",
  "We tested the models on the suite of HEAR 2021 tasks, which encompass a broad category of tasks. Preliminary results show (1) the proposed BECR can incur a more dispersed embedding on the test set, (2) BECR improves the PaSST model without extra computation complexity, and (3) STFT preprocessing outperforms CQT in all tasks we tested. Github:https://github.com/ankitshah009/general_audio_embedding_hear_2021\nAuthors: Ankit Shah, Shuyi Chen, Kejun Zhou, Yue Chen, B. Raj\nVenue: arXiv.org\nTldr: {'model': 'tldr@v2.0.0', 'text': 'Experiments with different front-end audio preprocessing methods are experiments, and a Batch Embedding Covariance Regularization (BECR) term is proposed to uncover a more holistic simulation of the frequency information received by the human auditory system.'}",
  "Faculty Name: bhiksha raj\nMetadata:\nPaperid: 74664618ad3b44eb191ba96fdff5b93f27a29ced\nTitle: Training on Foveated Images Improves Robustness to Adversarial Attacks\nYear: 2023\nAbstract: Deep neural networks (DNNs) have been shown to be vulnerable to adversarial attacks -- subtle, perceptually indistinguishable perturbations of inputs that change the response of the model. In the context of vision, we hypothesize that an important contributor to the robustness of human visual perception is constant exposure to low-fidelity visual stimuli in our peripheral vision. To investigate this hypothesis, we develop \\RBlur, an image transform that simulates the loss in fidelity of peripheral vision by blurring the image and reducing its color saturation based on the distance from a given fixation point. We show that compared to DNNs trained on the original images, DNNs trained on images transformed by \\RBlur are substantially more robust to adversarial attacks, as well as other, non-adversarial, corruptions, achieving up to 25\\% higher accuracy on perturbed data.",
  "We show that compared to DNNs trained on the original images, DNNs trained on images transformed by \\RBlur are substantially more robust to adversarial attacks, as well as other, non-adversarial, corruptions, achieving up to 25\\% higher accuracy on perturbed data.\nAuthors: Muhammad A Shah, B. Raj\nVenue: Neural Information Processing Systems\nTldr: {'model': 'tldr@v2.0.0', 'text': 'DNNs trained on images transformed by \\\\RBlur are substantially more robust to adversarial attacks, as well as other, non-adversarial, corruptions, achieving up to 25\\\\% higher accuracy on perturbed data.'}",
  "Faculty Name: bhiksha raj\nMetadata:\nPaperid: 7a25aa397ae2a7f82df87a936ce6ff7f03b7ac4c\nTitle: Token Prediction as Implicit Classification to Identify LLM-Generated Text\nYear: 2023\nAbstract: This paper introduces a novel approach for identifying the possible large language models (LLMs) involved in text generation. Instead of adding an additional classification layer to a base LM, we reframe the classification task as a next-token prediction task and directly fine-tune the base LM to perform it. We utilize the Text-to-Text Transfer Transformer (T5) model as the backbone for our experiments. We compared our approach to the more direct approach of utilizing hidden states for classification. Evaluation shows the exceptional performance of our method in the text classification task, highlighting its simplicity and efficiency. Furthermore, interpretability studies on the features extracted by our model reveal its ability to differentiate distinctive writing styles among various LLMs even in the absence of an explicit classifier.",
  "Evaluation shows the exceptional performance of our method in the text classification task, highlighting its simplicity and efficiency. Furthermore, interpretability studies on the features extracted by our model reveal its ability to differentiate distinctive writing styles among various LLMs even in the absence of an explicit classifier. We also collected a dataset named OpenLLMText, containing approximately 340k text samples from human and LLMs, including GPT3.5, PaLM, LLaMA, and GPT2.\nAuthors: Yutian Chen, Hao Kang, Vivian Zhai, Liangze Li, Rita Singh, Bhiksha Raj\nVenue: Conference on Empirical Methods in Natural Language Processing\nTldr: {'model': 'tldr@v2.0.0', 'text': 'Evaluation shows the exceptional performance of the method in the text classification task, highlighting its simplicity and efficiency, and interpretability studies on the features extracted by the model reveal its ability to differentiate distinctive writing styles among various LLMs even in the absence of an explicit classifier.'}",
  "Faculty Name: bhiksha raj\nMetadata:\nPaperid: 8665c864d71df1e918d2010778fc06712f4e5550\nTitle: Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations\nYear: 2023\nAbstract: Learning with reduced labeling standards, such as noisy label, partial label, and multiple label candidates, which we generically refer to as \\textit{imprecise} labels, is a commonplace challenge in machine learning tasks. Previous methods tend to propose specific designs for every emerging imprecise label configuration, which is usually unsustainable when multiple configurations of imprecision coexist. In this paper, we introduce imprecise label learning (ILL), a framework for the unification of learning with various imprecise label configurations. ILL leverages expectation-maximization (EM) for modeling the imprecise label information, treating the precise labels as latent variables.Instead of approximating the correct labels for training, it considers the entire distribution of all possible labeling entailed by the imprecise information.",
  "ILL leverages expectation-maximization (EM) for modeling the imprecise label information, treating the precise labels as latent variables.Instead of approximating the correct labels for training, it considers the entire distribution of all possible labeling entailed by the imprecise information. We demonstrate that ILL can seamlessly adapt to partial label learning, semi-supervised learning, noisy label learning, and, more importantly, a mixture of these settings. Notably, ILL surpasses the existing specified techniques for handling imprecise labels, marking the first unified framework with robust and effective performance across various challenging settings. We hope our work will inspire further research on this topic, unleashing the full potential of ILL in wider scenarios where precise labels are expensive and complicated to obtain.",
  "We hope our work will inspire further research on this topic, unleashing the full potential of ILL in wider scenarios where precise labels are expensive and complicated to obtain.\nAuthors: Hao Chen, Ankit Shah, Jindong Wang, R. Tao, Yidong Wang, Xingxu Xie, Masashi Sugiyama, Rita Singh, B. Raj\nVenue: arXiv.org\nTldr: {'model': 'tldr@v2.0.0', 'text': 'Imprecise label learning (ILL) is introduced, a framework for the unification of learning with various imprecise label configurations, marking the first unified framework with robust and effective performance across various challenging settings.'}",
  "Faculty Name: bhiksha raj\nMetadata:\nPaperid: 9f9cdced51568c623ec447bf0ea9709b383b5a0f\nTitle: Understanding and Mitigating the Label Noise in Pre-training on Downstream Tasks\nYear: 2023\nAbstract: Pre-training on large-scale datasets and then fine-tuning on downstream tasks have become a standard practice in deep learning. However, pre-training data often contain label noise that may adversely affect the generalization of the model. This paper aims to understand the nature of noise in pre-training datasets and to mitigate its impact on downstream tasks. More specifically, through extensive experiments of supervised pre-training models on synthetic noisy ImageNet-1K and YFCC15M datasets, we demonstrate that while slight noise in pre-training can benefit in-domain (ID) transfer performance, where the training and testing data share the same distribution, it always deteriorates out-of-domain (OOD) performance, where training and testing data distribution are different. We empirically verify that the reason behind is noise in pre-training shapes the feature space differently.",
  "We empirically verify that the reason behind is noise in pre-training shapes the feature space differently. We then propose a lightweight black-box tuning method (NMTune) to affine the feature space to mitigate the malignant effect of noise and improve generalization on both ID and OOD tasks, considering one may not be able to fully fine-tune or even access the pre-trained models. We conduct practical experiments on popular vision and language models that are pre-trained on noisy data for evaluation of our approach. Our analysis and results show the importance of this interesting and novel research direction, which we term Noisy Model Learning.\nAuthors: Hao Chen, Jindong Wang, Ankit Shah, Ran Tao, Hongxin Wei, Xingxu Xie, Masashi Sugiyama, Bhiksha Raj\nVenue: arXiv.org\nTldr: {'model': 'tldr@v2.0.0', 'text': 'A lightweight black-box tuning method (NMTune) is proposed to affine the feature space to mitigate the malignant effect of noise and improve generalization on both ID and OOD tasks, considering one may not be able to fully fine-tune or even access the pre-trained models.'}",
  "Faculty Name: bhiksha raj\nMetadata:\nPaperid: a6e3a10a6286967413e3406374bbeea533640030\nTitle: The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features\nYear: 2023\nAbstract: This work unveils the enigmatic link between phonemes and facial features. Traditional studies on voice-face correlations typically involve using a long period of voice input, including generating face images from voices and reconstructing 3D face meshes from voices. However, in situations like voice-based crimes, the available voice evidence may be short and limited. Additionally, from a physiological perspective, each segment of speech -- phoneme -- corresponds to different types of airflow and movements in the face. Therefore, it is advantageous to discover the hidden link between phonemes and face attributes. In this paper, we propose an analysis pipeline to help us explore the voice-face relationship in a fine-grained manner, i.e., phonemes v.s. facial anthropometric measurements (AM). We build an estimator for each phoneme-AM pair and evaluate the correlation through hypothesis testing.",
  "In this paper, we propose an analysis pipeline to help us explore the voice-face relationship in a fine-grained manner, i.e., phonemes v.s. facial anthropometric measurements (AM). We build an estimator for each phoneme-AM pair and evaluate the correlation through hypothesis testing. Our results indicate that AMs are more predictable from vowels compared to consonants, particularly with plosives. Additionally, we observe that if a specific AM exhibits more movement during phoneme pronunciation, it is more predictable. Our findings support those in physiology regarding correlation and lay the groundwork for future research on speech-face multimodal learning.\nAuthors: Liao Qu, X. Zou, Xiang Li, Yandong Wen, Rita Singh, B. Raj\nVenue: Interspeech\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This work proposes an analysis pipeline to help explore the voice-face relationship in a fine-grained manner, i.s. phonemes v. facial anthropometric measurements (AM), and indicates that AMs are more predictable from vowels compared to consonants, particularly with plosives.'}",
  "Faculty Name: bhiksha raj\nMetadata:\nPaperid: ac856b6b7b3f32fb34320b7170526d3ab15ba5f3\nTitle: Fairness Continual Learning Approach to Semantic Scene Understanding in Open-World Environments\nYear: 2023\nAbstract: Continual semantic segmentation aims to learn new classes while maintaining the information from the previous classes. Although prior studies have shown impressive progress in recent years, the fairness concern in the continual semantic segmentation needs to be better addressed. Meanwhile, fairness is one of the most vital factors in deploying the deep learning model, especially in human-related or safety applications. In this paper, we present a novel Fairness Continual Learning approach to the semantic segmentation problem. In particular, under the fairness objective, a new fairness continual learning framework is proposed based on class distributions. Then, a novel Prototypical Contrastive Clustering loss is proposed to address the significant challenges in continual learning, i.e., catastrophic forgetting and background shift. Our proposed loss has also been proven as a novel, generalized learning paradigm of knowledge distillation commonly used in continual learning.",
  "Then, a novel Prototypical Contrastive Clustering loss is proposed to address the significant challenges in continual learning, i.e., catastrophic forgetting and background shift. Our proposed loss has also been proven as a novel, generalized learning paradigm of knowledge distillation commonly used in continual learning. Moreover, the proposed Conditional Structural Consistency loss further regularized the structural constraint of the predicted segmentation. Our proposed approach has achieved State-of-the-Art performance on three standard scene understanding benchmarks, i.e., ADE20K, Cityscapes, and Pascal VOC, and promoted the fairness of the segmentation model.\nAuthors: Thanh-Dat Truong, Hoang-Quan Nguyen, B. Raj, Khoa Luu\nVenue: Neural Information Processing Systems\nTldr: {'model': 'tldr@v2.0.0', 'text': 'A novel Fairness Continual Learning approach to the semantic segmentation problem is presented, in particular, a new fairness continual learning framework is proposed based on class distributions and a novel Prototypical Contrastive Clustering loss is proposed to address the significant challenges in continual learning.'}",
  "Faculty Name: bhiksha raj\nMetadata:\nPaperid: b7e2074934985b6112b6bce8c3680b14e621fdfe\nTitle: Importance of negative sampling in weak label learning\nYear: 2023\nAbstract: Weak-label learning is a challenging task that requires learning from data\"bags\"containing positive and negative instances, but only the bag labels are known. The pool of negative instances is usually larger than positive instances, thus making selecting the most informative negative instance critical for performance. Such a selection strategy for negative instances from each bag is an open problem that has not been well studied for weak-label learning. In this paper, we study several sampling strategies that can measure the usefulness of negative instances for weak-label learning and select them accordingly. We test our method on CIFAR-10 and AudioSet datasets and show that it improves the weak-label classification performance and reduces the computational cost compared to random sampling methods. Our work reveals that negative instances are not all equally irrelevant, and selecting them wisely can benefit weak-label learning.",
  "We test our method on CIFAR-10 and AudioSet datasets and show that it improves the weak-label classification performance and reduces the computational cost compared to random sampling methods. Our work reveals that negative instances are not all equally irrelevant, and selecting them wisely can benefit weak-label learning.\nAuthors: Ankit Shah, Fuyu Tang, Zelin Ye, Rita Singh, Bhiksha Raj\nVenue: arXiv.org\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This work reveals that negative instances are not all equally irrelevant, and selecting them wisely can benefit weak-label learning, and reduces the computational cost compared to random sampling methods.'}",
  "Faculty Name: bhiksha raj\nMetadata:\nPaperid: d7911ff6f80bd9f053ef8d304f15791f510f5cda\nTitle: Completing Visual Objects via Bridging Generation and Segmentation\nYear: 2023\nAbstract: This paper presents a novel approach to object completion, with the primary goal of reconstructing a complete object from its partially visible components. Our method, named MaskComp, delineates the completion process through iterative stages of generation and segmentation. In each iteration, the object mask is provided as an additional condition to boost image generation, and, in return, the generated images can lead to a more accurate mask by fusing the segmentation of images. We demonstrate that the combination of one generation and one segmentation stage effectively functions as a mask denoiser. Through alternation between the generation and segmentation stages, the partial object mask is progressively refined, providing precise shape guidance and yielding superior object completion results. Our experiments demonstrate the superiority of MaskComp over existing approaches, e.g., ControlNet and Stable Diffusion, establishing it as an effective solution for object completion.",
  "Through alternation between the generation and segmentation stages, the partial object mask is progressively refined, providing precise shape guidance and yielding superior object completion results. Our experiments demonstrate the superiority of MaskComp over existing approaches, e.g., ControlNet and Stable Diffusion, establishing it as an effective solution for object completion.\nAuthors: Xiang Li, Yinpeng Chen, Chung-Ching Lin, Rita Singh, Bhiksha Raj, Zicheng Liu\nVenue: arXiv.org\nTldr: {'model': 'tldr@v2.0.0', 'text': 'The method, named MaskComp, delineates the completion process through iterative stages of generation and segmentation, and demonstrates that the combination of one generation and one segmentation stage effectively functions as a mask denoiser.'}",
  "Faculty Name: bhiksha raj\nMetadata:\nPaperid: dc157eba8bdb4cfe6ee65566d8295939ac5b4b37\nTitle: PaintSeg: Training-free Segmentation via Painting\nYear: 2023\nAbstract: The paper introduces PaintSeg, a new unsupervised method for segmenting objects without any training. We propose an adversarial masked contrastive painting (AMCP) process, which creates a contrast between the original image and a painted image in which a masked area is painted using off-the-shelf generative models. During the painting process, inpainting and outpainting are alternated, with the former masking the foreground and filling in the background, and the latter masking the background while recovering the missing part of the foreground object. Inpainting and outpainting, also referred to as I-step and O-step, allow our method to gradually advance the target segmentation mask toward the ground truth without supervision or training. PaintSeg can be configured to work with a variety of prompts, e.g. coarse masks, boxes, scribbles, and points.",
  "PaintSeg can be configured to work with a variety of prompts, e.g. coarse masks, boxes, scribbles, and points. Our experimental results demonstrate that PaintSeg outperforms existing approaches in coarse mask-prompt, box-prompt, and point-prompt segmentation tasks, providing a training-free solution suitable for unsupervised segmentation.\nAuthors: Xiang Li, Chung-Ching Lin, Yinpeng Chen, Zicheng Liu, Jinglu Wang, B. Raj\nVenue: arXiv.org\nTldr: {'model': 'tldr@v2.0.0', 'text': 'An adversarial masked contrastive painting process, which creates a contrast between the original image and a painted image in which a masked area is painted using off-the-shelf generative models, providing a training-free solution suitable for unsupervised segmentation.'}",
  "Faculty Name: bhiksha raj\nMetadata:\nPaperid: e146e5221c124d93f69516c5ae7e1b7b1822848e\nTitle: TAPLoss: A Temporal Acoustic Parameter Loss for Speech Enhancement\nYear: 2023\nAbstract: Speech enhancement models have greatly progressed in recent years, but still show limits in perceptual quality of their speech outputs. We propose an objective for perceptual quality based on temporal acoustic parameters. These are fundamental speech features that play an essential role in various applications, including speaker recognition and paralinguistic analysis. We provide a differentiable estimator for four categories of low-level acoustic descriptors involving: frequency-related parameters, energy or amplitude-related parameters, spectral balance parameters, and temporal features. Un-like prior work that looks at aggregated acoustic parameters or a few categories of acoustic parameters, our temporal acoustic parameter (TAP) loss enables auxiliary optimization and improvement of many fine-grained speech characteristics in enhancement workflows. We show that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility.",
  "We show that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility. We use data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time-frequency domain models can benefit from our method.\nAuthors: YUNYANG ZENG, Joseph Konan, Shuo Han, David Bick, Muqiao Yang, Anurag Kumar, Shinji Watanabe, B. Raj\nVenue: IEEE International Conference on Acoustics, Speech, and Signal Processing\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This work shows that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility and uses data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time-frequency domain models can benefit from the method.'}",
  "Faculty Name: bhiksha raj\nMetadata:\nPaperid: e2572e0adacfb116b19b25691e7f6b3749490a88\nTitle: Training Audio Captioning Models without Audio\nYear: 2023\nAbstract: Automated Audio Captioning (AAC) is the task of generating natural language descriptions given an audio stream. A typical AAC system requires manually curated training data of audio segments and corresponding text caption annotations. The creation of these audio-caption pairs is costly, resulting in general data scarcity for the task. In this work, we address this major limitation and propose an approach to train AAC systems using only text. Our approach leverages the multimodal space of contrastively trained audio-text models, such as CLAP. During training, a decoder generates captions conditioned on the pretrained CLAP text encoder. During inference, the text encoder is replaced with the pretrained CLAP audio encoder. To bridge the modality gap between text and audio embeddings, we propose the use of noise injection or a learnable adapter, during training.",
  "During training, a decoder generates captions conditioned on the pretrained CLAP text encoder. During inference, the text encoder is replaced with the pretrained CLAP audio encoder. To bridge the modality gap between text and audio embeddings, we propose the use of noise injection or a learnable adapter, during training. We find that the proposed text-only framework performs competitively with state-of-the-art models trained with paired audio, showing that efficient text-to-audio transfer is possible. Finally, we showcase both stylized audio captioning and caption enrichment while training without audio or human-created text captions.\nAuthors: Soham Deshmukh, Benjamin Elizalde, Dimitra Emmanouilidou, Bhiksha Raj, Rita Singh, Huaming Wang\nVenue: arXiv.org\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This work proposes an approach to train AAC systems using only text, and finds that the proposed text-only framework performs competitively with state-of-the-art models trained with paired audio, showing that efficient text-to-audio transfer is possible.'}",
  "Faculty Name: bhiksha raj\nMetadata:\nPaperid: f5a7a4fda49c657742072a2758f43b1cbcde3886\nTitle: Continual Contrastive Spoken Language Understanding\nYear: 2023\nAbstract: Recently, neural networks have shown impressive progress across diverse fields, with speech processing being no exception. However, recent breakthroughs in this area require extensive offline training using large datasets and tremendous computing resources. Unfortunately, these models struggle to retain their previously acquired knowledge when learning new tasks continually, and retraining from scratch is almost always impractical. In this paper, we investigate the problem of learning sequence-to-sequence models for spoken language understanding in a class-incremental learning (CIL) setting and we propose COCONUT, a CIL method that relies on the combination of experience replay and contrastive learning. Through a modified version of the standard supervised contrastive loss applied only to the rehearsal samples, COCONUT preserves the learned representations by pulling closer samples from the same class and pushing away the others. Moreover, we leverage a multimodal contrastive loss that helps the model learn more discriminative representations of the new data by aligning audio and text features.",
  "Moreover, we leverage a multimodal contrastive loss that helps the model learn more discriminative representations of the new data by aligning audio and text features. We also investigate different contrastive designs to combine the strengths of the contrastive loss with teacher-student architectures used for distillation. Experiments on two established SLU datasets reveal the effectiveness of our proposed approach and significant improvements over the baselines. We also show that COCONUT can be combined with methods that operate on the decoder side of the model, resulting in further metrics improvements.\nAuthors: Umberto Cappellazzo, Enrico Fini, Muqiao Yang, Daniele Falavigna, A. Brutti, Bhiksha Raj\nVenue: arXiv.org\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This paper investigates the problem of learning sequence-to-sequence models for spoken language understanding in a class-incremental learning (CIL) setting and proposes COCONUT, a CIL method that relies on the combination of experience replay and contrastive learning.'}",
  "Faculty Name: bhiksha raj\nMetadata:\nPaperid: f5b88ca9d74e8ddc679adcd07a292bd8481062fa\nTitle: Prompting Audios Using Acoustic Properties For Emotion Representation\nYear: 2023\nAbstract: Emotions lie on a continuum, but current models treat emotions as a finite valued discrete variable. This representation does not capture the diversity in the expression of emotion. To better represent emotions we propose the use of natural language descriptions (or prompts). In this work, we address the challenge of automatically generating these prompts and training a model to better learn emotion representations from audio and prompt pairs. We use acoustic properties that are correlated to emotion like pitch, intensity, speech rate, and articulation rate to automatically generate prompts i.e. 'acoustic prompts'. We use a contrastive learning objective to map speech to their respective acoustic prompts. We evaluate our model on Emotion Audio Retrieval and Speech Emotion Recognition. Our results show that the acoustic prompts significantly improve the model's performance in EAR, in various Precision@K metrics. In SER, we observe a 3.8% relative accuracy improvement on the Ravdess dataset.",
  "We evaluate our model on Emotion Audio Retrieval and Speech Emotion Recognition. Our results show that the acoustic prompts significantly improve the model's performance in EAR, in various Precision@K metrics. In SER, we observe a 3.8% relative accuracy improvement on the Ravdess dataset.\nAuthors: Hira Dhamyal, Benjamin Elizalde, Soham Deshmukh, Huaming Wang, Bhiksha Raj, Rita Singh\nVenue: arXiv.org\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This work addresses the challenge of automatically generating prompts and training a model to better learn emotion representations from audio and prompt pairs by using acoustic properties that are correlated to emotion like pitch, intensity, speech rate, and articulation rate to automatically generate prompts.'}",
  "Faculty Name: bhiksha raj\nMetadata:\nPaperid: f969f059b01be02f9995396b6cc397959b574635\nTitle: Pairwise Similarity Learning is SimPLE\nYear: 2023\nAbstract: In this paper, we focus on a general yet important learning problem, pairwise similarity learning (PSL). PSL subsumes a wide range of important applications, such as open-set face recognition, speaker verification, image retrieval and person re-identification. The goal of PSL is to learn a pairwise similarity function assigning a higher similarity score to positive pairs (i.e., a pair of samples with the same label) than to negative pairs (i.e., a pair of samples with different label). We start by identifying a key desideratum for PSL, and then discuss how existing methods can achieve this desideratum. We then propose a surprisingly simple proxy-free method, called SimPLE, which requires neither feature/proxy normalization nor angular margin and yet is able to generalize well in open-set recognition. We apply the proposed method to three challenging PSL tasks: open-set face recognition, image retrieval and speaker verification.",
  "We then propose a surprisingly simple proxy-free method, called SimPLE, which requires neither feature/proxy normalization nor angular margin and yet is able to generalize well in open-set recognition. We apply the proposed method to three challenging PSL tasks: open-set face recognition, image retrieval and speaker verification. Comprehensive experimental results on large-scale benchmarks show that our method performs significantly better than current state-of-the-art methods. Our project page is available at simple.is.tue.mpg.de.\nAuthors: Yandong Wen, Weiyang Liu, Yao Feng, Bhiksha Raj, Rita Singh, Adrian Weller, Michael J. Black, Bernhard Sch\u00f6lkopf\nVenue: IEEE International Conference on Computer Vision\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This work identifies a key desideratum for PSL, and proposes a surprisingly simple proxy-free method, called SimPLE, which requires neither feature/proxy normalization nor angular margin and yet is able to generalize well in open-set recognition.'}",
  "Faculty Name: bhiksha raj\nMetadata:\nPaperid: feecd2cfb7871a818ba514e8b4b3f9da482f17bc\nTitle: Synergy between human and machine approaches to sound/scene recognition and processing: An overview of ICASSP special session\nYear: 2023\nAbstract: Machine Listening, as usually formalized, attempts to perform a task that is, from our perspective, fundamentally human-performable, and performed by humans. Current automated models of Machine Listening vary from purely data-driven approaches to approaches imitating human systems. In recent years, the most promising approaches have been hybrid in that they have used data-driven approaches informed by models of the perceptual, cognitive, and semantic processes of the human system. Not only does the guidance provided by models of human perception and domain knowledge enable better, and more generalizable Machine Listening, in the converse, the lessons learned from these models may be used to verify or improve our models of human perception themselves. This paper summarizes advances in the development of such hybrid approaches, ranging from Machine Listening models that are informed by models of peripheral (human) auditory processes, to those that employ or derive semantic information encoded in relations between sounds.",
  "This paper summarizes advances in the development of such hybrid approaches, ranging from Machine Listening models that are informed by models of peripheral (human) auditory processes, to those that employ or derive semantic information encoded in relations between sounds. The research described herein was presented in a special session on\"Synergy between human and machine approaches to sound/scene recognition and processing\"at the 2023 ICASSP meeting.\nAuthors: L. Heller, Benjamin Elizalde, B. Raj, Soham Deshmukh\nVenue: arXiv.org\nTldr: {'model': 'tldr@v2.0.0', 'text': 'Advances in the development of hybrid approaches to Machine Listening models that are informed by models of peripheral (human) auditory processes, to those that employ or derive semantic information encoded in relations between sounds are summarized.'}",
  "List of 2023 Open Access papers by bhiksha raj are:\nFREDOM: Fairness Domain Adaptation Approach to Semantic Scene Understanding\nUTOPIA: Unconstrained Tracking Objects without Preliminary Examination via Cross-Domain Adaptation\nSoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning\nFixed Inter-Neuron Covariability Induces Adversarial Robustness\nUnderstanding political polarization using language models: A dataset and method\nProlonged school closure during the pandemic time in successive waves of COVID-19- vulnerability of children to sexual abuses \u2013 A case study in Tamil Nadu, India\nBASS: Block-wise Adaptation for Speech Summarization\nUnderstanding Political Polarisation using Language Models: A dataset and method\nAn Approach to Ontological Learning from Weak Labels\nRethinking Voice-Face Correlation: A Geometry View\nPaaploss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement\nImproving Perceptual Quality, Intelligibility, and Acoustics on VoIP Platforms\nApproach to Learning Generalized Audio Representation Through Batch Embedding Covariance Regularization and Constant-Q Transforms\nTraining on Foveated Images Improves Robustness to Adversarial",
  "Intelligibility, and Acoustics on VoIP Platforms\nApproach to Learning Generalized Audio Representation Through Batch Embedding Covariance Regularization and Constant-Q Transforms\nTraining on Foveated Images Improves Robustness to Adversarial Attacks\nImprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations\nThe Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features\nFairness Continual Learning Approach to Semantic Scene Understanding in Open-World Environments\nPaintSeg: Training-free Segmentation via Painting\nTAPLoss: A Temporal Acoustic Parameter Loss for Speech Enhancement\nSynergy between human and machine approaches to sound/scene recognition and processing: An overview of ICASSP special session\nLoFT: Local Proxy Fine-tuning For Improving Transferability Of Adversarial Attacks Against Large Language Model\nEvaluating Speech Synthesis by Training Recognizers on Synthetic Speech\nRethinking Audiovisual Segmentation with Semantic Quantization and Decomposition\nTowards Noise-Tolerant Speech-Referring Video Object Segmentation: Bridging Speech and Text\nPsychoacoustic Challenges Of Speech Enhancement On VoIP Platforms\nuSee: Unified Speech Enhancement and Editing with",
  "Training Recognizers on Synthetic Speech\nRethinking Audiovisual Segmentation with Semantic Quantization and Decomposition\nTowards Noise-Tolerant Speech-Referring Video Object Segmentation: Bridging Speech and Text\nPsychoacoustic Challenges Of Speech Enhancement On VoIP Platforms\nuSee: Unified Speech Enhancement and Editing with Conditional Diffusion Models\nToken Prediction as Implicit Classification to Identify LLM-Generated Text\nUnderstanding and Mitigating the Label Noise in Pre-training on Downstream Tasks\nImportance of negative sampling in weak label learning\nCompleting Visual Objects via Bridging Generation and Segmentation\nTraining Audio Captioning Models without Audio\nContinual Contrastive Spoken Language Understanding\nPrompting Audios Using Acoustic Properties For Emotion Representation\nPairwise Similarity Learning is SimPLE",
  "Title: Jeffrey Bigham -     Language Technologies Institute -     School of Computer Science - Carnegie Mellon University\n\nMeta Tags:\n<meta charset=\"utf-8\"/>\n<meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\"/>\n<meta content=\"width=device-width, initial-scale=1.0\" name=\"viewport\"/>\n<meta content=\"Bio of Jeffrey Bigham, Faculty Member at Carnegie Mellon University's Language Technologies Institute\" name=\"description\"/>\n<meta content=\"Carnegie Mellon University\" name=\"author\"/>\n<meta content=\"Affiliated Faculty\" name=\"categories-1\"/>\n<meta content=\"Faculty\" name=\"global-categories\"/>\n<meta content=\"Jeffrey Bigham - Language Technologies Institute - School of Computer Science - Carnegie Mellon University\" property=\"og:title\"/>\n<meta content=\"Bio of Jeffrey Bigham, Faculty Member at Carnegie Mellon University's Language Technologies Institute\" property=\"og:description\"/>\n<meta content=\"profile\" property=\"og:type\"/>\n<meta content=\"Firstname\" property=\"profile:Jeffrey\"/>\n<meta content=\"Lastname\" property=\"profile:Bigham\"/>\n<meta content=\"http://lti.cmu.edu//people/faculty/bigham-jeffrey.",
  "cmu.edu//people/faculty/bigham-jeffrey.html\" property=\"og:url\"/>\n<meta data-siteid=\"lti\" id=\"siteId\"/>\n<meta content=\"#9f0000\" name=\"msapplication-TileColor\"/>\n<meta content=\"//www.cmu.edu/favicon-144.png\" name=\"msapplication-TileImage\"/>\n\nContent:\nJeffrey \n                        Bigham\nAssociate Professor, Language Technologies Institute\nHuman-Computer Interaction Institute\nContact\n3525 \u2014Newell-Simon Hall\njbigham(through)andrew.cmu.edu\n412-945-0708\nResearch\nI work at the intersection of human-computer interaction, human computation, crowdsourcing, artificial intelligence and natural language processing.\nThe future my research envisions is one in which the intelligent systems that we have dreamed about for decades \u2014 that have inspired generations of computer scientists from its beginning \u2014 are brought about for the benefit of people in their everyday lives.\nMy work is achieving this vision by leveraging on-demand human labor to fill in for components that we cannot currently automate, and by building frameworks that allow groups to do together what even expert individuals cannot do alone.",
  "My work is achieving this vision by leveraging on-demand human labor to fill in for components that we cannot currently automate, and by building frameworks that allow groups to do together what even expert individuals cannot do alone. A crowd-powered world seems counter to the goals of computer science, but by creating and deploying the systems of our dreams, we will learn to create the machines that will someday realize them.\nHuman-Computer Interaction Institute\nPersonal Website\n\nLinks:\nhttps://hcii.cmu.edu/\nhttp://www.cs.cmu.edu/~jbigham/",
  "Title: Lei Li -     Language Technologies Institute -     School of Computer Science - Carnegie Mellon University\n\nMeta Tags:\n<meta charset=\"utf-8\"/>\n<meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\"/>\n<meta content=\"width=device-width, initial-scale=1.0\" name=\"viewport\"/>\n<meta content=\"Bio page for LTI faculty member Lei Li\" name=\"description\"/>\n<meta content=\"Carnegie Mellon University\" name=\"author\"/>\n<meta content=\"Core Faculty\" name=\"categories-1\"/>\n<meta content=\"AI;Machine Learning;Machine Translation\" name=\"categories-2\"/>\n<meta content=\"Faculty\" name=\"global-categories\"/>\n<meta content=\"Lei Li - Language Technologies Institute - School of Computer Science - Carnegie Mellon University\" property=\"og:title\"/>\n<meta content=\"Bio page for LTI faculty member Lei Li\" property=\"og:description\"/>\n<meta content=\"profile\" property=\"og:type\"/>\n<meta content=\"Firstname\" property=\"profile:Lei\"/>\n<meta content=\"Lastname\" property=\"profile:Li\"/>\n<meta content=\"http://cms-staging.andrew.cmu.edu/lti/people/faculty/bio.",
  "andrew.cmu.edu/lti/people/faculty/bio.html\" property=\"og:url\"/>\n<meta data-siteid=\"lti\" id=\"siteId\"/>\n<meta content=\"#9f0000\" name=\"msapplication-TileColor\"/>\n<meta content=\"//www.cmu.edu/favicon-144.png\" name=\"msapplication-TileImage\"/>\n\nContent:\nLei \n                        Li\nAssistant Professor, Language Technologies Institute\nContact\n6403 Gates & Hillman Centers\nleili(through)andrew.cmu.edu\n412-268-6355\nAddress\n5000 Forbes Avenue\nPittsburgh, PA 15213\nPersonal Website\n\nLinks:\nhttps://www.cs.cmu.edu/~leili/",
  "Faculty Name: bio\nMetadata:\nPaperid: 0312627ed2ce0f5327d8588c2f9d65afd61e53d3\nTitle: Characterisation and Dynamics of an Emerging Seagrass Meadow\nYear: 2023\nAbstract: Seagrasses are habitat-forming species that support biodiversity and a wide range of associated ecosystem services, from blue carbon capture to providing nursery areas for a variety of organisms. Their decline has been documented worldwide and is attributed to human impacts ranging from habitat loss and eutrophication to the effects of climate change. However, recent recovery trends have also been documented due to reductions in stressors, passive and active restoration, and even changes in environmental conditions owing to local management. In this study, we document for the first time the occurrence of Zostera noltei in the downstream area of the River Minho Estuary. This occurrence was unexpected given the hydrological conditions of the estuary, characterised by dredging and siltation. We reconstructed the occurrence and historical distribution of seagrass beds, and showed that they have existed in the region for more than a decade.",
  "This occurrence was unexpected given the hydrological conditions of the estuary, characterised by dredging and siltation. We reconstructed the occurrence and historical distribution of seagrass beds, and showed that they have existed in the region for more than a decade. The current distribution area was mapped using high-resolution multispectral remote sensing techniques, and in situ photoquadrats to complement the remote sensing information with an evaluation of the seagrass cover. A current seagrass area of 0.81 ha was found with an average cover of 70%. However, the Minho Estuary continues to be strongly affected by sediment deposition, which may affect the seagrass population in the long term. Continued surveys are recommended to confirm the long-term trend of colonisation of this important habitat, which ultimately provides so many benefits to coastal ecosystems and humankind.",
  "However, the Minho Estuary continues to be strongly affected by sediment deposition, which may affect the seagrass population in the long term. Continued surveys are recommended to confirm the long-term trend of colonisation of this important habitat, which ultimately provides so many benefits to coastal ecosystems and humankind.\nAuthors: M. Dolbeth, D. Costa, Manuel de Figueiredo Meyer, J. Gon\u00e7alves, A. Bio\nVenue: Remote Sensing\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This study document for the first time the occurrence of Zostera noltei in the downstream area of the River Minho Estuary, and reconstructed the occurrence and historical distribution of seagrass beds, and showed that they have existed in the region for more than a decade.'}",
  "Faculty Name: bio\nMetadata:\nPaperid: 0b2db5968605bbce549f75928c1b5468782bc2ed\nTitle: 491. Risk Factors for Severe Pediatric COVID-19: A Systematic Review\nYear: 2023\nAbstract: Abstract Background Optimal management of COVID-19 in children requires risk stratification based on comorbidities and demographic factors that can predispose to severe disease. The Pediatric Infectious Diseases Society (PIDS) Pediatric COVID-19 Therapies Task Force, comprised of pediatric infectious diseases physicians, intensivists, and pharmacists from 29 US hospitals, develops clinical guidance for pediatric COVID-19 management. In support of these efforts, a systematic review of peer-reviewed literature was conducted to synthesize the evidence for risk factors for severe pediatric COVID-19. Methods Medline, EMBASE, and CDC databases were searched to identify all relevant publications before July 1, 2022. Titles and abstracts were reviewed to identify studies that assessed for potential predictors of severe COVID-19 disease in children < 21 years.",
  "Methods Medline, EMBASE, and CDC databases were searched to identify all relevant publications before July 1, 2022. Titles and abstracts were reviewed to identify studies that assessed for potential predictors of severe COVID-19 disease in children < 21 years. Severe disease was defined by intensive care unit admission, invasive mechanical ventilation, multiorgan dysfunction, or death. A team of reviewers appraised eligible studies, extracted relevant data, and assessed the quality of evidence. Comorbidities and demographic factors were classified as definite, probable, or unlikely risk factors based on the certainty of association with severe COVID-19. Results Sixteen potential risk factors were evaluated based on evidence from 50 studies: 13 reviews/meta-analyses, 23 multi-center, and 14 single-center studies (Figure 1). Severe immunocompromise, obesity, diabetes, prematurity, and neurologic, cardiovascular, and chronic lung disease were classified as definite risk factors. Evidence was less consistent in support of sickle cell disease, mild/moderate immunocompromise, neurodevelopmental, and chronic liver disorders as risk factors.",
  "Severe immunocompromise, obesity, diabetes, prematurity, and neurologic, cardiovascular, and chronic lung disease were classified as definite risk factors. Evidence was less consistent in support of sickle cell disease, mild/moderate immunocompromise, neurodevelopmental, and chronic liver disorders as risk factors. Most studies found asthma, sex, mental health, chronic kidney disease, and inflammatory bowel disease to be unlikely risk factors. Many studies demonstrated that the magnitude of risk for comorbidities was modified by prior immunization, age, and medical complexity (i.e., multiple or poorly controlled comorbidities) (Figure 2).Figure 1. Evidence Review: Comorbidities and Severe COVID-19 in Children Obesity - BMI \u226595th percentile for age and sex per CDC growth curves.",
  "Evidence Review: Comorbidities and Severe COVID-19 in Children Obesity - BMI \u226595th percentile for age and sex per CDC growth curves. Severe immunocompromise - Any of the following: Receipt in the 3 months before COVID-19 diagnosis of chemotherapy for a solid tumor or hematologic malignancy, high-dose corticosteroids (e.g., prednisone >20mg/day for \u226514 days), or other systemic B- or T-cell-depleting immunosuppressive agents; hematopoietic stem cell transplant, CAR T cell therapy, or solid organ transplant within 100 days of COVID-19 diagnosis; human immunodeficiency virus infection and CD4 count <200; combined primary immunodeficiency disorders. Moderate immunocompromise - Routine receipt of non-lymphocyte-depleting immunosuppressive or immunomodulatory medications or prednisone <20 mg/day for inflammatory/immune-mediated disease.Figure 2. Risk Factors and Risk Modifiers for Severe Pediatric COVID-19 *Children who are less likely to gain protection from previous infections or vaccines.",
  "Risk Factors and Risk Modifiers for Severe Pediatric COVID-19 *Children who are less likely to gain protection from previous infections or vaccines. Definite risk factor - Evidence for increased risk for severe COVID-19, supported by large multicenter studies, meta-analyses, or systematic reviews. Probable risk factor - Evidence for increased risk, supported by small or single-center studies; this category also includes factors inconsistently associated with severe COVID-19, suggesting substantial uncertainty. Unlikely risk factor - Evidence against increased risk for severe COVID-19, supported by large multicenter studies, meta-analyses, or systematic reviews. Conclusion This study highlights key comorbidities and effect modifiers associated with severe COVID-19 in children. These findings can be used to facilitate risk stratification and inform management decisions.",
  "Conclusion This study highlights key comorbidities and effect modifiers associated with severe COVID-19 in children. These findings can be used to facilitate risk stratification and inform management decisions. Disclosures Zachary I. Willis, MD, MPH, Merck Sharp & Dohme Corp: Grant/Research Support|Pfizer Inc: Grant/Research Support Gabriela Maron, MD, Astellas Inc: Grant/Research Support|SymBio Pharma: Grant/Research Support Paul K. Sue, MDCM, Allovir, Inc: Participant in Industry Sponsored Trial|Gilead Sciences, Inc: Participant in Industry Sponsored Trial|Merck & Co.: Participant in Industry Sponsored Trial Scott H. James, MD, Bayer: Advisor/Consultant|Evrys: Grant/Research Support|Gilead: Grant/Research Support Mari M. Nakamura, MD, MPH, Gilead Sciences, Inc.: Grant/Research Support Joshua Wolf, MBBS, PhD, Karius Inc.: Grant/Research Support|Merck Inc.",
  ": Grant/Research Support Joshua Wolf, MBBS, PhD, Karius Inc.: Grant/Research Support|Merck Inc.: Participation in industry-sponsored research\nAuthors: C. R. Oliveira, Zachary I Willis, Gabriela M. Maron, Paul K Sue, Brenda I Anosike, L. Bio, Prachi Singh, Scott H James, Christine M Miller, Mari M Nakamura, Joshua Wolf\nVenue: Open Forum Infectious Diseases\nTldr: {'model': 'tldr@v2.0.0', 'text': 'A systematic review of peer-reviewed literature was conducted to synthesize the evidence for risk factors for severe pediatric COVID-19 and found asthma, sex, mental health, chronic kidney disease, and inflammatory bowel disease to be unlikely risk factors.'}",
  "Faculty Name: bio\nMetadata:\nPaperid: 0d983ae9fe2dd2bd354f9e1b5b1a99eef3c626db\nTitle: New Methodology for Intertidal Seaweed Biomass Estimation Using Multispectral Data Obtained with Unoccupied Aerial Vehicles\nYear: 2023\nAbstract: Seaweed assemblages include a variety of structuring species providing habitats, food and shelter for organisms from different trophic levels. Monitoring intertidal seaweed traditionally involves targeting small areas to collect data on species\u2019 biological traits, which is often labour intensive and covers only a small area of the rocky reef under study. Given the various applications for seaweeds and their compounds, there has been an increase in demand for biomass triggered by the development of new markets. Such biomass demand generates new challenges for biomass quantification and the definition of future in-take harvesting commercial quotas by regulating agencies. The use of Unoccupied Aerial Vehicles (UAVs) as a low-cost yet efficient monitoring solution, combined with new sensors such as multispectral cameras, has been proposed for mapping intertidal reefs and seaweed in particular.",
  "The use of Unoccupied Aerial Vehicles (UAVs) as a low-cost yet efficient monitoring solution, combined with new sensors such as multispectral cameras, has been proposed for mapping intertidal reefs and seaweed in particular. In this study, a new methodology was developed and validated to quantify intertidal seaweed biomass based on multispectral UAV imagery, which was made available through an easy-to-use QGIS plugin (named SWUAV_BIO) that automates such biomass estimation. This tool was applied to a case study where the standing stock of Fucus spp. beds located at Viana do Castelo rocky shore (northern Portugal) was assessed using UAV multispectral imagery, providing a reference for future UAV-based ecological studies. Although comparison with the in situ assessments showed that biomass was underestimated by 36%, the SWUAV_BIO plugin is a valuable tool, as it provides an expedited (albeit conservative) seaweed standing stock assessment that can be used to monitor seaweed populations, their changes, and assess the effect of harvesting. These data can be used for an informed and sustainable management of seaweed resources by the competent authorities.",
  "These data can be used for an informed and sustainable management of seaweed resources by the competent authorities.\nAuthors: D\u00e9bora Borges, L. Duarte, I. Costa, A. Bio, Joelen Silva, I. Sousa-Pinto, J. Gon\u00e7alves\nVenue: Remote Sensing\nTldr: None",
  "Faculty Name: bio\nMetadata:\nPaperid: 199205d8d1353999b80a05f6de3c6fa7706885eb\nTitle: Multimodal Imaging Findings of Cerebral Amyloid Angiopathy Related Inflammation With Unusual Clinical Manifestation: A Case Report\nYear: 2023\nAbstract: None\nAuthors: J. Koo, Mina Park, H. S. Yoo, B. Joo, S. Ahn, Jae-Hoon Lee, Young Hoon Ryu, Sang Hyun Suh\nVenue: Investigative Magnetic Resonance Imaging\nTldr: None",
  "Faculty Name: bio\nMetadata:\nPaperid: 218c8d3a5f8c7e1939b453e3360ad28c42eeff7e\nTitle: Application of a Multispectral UAS to Assess the Cover and Biomass of the Invasive Dune Species Carpobrotus edulis\nYear: 2023\nAbstract: Remote sensing can support dune ecosystem conservation. Unoccupied Aircraft Systems (UAS) equipped with multispectral cameras can provide information for identifying different vegetation species, including Carpobrotus edulis\u2014one of the most prominent alien species in Portuguese dune ecosystems. This work investigates the use of multispectral UAS for C. edulis identification and biomass estimation. A UAS with a five-band multispectral camera was used to capture images from the sand dunes of the C\u00e1vado River spit. Simultaneously, field samples of C. edulis were collected for laboratorial quantification of biomass through Dry Weight (DW).",
  "A UAS with a five-band multispectral camera was used to capture images from the sand dunes of the C\u00e1vado River spit. Simultaneously, field samples of C. edulis were collected for laboratorial quantification of biomass through Dry Weight (DW). Five supervised classification algorithms were tested to estimate the total area of C. edulis, with the Random Forest algorithm achieving the best results (C. edulis Producer Accuracy (PA) = 0.91, C. edulis User Accuracy (UA) = 0.80, kappa = 0.87, Overall Accuracy (OA) = 0.89). Sixteen vegetation indices (VIs) were assessed to estimate the Above-Ground Biomass (AGB) of C. edulis, using three regression models to correlate the sample areas VI and DW. An exponential regression model of the Renormalized Difference Vegetation Index (RDVI) presented the best fit for C. edulis DW (R2 = 0.86; p-value < 0.05; normalised root mean square error (NRMSE) = 0.09).",
  "An exponential regression model of the Renormalized Difference Vegetation Index (RDVI) presented the best fit for C. edulis DW (R2 = 0.86; p-value < 0.05; normalised root mean square error (NRMSE) = 0.09). This result was later used to estimate the total AGB in the area, which can be used for monitoring and management plans\u2014namely, removal campaigns.\nAuthors: Manuel de Figueiredo Meyer, J. Gon\u00e7alves, Jacinto Cunha, S. Ramos, A. Bio\nVenue: Remote Sensing\nTldr: {'model': 'tldr@v2.0.0', 'text': 'This work investigates the use of multispectral UAS for C. edulis identification and biomass estimation, and estimates the total AGB in the area, which can be used for monitoring and management plans\u2014namely, removal campaigns.'}",
  "Faculty Name: bio\nMetadata:\nPaperid: 241bf86642809dc59562aa1448fea36c075b6b8e\nTitle: Clinicoradiologic Characteristics of Intradural Extramedullary Conventional Spinal Ependymoma\nYear: 2023\nAbstract: Purpose Distinguishing intradural extramedullary (IDEM) spinal ependymoma from myxopapillary ependymoma is challenging due to the location of IDEM spinal ependymoma. This study aimed to investigate the utility of clinical and MR imaging features for differentiating between IDEM spinal and myxopapillary ependymomas. Materials and Methods We compared tumor size, longitudinal/axial location, enhancement degree/pattern, tumor margin, signal intensity (SI) of the tumor on T2-weighted images and T1-weighted image (T1WI), increased cerebrospinal fluid (CSF) SI caudal to the tumor on T1WI, and CSF dissemination of pathologically confirmed 12 IDEM spinal and 10 myxopapillary ependymomas.",
  "Furthermore, classification and regression tree (CART) was performed to identify the clinical and MR features for differentiating between IDEM spinal and myxopapillary ependymomas. Results Patients with IDEM spinal ependymomas were older than those with myxopapillary ependymomas (48 years vs. 29.5 years, p < 0.05). A high SI of the tumor on T1W1 was more frequently observed in IDEM spinal ependymomas than in myxopapillary ependymomas (p = 0.02). Conversely, myxopapillary ependymomas show CSF dissemination. Increased CSF SI caudal to the tumor on T1WI was observed more frequently in myxopapillary ependymomas than in IDEM spinal ependymomas (p < 0.05). Dissemination to the CSF space and increased CSF SI caudal to the tumor on T1WI were the most important variables in CART analysis. Conclusion Clinical and radiological variables may help differentiate between IDEM spinal and myxopapillary ependymomas.",
  "Dissemination to the CSF space and increased CSF SI caudal to the tumor on T1WI were the most important variables in CART analysis. Conclusion Clinical and radiological variables may help differentiate between IDEM spinal and myxopapillary ependymomas.\nAuthors: Seung Hyun Lee, Y. Cha, Y. Cho, Mina Park, B. Joo, Sang Hyun Suh, S. Ahn\nVenue: Journal of the Korean Society of Radiology\nTldr: {'model': 'tldr@v2.0.0', 'text': 'Investigation of the utility of clinical and MR imaging features for differentiating between IDEM spinal and myxopapillary ependymomas found dissemination to the CSF space and increased CSF SI caudal to the tumor on T1WI were the most important variables in CART analysis.'}",
  "Faculty Name: bio\nMetadata:\nPaperid: 3cdce526e8bce7c785c624e5a2afe842ccd73e85\nTitle: Magnetic Resonance Elastography for Clinicians and Researchers Unfamiliar With the Field\nYear: 2023\nAbstract: None\nAuthors: Seungtae Lee, B. Joo, Mina Park, S. Ahn, Sang Hyun Suh\nVenue: Investigative Magnetic Resonance Imaging\nTldr: None",
  "Faculty Name: bio\nMetadata:\nPaperid: 4aba9db4337078d1ca3585dbf7d9d42935044c17\nTitle: Epidemiological description of measles outbreaks following a mass vaccination Campaign in Bayelsa State, Nigeria\nYear: 2023\nAbstract: None\nAuthors: S. Abaya, D. Ogoina, B. Abaye, Tella Adedamola\nVenue: AfricArXiv\nTldr: None",
  "Faculty Name: bio\nMetadata:\nPaperid: 5761f39859863520a09eb2854f319dfc9720abef\nTitle: A Radiomics-Based Model for Potentially More Accurate Identification of Subtypes of Breast Cancer Brain Metastases\nYear: 2023\nAbstract: Purpose Breast cancer brain metastases (BCBM) may involve subtypes that differ from the primary breast cancer lesion. This study aimed to develop a radiomics-based model that utilizes preoperative brain MRI for multiclass classification of BCBM subtypes and to investigate whether the model offers better prediction accuracy than the assumption that primary lesions and their BCBMs would be of the same subtype (non-conversion model) in an external validation set. Materials and Methods The training and external validation sets each comprised 51 cases (102 cases total). Four machine learning classifiers combined with three feature selection methods were trained on radiomic features and primary lesion subtypes for prediction of the following four subtypes: 1) hormone receptor (HR)+/human epidermal growth factor receptor 2 (HER2)-, 2) HR+/HER2+, 3) HR-/HER2+, and 4) triple-negative.",
  "After training, the performance of the radiomics-based model was compared to that of the non-conversion model in an external validation set using accuracy and F1-macro scores. Results The rate of discrepant subtypes between primary lesions and their respective BCBMs were 25.5% (n=13 of 51) in the training set and 23.5% (n=12 of 51) in the external validation set. In the external validation set, the accuracy and F1-macro score of the radiomics-based model were significantly higher than those of the non-conversion model (0.902 vs. 0.765, p=0.004; 0.861 vs. 0.699, p=0.002). Conclusion Our radiomics-based model represents an incremental advance in the classification of BCBM subtypes, thereby facilitating a more appropriate personalized therapy.",
  "0.765, p=0.004; 0.861 vs. 0.699, p=0.002). Conclusion Our radiomics-based model represents an incremental advance in the classification of BCBM subtypes, thereby facilitating a more appropriate personalized therapy.\nAuthors: Seonghyeon Cho, B. Joo, Mina Park, S. Ahn, Sang Hyun Suh, Y. Park, S. Ahn, Seung-Koo Lee\nVenue: Yonsei medical journal\nTldr: {'model': 'tldr@v2.0.0', 'text': 'The radiomics-based model represents an incremental advance in the classification of BCBM subtypes, thereby facilitating a more appropriate personalized therapy.'}"
]