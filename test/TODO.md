#TODO - Priority
1. Langchain RAG Stack
- LLama 13B (very slow langchain because inference is per question)
- Base retriever + LLama13B is giving I don't know

2. Finetuning Langchain RAG Stack
3. Using Larger models with in-context learning (eg. llama-instruction-tuned); <Retriever>

#Then use some transformers good LLM to just see how good the inference is

