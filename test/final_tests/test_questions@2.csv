question,retrieved_docs,prompt
"What is another name for the vehicle being raced in sweepstakes?
","['cmubuggy_d403.txt', 'cmubuggy_d403.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is another name for the vehicle being raced in sweepstakes?

Context: An mural celebrating Buggy can be found in CMU's Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions. Sweepstakes Slang Buggy: Vehicle being raced and also a nickname for the competition. Chute: A section of the freeroll portion of the buggy course (near the southwestern end of Frew Street at its intersection with Schenley Drive) where buggies make the sharp righthand turn from Schenley Drive onto Frew Street. Chute Flagger: Team member who provides a signal for buggy drivers to know when to start the right-hand turn from Schenley Drive onto Frew Street. Driver: Person who travels with a buggy and controls the vehicles via steering and braking systems. Pushbar: Structure attached to a buggy that a person pushes to propel that buggy forward. Pusher: Person who propels a buggy via a pushbar along one of the five hills of the buggy course. Shell: Entire outer structure or covering of a buggy that determines that buggy's aerodynamic characteristics. Transition: Procedure whereby one pusher finishes pushing a buggy and the next pusher in sequence starts to push that same buggy. Members of Fringe, celebrating their 50th year of Buggy racing, allowed a rare visit into its shop for a behind-the-scenes tour. This year Fringe is planning to roll four different vehicles, built and maintained in the Fringe workshop in the basement of the East Campus Garage, known as the ""Froom."" ""We're used to saying everything with 'fr' in front of it but when we say something in front of other people, it gets them confused,"" said Fringe head mechanic Dave Singh, who will graduate in May with a bachelor's degree in mechanical engineering and biomedical engineering. Fringe vehicles often are named with the letter ""B,"" like Boson, Blueshift, Bissa and Bumper. Other teams, such as Apex, often use names that connotate fire, while the SDC (Student Dormitory Committee) team, uses names such as Vice, Bane, Avarice and Malice. This year's buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It's one of the few times spectators can see buggies up close.
Buggy Races Keep Rolling at Carnegie Mellon - News - Carnegie Mellon University Carnegie Mellon University ——— Search Search Search this site only News News › Stories › Archives › 2019 › April › Buggy Races Keep Rolling at Carnegie Mellon April 10, 2019 Buggy Races Keep Rolling at Carnegie Mellon In its 99th year, the tradition is a Spring Carnival treat By Heidi Opdyke opdyke(through)andrew.cmu.edu Media Inquiries Julie Mattera Marketing and Communications jmattera(through)cmu.edu 412-268-2902 Sweepstakes, also known as the Buggy Races, has come a long way at Carnegie Mellon University. The slick, torpedo-like vessels carrying drivers with nerves of steel are a far cry from the two-man teams that once changed places halfway through a race and rode in everything from rain barrels with bicycle wheels to three-wheeled ash cans 99 years ago. Today, it takes six people to maneuver the .84 -mile course around Schenley Park's Flagstaff Hill. But while five pushers and a driver navigate the course's hills, dozens of people are needed to make a successful race happen. A year of planning goes into just over two minutes of racing. The Machine The basics of a buggy are straightforward, but teams are often secretive in how they build the machines, in particular the way they brake, steer and what types of wheels are used. Each has a body, pushbar for runners to move the machine up the hills, wheels, a safety harness and driving and braking mechanisms. Some also include fairings, a type of housing around the wheels that help reduce drag, make the vehicle quieter and just looks cool. Fairings have been a key feature for the Fringe team in recent years, which is celebrating its 50th anniversary of Buggy. ""We have a reputation of being the quietest on the course,"" said Diya Nuxoll, who wrapped up her bachelor's degree in mechanical engineering in December and is working on an advanced studies master's degree in design and manufacturing. Nuxoll leads the Fringe team as one of two head mechanics. An mural celebrating Buggy can be found in CMU's Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions. Sweepstakes Slang Buggy: Vehicle being raced and also a nickname for the competition.
Answer: "
"What's the course number for large language models methods and application?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What's the course number for large language models methods and application?

Context: In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Artificial Intelligence and Future Markets:' with Course ID 11651 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'TBA' with Course ID 11651 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Shamos located in Building WEH, Room 5403.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'AI Innovation:' with Course ID 11654 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'TBA' with Course ID 11654 and Section A offers 12.0 units. The Class meets Monday between 07:00PM and 09:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Paulisick located in Building POS, Room 146.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Applied Machine Learning' with Course ID 11663 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rose located in Building SH, Room 105.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Large Language Models Methods and Application' with Course ID 11667 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ippolito, Xiong located in Building BH, Room A51.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Artificial Intelligence and Future Markets:' with Course ID 11651 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'TBA' with Course ID 11651 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Shamos located in Building WEH, Room 5403.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'AI Innovation:' with Course ID 11654 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'TBA' with Course ID 11654 and Section A offers 12.0 units. The Class meets Monday between 07:00PM and 09:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Paulisick located in Building POS, Room 146.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Applied Machine Learning' with Course ID 11663 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rose located in Building SH, Room 105.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Large Language Models Methods and Application' with Course ID 11667 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ippolito, Xiong located in Building BH, Room A51.
Answer: "
"When will the classes begin in the Fall 2024 semester?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When will the classes begin in the Fall 2024 semester?

Context: From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mini-2 Classes Begin is observed.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mid-Semester & Mini-1 grades due by 4 pm is observed.
On Friday, 27 October, 2023, during the Fall 2023 (F23) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
On Tuesday, 07 November, 2023, during the Fall 2023 (F23) semester, Democracy Day is observed, leading to No Classes, except Evening classes after 5 pm will still meet.
On Saturday, 11 November, 2023, during the Fall 2023 (F23) semester, Homecoming is observed.
From 13 November, 2023,Monday to 17 November, 2023,Friday marks the Spring 2024 Registration Week for Fall 2023 (F23) semester.
On Monday, 13 November, 2023, during the Fall 2023 (F23) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Wednesday, 15 November, 2023, during the Fall 2023 (F23) semester, Mini-2 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 22 November, 2023,Wednesday to 24 November, 2023,Friday marks the Thanksgiving Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Mini-2 pass/no pass & withdrawal deadline (2) is observed.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Faculty Course Evaluations open is observed.
On Friday, 08 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Last Day of Classes is observed.
On Monday, 02 September, 2024, during the Fall 2024 (F24) semester, Labor Day is observed, leading to No Classes & University Closed.
On Monday, 09 September, 2024, during the Fall 2024 (F24) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 18 September, 2024, during the Fall 2024 (F24) semester, Mini-1 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 30 September, 2024, during the Fall 2024 (F24) semester, Mini-1 Pass/no pass & withdrawal deadline (3) is observed.
On Monday, 30 September, 2024, during the Fall 2024 (F24) semester, Mini-1 Faculty Course Evaluations open is observed.
On Monday, 07 October, 2024, during the Fall 2024 (F24) semester, Semester drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 11 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Last Day of Classes is observed.
On Friday, 11 October, 2024, during the Fall 2024 (F24) semester, Mini-1 voucher election deadline (4) is observed.
On Saturday, 12 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Exams is observed.
On Saturday, 12 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Faculty Course Evaluations close is observed.
From 14 October, 2024,Monday to 18 October, 2023,Friday marks the Fall Break for Fall 2024 (F24) semester which leads to No Classes.
On Monday, 21 October, 2024, during the Fall 2024 (F24) semester, Mini-2 Classes Begin is observed.
On Wednesday, 23 October, 2024, during the Fall 2024 (F24) semester, Mid-Semester & Mini-1 grades due by 4 pm is observed.
On Friday, 25 October, 2024, during the Fall 2024 (F24) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
Answer: "
"In spring 2024, How many units is course 10315?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, How many units is course 10315?

Context: In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Introduction to Machine Learning (SCS Majors)' with Course ID 10315 and Section C offers 12.0 units. The Class meets Friday between 12:00PM and 12:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Virtue located in Building GHC, Room 4301.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Introduction to Machine Learning (SCS Majors)' with Course ID 10315 and Section D offers 12.0 units. The Class meets Friday between 01:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Virtue located in Building GHC, Room 4102.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Introduction to Machine Learning (SCS Majors)' with Course ID 10315 and Section E offers 12.0 units. The Class meets Friday between 01:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Virtue located in Building WEH, Room 4625.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Introduction to Machine Learning (SCS Majors)' with Course ID 10315 and Section F offers 12.0 units. The Class meets Friday between 03:00PM and 03:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Virtue located in Building WEH, Room 2302.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Art and Machine Learning' with Course ID 10335 and Section A offers 12.0 units. The Class meets Monday Wednesday between 07:00PM and 08:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kang located in Building TEP, Room 1403.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Deep Reinforcement Learning & Control' with Course ID 10403 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 12:30PM and 01:50PM ET.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Introduction to Machine Learning (SCS Majors)' with Course ID 10315 and Section C offers 12.0 units. The Class meets Friday between 12:00PM and 12:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Virtue located in Building GHC, Room 4301.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Introduction to Machine Learning (SCS Majors)' with Course ID 10315 and Section D offers 12.0 units. The Class meets Friday between 01:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Virtue located in Building GHC, Room 4102.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Introduction to Machine Learning (SCS Majors)' with Course ID 10315 and Section E offers 12.0 units. The Class meets Friday between 01:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Virtue located in Building WEH, Room 4625.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Introduction to Machine Learning (SCS Majors)' with Course ID 10315 and Section F offers 12.0 units. The Class meets Friday between 03:00PM and 03:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Virtue located in Building WEH, Room 2302.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Art and Machine Learning' with Course ID 10335 and Section A offers 12.0 units. The Class meets Monday Wednesday between 07:00PM and 08:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kang located in Building TEP, Room 1403.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Deep Reinforcement Learning & Control' with Course ID 10403 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 12:30PM and 01:50PM ET.
Answer: "
"In the TAPLoss paper, what does TAP stand for?
","['bhiksha raj_e146e5221c124d93f69516c5ae7e1b7b1822848e_metadata.txt', 'shinji watanabe_e146e5221c124d93f69516c5ae7e1b7b1822848e_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the TAPLoss paper, what does TAP stand for?

Context: Faculty Name: bhiksha raj
Paperid: e146e5221c124d93f69516c5ae7e1b7b1822848e
Title: TAPLoss: A Temporal Acoustic Parameter Loss for Speech Enhancement
Year: 2023
Abstract: Speech enhancement models have greatly progressed in recent years, but still show limits in perceptual quality of their speech outputs. We propose an objective for perceptual quality based on temporal acoustic parameters. These are fundamental speech features that play an essential role in various applications, including speaker recognition and paralinguistic analysis. We provide a differentiable estimator for four categories of low-level acoustic descriptors involving: frequency-related parameters, energy or amplitude-related parameters, spectral balance parameters, and temporal features. Un-like prior work that looks at aggregated acoustic parameters or a few categories of acoustic parameters, our temporal acoustic parameter (TAP) loss enables auxiliary optimization and improvement of many fine-grained speech characteristics in enhancement workflows. We show that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility. We use data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time-frequency domain models can benefit from our method.
Authors: YUNYANG ZENG, Joseph Konan, Shuo Han, David Bick, Muqiao Yang, Anurag Kumar, Shinji Watanabe, B. Raj
Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work shows that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility and uses data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time-frequency domain models can benefit from the method.'}
Url: https://arxiv.org/pdf/2302.08088
Faculty Name: shinji watanabe
Paperid: e146e5221c124d93f69516c5ae7e1b7b1822848e
Title: TAPLoss: A Temporal Acoustic Parameter Loss for Speech Enhancement
Year: 2023
Abstract: Speech enhancement models have greatly progressed in recent years, but still show limits in perceptual quality of their speech outputs. We propose an objective for perceptual quality based on temporal acoustic parameters. These are fundamental speech features that play an essential role in various applications, including speaker recognition and paralinguistic analysis. We provide a differentiable estimator for four categories of low-level acoustic descriptors involving: frequency-related parameters, energy or amplitude-related parameters, spectral balance parameters, and temporal features. Un-like prior work that looks at aggregated acoustic parameters or a few categories of acoustic parameters, our temporal acoustic parameter (TAP) loss enables auxiliary optimization and improvement of many fine-grained speech characteristics in enhancement workflows. We show that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility. We use data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time-frequency domain models can benefit from our method.
Authors: YUNYANG ZENG, Joseph Konan, Shuo Han, David Bick, Muqiao Yang, Anurag Kumar, Shinji Watanabe, B. Raj
Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work shows that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility and uses data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time-frequency domain models can benefit from the method.'}
Url: https://arxiv.org/pdf/2302.08088
Answer: "
"What is the purpose of the ACL 60/60 evaluation sets?
","['mona diab_c5849f406e8263806a84e1a407ec0e0fe131bd5c_metadata.txt', 'shinji watanabe_f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b_content_2.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the purpose of the ACL 60/60 evaluation sets?

Context: Faculty Name: mona diab
Paperid: c5849f406e8263806a84e1a407ec0e0fe131bd5c
Title: Evaluating Multilingual Speech Translation under Realistic Conditions with Resegmentation and Terminology
Year: 2023
Abstract: We present the ACL 60/60 evaluation sets for multilingual translation of ACL 2022 technical presentations into 10 target languages. This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics.
Authors: Elizabeth Salesky, Kareem Darwish, Mohamed Al-Badrashiny, Mona T. Diab, J. Niehues
Venue: International Workshop on Spoken Language Translation
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics.'}
Url: https://aclanthology.org/2023.iwslt-1.2.pdf
B.2 Simultaneous SLT B.3 Automatic Subtitling B.4 Multilingual Speech Translation Below we show the Multilingual task (§5) results and overall rankings, ordered according to the average chrF across all 10 target languages after resegmentation to the reference translations. We also compare to the Offline submissions on the ACL 60-60 evaluation set on the 3 language pairs used for the Offline task. Finally, we show the scores for each metric (chrF, COMET, BLEU) per language pair for all systems. B.5 Speech-to-Speech Translation
Answer: "
"In summer 2024, What is the last day of Mini-5 classes?
","['Meta-Data-Calendar-2324.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In summer 2024, What is the last day of Mini-5 classes?

Context: Event: 'Mini-5 course drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-07', Day: 'Friday', Event: 'Mini-5 pass/no pass & withdrawal deadline (3)', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-14', Day: 'Friday', Event: 'Mini-5 Faculty Course Evaluations open', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-18', Day: 'Tuesday', Event: 'Semester course drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Summer One 2024 (M24)'
Date: '2022-06-19', Day: 'Wednesday', Event: 'Juneteenth; University Closed & No Classes', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-20', Day: 'Thursday', Event: 'Mini-5 Last Day of Classes', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-20', Day: 'Thursday', Event: 'Mini-5 voucher deadline (4)', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-21', Day: 'Friday', Event: 'Mini-5 Final Exams', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-21', Day: 'Friday', Event: 'Mini-5 Faculty Course Evaluations close ', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-24', Day: 'Monday', Event: 'Mini-6 First Day of Classes', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-25', Day: 'Tuesday', Event: 'Mini-5 Final Grades Due by 4 pm', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-28', Day: 'Friday', Event: 'Mini-6 add, audit, & tuition adjustment drop deadline (1)', Semester: 'Summer One 2024 (M24)'
Date: '2024-07-04', Day: 'Thursday', Event: 'Independence Day; University Closed & No Classes',
On Friday, 13 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Faculty Course Evaluations open is observed.
On Tuesday, 17 June, 2025, during the Summer 2025 (M25) semester, Summer All course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Wednesday, 18 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Last Day of Classes is observed.
On Wednesday, 18 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 voucher deadline is observed.
On Thursday, 19 June, 2025, during the Summer 2025 (M25) semester, Juneteenth is observed, leading to University Closed & No Classes.
On Friday, 20 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Final Exams is observed.
On Friday, 20 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Faculty Course Evaluations close is observed.
On Monday, 23 June, 2025, during the Summer 2025 (M25) semester, Summer Two & Mini-6 First Day of Classes is observed.
On Tuesday, 24 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Final Grades Due by 4 pm is observed.
On Friday, 27 June, 2025, during the Summer 2025 (M25) semester, Summer Two & Mini-6 add, audit, & tuition adjustment drop deadline (1) is observed.
On Thursday, 03 July, 2025, during the Summer 2025 (M25) semester, Summer All pass/no pass & withdrawal grade deadline (3) is observed.
On Friday, 04 July, 2025, during the Summer 2025 (M25) semester, Independence Day is observed, leading to University Closed & No Classes.
On Monday, 07 July, 2025, during the Summer 2025 (M25) semester, Summer Two & Mini-6 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
Answer: "
"What number do all of the Drama classes start with?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What number do all of the Drama classes start with?

Context: In Semester Fall 2023, from the department of Drama, the subject titled 'Dramaturgy' with Course ID 54426 and Section B offers 6-21 units. The Class meets Monday Tuesday Wednesday Thursday Friday between 06:30PM and 10:30PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Good located in Building TBA, Room None.
In Semester Fall 2023, from the department of Drama, the subject titled 'Dramaturgy' with Course ID 54426 and Section NA offers 6-21 units. The Class meets Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor NA located in Building TBA, Room None.
In Semester Fall 2023, from the department of Drama, the subject titled 'Dramaturgy' with Course ID 54426 and Section NA offers 6-21 units. The Class meets Saturday between 11:00AM and 03:00PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor NA located in Building TBA, Room None.
In Semester Fall 2023, from the department of Drama, the subject titled 'Production Prep: Lear:' with Course ID 54426 and Section NA offers 6-21 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Drama, the subject titled 'Production Management' with Course ID 54426 and Section C offers 6-21 units. The Class meets Monday Tuesday Wednesday Thursday Friday between 06:30PM and 10:30PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Holcomb located in Building TBA, Room None.
In Semester Fall 2023, from the department of Drama, the subject titled 'Production Management' with Course ID 54426 and Section NA offers 6-21 units. The Class meets Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor NA located in Building TBA, Room None.
In Semester Fall 2023, from the department of Drama, the subject titled 'Dramaturgy' with Course ID 54426 and Section B offers 6-21 units. The Class meets Monday Tuesday Wednesday Thursday Friday between 06:30PM and 10:30PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Good located in Building TBA, Room None.
In Semester Fall 2023, from the department of Drama, the subject titled 'Dramaturgy' with Course ID 54426 and Section NA offers 6-21 units. The Class meets Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor NA located in Building TBA, Room None.
In Semester Fall 2023, from the department of Drama, the subject titled 'Dramaturgy' with Course ID 54426 and Section NA offers 6-21 units. The Class meets Saturday between 11:00AM and 03:00PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor NA located in Building TBA, Room None.
In Semester Fall 2023, from the department of Drama, the subject titled 'Production Prep: Lear:' with Course ID 54426 and Section NA offers 6-21 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Drama, the subject titled 'Production Management' with Course ID 54426 and Section C offers 6-21 units. The Class meets Monday Tuesday Wednesday Thursday Friday between 06:30PM and 10:30PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Holcomb located in Building TBA, Room None.
In Semester Fall 2023, from the department of Drama, the subject titled 'Production Management' with Course ID 54426 and Section NA offers 6-21 units. The Class meets Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor NA located in Building TBA, Room None.
Answer: "
"Carnegie Mellon University is home to how many members of the National Academy of Medicine (NAM)? 
","['handbook-msaii-2022-2023.txt', 'fact_sheet_d407.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Carnegie Mellon University is home to how many members of the National Academy of Medicine (NAM)? 

Context: 1.7 
Office of International Education (OIE) . 44 
14.1.8 
Veterans and Military Community . 45 
14.1.9 
Carnegie Mellon Ethics Hotline . 45 
14.2 
Key Offices for Academic & Research Support . 45 
14.2.1 
Computing and Information Resources . 46 
14.2.2 
Student Academic Success Center . 46 
14.2.3 
University Libraries . 48 
14.2.4 
Research at Carnegie Mellon . 48 
14.2.5 
Office of Research Integrity & Compliance . 48 
14.3 
Key Offices for Health, Wellness & Safety . 49 
14.3.1 
Counseling & Psychological Services . 49 
14.3.2 
Health Services . 49 
14.3.3 
Campus Wellness . 49 
14.3.4 
Religious and Spiritual Life Initiatives (RSLI) . 50 
14.3.5 
University Police . 50 
14.3.6 
Shuttle and Escort Services . 50 
14.4 
The WORD . 50 
 
 
 
6 
 
1 
Welcome 
Welcome to Carnegie Mellon University!  Being in the CMU School of Computer Science (SCS) 
is a unique experience because of the size of the school (over 2000 students, staff and faculty), 
the quality of its members, the variety of research being conducted, and unparalleled learning 
opportunities.  We encourage you to take advantage of CMU offerings, both inside and outside 
SCS, and also to explore Pittsburgh and southwest Pennsylvania to the extent your schedule 
permits.   What you will find is an environment that encourages and rewards hard work and 
accomplishment.  Pittsburgh has long been an industrial area whose residents build things.  In 
SCS we build things, and you will also.
Carnegie Mellon University has been a birthplace 
of innovation since its founding in 1900.
Today, CMU is a global leader bringing groundbreaking 
ideas to market and creating successful startup 
businesses. Our award-winning faculty are renowned for 
working closely with students to solve major scientific, 
technological and societal challenges. We put a strong 
emphasis on creating things — from art to robots. We 
have become a model for economic development in 
forming partnerships with companies such as Uber, 
Google and Disney. Our students are recruited by some 
of the world’s most innovative companies.
MANAGEMENT 
INFORMATION 
SYSTEMS
U.S. News & World Report, 2022
#1
ARTIFICIAL 
INTELLIGENCE
U.S. News & World Report, 2022
#1
AMONG U.S. 
UNIVERSITIES
Times Higher Education  
World University 
Ranking, 2023
#19
SOFTWARE 
ENGINEERING
U.S. News & World Report, 2022
COLLEGE OF 
ENGINEERING
U.S. News & World Report, 2022
#1
#4
#1
SCHOOL OF 
COMPUTER 
SCIENCE
U.S. News & World Report, 2022
UNIVERSITY  
IN THE WORLD
Times Higher Education  
World University  
Ranking, 2023
#28
TIME-BASED/ 
NEW MEDIA
U.S. News & World Report, 2019
#1
1,529  
FACULTY
16,779 
STUDENTS
Graduate
Undergraduate
117,257 
ALUMNI (LIVING)
85%  U.S.
15%  International
35%  U.S. 
65%  International
76%  U.S. 
24%  International
GLOBAL COMMUNITY
Students representing
126 countries
Faculty representing
56 countries
Alumni representing 
148 countries
OF COMPUTER 
SCIENCE’S FIRST-
YEAR STUDENTS  
WERE WOMEN  
IN 2019
Nearly triple the  
national average
“ My Heart is in the Work.” 
 
Andrew Carnegie, Founder 
 
November 15, 1900
49.8%
87%  U.S.
Answer: "
"What class room was advanced NLP taught last semester?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What class room was advanced NLP taught last semester?

Context: The Class meets Tuesday between 07:00PM and 08:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Lyu, Wang located in Building WEH, Room 5415.
In Semester Spring 2024, from the department of StuCo (Student Led Courses), the subject titled 'Student Taught Courses (StuCo): Every Day Carry & Community' with Course ID 98019 and Section A offers 3.0 units. The Class meets Wednesday between 07:00PM and 08:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor John located in Building WEH, Room 5304.
In Semester Spring 2024, from the department of StuCo (Student Led Courses), the subject titled 'Student Taught Courses (StuCo): Deconstructing the Barbie Cinematic Universe:' with Course ID 98020 and Section NA offers 3.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of StuCo (Student Led Courses), the subject titled 'StuCo: Deconstructing the Barbie Cinematic Universe' with Course ID 98020 and Section A offers 3.0 units. The Class meets Wednesday between 07:00PM and 08:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Thai-Tang located in Building GHC, Room 4211.
In Semester Spring 2024, from the department of StuCo (Student Led Courses), the subject titled 'Student Taught Courses (StuCo): NLP Ethics in a Nutshell' with Course ID 98031 and Section W3 offers 3.0 units. The Class meets Sunday Tuesday between 05:20PM and 06:10PM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Xiao located in Building CMB, Room 1031.
In Semester Spring 2024, from the department of StuCo (Student Led Courses), the subject titled 'Student Taught Courses (StuCo): Introduction to GO' with Course ID 98039 and Section A offers 3.0 units.
The Class meets Tuesday between 07:00PM and 08:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Lyu, Wang located in Building WEH, Room 5415.
In Semester Spring 2024, from the department of StuCo (Student Led Courses), the subject titled 'Student Taught Courses (StuCo): Every Day Carry & Community' with Course ID 98019 and Section A offers 3.0 units. The Class meets Wednesday between 07:00PM and 08:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor John located in Building WEH, Room 5304.
In Semester Spring 2024, from the department of StuCo (Student Led Courses), the subject titled 'Student Taught Courses (StuCo): Deconstructing the Barbie Cinematic Universe:' with Course ID 98020 and Section NA offers 3.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of StuCo (Student Led Courses), the subject titled 'StuCo: Deconstructing the Barbie Cinematic Universe' with Course ID 98020 and Section A offers 3.0 units. The Class meets Wednesday between 07:00PM and 08:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Thai-Tang located in Building GHC, Room 4211.
In Semester Spring 2024, from the department of StuCo (Student Led Courses), the subject titled 'Student Taught Courses (StuCo): NLP Ethics in a Nutshell' with Course ID 98031 and Section W3 offers 3.0 units. The Class meets Sunday Tuesday between 05:20PM and 06:10PM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Xiao located in Building CMB, Room 1031.
In Semester Spring 2024, from the department of StuCo (Student Led Courses), the subject titled 'Student Taught Courses (StuCo): Introduction to GO' with Course ID 98039 and Section A offers 3.0 units.
Answer: "
"Which LTI faculty member is an author on ""Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation""?
","['eric xing_ab70103b8cc85fd1cd52200aa134c58c7e9c0e03_metadata.txt', 'yang yiming_papers.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which LTI faculty member is an author on ""Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation""?

Context: Faculty Name: eric xing
Paperid: ab70103b8cc85fd1cd52200aa134c58c7e9c0e03
Title: FedNAR: Federated Optimization with Normalized Annealing Regularization
Year: 2023
Abstract: Weight decay is a standard technique to improve generalization performance in modern deep neural network optimization, and is also widely adopted in federated learning (FL) to prevent overfitting in local clients. In this paper, we first explore the choices of weight decay and identify that weight decay value appreciably influences the convergence of existing FL algorithms. While preventing overfitting is crucial, weight decay can introduce a different optimization goal towards the global objective, which is further amplified in FL due to multiple local updates and heterogeneous data distribution. To address this challenge, we develop {\it Federated optimization with Normalized Annealing Regularization} (FedNAR), a simple yet effective and versatile algorithmic plug-in that can be seamlessly integrated into any existing FL algorithms. Essentially, we regulate the magnitude of each update by performing co-clipping of the gradient and weight decay. We provide a comprehensive theoretical analysis of FedNAR's convergence rate and conduct extensive experiments on both vision and language datasets with different backbone federated optimization algorithms. Our experimental results consistently demonstrate that incorporating FedNAR into existing FL algorithms leads to accelerated convergence and heightened model accuracy. Moreover, FedNAR exhibits resilience in the face of various hyperparameter configurations. Specifically, FedNAR has the ability to self-adjust the weight decay when the initial specification is not optimal, while the accuracy of traditional FL algorithms would markedly decline. Our codes are released at \href{https://github.com/ljb121002/fednar}{https://github.com/ljb121002/fednar}.
Authors: Junbo Li, Ang Li, Chong Tian, Qirong Ho, Eric P. Xing, Hongyi Wang
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper develops FedNAR, a simple yet effective and versatile algorithmic plug-in that can be seamlessly integrated into any existing FL algorithms and has the ability to self-adjust the weight decay when the initial specification is not optimal, while the accuracy of traditional FL algorithms would markedly decline.'}
Url: https://arxiv.org/pdf/2310.03163
List of 2023 Open Access papers by yang yiming are:
Functional targeted therapy for glioma based on platelet membrane-coated nanogels
Dual Responsive Magnetic Drug Delivery Nanomicelles with Tumor Targeting for Enhanced Cancer Chemo/Magnetothermal Synergistic Therapy
Expression of ALCAM in Clinical Colon Cancer and Relationship With Patients’ Treatment Responses
Claudin-10 in the Blood Brain Barrier Function of Cerebral Endothelial Cells and Transendothelial Invasion of Breast Cancer Cells
Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation
Automatic synchrotron tomographic alignment schemes based on genetic algorithms and human-in-the-loop software
High CD8+tumor-infiltrating lymphocytes indicate severe exhaustion and poor prognosis in angioimmunoblastic T-cell lymphoma
Numerical Investigation of Fatigue Crack Propagation Behaviour of 550E High-Performance Steel
Association between albumin-to-globulin ratio and the risk of overall survival in advanced non-small cell lung cancer patients with anlotinib treatment: a retrospective cohort study
Analysis of Volatile Components in Dried Fruits and Branch Exudates of Schisandra chinensis with Different Fruit Colors Using GC-IMS Technology
Secreted endogenous macrosomes reduce Aβ burden and ameliorate Alzheimer’s disease
DIFUSCO: Graph-based Diffusion Solvers for Combinatorial Optimization
Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs
Aligning Large Multimodal Models with Factually Augmented RLHF
An Experimental Study on Secondary Transfer Performances of Prestress after Anchoring Failure of Steel Wire Strands
MRI Features for Predicting Microvascular Invasion and Postoperative Recurrence in Hepatocellular Carcinoma Without Peritumoral Hypointensity
Impact of local governments’ construction land allocation strategies on innovation-driven development of China
16p11.2 CNV gene Doc2α functions in neurodevelopment and social behaviors through interaction with Secretagogin.
Chinese EFL learners different from English natives in cataphora resolution: Evidence from eye-tracking studies
Strain-driven Kovacs-like memory effect in glasses
The Relationship between the Serum NLRP1 Level and Coronary Lesions in Patients with Coronary Artery Disease
Matrine induces ferroptosis in cervical cancer through activation of piezo1 channel.
Answer: "
"At what conference was ""Fully Unsupervised Topic Clustering of Unlabelled Spoken Audio Using Self-Supervised Representation Learning and Topic Model"" published?
","['shinji watanabe_debb65ab30ceef2faef0e4af560a67f2abd03d14_metadata.txt', 'shinji watanabe_1dc2d0f43df7f7a7847817203411357eca79a5b3_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: At what conference was ""Fully Unsupervised Topic Clustering of Unlabelled Spoken Audio Using Self-Supervised Representation Learning and Topic Model"" published?

Context: Faculty Name: shinji watanabe
Paperid: debb65ab30ceef2faef0e4af560a67f2abd03d14
Title: Fully Unsupervised Topic Clustering of Unlabelled Spoken Audio Using Self-Supervised Representation Learning and Topic Model
Year: 2023
Abstract: Unsupervised topic clustering of spoken audio is an important research topic for zero-resourced unwritten languages. A classical approach is to find a set of spoken terms from only the audio based on dynamic time warping or generative modeling (e.g., hidden Markov model), and apply a topic model to classify topics. The spoken term discovery is the most important and difficult part. In this paper, we propose to combine self-supervised representation learning (SSRL) methods as a component of spoken term discovery and probabilistic topic models. Most SSRL methods pre-train a model which predicts high-quality pseudo labels generated from an audio-only corpus. These pseudo labels can be used to produce a sequence of pseudo subwords by applying deduplication and a subword model. Then, we apply a topic model based on latent Dirichlet allocation for these pseudo-subword sequences in an unsupervised manner. The clustering performance is evaluated on the Fisher corpus using normalized mutual information. We confirm the improvement of the proposed method and its effectiveness compared to an existing approach using dynamic time warping and topic models although the experimental setups are not directly comparable.
Authors: Takashi Maekaku, Yuya Fujita, Xuankai Chang, Shinji Watanabe
Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes to combine self-supervised representation learning (SSRL) methods as a component of spoken term discovery and probabilistic topic models and confirms the improvement of the proposed method and its effectiveness compared to an existing approach using dynamic time warping and topic models.'}
Url: N/A
6, pp. 1179–1210, 2022. [9] S. Schneider et al., “wav2vec: Unsupervised Pre-Training for Speech Recognition,” in Proc. Interspeech, 2019, pp. 3465– 3469. [10] A. Baevski et al., “wav2vec 2.0: A framework for selfsupervised learning of speech representations,” in Proc. NeurIPS, H. Larochelle et al., Eds., vol. 33, 2020, pp. 12 449– 12 460. [11] W.-N. Hsu et al., “HuBERT: Self-supervised speech representation learning by masked prediction of hidden units,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451–3460, 2021. [12] S. Chen et al., “WavLM: Large-scale self-supervised pretraining for full stack speech processing,” IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505– 1518, 2022. [13] C. Wang et al., “Improving self-supervised learning for speech recognition with intermediate layer supervision,” in Proc. ICASSP, 2022, pp. 7092–7096. [14] M. Caron et al., “Deep clustering for unsupervised learning of visual features,” in Proceedings of the European conference on computer vision (ECCV), 2018, pp. 132–149. [15] J. Devlin et al., “BERT: Pre-training of deep bidirectional transformers for language understanding,” in Proc. ACL, 2019, pp. 4171–4186. [16] H.-J. Chang, S.-w. Yang, and H.-y. Lee, “DistilHuBERT: Speech representation learning by layer-wise distillation of hidden-unit BERT,” in Proc. ICASSP, 2022. [17] Y. Lee et al., “FitHuBERT: Going Thinner and Deeper for Knowledge Distillation of Speech Self-Supervised Models,” in Proc. INTERSPEECH, 2022.
Answer: "
"What is the full name of the conference where the paper TAPLoss: A Temporal Acoustic Parameter Loss for Speech Enhancement, got published?
","['bhiksha raj_e146e5221c124d93f69516c5ae7e1b7b1822848e_content_2.txt', 'shinji watanabe_e146e5221c124d93f69516c5ae7e1b7b1822848e_content_2.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the full name of the conference where the paper TAPLoss: A Temporal Acoustic Parameter Loss for Speech Enhancement, got published?

Context: Title: TAPLOSS: A TEMPORAL ACOUSTIC PARAMETER LOSS FOR SPEECH ENHANCEMENT
Authors: Yunyang Zeng, Joseph Konan, Shuo Han, David Bick, Muqiao Yang, Anurag Kumar, Shinji Watanabe, Bhiksha Raj
Section: 6. REFERENCES
P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, “Audio set: An ontology and human-labeled dataset for audio events,” in Proc. ICASSP, IEEE, 2017, pp. 776–780. [30] P. C. Loizou, Speech Enhancement: Theory and Practice, 2nd. USA: CRC Press, Inc., 2013. [31] C. K. A. Reddy, V. Gopal, and R. Cutler, “DNSMOS P.835: A nonintrusive perceptual objective speech quality metric to evaluate noise suppressors,” in Proc. ICASSP, 2022, pp. 886–890. [32] P. Manocha, B. Xu, and A. Kumar, “NORESQA: A framework for speech quality assessment using non-matching references,” in ThirtyFifth Conference on Neural Information Processing Systems, 2021. [33] J. Towns, T. Cockerill, M. Dahan, I. Foster, K. Gaither, A. Grimshaw, V. Hazlewood, S. Lathrop, D. Lifka, G. D. Peterson, R. Roskies, J. R. Scott, and N. Wilkins-Diehr, “Xsede: Accelerating scientific discovery,” Computing in Science & Engineering, vol. 16, no. 5, pp. 62–74, Sep. 2014. [34] N. A. Nystrom, M. J. Levine, R. Z. Roskies, and J. R. Scott, “Bridges: A uniquely flexible hpc resource for new communities and data analytics,” in Proceedings of the 2015 XSEDE Conference: Scientific Advancements Enabled by Enhanced Cyberinfrastructure, 2015, pp. 1– 8.
Title: TAPLOSS: A TEMPORAL ACOUSTIC PARAMETER LOSS FOR SPEECH ENHANCEMENT
Authors: Yunyang Zeng, Joseph Konan, Shuo Han, David Bick, Muqiao Yang, Anurag Kumar, Shinji Watanabe, Bhiksha Raj
Section: 6. REFERENCES
P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, “Audio set: An ontology and human-labeled dataset for audio events,” in Proc. ICASSP, IEEE, 2017, pp. 776–780. [30] P. C. Loizou, Speech Enhancement: Theory and Practice, 2nd. USA: CRC Press, Inc., 2013. [31] C. K. A. Reddy, V. Gopal, and R. Cutler, “DNSMOS P.835: A nonintrusive perceptual objective speech quality metric to evaluate noise suppressors,” in Proc. ICASSP, 2022, pp. 886–890. [32] P. Manocha, B. Xu, and A. Kumar, “NORESQA: A framework for speech quality assessment using non-matching references,” in ThirtyFifth Conference on Neural Information Processing Systems, 2021. [33] J. Towns, T. Cockerill, M. Dahan, I. Foster, K. Gaither, A. Grimshaw, V. Hazlewood, S. Lathrop, D. Lifka, G. D. Peterson, R. Roskies, J. R. Scott, and N. Wilkins-Diehr, “Xsede: Accelerating scientific discovery,” Computing in Science & Engineering, vol. 16, no. 5, pp. 62–74, Sep. 2014. [34] N. A. Nystrom, M. J. Levine, R. Z. Roskies, and J. R. Scott, “Bridges: A uniquely flexible hpc resource for new communities and data analytics,” in Proceedings of the 2015 XSEDE Conference: Scientific Advancements Enabled by Enhanced Cyberinfrastructure, 2015, pp. 1– 8.
Answer: "
"Who used the first emoticon at CMU?
","['25things_d400.txt', '25things_d400.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who used the first emoticon at CMU?

Context: It was developed by future Turing Award winner and future SCS dean Raj Reddy along with his students. Their work on subsequent systems established many of the principles that still underlie speech recognition software. 5. Emoticons, 1982Sure, it was just a joke, but (for better or worse) it’s endured for more than three decades. CMU researcher Scott Fahlman created the emoticon to clear up misunderstandings on computer message boards. We’ve been looking at the world sideways ever since. :-) 6. Andrew project, 1982It was long the dream of computer scientists to put a workstation in every home and office, but no one had actually tried to accomplish it until researchers from Carnegie Mellon University and IBM launched the Andrew Project. Soon, every student, faculty member and employee had access to email, word processing, file-transfer services and graphics programs, and CMU was the most-wired campus in the world. 7. Autonomous robots, 1983Thanks to Red Whittaker (E'75,'79), robots moved off of the assembly lines and into places no human ever could go. His Robotic Reconnaissance Vehicle spent four years inspecting and cleaning up the contaminated reactor building at the crippled Three Mile Island nuclear plant. 8. User interfaces, 1983Why should humans adapt to fit computers? Shouldn’t computers adapt to fit humans? That was the attitude of CMU researchers, who applied design principles to computer science to develop better, easier-to-use interfaces. They called the new field “human-computer interaction.” 9. Machine translation, 1984Every “Star Trek” fan knows about the universal translator. Scientists in the Language Technologies Institute are moving those gadgets from science fiction to real life. Their pioneering systems include handheld, portable speech-to-speech translators, just like those depicted on the USS Enterprise. 10. Mach kernel, 1985In computer parlance, a “kernel” is the heart of an operating system, passing input and output requests to and from the processor. At the core of all modern Apple devices --- including iPhones, iPads and MacBooks --- is the Mach kernel, developed at CMU under the leadership of then-professor Rick Rashid. 11. Computer chess, 1990Could a computer play chess at the level of the world’s best players?
At the core of all modern Apple devices --- including iPhones, iPads and MacBooks --- is the Mach kernel, developed at CMU under the leadership of then-professor Rick Rashid. 11. Computer chess, 1990Could a computer play chess at the level of the world’s best players? For many years, it was considered the “holy grail” of artificial intelligence. Hitech, developed by CMU researcher Hans Berliner (CS’74), was the first computer to achieve grandmaster status. CMU alumni played key roles in developing “Deep Blue,” the IBM machine that beat human chess champion Garry Kasparov in 1997. 12. Java, 1991 As a CMU grad student, James Gosling (CS’83) worked on the Andrew project, which stressed interoperability between computers, whether they were Macs, IBMs or Unix machines. Those lessons served Gosling well when he developed Java, the first programming language able to run on almost any platform. 13. Email attachments, 1992Steve Jobs liked the email system built into CMU’s Andrew so much that he tried to hire Nathaniel Borenstein (CS’81,’85) and the rest of his team to create a similar program for Apple. Borenstein didn’t take the offer, but he did like Jobs’ idea about attaching documents to email. Borenstein went on to develop the MIME standard that’s used by all email programs to send photos and other files over the Internet. 14. Web search engines, 1994The World Wide Web was still in its toddler stage when CMU researcher Michael “Fuzzy” Mauldin (CS’83,’89) developed one of the first successful search engines, Lycos. It was the most-visited site on the Web by 1999. 15. Model checking, 1994CMU professor Edmund Clarke had long stressed the importance of verifying computer hardware and software through a formal problem-solving technique called “model checking.” In 1994, his arguments gained new weight with the discovery that Intel’s amazing new Pentium chip made errors on certain math problems. Clarke would go on to receive the Turing Award for his role in the development of model checking. 16.
Answer: "
"Who is the PI of CLAW Lab?
","['emma strubell_667ba2e8f1933b6c32e9672012526904b4c5dc31_content_1.txt', 'daniel fried_3e4085e5869f1b7959707a1e1d7d273b6057eb4e_content_3.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who is the PI of CLAW Lab?

Context: Title: Surveying (Dis)Parities and Concerns of Compute Hungry NLP Research
Authors: Ji-Ung Lee, Haritz Puerto, Betty van Aken, Yuki Arase, Jessica Zosa Forde, Leon Derczynski, Andreas Rücklé, Iryna Gurevych, Roy Schwartz, Emma Strubell, Jesse Dodge
Section: 5.1 Analysis
well as instructing reviewers than senior researchers (11– 15 years with a p-value of 0.089 and 16+ years with a p-value of 0.085). Job Sector. In terms of the job sector, we again find no significant differences with respect to reviewers asking for too expensive experiments (𝑄10) or critique being justified (𝑄12). Interestingly, respondents from small industry received fewer such requests (𝑄11) compared to post-docs (p-value = 0.024), PIs (p-value= 0.061), and larger industry (p-value = 0.087). The most concerning trend can be observed when comparing the different groups with respect to their lack of compute resources to 4https://2022.naacl.org/blog/ reproducibility-track/ reproduce experiments (𝑄13, Fig. 8); where we find significant differences and conduct pairwise analyses.5 In general, students suffered most, with a significant difference compared to the large industry sector with a p-value of 0.002 < 0.005 = 𝛼 (Bonferroni-corrected). We further find substantial differences between students and academic PIs (pvalue = 0.026) and between academic post-docs and large industry labs (p-value = 0.088). We find no substantial differences when it comes to actionable items for the *CL community (𝑄14– 𝑄17), indicating that implementing popular ideas would be welcomed by all groups. However, we find some differences when it comes to encouraging the release of models (𝑄18). For instance, Figure 9d shows that academic post-docs had a higher preference towards reviewers rewarding papers that promise to release models than academic PIs. Also, participants from small industry would prefer visual branding over awards in contrast to large industry.
Title: StarCoder: may the source be with you!
Authors: Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries
Section: 1 Introduction
AI documentation such as model cards (Mitchell et al., 2019); – We incorporate a new attribution tool into the VSCode demo that can help users detect and locate model generations that may have been copied from the training set. This is achieved through a two-step process that involves a lightweight membership check followed by a search over a BM25 index (Section 9); and – We have significantly improved the PII redaction pipeline by collecting a PII dataset containing 12,000 files with 22,950 annotated entities. We fine-tuned our own encoder model (StarEncoder) on this dataset, resulting in a robust PII detection model (Section 4).
Answer: "
"In the BiasX paper, how much do imperfect machine-generated explanations help in correctly identifying subtly (non-)toxic content?
","['maarten sap_85a5ffc509fa50c96b415e09ae87fb6e5f435b37_metadata.txt', 'maarten sap_85a5ffc509fa50c96b415e09ae87fb6e5f435b37_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the BiasX paper, how much do imperfect machine-generated explanations help in correctly identifying subtly (non-)toxic content?

Context: Faculty Name: maarten sap
Paperid: 85a5ffc509fa50c96b415e09ae87fb6e5f435b37
Title: BiasX: ""Thinking Slow"" in Toxic Content Moderation with Explanations of Implied Social Biases
Year: 2023
Abstract: Toxicity annotators and content moderators often default to mental shortcuts when making decisions. This can lead to subtle toxicity being missed, and seemingly toxic but harmless content being over-detected. We introduce BiasX, a framework that enhances content moderation setups with free-text explanations of statements' implied social biases, and explore its effectiveness through a large-scale crowdsourced user study. We show that indeed, participants substantially benefit from explanations for correctly identifying subtly (non-)toxic content. The quality of explanations is critical: imperfect machine-generated explanations (+2.4% on hard toxic examples) help less compared to expert-written human explanations (+7.2%). Our results showcase the promise of using free-text explanations to encourage more thoughtful toxicity moderation.
Authors: Yiming Zhang, Sravani Nanduri, Liwei Jiang, Tongshuang Wu, Maarten Sap
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': ""BiasX, a framework that enhances content moderation setups with free-text explanations of statements' implied social biases, is introduced and it is shown that indeed, participants substantially benefit from explanations for correctly identifying subtly (non-)toxic content.""}
Url: http://arxiv.org/pdf/2305.13589
Title: BIASX: “Thinking Slow” in Toxic Content Moderation with Explanations of Implied Social Biases
Authors: Yiming Zhang, Sravani Nanduri, Liwei Jiang, Tongshuang Wu, Maarten Sap
Section: 4 Results and Discussion
the correctness of explanations to the accuracy of participants.7 On the hard toxic set, 60% of model explanations are accurate, which leads to 56.4% worker accuracy, a -7.7% drop from the HUMAN-EXPL condition where workers always have access to correct explanations. Figure 4b shows an example where the model explains an implicitly toxic statement as harmless and misleads content moderators (39.8% in MODEL-EXPL vs. 55.4% in NO-EXPL). On a positive note, expert-written explanations still improve moderator performance over baselines, highlighting the potential of our framework with higher quality explanations and serving as a proof-of-concept of BIASX, while motivating future work to explore methods to generate higher-quality explanations using techniques such as chain-of-thought (Camburu et al., 2018; Wei et al., 2022) and self-consistency (Wang et al., 2023) prompting.
Answer: "
"What is Graham Neubig's job title?
","['graham neubig_9bce3661f01825ad56dc9d2b3d254fd9e3792360_metadata.txt', 'graham neubig_cc1705fe421c70d85254b557634bd4669fdd49b0_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is Graham Neubig's job title?

Context: Faculty Name: graham neubig
Paperid: 9bce3661f01825ad56dc9d2b3d254fd9e3792360
Title: Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach
Year: 2023
Abstract: Humans work together to solve common problems by having discussions, explaining, and agreeing or disagreeing with each other. Similarly, if a system can have discussions with humans when solving tasks, it can improve the system's performance and reliability. In previous research on explainability, it has only been possible for the system to make predictions and for humans to ask questions about them rather than having a mutual exchange of opinions. This research aims to create a dataset and computational framework for systems that discuss and refine their predictions through dialogue. Through experiments, we show that the proposed system can have beneficial discussions with humans improving the accuracy by up to 25 points in the natural language inference task.
Authors: Masahiro Kaneko, Graham Neubig, Naoaki Okazaki
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This research aims to create a dataset and computational framework for systems that discuss and refine their predictions through dialogue and shows that the proposed system can have beneficial discussions with humans improving the accuracy by up to 25 points in the natural language inference task.'}
Url: http://arxiv.org/pdf/2305.11789
Faculty Name: graham neubig
Paperid: cc1705fe421c70d85254b557634bd4669fdd49b0
Title: DataFinder: Scientific Dataset Recommendation from Natural Language Descriptions
Year: 2023
Abstract: Modern machine learning relies on datasets to develop and validate research ideas. Given the growth of publicly available data, finding the right dataset to use is increasingly difficult. Any research question imposes explicit and implicit constraints on how well a given dataset will enable researchers to answer this question, such as dataset size, modality, and domain. We operationalize the task of recommending datasets given a short natural language description of a research idea, to help people find relevant datasets for their needs. Dataset recommendation poses unique challenges as an information retrieval problem; datasets are hard to directly index for search and there are no corpora readily available for this task. To facilitate this task, we build the DataFinder Dataset which consists of a larger automatically-constructed training set (17.5K queries) and a smaller expert-annotated evaluation set (392 queries). Using this data, we compare various information retrieval algorithms on our test set and present a superior bi-encoder retriever for text-based dataset recommendation. This system, trained on the DataFinder Dataset, finds more relevant search results than existing third-party dataset search engines. To encourage progress on dataset recommendation, we release our dataset and models to the public.
Authors: Vijay Viswanathan, Luyu Gao, Tongshuang Sherry Wu, Pengfei Liu, Graham Neubig
Venue: Annual Meeting of the Association for Computational Linguistics
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work operationalizes the task of recommending datasets given a short natural language description of a research idea, to help people find relevant datasets for their needs, and builds the DataFinder Dataset, a larger automatically-constructed training set and a smaller expert-annotated evaluation set.'}
Url: http://arxiv.org/pdf/2305.16636
Answer: "
"In fall 2023, What is the title of course 05291?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, What is the title of course 05291?

Context: In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Applied Research Methods' with Course ID 05816 and Section A offers 12.0 units. The Class meets Monday Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Carvalho located in Building 3SC, Room 172.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'E-Learning Design Principles and Methods' with Course ID 05823 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Koedinger located in Building POS, Room A35.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Personalized Online Learning' with Course ID 05832 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 03:00PM and 04:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Aleven located in Building WEH, Room 5409.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Applied Machine Learning' with Course ID 05834 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rose located in Building SH, Room 105.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Interactive Data Science:' with Course ID 05839 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Interactive Data Science' with Course ID 05839 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 12:30PM and 01:50PM ET.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Applied Research Methods' with Course ID 05816 and Section A offers 12.0 units. The Class meets Monday Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Carvalho located in Building 3SC, Room 172.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'E-Learning Design Principles and Methods' with Course ID 05823 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Koedinger located in Building POS, Room A35.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Personalized Online Learning' with Course ID 05832 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 03:00PM and 04:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Aleven located in Building WEH, Room 5409.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Applied Machine Learning' with Course ID 05834 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rose located in Building SH, Room 105.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Interactive Data Science:' with Course ID 05839 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Interactive Data Science' with Course ID 05839 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 12:30PM and 01:50PM ET.
Answer: "
"How large are the trained models by the authors of SantaCoder paper?
","['daniel fried_1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a_metadata.txt', 'chenyan xiong_a57b90cfc2eab46b773e65240d4ff910f05f989e_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How large are the trained models by the authors of SantaCoder paper?

Context: Faculty Name: daniel fried
Paperid: 1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a
Title: SantaCoder: don't reach for the stars!
Year: 2023
Abstract: The BigCode project is an open-scientific collaboration working on the responsible development of large language models for code. This tech report describes the progress of the collaboration until December 2022, outlining the current state of the Personally Identifiable Information (PII) redaction pipeline, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data. We train 1.1B parameter models on the Java, JavaScript, and Python subsets of The Stack and evaluate them on the MultiPL-E text-to-code benchmark. We find that more aggressive filtering of near-duplicates can further boost performance and, surprisingly, that selecting files from repositories with 5+ GitHub stars deteriorates performance significantly. Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and infilling on the Java, JavaScript, and Python portions of MultiPL-E, despite being a substantially smaller model. All models are released under an OpenRAIL license at https://hf.co/bigcode.
Title: Structure-Aware Language Model Pretraining Improves Dense Retrieval on Structured Data
Authors: Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yu Gu, Zhiyuan Liu
Section: A Appendix
parsed are also filtered out, because the data processing details are not available4. During continuous pretraining, we set the batch size, learning rate and epoch as 128, 5e-5 and 10, respectively. During finetuning, we set the learning rate as 2e-5 and 1e-5 for CodeSearch and Adv, and set batch size and epoch as 128 and 12. We use inbatch negatives with one hard negative for finetuning and the hard negative is randomly sampled from the top 100 retrieved negative codes by pretrained SANTA. The warm-up ratio is 0.1. The performance of SANTA on CodeSearch and Adv is shown in Table 6. Under the zeroshot setting, SANTA still outperforms CodeRetriever (Li et al., 2022) with about 2% improvements, which shows that the advances of SANTA can be generalized to different structured data retrieval tasks. Moreover, SANTA also shows strong zero-shot ability by achieving comparable performance with the finetuned CodeBERT, GraphCodeBERT and CodeT5 models. After finetuning, SANTA achieves more than 3.7% improvements over CodeT5 on CodeSearch. All these encouraged experiment results further demonstrate that our structure-aware pretraining method indeed helps language models to capture the structure semantics behind the text data. The retrieval performance on Adv dataset illustrates that the retrieval effectiveness of SANTA can be further improved by increasing the batch size from 16 to 128. 3https://github.com/github/CodeSearchNet/blob/ master/notebooks/ExploreData.ipynb 4https://github.com/salesforce/CodeT5/issues/ 64
Answer: "
"What is David Garlan's two word title; this is not the one with the word professor in it?
","['handbook-msaii-2022-2023.txt', 'david mortensen_659be1ff350634f50cc066d258ee6a45e697e552_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is David Garlan's two word title; this is not the one with the word professor in it?

Context: 6 Academic Calendar 
The Academic Calendar can be found at https://www.cmu.edu/hub/calendar/index.html and 
provides information on all deadlines including registration dates, class start dates, add/drop 
deadlines, exam dates and more. 
7 School and Departmental Information 
The following are key personnel with whom you may interact during your time at Carnegie 
Mellon: 
 
Martial Hebert 
 
 
 
 
 
Dean, School of Computer Science 
9 
 
 
University Professor 
 
GHC 6105 
 
412-268-5704 
 
hebert@cs.cmu.edu  
https://www.cs.cmu.edu/~hebert/  
 
David Garlan 
Associate Dean for Master’s Programs, SCS 
Professor 
TCS 420 
garlan@cs.cmu.edu 
http://www.cs.cmu.edu/~garlan/academics.htm  
 
Carolyn Rosé  
 
 
 
 
Interim Director, Language Technologies Institute 
 
Professor 
 
GHC 5415 
 
412-268-7130 
 
cprose@cs.cmu.edu 
8 MSAII Policies 
The Language Technologies Institute (LTI) has prepared this statement of policies, program 
requirements, guidance, process and procedures for students in the M.S. in Artificial Intelligence 
and Innovation (MSAII) program. A copy of this handbook is also available online at the 
program website.   
The University Student guide, The Word, which details university-wide policies, is also 
available online at www.cmu.edu/student-affairs/theword.     
Additional resources specific to graduate students can be found at 
www.cmu.edu/graduate/policies/    
8.1 The MSAII Degree 
The M.S. in Artificial Intelligence and Innovation (MSAII) is a professional master's program 
offered by the Language Technologies Institute (LTI) in the School of Computer Science at 
Carnegie Mellon University. This program seeks to educate students in the innovative use of 
artificial intelligence to create practical solutions in a wide variety of application areas. As 
artificial intelligence matures, there is a great need for students who can envision, design, plan 
and deliver solutions that integrate AI technologies such as deep learning, natural language 
processing, robotics and big data analytics into new applications.  The program stresses both 
10 
 
intrapreneurship and entrepreneurship and encourages students to develop ideas for startups 
and for introduction into established companies.
Faculty Name: david mortensen
Paperid: 659be1ff350634f50cc066d258ee6a45e697e552
Title: SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing
Year: 2023
Abstract: In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.
Authors: Taiqi He, Lindia Tjuatja, Nathaniel R. Robinson, Shinji Watanabe, David R. Mortensen, Graham Neubig, L. Levin
Venue: Special Interest Group on Computational Morphology and Phonology Workshop
Tldr: {'model': 'tldr@v2.0.0', 'text': 'In this submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), approaches to data augmentation and modeling across seven low-resource languages are explored and token classification models are found to be the best performing.'}
Url: https://aclanthology.org/2023.sigmorphon-1.22.pdf
Answer: "
"In fall 2023, Who is the instructor for unit 02718?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, Who is the instructor for unit 02718?

Context: In Semester Fall 2023, from the department of Computational Biology, the subject titled 'M.S. Thesis Research' with Course ID 02700 and Section A offers 1-12 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Garrand located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'CPCB Course - Current Topics in Computational Biology' with Course ID 02701 and Section A offers 3.0 units. The Class meets Monday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ma located in Building WEH, Room 5403.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Biology Seminar' with Course ID 02702 and Section A offers 3.0 units. The Class meets Friday between 10:30AM and 12:00PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Carja located in Building OFF, Room PITT.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Methods for Biological Modeling and Simulation' with Course ID 02712 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Schwartz, Carja located in Building HH, Room B131.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Medicine' with Course ID 02718 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Wu located in Building WEH, Room 5403.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Laboratory Methods for Computational Biologists' with Course ID 02760 and Section A offers 9.0 units. The Class meets Tuesday between 11:00AM and 12:50PM ET.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'M.S. Thesis Research' with Course ID 02700 and Section A offers 1-12 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Garrand located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'CPCB Course - Current Topics in Computational Biology' with Course ID 02701 and Section A offers 3.0 units. The Class meets Monday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ma located in Building WEH, Room 5403.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Biology Seminar' with Course ID 02702 and Section A offers 3.0 units. The Class meets Friday between 10:30AM and 12:00PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Carja located in Building OFF, Room PITT.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Methods for Biological Modeling and Simulation' with Course ID 02712 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Schwartz, Carja located in Building HH, Room B131.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Medicine' with Course ID 02718 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Wu located in Building WEH, Room 5403.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Laboratory Methods for Computational Biologists' with Course ID 02760 and Section A offers 9.0 units. The Class meets Tuesday between 11:00AM and 12:50PM ET.
Answer: "
"What is the name of the proposed approach that extends pretrained transformer models to handle unlimited input lengths?
","['graham neubig_dbc368bc8b49347dd27679894524fa62f88492c9_metadata.txt', 'graham neubig_dbc368bc8b49347dd27679894524fa62f88492c9_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the name of the proposed approach that extends pretrained transformer models to handle unlimited input lengths?

Context: Faculty Name: graham neubig
Paperid: dbc368bc8b49347dd27679894524fa62f88492c9
Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input
Year: 2023
Abstract: Since the proposal of transformers, these models have been limited to bounded input lengths, because of their need to attend to every token in the input. In this work, we propose Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores. This kNN index can be kept on either the GPU or CPU memory and queried in sub-linear time; this way, we can index practically unlimited input sequences, while every attention head in every decoder layer retrieves its top-k keys, instead of attending to every key. We evaluate Unlimiformer on several long-document and book-summarization benchmarks, showing that it can process even 500k token-long inputs from the BookSum dataset, without any input truncation at test time. We demonstrate that Unlimiformer improves pretrained models such as BART and Longformer by extending them to unlimited inputs without additional learned weights and without modifying their code. We make our code and models publicly available at https://github.com/abertsch72/unlimiformer .
Authors: Amanda Bertsch, Uri Alon, Graham Neubig, Matthew R. Gormley
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work proposes Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores.'}
Url: http://arxiv.org/pdf/2305.01625
Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input
Authors: Amanda Bertsch, Uri Alon, Graham Neubig, Matthew R. Gormley
Section: G Additional Related Work
Long-document summarization Prior work has proposed several strategies for long-document summarization. In particular, many methods select a subsection of input to summarize using TF-IDF (Liu* et al., 2018), smaller retriever models (Liu and Lapata, 2019), or sentence similarity metrics (Bajaj et al., 2021). An orthogonal approach is to summarize chunks of the input, then combine and condense these sub-summaries into a global summary, either using vanilla transformer models (Kryściński et al. (2021), Zhang et al. (2022), (Zhang et al., 2021)) or a specialized architecture (Liu and Lapata (2019), Grail et al. (2021)). Other work has focused on expanding the amount of text that can be processed, by applying long-context transformers or developing new long-context methods (Huang et al., 2021). However, these methods all suffer from cascading errors: if the initial trimming or chunk summarization steps remove important information, there is no way to recover that information in the downstream summary. Retrieval-augmented transformers Interpolating language model probabilities with nearest neighbors retrieval from an external datastore was originally proposed by Khandelwal et al. (2019). Additional work in this space has improved the selection of neighbors (Drozdov et al., 2022) or added structure to the datastore (Alon et al., 2022). Despite the shared use of retrieval, all these works retrieve from an external datastore, while Unlimiformer retrieves from a single input example, independently from external cumbersome sources. Borgeaud et al. (2022) incorporate retrieval from the external datastore into the architecture, which requires pretaining the model from scratch; in contrast, Unlimiformer leverages any already-pretrained model, and thus can be applied to future models as well. Other efficient processing methods Outside of retrieval, many other works have attempted to combine inputs encoded across multiple context windows to process long inputs.
Answer: "
"In fall 2024, What is the deadline for Mini-2 drop and withdrawal grade assigned after this date?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-2425.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2024, What is the deadline for Mini-2 drop and withdrawal grade assigned after this date?

Context: On Monday, 02 September, 2024, during the Fall 2024 (F24) semester, Labor Day is observed, leading to No Classes & University Closed.
On Monday, 09 September, 2024, during the Fall 2024 (F24) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 18 September, 2024, during the Fall 2024 (F24) semester, Mini-1 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 30 September, 2024, during the Fall 2024 (F24) semester, Mini-1 Pass/no pass & withdrawal deadline (3) is observed.
On Monday, 30 September, 2024, during the Fall 2024 (F24) semester, Mini-1 Faculty Course Evaluations open is observed.
On Monday, 07 October, 2024, during the Fall 2024 (F24) semester, Semester drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 11 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Last Day of Classes is observed.
On Friday, 11 October, 2024, during the Fall 2024 (F24) semester, Mini-1 voucher election deadline (4) is observed.
On Saturday, 12 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Exams is observed.
On Saturday, 12 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Faculty Course Evaluations close is observed.
From 14 October, 2024,Monday to 18 October, 2023,Friday marks the Fall Break for Fall 2024 (F24) semester which leads to No Classes.
On Monday, 21 October, 2024, during the Fall 2024 (F24) semester, Mini-2 Classes Begin is observed.
On Wednesday, 23 October, 2024, during the Fall 2024 (F24) semester, Mid-Semester & Mini-1 grades due by 4 pm is observed.
On Friday, 25 October, 2024, during the Fall 2024 (F24) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
Day: 'Friday', Event: 'Mini-1 voucher election deadline (4)', Semester: 'Fall 2024 (F24)'
Date: '2024-10-12', Day: 'Saturday', Event: 'Mini-1 Exams', Semester: 'Fall 2024 (F24)'
Date: '2024-10-12', Day: 'Saturday', Event: 'Mini-1 Faculty Course Evaluations close', Semester: 'Fall 2024 (F24)'
Start Date: '2024-10-14', End Date: '2023-10-18', Days: 'Monday to Friday', Event: 'Fall Break; No Classes', Semester: 'Fall 2024 (F24)'
Date: '2024-10-21', Day: 'Monday', Event: 'Mini-2 Classes Begin ', Semester: 'Fall 2024 (F24)'
Date: '2024-10-23', Day: 'Wednesday', Event: 'Mid-Semester & Mini-1 grades due by 4 pm', Semester: 'Fall 2024 (F24)'
Date: '2024-10-25', Day: 'Friday', Event: 'Mini-2 add, audit & tuition adjustment drop deadline (1)', Semester: 'Fall 2024 (F24)'
Date: '2024-11-05', Day: 'Tuesday', Event: 'Democracy Day; No Classes, except Evening classes after 5 pm will still meet', Semester: 'Fall 2024 (F24)'
Date: '2024-11-11', Day: 'Monday', Event: 'Semester pass/no pass & withdrawal deadline (3)', Semester: 'Fall 2024 (F24)'
Date: '2024-11-13', Day: 'Wednesday', Event: 'Mini-2 drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Fall 2024 (F24)'
Start Date: '2024-11-18', End Date: '2024-11-22', Days: 'Monday to Friday', Event: 'Spring 2025 Registration Week', Semester: 'Fall 2024 (F24)'
Date: '2024-11-25', Day: 'Monday', Event: 'Mini-2 pass/no pass & withdrawal deadline (3)', Semester: 'Fall 2024 (F24)'
Date: '2024-11-25', Day: 'Monday',
Answer: "
"When was aluminum first used to build buggies?
","['cmubuggy_d403.txt', 'cmubuggy_d403.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When was aluminum first used to build buggies?

Context: This year's buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It's one of the few times spectators can see buggies up close. Dave Singh explains how Fringe builds its buggies. Each team keeps its actual processes a secret, but each buggy has certain features such as a body, pushbar, wheels and driving and braking mechanisms. Behind Singh are other buggies built by Fringe. Behind the Wheel Drivers, have the closest connection to the vehicles, aside from the mechanics who spent countless hours building and maintaining individual buggies. While races are brief, each driver must log a number of practice runs to qualify to race. Teams practice nights and weekends throughout the fall and spring semesters as weather permits. Buggies are often built around drivers, so the fit can be snug, and drivers are often less than 5 feet, 3 inches in height. Most — but not all — are women. Tishya Girdhar, a junior in neuroscience, is social chair for Fringe. She came to CMU wanting to drive, but first she had to get over being claustrophobic. She started as a mechanic and got behind the wheel for last year's Sweepstakes. ""Our team's philosophy is to teach everyone how to do everything,"" Girdhar said. ""But I came to CMU wanting to drive. I wanted to drive so badly."" Amy Chen demonstrates how a driver is positioned inside of Boson, a Fringe buggy built in 2016. Safety is a top priority for teams. All drivers are required to have five pieces of safety gear: mouth guard, goggles, a harness that includes three points of contact to the body of the buggy, gloves and a helmet. And during practice runs and races, flaggers man the course to let drivers know if it's safe to proceed. The streets of the race course haven't changed, but the condition of the course varies due to potholes, which can make or break a fast run. The last course records were set in 2017, but this year potholes are making for some challenging conditions, said Amy Chen, a senior in psychology who has been driving for four years.
Buggy Races Keep Rolling at Carnegie Mellon - News - Carnegie Mellon University Carnegie Mellon University ——— Search Search Search this site only News News › Stories › Archives › 2019 › April › Buggy Races Keep Rolling at Carnegie Mellon April 10, 2019 Buggy Races Keep Rolling at Carnegie Mellon In its 99th year, the tradition is a Spring Carnival treat By Heidi Opdyke opdyke(through)andrew.cmu.edu Media Inquiries Julie Mattera Marketing and Communications jmattera(through)cmu.edu 412-268-2902 Sweepstakes, also known as the Buggy Races, has come a long way at Carnegie Mellon University. The slick, torpedo-like vessels carrying drivers with nerves of steel are a far cry from the two-man teams that once changed places halfway through a race and rode in everything from rain barrels with bicycle wheels to three-wheeled ash cans 99 years ago. Today, it takes six people to maneuver the .84 -mile course around Schenley Park's Flagstaff Hill. But while five pushers and a driver navigate the course's hills, dozens of people are needed to make a successful race happen. A year of planning goes into just over two minutes of racing. The Machine The basics of a buggy are straightforward, but teams are often secretive in how they build the machines, in particular the way they brake, steer and what types of wheels are used. Each has a body, pushbar for runners to move the machine up the hills, wheels, a safety harness and driving and braking mechanisms. Some also include fairings, a type of housing around the wheels that help reduce drag, make the vehicle quieter and just looks cool. Fairings have been a key feature for the Fringe team in recent years, which is celebrating its 50th anniversary of Buggy. ""We have a reputation of being the quietest on the course,"" said Diya Nuxoll, who wrapped up her bachelor's degree in mechanical engineering in December and is working on an advanced studies master's degree in design and manufacturing. Nuxoll leads the Fringe team as one of two head mechanics. An mural celebrating Buggy can be found in CMU's Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions. Sweepstakes Slang Buggy: Vehicle being raced and also a nickname for the competition.
Answer: "
"Where will the Phi Beta Kappa Initiation Ceremony (not Reception) be held on May 9?
","['Commencement.txt', 'Commencement.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Where will the Phi Beta Kappa Initiation Ceremony (not Reception) be held on May 9?

Context: Main Commencement Ceremony
Bachelor’s, master’s and doctoral degree candidates and their guests are invited to join the main commencement ceremony on Sunday, May 12, for the conferral of all degrees.
The main ceremony will include remarks from the president, keynote speaker, student speaker and academic deans, in addition to recognition of the honorary degree recipients.
There is no limit on number of guests who can attend the main commencement ceremony and tickets are not needed.
The ceremony will take place on CMU’s campus beginning at 10 a.m. and will be approximately 1.5 hours long. All guests must be seated by 9:15 a.m. for the start of the student procession. Access to guest seating will be restricted once the student procession begins.

Diploma Ceremonies
In addition to the main commencement ceremony, each college/school/department will host a diploma ceremony to recognize all graduating students. Diploma ceremonies will be held over the course of the weekend (Friday, May 10–Sunday, May 12).
Diploma ceremonies will include the presentation of diplomas to graduates, hooding of doctoral candidates and remarks from their college/school/department leadership. Each ceremony is organized and customized by their college/school/department.
Diploma ceremonies will take place both on and off campus. Locations and times for diploma ceremonies will be provided soon.
Diploma ceremonies typically include a reception and the length of time for each ceremony varies based on the number of graduates.
More details on the weekend schedule, including a diploma ceremony schedule, will be provided in the coming weeks. 

Thursday, May 9
Phi Beta Kappa Initiation Ceremony
Ceremony: 2–3 p.m.
McConomy Auditorium, Cohon University Center
Reception: 3–4 p.m.
Connan Room, Cohon University Center

Contact:
Joseph Devine
jd0x@andrew.cmu.edu 

Joanne Ursenbach
joanneu@andrew.cmu.edu 

President's Graduates Toast (bachelor's students)
3:30–4:30 p.m.
Location TBD
Registration required. Invitation, along with registration details, will be sent in late April.

First Gen Graduation Recognition
Reception: 5-5:30 p.m.
Alumni Concert Hall, College of Fine Arts
Ceremony: 5:30-6:30p.m.
Invitation, along with registration details, will be sent in late April.

First Gen Graduation Recognition
Reception: 5-5:30 p.m.
Alumni Concert Hall, College of Fine Arts
Ceremony: 5:30-6:30p.m.
Kresge Theater, College of Fine Arts

Contact:
M. Shernell Smith
mssmith@andrew.cmu.edu 

Sam Colavecchio
scolavec@andrew.cmu.edu 
412-268-7733


Friday, May 10
Diploma Ceremonies
Various times

Senior Leadership Recognition Ceremony
4–5:30 p.m.
Wiegand Gym, Cohon University Center
Undergraduate students and their guests by invitation only. This ceremony recognizes nominated seniors who have reflected upon their specific leadership contributions during their time at CMU.


Saturday, May 11
Diploma Ceremonies
Various times

Center for Student Diversity and Inclusion Ceremony
Reception: 12:30 p.m.
Program: 1-2:15 p.m.
Simmons Auditorium, Tepper Building

Contact:
M. Shernell Smith
mssmith@andrew.cmu.edu 

Rowshan Lang
rowshan@andrew.cmu.edu
Naval ROTC Commissioning
Ceremony: 1:30-2:30 p.m.
Auditorium, Soldiers & Sailors Memorial Hall & Museum *
4141 Fifth Avenue, Pittsburgh, PA 15213
Contact:
Mike Danko
mdanko@andrew.cmu.edu 

The President’s Reception in honor of CMU’s Doctoral Candidates
4–6 p.m.
Tepper Building Atrium


Sunday, May 12
Gesling Stadium opens to guests
8 a.m.

Robing and procession for graduates
9–10 a.m.
Various locations across campus

Student procession begins
9:15 a.m.
All guests in stadium must be seated. Access to guest seating will be restricted once the procession begins.

Commencement Ceremony
10–11:30 a.m.
Gesling Stadium, CMU’s campus

Diploma Ceremonies
Various times
Answer: "
"In fall 2024, What is the deadline for Mini-1 drop and withdrawal grade assigned after this date?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-2324.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2024, What is the deadline for Mini-1 drop and withdrawal grade assigned after this date?

Context: On Monday, 02 September, 2024, during the Fall 2024 (F24) semester, Labor Day is observed, leading to No Classes & University Closed.
On Monday, 09 September, 2024, during the Fall 2024 (F24) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 18 September, 2024, during the Fall 2024 (F24) semester, Mini-1 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 30 September, 2024, during the Fall 2024 (F24) semester, Mini-1 Pass/no pass & withdrawal deadline (3) is observed.
On Monday, 30 September, 2024, during the Fall 2024 (F24) semester, Mini-1 Faculty Course Evaluations open is observed.
On Monday, 07 October, 2024, during the Fall 2024 (F24) semester, Semester drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 11 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Last Day of Classes is observed.
On Friday, 11 October, 2024, during the Fall 2024 (F24) semester, Mini-1 voucher election deadline (4) is observed.
On Saturday, 12 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Exams is observed.
On Saturday, 12 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Faculty Course Evaluations close is observed.
From 14 October, 2024,Monday to 18 October, 2023,Friday marks the Fall Break for Fall 2024 (F24) semester which leads to No Classes.
On Monday, 21 October, 2024, during the Fall 2024 (F24) semester, Mini-2 Classes Begin is observed.
On Wednesday, 23 October, 2024, during the Fall 2024 (F24) semester, Mid-Semester & Mini-1 grades due by 4 pm is observed.
On Friday, 25 October, 2024, during the Fall 2024 (F24) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
Date: '2023-08-28', Day: 'Monday', Event: 'Semester & Mini-1 Classes Begin', Semester: 'Fall 2023 (F23)'
Date: '2023-09-01', Day: 'Friday', Event: 'Mini-1 add, audit & tuition adjustment drop deadline  (1)', Semester: 'Fall 2023 (F23)'
Date: '2023-09-04', Day: 'Monday', Event: 'Labor Day; No Classes & University Closed', Semester: 'Fall 2023 (F23)'
Date: '2023-09-11', Day: 'Monday', Event: 'Semester add, audit & tuition adjustment drop deadline (1)', Semester: 'Fall 2023 (F23)'
Date: '2023-09-20', Day: 'Wednesday', Event: 'Mini-1 drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Fall 2023 (F23)'
Date: '2023-10-02', Day: 'Monday', Event: 'Mini-1 Pass/no pass & withdrawal deadline (3)', Semester: 'Fall 2023 (F23)'
Date: '2023-10-02', Day: 'Monday', Event: 'Mini-1 Faculty Course Evaluations open', Semester: 'Fall 2023 (F23)'
Date: '2023-10-09', Day: 'Monday', Event: 'Semester drop deadline; withdrawal grade assigned after this date', Semester: 'Fall 2023 (F23)'
Date: '2023-10-13', Day: 'Friday', Event: 'Mini-1 Last Day of Classes', Semester: 'Fall 2023 (F23)'
Date: '2023-10-13', Day: 'Friday', Event: 'Mini-1 voucher election deadline (4)', Semester: 'Fall 2023 (F23)'
Start Date: '2023-10-13', End Date: '2023-10-14', Days: 'Friday to Saturday', Event: 'Family Weekend', Semester: 'Fall 2023 (F23)'
Date: '2023-10-14', Day: 'Saturday', Event: 'Mini-1 Exams', Semester: 'Fall 2023 (F23)'
Date: '2023-10-14', Day: 'Saturday', Event: 'Mini-1 Faculty Course Evaluations close',
Answer: "
"How much decrease in memory consumption (multi GPU setup) does SAMA showcase in large-scale meta learning benchmarks?
","['emma strubell_a815c3209e7baff4466dbf6e129129511f842b7e_metadata.txt', 'eric xing_a815c3209e7baff4466dbf6e129129511f842b7e_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How much decrease in memory consumption (multi GPU setup) does SAMA showcase in large-scale meta learning benchmarks?

Context: Faculty Name: emma strubell
Paperid: a815c3209e7baff4466dbf6e129129511f842b7e
Title: Making Scalable Meta Learning Practical
Year: 2023
Abstract: Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and 2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains.
Authors: Sang Keun Choe, Sanket Vaibhav Mehta, Hwijeen Ahn, W. Neiswanger, Pengtao Xie, Emma Strubell, Eric P. Xing
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients.'}
Url: https://arxiv.org/pdf/2310.05674
Faculty Name: eric xing
Paperid: a815c3209e7baff4466dbf6e129129511f842b7e
Title: Making Scalable Meta Learning Practical
Year: 2023
Abstract: Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and 2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains.
Authors: Sang Keun Choe, Sanket Vaibhav Mehta, Hwijeen Ahn, W. Neiswanger, Pengtao Xie, Emma Strubell, Eric P. Xing
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients.'}
Url: https://arxiv.org/pdf/2310.05674
Answer: "
"When does the Spring 2025 course registeration start for masters students?
","['Meta-Data-Calendar-2425.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When does the Spring 2025 course registeration start for masters students?

Context: Day: 'Monday', Event: 'Mini-3 Faculty Course Evaluations open', Semester: 'Spring 2025 (S25)'
Date: '2025-02-24', Day: 'Monday', Event: 'Semester course drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Spring 2025 (S25)'
Date: '2025-02-28', Day: 'Friday', Event: 'Mini-3 Last Day of Classes ', Semester: 'Spring 2025 (S25)'
Date: '2025-02-28', Day: 'Friday', Event: 'Mini-3 voucher deadline (4)', Semester: 'Spring 2025 (S25)'
Date: '2025-03-01', Day: 'Saturday', Event: 'Mini-3 Exams', Semester: 'Spring 2025 (S25)'
Date: '2025-03-01', Day: 'Saturday', Event: 'Mini-3 Faculty Course Evaluations close', Semester: 'Spring 2025 (S25)'
Start Date: '2025-03-03', End Date: '2023-03-07', Days: 'Monday to Friday', Event: 'Spring Break; No Classes', Semester: 'Spring 2025 (S25)'
Date: '2025-03-10', Day: 'Monday', Event: 'First day of Mini-4 Classes', Semester: 'Spring 2025 (S25)'
Date: '2025-03-11', Day: 'Tuesday', Event: 'Summer 2025 Registration Opens', Semester: 'Spring 2025 (S25)'
Date: '2025-03-12', Day: 'Wednesday', Event: 'Mid-Semester & Mini-3 grades due by 4 pm', Semester: 'Spring 2025 (S25)'
Date: '2025-03-14', Day: 'Friday', Event: 'Mini-4 add, audit & tuition adjustment drop deadline (1)', Semester: 'Spring 2025 (S25)'
Date: '2025-03-31', Day: 'Monday', Event: 'Semester pass/no pass & withdrawal deadline', Semester: 'Spring 2025 (S25)'
Date: '2025-04-02', Day: 'Wednesday', Event: 'Mini-4 course drop deadline; withdrawal grade assigned after this date (2)',
On Tuesday, 12 March, 2024, during the Spring 2024 (S24) semester, Summer 2024 Registration Opens is observed.
On Friday, 15 March, 2024, during the Spring 2024 (S24) semester, Mini-4 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 01 April, 2024, during the Spring 2024 (S24) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Wednesday, 03 April, 2024, during the Spring 2024 (S24) semester, Mini-4 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 11 April, 2024,Thursday to 13 April, 2024,Saturday marks the Spring Carnival for Spring 2024 (S24) semester which leads to No Classes.
From 15 April, 2024,Monday to 19 April, 2024,Friday marks the Fall 2024 Registration Week for Spring 2024 (S24) semester.
On Monday, 15 April, 2024, during the Spring 2024 (S24) semester, Mini-4 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 15 April, 2024, during the Spring 2024 (S24) semester, Semester & Mini-4 Faculty Course Evaluations open is observed.
On Friday, 26 April, 2024, during the Spring 2024 (S24) semester, Last Day of Classes is observed.
On Friday, 26 April, 2024, during the Spring 2024 (S24) semester, Semester & Mini-4 voucher deadline (4) is observed.
From 29 April, 2024,Monday to 30 April, 2024,Tuesday marks the Final Examinations for Spring 2024 (S24) semester.
On Wednesday, 01 May, 2024, during the Spring 2024 (S24) semester, Reading Day is observed.
From 02 May, 2024,Thursday to 03 May, 2024,Friday marks the Final Examinations for Spring 2024 (S24) semester.
From 04 May, 2024,Saturday to 05 May, 2024,Sunday marks the Reading Days for Spring 2024 (S24) semester.
Answer: "
"In fall 2023, What is the location of course 05317?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, What is the location of course 05317?

Context: In Semester Fall 2023, from the department of Institute for Politics and Strategy, the subject titled 'International Organizations and Law' with Course ID 84313 and Section A2 offers 6.0 units. The Class meets Monday between 02:30PM and 05:20PM ET. Students attend lectures at the Washington, District of Columbia:Dulles location,led by experienced instructor Grise located in Building MDC, Room 522.
In Semester Fall 2023, from the department of Institute for Politics and Strategy, the subject titled 'Military Strategy and Doctrine' with Course ID 84328 and Section A offers 9.0 units. The Class meets Monday Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Toukan located in Building POS, Room 146.
In Semester Fall 2023, from the department of Institute for Politics and Strategy, the subject titled 'Asian Strategies' with Course ID 84329 and Section A2 offers 6.0 units. The Class meets Monday Wednesday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Cronin located in Building WEH, Room 5320.
In Semester Fall 2023, from the department of Institute for Politics and Strategy, the subject titled 'US China Relations' with Course ID 84335 and Section A1 offers 6.0 units. The Class meets Wednesday between 05:30PM and 08:20PM ET. Students attend lectures at the Washington, District of Columbia:Dulles location,led by experienced instructor Hansen located in Building MDC, Room 522.
In Semester Fall 2023, from the department of Institute for Politics and Strategy, the subject titled 'Seminar in Public Policy Research' with Course ID 84339 and Section A offers 12.0 units. The Class meets Tuesday between 05:30PM and 08:20PM ET. Students attend lectures at the Washington, District of Columbia:Dulles location,led by experienced instructor Marcellino located in Building MDC, Room 522.
In Semester Fall 2023, from the department of Institute for Politics and Strategy, the subject titled 'Bias, Objectivity, and the Media's Role in Politics' with Course ID 84351 and Section A2 offers 6.0 units.
In Semester Fall 2023, from the department of Institute for Politics and Strategy, the subject titled 'International Organizations and Law' with Course ID 84313 and Section A2 offers 6.0 units. The Class meets Monday between 02:30PM and 05:20PM ET. Students attend lectures at the Washington, District of Columbia:Dulles location,led by experienced instructor Grise located in Building MDC, Room 522.
In Semester Fall 2023, from the department of Institute for Politics and Strategy, the subject titled 'Military Strategy and Doctrine' with Course ID 84328 and Section A offers 9.0 units. The Class meets Monday Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Toukan located in Building POS, Room 146.
In Semester Fall 2023, from the department of Institute for Politics and Strategy, the subject titled 'Asian Strategies' with Course ID 84329 and Section A2 offers 6.0 units. The Class meets Monday Wednesday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Cronin located in Building WEH, Room 5320.
In Semester Fall 2023, from the department of Institute for Politics and Strategy, the subject titled 'US China Relations' with Course ID 84335 and Section A1 offers 6.0 units. The Class meets Wednesday between 05:30PM and 08:20PM ET. Students attend lectures at the Washington, District of Columbia:Dulles location,led by experienced instructor Hansen located in Building MDC, Room 522.
In Semester Fall 2023, from the department of Institute for Politics and Strategy, the subject titled 'Seminar in Public Policy Research' with Course ID 84339 and Section A offers 12.0 units. The Class meets Tuesday between 05:30PM and 08:20PM ET. Students attend lectures at the Washington, District of Columbia:Dulles location,led by experienced instructor Marcellino located in Building MDC, Room 522.
In Semester Fall 2023, from the department of Institute for Politics and Strategy, the subject titled 'Bias, Objectivity, and the Media's Role in Politics' with Course ID 84351 and Section A2 offers 6.0 units.
Answer: "
"What is ValuePrism?
","['maarten sap_d655f652d02251b45db43181c5e3c73dfc59cd51_metadata.txt', 'maarten sap_d655f652d02251b45db43181c5e3c73dfc59cd51_content_5.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is ValuePrism?

Context: Faculty Name: maarten sap
Paperid: d655f652d02251b45db43181c5e3c73dfc59cd51
Title: Value Kaleidoscope: Engaging AI with Pluralistic Human Values, Rights, and Duties
Year: 2023
Abstract: Human values are crucial to human decision-making. Value pluralism is the view that multiple correct values may be held in tension with one another (e.g., when considering lying to a friend to protect their feelings, how does one balance honesty with friendship?). As statistical learners, AI systems fit to averages by default, washing out these potentially irreducible value conflicts. To improve AI systems to better reflect value pluralism, the first-order challenge is to explore the extent to which AI systems can model pluralistic human values, rights, and duties as well as their interaction. We introduce ValuePrism, a large-scale dataset of 218k values, rights, and duties connected to 31k human-written situations. ValuePrism's contextualized values are generated by GPT-4 and deemed high-quality by human annotators 91% of the time. We conduct a large-scale study with annotators across diverse social and demographic backgrounds to try to understand whose values are represented. With ValuePrism, we build Kaleido, an open, light-weight, and structured language-based multi-task model that generates, explains, and assesses the relevance and valence (i.e., support or oppose) of human values, rights, and duties within a specific context. Humans prefer the sets of values output by our system over the teacher GPT-4, finding them more accurate and with broader coverage. In addition, we demonstrate that Kaleido can help explain variability in human decision-making by outputting contrasting values. Finally, we show that Kaleido's representations transfer to other philosophical frameworks and datasets, confirming the benefit of an explicit, modular, and interpretable approach to value pluralism. We hope that our work will serve as a step to making more explicit the implicit values behind human decision-making and to steering AI systems to make decisions that are more in accordance with them.
Title: VALUE KALEIDOSCOPE : Engaging AI with Pluralistic Human Values, Rights, and Duties
Authors: Taylor Sorensen, Liwei Jiang, Jena D. Hwang, Valentina Pyatkin, Peter West, Nouha Dziri, Ximing Lu, Kavel Rao, Chandra Bhagavatula, Maarten Sap, John Tasioulas, Yejin Choi
Section: B Output Examples
, like all children, have the right to an education that promotes their growth and development. Showing a picture can be an opportunity to help them learn about the world around them. [supports] 133 134 Duties: 135 - Duty to respect the child’s abilities: Recognizing and respecting the child’s abilities and limitations, such as their visual limitations, is important in building a strong foundation for their future success. [either ] 136 - Duty to Educate: As a parent, you have a duty to educate your child , which may involve finding ways to make visually-impaired objects accessible. [supports] 137 ------------------------- 138 Giving your friend money for his birthday--> 139 Values: 140 - Friendship: Giving money to a friend on their birthday can strengthen the bond between the two individuals, demonstrating care and support. [supports] 141 - Reciprocity: If your friend has given you money in the past or if it is a tradition, you might feel a sense of duty to reciprocate the kindness by giving them money for their birthday. [supports] 142 - Autonomy: By giving your friend money, you are enabling them to make their own decisions and choices about how to spend the money. [supports] 143 - Happiness: Giving your friend money can bring happiness to your friend and possibly improve their well-being. [supports] 144 - Financial responsibility: Giving money to someone might not be financially responsible if they may use the money for harmful purposes. [either] 145 - Gratitude: If your friend has done something for you in the past, giving them money can be a way to show appreciation and gratitude. [supports] 146 147 Rights: 148 - Right to financial autonomy: Your friend has the right to manage their own finances and make decisions about how to spend the money you give them.
Answer: "
"What percentage of CMU's Computer Science first year students in 2019 were women?
","['fact_sheet_d407.txt', 'fact_sheet_d407.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What percentage of CMU's Computer Science first year students in 2019 were women?

Context: Carnegie Mellon University has been a birthplace 
of innovation since its founding in 1900.
Today, CMU is a global leader bringing groundbreaking 
ideas to market and creating successful startup 
businesses. Our award-winning faculty are renowned for 
working closely with students to solve major scientific, 
technological and societal challenges. We put a strong 
emphasis on creating things — from art to robots. We 
have become a model for economic development in 
forming partnerships with companies such as Uber, 
Google and Disney. Our students are recruited by some 
of the world’s most innovative companies.
MANAGEMENT 
INFORMATION 
SYSTEMS
U.S. News & World Report, 2022
#1
ARTIFICIAL 
INTELLIGENCE
U.S. News & World Report, 2022
#1
AMONG U.S. 
UNIVERSITIES
Times Higher Education  
World University 
Ranking, 2023
#19
SOFTWARE 
ENGINEERING
U.S. News & World Report, 2022
COLLEGE OF 
ENGINEERING
U.S. News & World Report, 2022
#1
#4
#1
SCHOOL OF 
COMPUTER 
SCIENCE
U.S. News & World Report, 2022
UNIVERSITY  
IN THE WORLD
Times Higher Education  
World University  
Ranking, 2023
#28
TIME-BASED/ 
NEW MEDIA
U.S. News & World Report, 2019
#1
1,529  
FACULTY
16,779 
STUDENTS
Graduate
Undergraduate
117,257 
ALUMNI (LIVING)
85%  U.S.
15%  International
35%  U.S. 
65%  International
76%  U.S. 
24%  International
GLOBAL COMMUNITY
Students representing
126 countries
Faculty representing
56 countries
Alumni representing 
148 countries
OF COMPUTER 
SCIENCE’S FIRST-
YEAR STUDENTS  
WERE WOMEN  
IN 2019
Nearly triple the  
national average
“ My Heart is in the Work.” 
 
Andrew Carnegie, Founder 
 
November 15, 1900
49.8%
87%  U.S.
13%  International
SCHOOLS AND COLLEGES
College of Engineering 
College of Fine Arts
Dietrich College of Humanities  
and Social Sciences
Heinz College of Information Systems 
and Public Policy
Mellon College of Science
School of Computer Science
Tepper School of Business
cmu.edu/assets/pdfs/cmu-fact-sheet.pdf    |   cmu.edu/about/awards.html
March 2020
BIG IDEAS THAT SHAPE YOUR WORLD START HERE
1,000+ companies greenlighted 
across the U.S. and internationally 
Cognitive tutors, which improve 
the math skills of 500,000+ 
students each year 
The first U.S. school to award  
a degree in drama in 1914
James Gosling (SCS 1983) 
invented the Java computer 
programming language.
The first exclusive higher 
education partner of the Tony 
Awards®, co-creating the first, 
national recognition program 
honoring K-12 theatre educators 
The first smile in an email, created 
in 1982 by professor Scott Fahlman
Kevlar Fiber, invented by alumna 
Stephanie Kwolek (MM 1946)
The Last Lecture, written  
by alumni Randy Pausch (SCS 1988)  
and Jeffrey Zaslow (DC 1980)
In 1919, CMU’s first Ph.D. was 
awarded to Mao Yisheng, father  
of Chinese bridge construction. 
Duolingo, invented by professor  
Luis von Ahn (CS 2003, 2005), had  
10 million downloads in 12 months 
and was named Apple’s 2013 app  
of the year.
:-)
LEADERSHIP
Farnam Jahanian  
President
David Coulter 
Chair, Board of Trustees
James H. Garrett Jr.
Answer: "
"In spring 2024, What is the deadline for Mini-3 pass/no pass and withdrawal?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, What is the deadline for Mini-3 pass/no pass and withdrawal?

Context: On Wednesday, 05 February, 2025, during the Spring 2025 (S25) semester, Mini-3 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 17 February, 2025, during the Spring 2025 (S25) semester, Mini-3 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 17 February, 2025, during the Spring 2025 (S25) semester, Mini-3 Faculty Course Evaluations open is observed.
On Monday, 24 February, 2025, during the Spring 2025 (S25) semester, Semester course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 28 February, 2025, during the Spring 2025 (S25) semester, Mini-3 Last Day of Classes is observed.
On Friday, 28 February, 2025, during the Spring 2025 (S25) semester, Mini-3 voucher deadline (4) is observed.
On Saturday, 01 March, 2025, during the Spring 2025 (S25) semester, Mini-3 Exams is observed.
On Saturday, 01 March, 2025, during the Spring 2025 (S25) semester, Mini-3 Faculty Course Evaluations close is observed.
From 03 March, 2025,Monday to 07 March, 2023,Friday marks the Spring Break for Spring 2025 (S25) semester which leads to No Classes.
On Monday, 10 March, 2025, during the Spring 2025 (S25) semester, First day of Mini-4 Classes is observed.
On Tuesday, 11 March, 2025, during the Spring 2025 (S25) semester, Summer 2025 Registration Opens is observed.
On Wednesday, 12 March, 2025, during the Spring 2025 (S25) semester, Mid-Semester & Mini-3 grades due by 4 pm is observed.
On Friday, 14 March, 2025, during the Spring 2025 (S25) semester, Mini-4 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 31 March, 2025, during the Spring 2025 (S25) semester, Semester pass/no pass & withdrawal deadline is observed.
On Monday, 22 January, 2024, during the Spring 2024 (S24) semester, Mini-3 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 29 January, 2024, during the Spring 2024 (S24) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Wednesday, 07 February, 2024, during the Spring 2024 (S24) semester, Mini-3 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 19 February, 2024, during the Spring 2024 (S24) semester, Mini-3 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 19 February, 2024, during the Spring 2024 (S24) semester, Mini-3 Faculty Course Evaluations open is observed.
On Monday, 26 February, 2024, during the Spring 2024 (S24) semester, Semester course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 01 March, 2024, during the Spring 2024 (S24) semester, Mini-3 Last Day of Classes is observed.
On Friday, 01 March, 2024, during the Spring 2024 (S24) semester, Mini-3 voucher deadline (4) is observed.
On Saturday, 02 March, 2024, during the Spring 2024 (S24) semester, Mini-3 Exams is observed.
On Saturday, 02 March, 2024, during the Spring 2024 (S24) semester, Mini-3 Faculty Course Evaluations close is observed.
From 04 March, 2024,Monday to 08 March, 2024,Friday marks the Spring Break for Spring 2024 (S24) semester which leads to No Classes.
On Monday, 11 March, 2024, during the Spring 2024 (S24) semester, First day of Mini-4 Classes is observed.
On Monday, 11 March, 2024, during the Spring 2024 (S24) semester, Mid-Semester & Mini-3 grades due by 4 pm is observed.
On Tuesday, 12 March, 2024, during the Spring 2024 (S24) semester, Summer 2024 Registration Opens is observed.
Answer: "
"What's the paper title for the paper that released a method called IPA?
","['bhiksha raj_22c9eb4868c5cabb26d132e0a160b9a093579f08_metadata.txt', 'bhiksha raj_4e6a004e4f9a3b489004a2efb5b25b0bcd0f48b5_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What's the paper title for the paper that released a method called IPA?

Context: Faculty Name: bhiksha raj
Paperid: 22c9eb4868c5cabb26d132e0a160b9a093579f08
Title: Understanding political polarization using language models: A dataset and method
Year: 2023
Abstract: Our paper aims to analyze political polarization in US political system using language models, and thereby help candidates make an informed decision. The availability of this information will help voters understand their candidates' views on the economy, healthcare, education, and other social issues. Our main contributions are a dataset extracted from Wikipedia that spans the past 120 years and a language model‐based method that helps analyze how polarized a candidate is. Our data are divided into two parts, background information and political information about a candidate, since our hypothesis is that the political views of a candidate should be based on reason and be independent of factors such as birthplace, alma mater, and so forth. We further split this data into four phases chronologically, to help understand if and how the polarization amongst candidates changes. This data has been cleaned to remove biases. To understand the polarization, we begin by showing results from some classical language models in Word2Vec and Doc2Vec. And then use more powerful techniques like the Longformer, a transformer‐based encoder, to assimilate more information and find the nearest neighbors of each candidate based on their political view and their background. The code and data for the project will be available here: “https://github.com/samirangode/Understanding_Polarization”
Authors: Samiran Gode, Supreeth Bare, B. Raj, H. Yoo
Venue: The AI Magazine
Tldr: {'model': 'tldr@v2.0.0', 'text': 'The main contributions are a dataset extracted from Wikipedia that spans the past 120 years and a language model‐based method that helps analyze how polarized a candidate is.'}
Url: https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/aaai.12104
Faculty Name: bhiksha raj
Paperid: 4e6a004e4f9a3b489004a2efb5b25b0bcd0f48b5
Title: Understanding Political Polarisation using Language Models: A dataset and method
Year: 2023
Abstract: Our paper aims to analyze political polarization in US political system using Language Models, and thereby help candidates make an informed decision. The availability of this information will help voters understand their candidates views on the economy, healthcare, education and other social issues. Our main contributions are a dataset extracted from Wikipedia that spans the past 120 years and a Language model based method that helps analyze how polarized a candidate is. Our data is divided into 2 parts, background information and political information about a candidate, since our hypothesis is that the political views of a candidate should be based on reason and be independent of factors such as birthplace, alma mater, etc. We further split this data into 4 phases chronologically, to help understand if and how the polarization amongst candidates changes. This data has been cleaned to remove biases. To understand the polarization we begin by showing results from some classical language models in Word2Vec and Doc2Vec. And then use more powerful techniques like the Longformer, a transformer based encoder, to assimilate more information and find the nearest neighbors of each candidate based on their political view and their background.
Authors: Samiran Gode, Supreeth Bare, B. Raj, H. Yoo
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A dataset extracted from Wikipedia that spans the past 120 years and a Language model based method that helps analyze how polarized a candidate is are used to understand the polarization.'}
Url: http://arxiv.org/pdf/2301.00891
Answer: "
"In the paper ""Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning"", on which tasks has IPA shown significant improvements?
","['sean welleck_papers.txt', 'chenyan xiong_24811cadf16519910f643b6084107164e6ca4219_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the paper ""Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning"", on which tasks has IPA shown significant improvements?

Context: List of 2023 Open Access papers by sean welleck are:
Self-Refine: Iterative Refinement with Self-Feedback
Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning
Faculty Name: chenyan xiong
Paperid: 24811cadf16519910f643b6084107164e6ca4219
Title: Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In
Year: 2023
Abstract: Retrieval augmentation can aid language models (LMs) in knowledge-intensive tasks by supplying them with external information. Prior works on retrieval augmentation usually jointly fine-tune the retriever and the LM, making them closely coupled. In this paper, we explore the scheme of generic retrieval plug-in: the retriever is to assist target LMs that may not be known beforehand or are unable to be fine-tuned together. To retrieve useful documents for unseen target LMs, we propose augmentation-adapted retriever (AAR), which learns LM’s preferences obtained from a known source LM. Experiments on the MMLU and PopQA datasets demonstrate that our AAR trained with a small source LM is able to significantly improve the zero-shot generalization of larger target LMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates that the preferences of different LMs overlap, enabling AAR trained with a single source LM to serve as a generic plug-in for various target LMs. Our code is open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.
Authors: Zichun Yu, Chenyan Xiong, S. Yu, Zhiyuan Liu
Venue: Annual Meeting of the Association for Computational Linguistics
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes augmentation-adapted retriever (AAR), which learns LM’s preferences obtained from a known source LM to assist target LMs that may not be known beforehand or are unable to be fine-tuned together in a generic retrieval plug-in.'}
Url: http://arxiv.org/pdf/2305.17331
Answer: "
"How many credits is 11824 worth?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many credits is 11824 worth?

Context: In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Quantitative Evaluation of Language Technologies:' with Course ID 11801 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Quantitative evaluation of language technologies' with Course ID 11801 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Diaz located in Building WEH, Room 5421.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Advanced Linguistics Seminar' with Course ID 11821 and Section A offers 6.0 units. The Class meets Monday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin located in Building PH, Room A19C.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'ConLanging: Lrng. Ling. & Lang Tech via Constru Artif. Lang.' with Course ID 11823 and Section A offers 12.0 units. The Class meets Monday Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin located in Building GHC, Room 4301.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Subword Modeling' with Course ID 11824 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Mortensen located in Building PH, Room 100.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Ethics, Social Biases, and Positive Impact in Language Technologies' with Course ID 11830 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Quantitative Evaluation of Language Technologies:' with Course ID 11801 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Quantitative evaluation of language technologies' with Course ID 11801 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Diaz located in Building WEH, Room 5421.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Advanced Linguistics Seminar' with Course ID 11821 and Section A offers 6.0 units. The Class meets Monday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin located in Building PH, Room A19C.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'ConLanging: Lrng. Ling. & Lang Tech via Constru Artif. Lang.' with Course ID 11823 and Section A offers 12.0 units. The Class meets Monday Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin located in Building GHC, Room 4301.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Subword Modeling' with Course ID 11824 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Mortensen located in Building PH, Room 100.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Ethics, Social Biases, and Positive Impact in Language Technologies' with Course ID 11830 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET.
Answer: "
"In fall 2023, What is the title of course 05391?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, What is the title of course 05391?

Context: In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Designing Human-Centered Software' with Course ID 05391 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Harrison located in Building 4SC, Room 104.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Designing Human Centered Software:' with Course ID 05391 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Designing Human-Centered Software' with Course ID 05391 and Section B offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Carrington located in Building 3SC, Room 172.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Designing Human Centered Software:' with Course ID 05391 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Designing Human-Centered Software' with Course ID 05391 and Section C offers 12.0 units. The Class meets Tuesday Thursday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Wu located in Building 3SC, Room 172.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Designing Human-Centered Software' with Course ID 05391 and Section D offers 12.0 units. The Class meets Tuesday Thursday between 08:00AM and 09:20AM ET.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Designing Human-Centered Software' with Course ID 05391 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Harrison located in Building 4SC, Room 104.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Designing Human Centered Software:' with Course ID 05391 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Designing Human-Centered Software' with Course ID 05391 and Section B offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Carrington located in Building 3SC, Room 172.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Designing Human Centered Software:' with Course ID 05391 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Designing Human-Centered Software' with Course ID 05391 and Section C offers 12.0 units. The Class meets Tuesday Thursday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Wu located in Building 3SC, Room 172.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Designing Human-Centered Software' with Course ID 05391 and Section D offers 12.0 units. The Class meets Tuesday Thursday between 08:00AM and 09:20AM ET.
Answer: "
"Who is the first author on ""Extracting Training Data from Diffusion Models""?
","['daphne ippolito_2e965b5d97c2d6fb4af284307735be39283792ba_content_1.txt', 'daphne ippolito_2e965b5d97c2d6fb4af284307735be39283792ba_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who is the first author on ""Extracting Training Data from Diffusion Models""?

Context: Title: Extracting Training Data from Diffusion Models
Authors: Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramèr, Borja Balle, Daphne Ippolito, Eric Wallace, Ann Graham Lotz
Section: 9 Discussion and Conclusion
each wrote the corresponding sections of the paper. • Jamie performed the membership inference attacks and inpainting attacks on CIFAR-10 diffusion models, and Nicholas performed the diffusion extraction experiments; each wrote the corresponding sections of the paper. • Matthew ran experiments for canary memorization and wrote the corresponding section of the paper. • Florian and Vikash performed preliminary experiments on memorization in GANs, and Milad and Vikash ran the experiments included in the paper. • Milad ran the membership inference experiments on GANs. • Vikash ran extraction experiments on pretrained GANs. • Daphne and Florian improved figure clarity and presentation. • Daphne, Borja, and Eric edited the paper and contributed to paper framing. • Nicholas organized the project and wrote the initial paper draft. Acknowledgements and Conflicts of Interest The authors are grateful to Tom Goldstein, Olivia Wiles, Katherine Lee, Austin Tarango, Ian Wilbur, Jeff Dean, Andreas Terzis, Robin Rombach, and Andreas Blattmann for comments on early drafts of this paper. Nicholas, Milad, Matthew, and Daphne are employed at Google, and Jamie and Borja are employed at DeepMind, companies that both train large machine learning models (including diffusion models) on both public and private datasets. Eric Wallace is supported by the Apple Scholars in AI/ML Fellowship.
Title: Extracting Training Data from Diffusion Models
Authors: Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramèr, Borja Balle, Daphne Ippolito, Eric Wallace, Ann Graham Lotz
Section: F Additional GAN Extraction Results
Figure 24 and Figure 25 contain additional examples extracted from GANs trained on CIFAR-10. 10https://github.com/POSTECH-CVLab/PyTorch-StudioGAN
Answer: "
"In fall 2023, Who is the instructor for course 05315?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, Who is the instructor for course 05315?

Context: In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Applied Research Methods' with Course ID 05816 and Section A offers 12.0 units. The Class meets Monday Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Carvalho located in Building 3SC, Room 172.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'E-Learning Design Principles and Methods' with Course ID 05823 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Koedinger located in Building POS, Room A35.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Personalized Online Learning' with Course ID 05832 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 03:00PM and 04:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Aleven located in Building WEH, Room 5409.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Applied Machine Learning' with Course ID 05834 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rose located in Building SH, Room 105.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Interactive Data Science:' with Course ID 05839 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Interactive Data Science' with Course ID 05839 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 12:30PM and 01:50PM ET.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Applied Research Methods' with Course ID 05816 and Section A offers 12.0 units. The Class meets Monday Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Carvalho located in Building 3SC, Room 172.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'E-Learning Design Principles and Methods' with Course ID 05823 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Koedinger located in Building POS, Room A35.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Personalized Online Learning' with Course ID 05832 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 03:00PM and 04:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Aleven located in Building WEH, Room 5409.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Applied Machine Learning' with Course ID 05834 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rose located in Building SH, Room 105.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Interactive Data Science:' with Course ID 05839 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Interactive Data Science' with Course ID 05839 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 12:30PM and 01:50PM ET.
Answer: "
"How much does it cost to apply for the MLT program if an application is submitted on the day before the deadline?
","['mlt-student-handbook-2023-2024.txt', 'mlt-student-handbook-2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How much does it cost to apply for the MLT program if an application is submitted on the day before the deadline?

Context: 5.1.4 Independent Study 
For an Independent Study to satisfy an MLT students coursework requirements, it must be 
approved by the MLT Program Director in advance.  In consultation with the CMU faculty 
member who will be supervising the Independent Study, the student should produce a short 
(half-page to one page) description of the goals of the Independent Study, and how the results 
will be evaluated, and submit it to the Program Director before the end of the Add period of the 
semester of the proposed study. The study should be planned so that it is finished in time for 
the supervising faculty member to give a grade at the end of the semester.  Independent 
studies may be undertaken for 6 or 12 units. Normally only one Independent Study would be 
approved during a students MLT coursework. 
5.1.5 Transfer Credit 
An equivalent graduate course previously completed at another institution may be permitted to 
satisfy one of the MLT course requirements. The decision on whether a course may be 
transferred is made by the MLT Program Director. Typically, the student will provide the 
Program Director with the syllabus of the external course, and the Program Director will use 
that and the students transcript to make the decision.  
See the section on Definition of transfer credit versus course exemption. 
All MLT students are required to take a minimum of 96 units of coursework at CMU.  
5.1.6 External Internships 
MLT students may only engage in internships if their advisors recommend it, since the program 
is 24 months, including summers. International students are required to consult with the Office 
of International Education for eligibility before seeking an internship/co-op or signing an offer 
contract. 
We caution all students to be aware of potential intellectual property (IP) problems with 
internships, and to review any IP agreements with their advisors before signing them. It is 
possible to lose ownership of your ideas. 
Important: See also section 4.2.3, External Employment/Consulting. 
MLT Graduate Student Handbook 
Page 22 
 
5.1.7 Transferring Into the MLT Program 
Transfers into the MLT program are not permitted during a students first semester at CMU. 
Students must begin their study at CMU in the program that admitted them; this is a university 
policy.
It is to the student's advantage to avoid switching advisors, especially late in 
their graduate studies, because forging a strong student-advisor relationship takes time. 
4.3 Optional Masters Thesis 
MLT students may optionally elect to complete a Masters Thesis; it is not required. Students 
who intend to apply for the PhD program should consider the MLT Thesis as one of several 
ways to demonstrate their research potential. Other possibilities include innovative research in 
a research project or class project work. 
The MLT Thesis must be completed according to the following guidelines. Note in particular the 
October 31 proposal deadline! 
 There are two course components required for students who elect to complete the M.S. 
Thesis: 11-928  Masters Thesis I (with the chosen thesis advisor, typically for 12 units, 
typically in the Fall of their second year) and 11-929  Masters Thesis II in the following 
Spring. 
 Each of these two courses counts towards 6 units of MLT-level LTI course credit; if the 
student goes on to an LTI PhD, the second course (11-929) counts towards one PhD lab 
requirement.   
 The MLT thesis may focus on either a significant research result or a significant 
implementation of a new technology. The student should work with their advisor to 
define an interesting but bounded thesis research project. The MLT thesis culminates in 
MLT Graduate Student Handbook 
Page 16 
 
a publication-quality paper that clearly describes the work in terms of its research 
significance. 
 The MLT thesis committee will be composed of a thesis advisor and two additional 
readers, to be chosen by the student in consultation with the advisor, during the Fall 
Masters Thesis I course. At least 2 of the 3 committee members must be SCS faculty 
working in Language Technologies. 
 To receive a final grade for the first semester's course, the student must have prepared 
a written proposal of at least five pages, describing the scientific thesis to be 
investigated, any relevant research already completed by the student, a comparison 
with related work by others, and a detailed description of the work to be done in the 
next semester. The proposal MUST be accepted by the committee by the end of 
October, or the student will not be allowed to enroll in 11-929  Masters Thesis II!
Answer: "
"How many credits is the MIIS-16 program?
","['miis-handbook_2023-2024.txt', 'miis-handbook_2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many credits is the MIIS-16 program?

Context: degree offered by the School of Computer Science without prior 
approval. (SCS policy) 
6.1.2 Duration of Study 
MIIS-16 students enrolled for full-time study are expected to complete the degree in three 
semesters of academic study and one summer internship (16 months total).  
MIIS-21 students enrolled for full-time study are expected to complete the degree in four 
semesters of academic study and one summer internship (21 months total).  
MIIS-16 students enrolled for part-time study are expected to complete the program in six 
semesters of academic study and one summer internship (27 months total).   
MIIS-21 students enrolled for part-time study are expected to complete the program in seven 
semesters of academic study and one summer internship (31 months total). 
See also the ‘Statute of Limitations’ policy. 
6.1.3 Deferral 
MIIS students are given the opportunity to defer their enrollment. The deferral request must 
be approved by the program director. The deferral can only be used once for the period of one 
academic year. 
6.1.4 Maximum and Minimum Course Loads 
A student in the program may not take more than sixty (60) units per semester without 
permission from the program director. 
A student in the program must be registered for a minimum thirty-six (36) units per semester 
to be considered a full-time student or twelve (12) units per semester to be considered a part-
time student. All international students are required by US Federal law to maintain full-time 
status. Reduced Course Load is not permitted for MIIS students. Failure to maintain full-time 
status will result in loss of a student visa (and, therefore, “permit of stay”). (MIIS policy) 
6.1.5 Transfer Credit 
An equivalent graduate course previously completed at Carnegie Mellon, or another 
institution may be permitted to satisfy one of the MIIS breadth requirements. The decision on 
whether a course may be used to satisfy a breadth requirement is made by the MIIS Program 
MIIS Graduate Student Handbook 
Page 25 
 
Director. Typically, the student will provide the Program Director with the syllabus of the 
external course, and the Program Director will use that and the student’s transcript to make 
the decision.
Under U.S. Federal Title IV regulations, 
student eligibility for federal financial aid is contingent upon enrollment in and successful 
completion of courses that are counted as credit toward their current degree program. To 
receive the maximum amount of federal financial aid for which they may be eligible, students 
must enroll each semester in at least 36 units that count toward their current degree level. (See 
separate guidance regarding integrated degree completion.) Students should consult with their 
designated college liaison in The HUB regarding billing and financial aid, particularly for early 
completion, longer-than standard completion, or integrated undergraduate and master’s degree 
programs.   
 
International Students 
Immigration status for students in F-1 and J-1 nonimmigrant status is tied to making normal 
progress toward completing degree requirements. Therefore, F-1 and J-1 students who are 
considering completing their degree requirements early, anticipating longer-than-standard 
completion, or moving from an undergraduate to a graduate student classification (integrated 
undergraduate-graduate study) should consult with their designated advisor in the Office of 
International Education (OIE) to ensure compliance with immigration regulations. 
MIIS Graduate Student Handbook 
Page 13 
 
4 MIIS Degree Requirements and Related Policies/Protocols 
4.1 Program Options 
The MIIS degree is offered in two options: 
Option 1. Standard MIIS degree (MIIS-16) - A 16-month track that is completed in three 
academic semesters (fall, spring, fall) and a summer internship.  
Option 2. MIIS: Advanced Study degree (MIIS-21) - A 21-month track that is completed in 
four academic semesters (fall, spring, fall, spring) and a summer internship. 
MIIS: Advanced Study track offers in depth degree in one of the following areas of 
concentration: 
• Human Language for Language Technologies 
• Language Technology Application 
• Machine Learning for Language Technologies 
Part-time options are available in some cases. 
4.2 Required Units for Degree Attainment 
To complete the Master of Science in Intelligent Information Systems, a student must satisfy 
three types of requirements. Curricular requirements ensure that MIIS students receive 
instruction in core intelligent information systems technologies while also allowing an 
opportunity to specialize in areas of personal interest. Practice requirements are opportunities 
to apply and hone new skills while building state-of-the-art systems.
Answer: "
"When was the first U.S. drama degree awarded at Carnegie Tech?
","['cmuhistory_d402.txt', 'cmuhistory_d402.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When was the first U.S. drama degree awarded at Carnegie Tech?

Context: The first U.S. drama degree was awarded in 1914 at Carnegie Tech. It began offering graduate degrees. In 1919, the first doctorate (in civil engineering) was awarded to Mao Yisheng, a student from China. It laid the groundwork for a research institution, recruiting leading scientists, offering sponsored fellowships with government and industry leaders and pioneering nontraditional interdisciplinary research, which brought together physicists, chemists and metallurgists, for example. Interdisciplinary research would become the hallmark of Carnegie Mellon research. It initiated the 'Carnegie Plan' in 1938, a new curriculum that required science and engineer students to take courses in humanities and social sciences in order to better understand the needs of society. Carnegie died in 1919, but his vision for an educated public lived on after him. Carnegie Tech - Post-war Years With the end of World War II, the latter half of the 20th century brought unprecedented growth to Carnegie Tech. In 1956, the arrival of the first IBM computer to campus was revolutionary, initiating a university culture where information technology pervaded virtually all areas of study. University culture also changed in 1973 when Margaret Morrison closed and women joined their male peers in classrooms and dorms. The times were changing, and Tech positioned itself at the forefront, opening three new schools: 1948: The Graduate School of Industrial Administration, later renamed the David A. Tepper School of Business, focusing on quantitative analysis and pioneering the field of management science. 1968: School of Urban and Public Affairs, later renamed the H. John Heinz III College, providing graduate training for work in the public sector. 1986: School of Computer Science, pioneering computing and artificial intelligence, led by interdisciplinary efforts of Allen Newell and Herbert Simon. Carnegie Mellon University In 1967, Carnegie Tech merged with the Mellon Institute, a science research center founded by the Mellon family of Pittsburgh, to become known as Carnegie Mellon University. The merger built upon a long history of support from the Mellons. It allowed Carnegie Mellon to establish the last of its current pillars: the Mellon College of Science and the College of Humanities and Social Sciences, now known as Marianna Brown Dietrich College of Humanities and Social Sciences.
History - CMU - Carnegie Mellon University Carnegie Mellon University ——— Search Search CMU › About › History Andrew Carnegie A self-educated ""working boy"" who loved books, Andrew Carnegie emigrated from Scotland in 1848 and settled in Pittsburgh, Pa. Attending night school and borrowing books, Carnegie went from factory worker in a textile mill to successful entrepreneur and industrialist. He rose to prominence by founding what became the world's largest steel producing company by the end of the 19th century. Carnegie Technical Schools At one point the richest man in the world, Carnegie believed that ""to die rich is to die disgraced."" He turned his attention to writing, social activism and philanthropy, determined to establish educational opportunities for the general public where few existed. In 1900, he donated $1 million for the creation of a technical institute for the city of Pittsburgh, envisioning a school where working-class men and women of Pittsburgh could learn practical skills, trades and crafts that would enhance their careers, lives and communities. ""My heart is in the work,"" he stated, which would become part of the school's official motto. The Carnegie Technical Schools offered two- and three-year certificates in the arts as well as in engineering disciplines and included a college for women, Margaret Morrison Carnegie College. Carnegie Tech – Early Years Soon faced with the demand for baccalaureate programs, Carnegie Technical Schools began offering bachelor's degrees through its College of Engineering and College of Fine Arts, becoming the Carnegie Institute of Technology, or ""Carnegie Tech."" During the first half of the 20th century, with support from Andrew Carnegie and other funders, Carnegie Tech laid the foundation for a school on the cutting edge. Some key developments were: It expanded from two buildings into an elegant 20th century campus designed in the beaux arts architectural style, housing a wealth of machine shops, studios and laboratories — the hands-on center of learning that persists today. It pioneered conservatory degree programs in music and drama, in addition to visual art and design programs. The first U.S. drama degree was awarded in 1914 at Carnegie Tech. It began offering graduate degrees. In 1919, the first doctorate (in civil engineering) was awarded to Mao Yisheng, a student from China.
Answer: "
"What can PhD students use LTI's computer cluster for?
","['miis-handbook_2023-2024.txt', 'mlt-student-handbook-2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What can PhD students use LTI's computer cluster for?

Context: 2.1.4 Computers for MS Students 
Students are expected to provide their own laptop computers that can be used to access 
university resources and complete course assignments.  Laptops running Windows, MacOS, 
and Linux software are all acceptable. 
Master’s students will be given a CS user id.  A CS user id is required to use the LTI computer 
cluster, and other SCS services.  The School of Computer Science has a Help Center located at 
MIIS Graduate Student Handbook 
Page 11 
 
GHC 4201.  They can be contacted at help@cs.cmu.edu, extension 8-4231 from a campus 
phone, or 412-268-4231 from an outside line. 
MS students will be given access to the LTI’s computer cluster on an as-needed basis, to be 
used for course assignments, directed study projects, and/or the capstone project.  The LTI 
cluster provides storage and computation for projects involving large datasets and/or lengthy 
computation.  
3 CMU Degree Completion and Certification 
3.1 Standard Degree Requirements & Degree Certification 
Carnegie Mellon graduate students are expected to complete their degree requirements within 
the standard length of time for their program of study as outlined in the relevant Graduate 
Student Handbook. Standard program lengths for graduate students vary significantly ranging 
from two semesters for some full-time master’s programs to several or more years for doctoral 
programs. Upon completion of the graduate program degree requirements, the degree will be 
certified by the student’s academic program in the semester in which the student completes 
the requirements. 
Early Completion 
Graduate students who consider the completion of all degree requirements in less than the 
standard length of time for their program of study may consult with their degree-granting 
program or department to determine if early degree certification is allowed and under what 
circumstances. 
 
Extended or Longer-than-Standard Completion 
Longer-than-standard degree completion may occur due to academic interruptions in making 
progress toward the degree as defined by the academic program, interruptions of full-time study 
or progress towards the degree due to serious, documented medical issues, or other unusual or 
unforeseen circumstances. 
 
Master’s students who require longer than the standard time to complete their degree 
requirements are expected to remain in close contact with their graduate program, and will be 
certified at the end of the semester in which they have completed their degree requirements.
LTIs printers 
are located in GHC 5404 and GHC 6604. The School of Computer Science provides a number of 
black-and-white and color printers for use by students.  The SCS Computer Facilities publishes a 
list of printers online at http://www.cs.cmu.edu/~help/printing/.  
 
MLT Graduate Student Handbook 
Page 11 
 
2.3 Office Space for MS Students 
To help them create a sense of community, full time students in the LTIs MLT program have 
access to a shared office space. 
2.4 Computers for MS Students 
Students are expected to provide their own laptop computers that can be used to access 
university resources and complete course assignments. Laptops running Windows, MacOS, and 
Linux software are all acceptable. 
MS students will be given a CS user ID. A CS user ID is required to use the LTI computer cluster, 
department printers, and other SCS services. The School of Computer Science has a Help Center 
located at 4203 GHC.  They can be contacted at help@cs.cmu.edu, extension 8-4231 from a 
campus phone, or 412-268-4231 from an outside line. 
MS students will be given access to the LTIs computer cluster on an as-needed basis, to be 
used for course assignments, directed study projects, and/or capstone projects. The LTI cluster 
provides storage and computation for projects involving large datasets and/or lengthy 
computation.  
3 Masters Degree Completion and Certification 
3.1 Standard Degree Requirements and Degree Certification 
3.1.1 Graduate Students 
Carnegie Mellon graduate students are expected to complete their degree requirements within 
the standard length of time for their program of study as outlined in the relevant Graduate 
Student Handbook. Standard program lengths for graduate students vary significantly-- ranging 
from two semesters for some full-time masters programs to several or more years for doctoral 
programs. Upon completion of the graduate program degree requirements, the degree will be 
certified by the students academic program in the semester in which the student completes 
the requirements. 
3.1.2 Early Completion 
Graduate students who consider the completion of all degree requirements in less than the 
standard length of time for their program of study may consult with their degree-granting 
program or department to determine if early degree certification is allowed and under what 
circumstances.
Answer: "
"Who is teaching the Advanced Topics in Multimodal Machine Learning in Spring 2024?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who is teaching the Advanced Topics in Multimodal Machine Learning in Spring 2024?

Context: The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Sap located in Building POS, Room 153.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Quantum Computing, Cryptography and Machine Learning Lab' with Course ID 11860 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Justice, Singh, Ramakrishnan located in Building DH, Room 1211.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Large Language Model Systems' with Course ID 11868 and Section A offers 12.0 units. The Class meets Monday Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Li located in Building POS, Room A35.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Advanced Topics in Multimodal Machine Learning' with Course ID 11877 and Section A offers VAR units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Liang, Fried located in Building WEH, Room 4709.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Neural Code Generation:' with Course ID 11891 and Section NA offers VAR units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Neural Code Generation' with Course ID 11891 and Section A offers VAR units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Fried, Welleck located in Building WEH, Room 4625.
The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Sap located in Building POS, Room 153.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Quantum Computing, Cryptography and Machine Learning Lab' with Course ID 11860 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Justice, Singh, Ramakrishnan located in Building DH, Room 1211.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Large Language Model Systems' with Course ID 11868 and Section A offers 12.0 units. The Class meets Monday Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Li located in Building POS, Room A35.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Advanced Topics in Multimodal Machine Learning' with Course ID 11877 and Section A offers VAR units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Liang, Fried located in Building WEH, Room 4709.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Neural Code Generation:' with Course ID 11891 and Section NA offers VAR units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Neural Code Generation' with Course ID 11891 and Section A offers VAR units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Fried, Welleck located in Building WEH, Room 4625.
Answer: "
"What language did Meloni et al (2021) achieve state-of-the-art results on for protoform reconstruction?
","['david mortensen_c5c6d006e399386c99068daba138021a62d6cc17_metadata.txt', 'david mortensen_c5c6d006e399386c99068daba138021a62d6cc17_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What language did Meloni et al (2021) achieve state-of-the-art results on for protoform reconstruction?

Context: Faculty Name: david mortensen
Paperid: c5c6d006e399386c99068daba138021a62d6cc17
Title: Transformed Protoform Reconstruction
Year: 2023
Abstract: Protoform reconstruction is the task of inferring what morphemes or words appeared like in the ancestral languages of a set of daughter languages. Meloni et al (2021) achieved the state-of-the-art on Latin protoform reconstruction with an RNN-based encoder-decoder with attention model. We update their model with the state-of-the-art seq2seq model: the Transformer. Our model outperforms their model on a suite of different metrics on two different datasets: their Romance data of 8,000 cognates spanning 5 languages and a Chinese dataset (Hou 2004) of 800+ cognates spanning 39 varieties. We also probe our model for potential phylogenetic signal contained in the model. Our code is publicly available at https://github.com/cmu-llab/acl-2023.
Authors: Young Min Kim, Kalvin Chang, Chenxuan Cui, David R. Mortensen
Venue: Annual Meeting of the Association for Computational Linguistics
Tldr: {'model': 'tldr@v2.0.0', 'text': 'The Meloni et al (2021) model is updated with the state-of-the-art seq2seq model: the Transformer, which outperforms their model on a suite of different metrics on two different datasets: their Romance data of 8,000 cognate spanning 5 languages and a Chinese dataset (Hou 2004) of 800+ cognates spanning 39 varieties.'}
Url: https://arxiv.org/pdf/2307.01896
Title: Transformed Protoform Reconstruction
Authors: Young Min Kim, Kalvin Chang, Chenxuan Cui, David Mortensen
Section: 7 Conclusion
espeak-ng’s Latin G2P, from which Meloni et al. (2021) obtain their phonetic Romance dataset. For most language families, protoforms are not attested. In fact, as the term is often used, protoform refers to a form that is inferred only through linguists’ comparative method. We adopt the other usage for simplicity. In practice, our approach would require reconstructions made by a linguist to serve as training labels for cognate sets. 5In the case of Chinese, only equivalence classes of pronunciations and not exact pronunciations are recorded.
Answer: "
"Carnegie Mellon University (CMU) Athletics Hall of Fame was established in which year?
","['tartanfacts_d404_metadata.txt', 'tartanfacts_d404.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Carnegie Mellon University (CMU) Athletics Hall of Fame was established in which year?

Context: {
  ""description"": ""p><strong>Who founded Carnegie Mellon University?<br>\n\n</strong> Carnegie Technical Schools was founded in 1900 by Andrew Carnegie. Twelve years late ..."",
  ""viewport"": ""width=device-width, initial-scale=1"",
  ""msapplication-TileColor"": ""#c41230"",
  ""msapplication-config"": ""/assets/favicons/browserconfig.xml"",
  ""theme-color"": ""#c41230"",
  ""fb:app_id"": ""280467664480"",
  ""og:locale"": ""en_US"",
  ""og:title"": ""Tartan Facts"",
  ""dcterms.title"": ""Tartan Facts"",
  ""og:description"": ""Tartan Facts"",
  ""dcterms.description"": ""Tartan Facts"",
  ""og:image"": ""https://athletics.cmu.edu/images/setup/thumbnail_default.jpg?max_width=1200&max_height=675"",
  "" og:image:alt"": ""Carnegie Mellon University Athletics thumbnail"",
  ""og:site_name"": ""Carnegie Mellon University Athletics"",
  ""og:url"": ""https://athletics.cmu.edu/athletics/tartanfacts"",
  ""dcterms.identifier"": ""https://athletics.cmu.edu/athletics/tartanfacts"",
  ""og:type"": ""article"",
  ""dcterms.type"": ""article""
}
Topic category is tartanfacts
In keeping with tradition, students, faculty, staff and alumni voted to name the new official mascot Scotty. But it's not just the costumed mascot that voters named. The live dog is also known as Scotty to the Carnegie Mellon community.
About Scotty
|
Celebrating Scotty
|
Performer Requests
Department Quick Facts
Name of School: Carnegie Mellon University
City/Zip: Pittsburgh, PA 15213
Founded: 1900
Enrollment (undergrad): 6,982- Fall 2021
Enrollment (graduate and doctoral): 7,062 - Fall 2021
Alumni: 102,577
Nickname: Tartans
Mascot: Scottie Dog
School Colors: Cardinal and Gray
Football Stadium and Track and Field: Gesling Stadium
Capacity: 3,500
Surface: FieldTurf
Basketball/Volleyball Gym: Wiegand Gymnasium
Capacity: 500
Soccer Stadium: CMU Soccer Field
Capacity: 250
Surface: FieldTurf
Affiliation: NCAA Division III
Conference: University Athletic Association | Presidents' Athletic Conference (Football Only)
President: Dr. Farnam Jahanian
Alma Mater, Year: University of Texas at Austin
Athletic Director: Dr. Josh Centor
Alma Mater, Year: Brandeis, 2004
Athletic Department Phone: 412-268-8054
Mailing Address:  5000 Forbes Avenue / Pittsburgh, PA 15213
Answer: "
"Which department in the School of Computer Science was formed in 2006?
","['history_d401.txt', 'history_d401.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which department in the School of Computer Science was formed in 2006?

Context: Drawing on Shaw’s plan and also on the work of other faculty members, an undergraduate program in computer science was created during the 1989-90 academic year. Seven CS majors were admitted to the program as sophomores. Another 73 undergraduates were admitted in 1990–91. By 1995, there were 401 undergraduates in the School of Computer Science; in fall 2013, more than 600 undergraduates made up about 37 percent of student enrollment at SCS, along with more than 600 master’s degree students.New departments, new areas of studyAlong the way, the Center for Machine Translation became the Language Technologies Institute, and other new departments were formed, including the Human-Computer Interaction Institute (1993), the Institute for Software Research (1999), the Machine Learning Department (2006) and the Ray and Stephanie Lane Center for Computational Biology (2009). SCS’s seven degree-granting departments draw faculty and students from a wide variety of disciplines, including engineering, mathematics, social sciences, linguistics and design.Committed to extending our founders’ visionThe School of Computer Science at Carnegie Mellon University enters its second quarter century as a world-leading educational and research institution, embracing all facets of computing. Its graduate programs are consisted ranked with the best in the world by a leading U.S. magazine, while its undergraduate programs are also rated the best in the U.S. by corporate recruiters. In 2013, SCS had 284 faculty members and a total student enrollment of nearly 1,700, including undergraduate, master’s and Ph.D. students, and conducted $124 million in research. Indeed, by itself, the Robotics Institute is the largest university robotics research group in the world, with more than 500 people and more than 100 ongoing research projects. A half-century ago, Perlis, Simon and Newell outlined a vision for computer science. The School of Computer Science at CMU remains committed to continuing and extending their vision in the context of big data and connected computing in the 21st century.
The experiment, which lasted two years, was an unqualified success. Separately, and also in 1986, the Pittsburgh Supercomputing Center was created as a joint effort between CMU, the University of Pittsburgh and Westinghouse Electric Corp.SCS is officially formedCMU’s Faculty Senate in the fall of 1988 agreed to President Richard Cyert’s plan to elevate the Department of Computer Science to college status. In addition to the Computer Science Department, SCS also incorporated the Robotics Institute, the Center for Machine Translation, and researchers from the Information Technology Center, which had developed Andrew. On Dec. 13, 1988, Cyert (H’89) told faculty and staff that Habermann had been appointed CMU’s first Dean of Computer Science, effective Dec. 1, and that the School of Computer Science would soon begin operations. SCS made its formal debut on Dec. 22, 1988, with a reception in the Wherrett Room of Skibo Hall, CMU’s student union. The official announcement of CMU’s new “graduate School of Computer Science” was made Jan. 3, 1989.Undergraduate degrees beginFor several years, undergraduates interested in computer science pursued an “applied math/CS” bachelor’s degree offered by the Mathematics Department. CSD professor Mary Shaw (CS’72) led CMU’s first effort to design an undergraduate curriculum solely in computer science. She and her colleagues were guided by the Carnegie Plan—guidelines established in 1938 under Carnegie Tech President Robert Doherty (A’40, E’48, H’50), outlining the principles of a sound professional education. Drawing on Shaw’s plan and also on the work of other faculty members, an undergraduate program in computer science was created during the 1989-90 academic year. Seven CS majors were admitted to the program as sophomores. Another 73 undergraduates were admitted in 1990–91.
Answer: "
"What does SPAE stand for?
","['alexander hauptmann_376f494126d1ea4f571ea0263c43ac2b6331800a_metadata.txt', 'yonatan bisk_papers.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What does SPAE stand for?

Context: Faculty Name: alexander hauptmann
Paperid: 376f494126d1ea4f571ea0263c43ac2b6331800a
Title: SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs
Year: 2023
Abstract: In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.
Authors: Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A. Ross, Irfan Essa, Yonatan Bisk, Ming Yang, K. Murphy, A. Hauptmann, Lu Jiang
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.'}
Url: http://arxiv.org/pdf/2306.17842
List of 2023 Open Access papers by yonatan bisk are:
SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs
HomeRobot: Open-Vocabulary Mobile Manipulation
Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents
MOSAIC: Learning Unified Multi-Sensory Object Property Representations for Robot Perception
Reasoning about the Unseen for Efficient Outdoor Object Navigation
The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment
WebArena: A Realistic Web Environment for Building Autonomous Agents
Computational Language Acquisition with Theory of Mind
Answer: "
"In which conference was HomeRobot published?
","['yonatan bisk_papers.txt', 'shinji watanabe_0b234f749c6aebf5b1ec61fa0b2ac0d348ad08ed_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In which conference was HomeRobot published?

Context: List of 2023 Open Access papers by yonatan bisk are:
SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs
HomeRobot: Open-Vocabulary Mobile Manipulation
Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents
MOSAIC: Learning Unified Multi-Sensory Object Property Representations for Robot Perception
Reasoning about the Unseen for Efficient Outdoor Object Navigation
The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment
WebArena: A Realistic Web Environment for Building Autonomous Agents
Computational Language Acquisition with Theory of Mind
PMLR, 2022, pp. 10557–10574. [21] Kenneth Heafield, “Kenlm: Faster and smaller language model queries,” in Proceedings of the Sixth Workshop on Statistical Machine Translation, USA, 2011, WMT ’11, p. 187–197, Association for Computational Linguistics. [22] Pengcheng Guo, Florian Boyer, Xuankai Chang, Tomoki Hayashi, Yosuke Higuchi, et al., “Recent developments on espnet toolkit boosted by conformer,” in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 5874–5878. [23] Dmitriy Serdyuk, Otavio Braga, and Olivier Siohan, “Transformerbased video front-ends for audio-visual speech recognition for single and multi-person video,” in Interspeech, 2022, pp. 2833–2837. [24] Bowen Shi, Wei-Ning Hsu, Kushal Lakhotia, and Abdelrahman Mohamed, “Learning audio-visual speech representation by masked multimodal cluster prediction,” in ICLR, 2022. [25] Pingchuan Ma, Alexandros Haliassos, Adriana Fernandez-Lopez, Honglie Chen, Stavros Petridis, et al., “Auto-avsr: Audio-visual speech recognition with automatic labels,” in ICASSP, 2023, pp. 1–5. [26] Pingchuan Ma, Niko Moritz, Stavros Petridis, Christian Fuegen, and Maja Pantic, “Streaming audio-visual speech recognition with alignment regularization,” in Interspeech, 2023. [27] Ludwig Kürzinger, Dominik Winkelbauer, Lujun Li, Tobias Watzel, and Gerhard Rigoll, “Ctc-segmentation of large corpora for german end-to-end speech recognition,” in International Conference on Speech and Computer. Springer, 2020, pp. 267–278.
Answer: "
"Carnegie Mellon University is home to how many members of the National Academy of Sciences (NAS)? 
","['fact_sheet_d407.txt', 'handbook-msaii-2022-2023.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Carnegie Mellon University is home to how many members of the National Academy of Sciences (NAS)? 

Context: Carnegie Mellon University has been a birthplace 
of innovation since its founding in 1900.
Today, CMU is a global leader bringing groundbreaking 
ideas to market and creating successful startup 
businesses. Our award-winning faculty are renowned for 
working closely with students to solve major scientific, 
technological and societal challenges. We put a strong 
emphasis on creating things — from art to robots. We 
have become a model for economic development in 
forming partnerships with companies such as Uber, 
Google and Disney. Our students are recruited by some 
of the world’s most innovative companies.
MANAGEMENT 
INFORMATION 
SYSTEMS
U.S. News & World Report, 2022
#1
ARTIFICIAL 
INTELLIGENCE
U.S. News & World Report, 2022
#1
AMONG U.S. 
UNIVERSITIES
Times Higher Education  
World University 
Ranking, 2023
#19
SOFTWARE 
ENGINEERING
U.S. News & World Report, 2022
COLLEGE OF 
ENGINEERING
U.S. News & World Report, 2022
#1
#4
#1
SCHOOL OF 
COMPUTER 
SCIENCE
U.S. News & World Report, 2022
UNIVERSITY  
IN THE WORLD
Times Higher Education  
World University  
Ranking, 2023
#28
TIME-BASED/ 
NEW MEDIA
U.S. News & World Report, 2019
#1
1,529  
FACULTY
16,779 
STUDENTS
Graduate
Undergraduate
117,257 
ALUMNI (LIVING)
85%  U.S.
15%  International
35%  U.S. 
65%  International
76%  U.S. 
24%  International
GLOBAL COMMUNITY
Students representing
126 countries
Faculty representing
56 countries
Alumni representing 
148 countries
OF COMPUTER 
SCIENCE’S FIRST-
YEAR STUDENTS  
WERE WOMEN  
IN 2019
Nearly triple the  
national average
“ My Heart is in the Work.” 
 
Andrew Carnegie, Founder 
 
November 15, 1900
49.8%
87%  U.S.
1.7 
Office of International Education (OIE) . 44 
14.1.8 
Veterans and Military Community . 45 
14.1.9 
Carnegie Mellon Ethics Hotline . 45 
14.2 
Key Offices for Academic & Research Support . 45 
14.2.1 
Computing and Information Resources . 46 
14.2.2 
Student Academic Success Center . 46 
14.2.3 
University Libraries . 48 
14.2.4 
Research at Carnegie Mellon . 48 
14.2.5 
Office of Research Integrity & Compliance . 48 
14.3 
Key Offices for Health, Wellness & Safety . 49 
14.3.1 
Counseling & Psychological Services . 49 
14.3.2 
Health Services . 49 
14.3.3 
Campus Wellness . 49 
14.3.4 
Religious and Spiritual Life Initiatives (RSLI) . 50 
14.3.5 
University Police . 50 
14.3.6 
Shuttle and Escort Services . 50 
14.4 
The WORD . 50 
 
 
 
6 
 
1 
Welcome 
Welcome to Carnegie Mellon University!  Being in the CMU School of Computer Science (SCS) 
is a unique experience because of the size of the school (over 2000 students, staff and faculty), 
the quality of its members, the variety of research being conducted, and unparalleled learning 
opportunities.  We encourage you to take advantage of CMU offerings, both inside and outside 
SCS, and also to explore Pittsburgh and southwest Pennsylvania to the extent your schedule 
permits.   What you will find is an environment that encourages and rewards hard work and 
accomplishment.  Pittsburgh has long been an industrial area whose residents build things.  In 
SCS we build things, and you will also.
Answer: "
"In which year was the official Scotty costume unveiled?
","['mascot_d405.txt', 'tartanfacts_d404.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In which year was the official Scotty costume unveiled?

Context: About Scotty
The Scottish terrier has long been a familiar figure around Carnegie Mellon's campus. For years students have suited up in an unofficial Scottish terrier costume to excite the fans at athletic events. But the relationship between the Scottish terrier breed and Carnegie Mellon far precedes anybody doing somersaults in a dog costume. Andrew Carnegie, founder of the university, kept a Scottish terrier as his pet.
Scotty's road from popular icon to official mascot of the university began in 2006. Carnegie Mellon formed a Mascot Identity Task Force in November 2006, which consisted of students, faculty, staff and alumni. The Task Force was co-chaired by Director of Athletics Susan Bassett and Dean of Student Affairs Jennifer Church.
The mascot selection process included a series of surveys and a university Town Hall meeting. Nearly 78 percent of 2,370 students surveyed in February 2007 voted for the Scottish terrier, and approximately 25 percent of 400 alumni surveyed thought the Scottish terrier was already the mascot.
In the spring, the Task Force partnered with SME Branding — a firm with more than 17 years of experience creating mascots for professional sports teams and universities — to develop the graphics for the mascot. During October, students and alumni reviewed potential mascot images in focus groups.
Carnegie Mellon's official mascot debuted at the Nov. 10, 2007 home football game. The graphic features a profile of a distinguished, bold Scottish terrier sporting a plaid scarf around his neck. The dog is contained in a shield, representing Carnegie Mellon's Scottish heritage.
The Task Force then partnered with a mascot costume company to design our Scottish terrier in the winter of 2007. The official Scotty costume was unveiled at the 2008 Spring Carnival.
The
Scottish terrier breed
is known for its keen, alert and intelligent expression. Its temperament is described as determined and thoughtful while its physical aspects exemplify strength, power and agility in a small package. Many of these traits are also apparent throughout the university, making the Scottish terrier a natural choice for Carnegie Mellon's mascot.
Fun Fact
In keeping with tradition, students, faculty, staff and alumni voted to name the new official mascot Scotty. But it's not just the costumed mascot that voters named. The live dog is also known as Scotty to the Carnegie Mellon community.
About Scotty
|
Celebrating Scotty
|
Performer Requests
Department Quick Facts
Name of School: Carnegie Mellon University
City/Zip: Pittsburgh, PA 15213
Founded: 1900
Enrollment (undergrad): 6,982- Fall 2021
Enrollment (graduate and doctoral): 7,062 - Fall 2021
Alumni: 102,577
Nickname: Tartans
Mascot: Scottie Dog
School Colors: Cardinal and Gray
Football Stadium and Track and Field: Gesling Stadium
Capacity: 3,500
Surface: FieldTurf
Basketball/Volleyball Gym: Wiegand Gymnasium
Capacity: 500
Soccer Stadium: CMU Soccer Field
Capacity: 250
Surface: FieldTurf
Affiliation: NCAA Division III
Conference: University Athletic Association | Presidents' Athletic Conference (Football Only)
President: Dr. Farnam Jahanian
Alma Mater, Year: University of Texas at Austin
Athletic Director: Dr. Josh Centor
Alma Mater, Year: Brandeis, 2004
Athletic Department Phone: 412-268-8054
Mailing Address:  5000 Forbes Avenue / Pittsburgh, PA 15213
Answer: "
"At which conference venue was the framework tax published?
","['emma strubell_b777aa86b5a1d49ce8eababc5c2ee56d3562801e_metadata.txt', 'yonatan bisk_b777aa86b5a1d49ce8eababc5c2ee56d3562801e_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: At which conference venue was the framework tax published?

Context: Faculty Name: emma strubell
Paperid: b777aa86b5a1d49ce8eababc5c2ee56d3562801e
Title: The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment
Year: 2023
Abstract: Increased focus on the computational efficiency of NLP systems has motivated the design of efficient model architectures and improvements to underlying hardware accelerators. However, the resulting increases in computational throughput and reductions in floating point operations have not directly translated to improvements in wall-clock inference latency. We demonstrate that these discrepancies can be largely attributed to bottlenecks introduced by deep learning frameworks. We denote this phenomenon as the \textit{framework tax}, and observe that the disparity is growing as hardware speed increases over time. In this work, we examine this phenomenon through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency. Code is available at https://github.com/JaredFern/Framework-Tax.
Authors: Jared Fernandez, Jacob Kahn, Clara Na, Yonatan Bisk, Emma Strubell
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work examines the effects of model design decisions, framework paradigms, and hardware platforms on total model latency through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency.'}
Url: http://arxiv.org/pdf/2302.06117
Faculty Name: yonatan bisk
Paperid: b777aa86b5a1d49ce8eababc5c2ee56d3562801e
Title: The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment
Year: 2023
Abstract: Increased focus on the computational efficiency of NLP systems has motivated the design of efficient model architectures and improvements to underlying hardware accelerators. However, the resulting increases in computational throughput and reductions in floating point operations have not directly translated to improvements in wall-clock inference latency. We demonstrate that these discrepancies can be largely attributed to bottlenecks introduced by deep learning frameworks. We denote this phenomenon as the \textit{framework tax}, and observe that the disparity is growing as hardware speed increases over time. In this work, we examine this phenomenon through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency. Code is available at https://github.com/JaredFern/Framework-Tax.
Authors: Jared Fernandez, Jacob Kahn, Clara Na, Yonatan Bisk, Emma Strubell
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work examines the effects of model design decisions, framework paradigms, and hardware platforms on total model latency through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency.'}
Url: http://arxiv.org/pdf/2302.06117
Answer: "
"In fall 2023, What is the deadline for adding, auditing, and tuition adjustment drop for Mini-2 (deadline 1)?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, What is the deadline for adding, auditing, and tuition adjustment drop for Mini-2 (deadline 1)?

Context: On Friday, 25 October, 2024, during the Fall 2024 (F24) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
On Tuesday, 05 November, 2024, during the Fall 2024 (F24) semester, Democracy Day is observed, leading to No Classes, except Evening classes after 5 pm will still meet.
On Monday, 11 November, 2024, during the Fall 2024 (F24) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Wednesday, 13 November, 2024, during the Fall 2024 (F24) semester, Mini-2 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 18 November, 2024,Monday to 22 November, 2024,Friday marks the Spring 2025 Registration Week for Fall 2024 (F24) semester.
On Monday, 25 November, 2024, during the Fall 2024 (F24) semester, Mini-2 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 25 November, 2024, during the Fall 2024 (F24) semester, Semester & Mini-2 Faculty Course Evaluations open is observed.
From 27 November, 2024,Wednesday to 29 November, 2023,Friday marks the Thanksgiving Break for Fall 2024 (F24) semester which leads to No Classes.
On Friday, 06 December, 2024, during the Fall 2024 (F24) semester, Semester & Mini-2 Last Day of Classes is observed.
On Friday, 06 December, 2024, during the Fall 2024 (F24) semester, Semester & Mini-2 voucher deadline (4) is observed.
From 09 December, 2024,Monday to 10 December, 2023,Tuesday marks the Final Exams for Fall 2024 (F24) semester.
On Monday, 11 December, 2024, during the Fall 2024 (F24) semester, Reading Day is observed.
From 12 December, 2024,Thursday to 13 December, 2023,Friday marks the Final Exams for Fall 2024 (F24) semester.
On Monday, 02 September, 2024, during the Fall 2024 (F24) semester, Labor Day is observed, leading to No Classes & University Closed.
On Monday, 09 September, 2024, during the Fall 2024 (F24) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 18 September, 2024, during the Fall 2024 (F24) semester, Mini-1 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 30 September, 2024, during the Fall 2024 (F24) semester, Mini-1 Pass/no pass & withdrawal deadline (3) is observed.
On Monday, 30 September, 2024, during the Fall 2024 (F24) semester, Mini-1 Faculty Course Evaluations open is observed.
On Monday, 07 October, 2024, during the Fall 2024 (F24) semester, Semester drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 11 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Last Day of Classes is observed.
On Friday, 11 October, 2024, during the Fall 2024 (F24) semester, Mini-1 voucher election deadline (4) is observed.
On Saturday, 12 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Exams is observed.
On Saturday, 12 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Faculty Course Evaluations close is observed.
From 14 October, 2024,Monday to 18 October, 2023,Friday marks the Fall Break for Fall 2024 (F24) semester which leads to No Classes.
On Monday, 21 October, 2024, during the Fall 2024 (F24) semester, Mini-2 Classes Begin is observed.
On Wednesday, 23 October, 2024, during the Fall 2024 (F24) semester, Mid-Semester & Mini-1 grades due by 4 pm is observed.
On Friday, 25 October, 2024, during the Fall 2024 (F24) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
Answer: "
"What year was ""End-to-End Speech Recognition: A Survey"" published?
","['shinji watanabe_f80c354908efd4d5617878e36e35446016534190_content_1.txt', 'shinji watanabe_92b3753e56f5d85fd57a32084f52476839cc7221_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What year was ""End-to-End Speech Recognition: A Survey"" published?

Context: 7. REFERENCES [1] R. Prabhavalkar et al., “End-to-end speech recognition: A survey,” arXiv preprint arXiv:2303.03329, 2023. [2] J. Li et al., “Recent advances in end-to-end automatic speech recognition,” APSIPA Transactions on Signal and Information Processing, vol. 11, no. 1, [3] S. Watanabe et al., “ESPnet: End-to-end speech processing toolkit,” in Proc. Interspeech, 2018. [4] A. Graves et al., “Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks,” in ICML 2006, vol. 148, 2006, pp. 369–376. [5] A. Graves, “Sequence transduction with recurrent neural networks,” CoRR, vol. abs/1211.3711, 2012. [6] J. Chorowski et al., “Attention-based models for speech recognition,” in Proc. NeurIPS, 2015, pp. 577–585. [7] A. Graves, A. Mohamed, and G. E. Hinton, “Speech recognition with deep recurrent neural networks,” in Proc. ICASSP, 2013. [8] G. Saon et al., “Advancing RNN transducer technology for speech recognition,” in Proc. ICASSP, 2021, pp. 5654–5658. [9] W. Chan et al., “Listen,
Title: ONE MODEL TO RULE THEM ALL ? TOWARDS END-TO-END JOINT SPEAKER DIARIZATION AND SPEECH RECOGNITION
Authors: Samuele Cornell, Jee-weon Jung, Shinji Watanabe, Stefano Squartini
Section: 6. REFERENCES
[1] J. S. Garofolo, J. G. Fiscus, A. F. Martin et al., “Nist rich transcription 2002 evaluation: A preview.,” in LREC, 2002. [2] T. J. Park, N. Kanda, D. Dimitriadis et al., “A review of speaker diarization: Recent advances with deep learning,” Computer Speech & Language, vol. 72, pp. 101317, 2022. [3] R. Prabhavalkar, T. Hori, T. N. Sainath et al., “End-to-end speech recognition: A survey,” arXiv preprint arXiv:2303.03329, 2023. [4] J. Barker, S. Watanabe, E. Vincent and J. Trmal, “The fifth CHiME speech separation and recognition challenge: Dataset, task and baselines,” in Proc. InterSpeech, 2018. [5] S. Watanabe, M. Mandel, J. Barker et al., “CHiME-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings,” in CHiME Workshop, 2020. [6] J. G. Fiscus, J. Ajot and J. S. Garofolo, “The rich transcription 2007 meeting recognition evaluation,” in International Evaluation Workshop on Rich Transcription. Springer, 2007, pp. 373–389. [7] S. Cornell, M. Wiesner, S. Watanabe et al., “The chime-7 dasr challenge: Distant meeting transcription with multiple devices in diverse scenarios,” CHiME Workshop, 2023. [8] F. Yu, S. Zhang, Y. Fu et al., “M2MeT: The ICASSP 2022 multichannel multi-party meeting transcription challenge,” in Proc. ICASSP, 2022.
Answer: "
"Does CMU discriminate based on race?
","['mlt-student-handbook-2023-2024.txt', 'handbook_phd_2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Does CMU discriminate based on race?

Context: In addition to this departmental graduate student handbook, 
the following resources are available to assist you in understanding community expectations: 
 The Word/Student Handbook: 
 www.cmu.edu/student-affairs/theword//index.html  
 Academic Integrity Policy:  
www.cmu.edu/policies/student-and-student-
life/academic-integrity.html    
 University Policies Website:  
www.cmu.edu/policies/  
 Office of Graduate and Postdoctoral Affairs: 
 
www.cmu.edu/graduate/policies/index.html 
Due to the changing nature of conditions and expectations surrounding public health and safety 
requirements, please visit www.cmu.edu/coronavirus for the most up to date information. 
Please see Appendix A for additional information about The Word and other university 
resources. 
1.5 Carnegie Mellon University Statement of Assurance  
Carnegie Mellon University does not discriminate in admission, employment or administration 
of its programs or activities on the basis of race, color, national origin, sex, handicap or 
disability, age, sexual orientation, gender identity, religion, creed, ancestry, belief, veteran 
status or genetic information. Furthermore, Carnegie Mellon University does not discriminate 
and is required not to discriminate in violation of federal, state or local laws or executive 
orders. 
Inquiries concerning the application of and compliance with this statement should be directed 
to the university ombudsman, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA 
15213, telephone 412-268-1018.  Obtain general information about Carnegie Mellon University 
by calling 412-268-2000. 
Carnegie Mellon University publishes an annual campus security and fire safety report 
describing the university's security, alcohol and drug, sexual assault and fire safety policies, and 
containing statistics about the number and type of crimes committed on the campus, and the 
number and cause of fires in campus residence facilities during the preceding three years. You 
can obtain a copy by contacting the Carnegie Mellon Police Department at 412-268-2323. The 
annual security and fire safety report also is available online at 
www.cmu.edu/police/annualreports.
Some doctoral course-sections follow a separate Academic Calendar. 
1.5 
Carnegie Mellon University Statement of Assurance  
Carnegie Mellon University does not discriminate in admission, employment, or administration 
of its programs or activities on the basis of race, color, national origin, sex, handicap or disability, 
age, sexual orientation, gender identity, religion, creed, ancestry, belief, veteran status, or genetic 
information.  Furthermore, Carnegie Mellon University does not discriminate and is required not 
to discriminate in violation of federal, state, or local laws or executive orders. 
Inquiries concerning the application of a compliance with this statement should be directed to 
the university ombudsperson, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA 
15213 (412-268-1018). Obtain general information about Carnegie Mellon University by calling 
412-268-2000. 
Carnegie Mellon University publishes an annual campus security and fire safety report describing 
the universitys security, alcohol and drug, sexual assault, and fire safety policies, and containing 
statistics about the number and type of crimes committed on campus, and the number and cause 
of fires in campus residence facilities during the preceding three years. You can obtain a copy by 
contacting the Carnegie Mellon Police Department at 412-268-2323. The annual security and fire 
safety report also is available online at www.cmu.edu/police/annualreports . 
Information regarding the applicable grievance procedures for alleged violations 
of the Statement of Assurance is available at 
https://www.cmu.edu/policies/forms-and-documents/soa-violations.pdf.   
The Office for Institutional Equity and Title IX may be reached at 412-268-7125  
LTI Ph.D. Graduate Student Handbook 
Page 12 
 
or  institutionalequity@cmu.edu. 
1.6 
The Carnegie Mellon Code 
Students at Carnegie Mellon, because they are members of an academic community dedicated to 
the achievement of excellence, are expected to meet the highest standards of personal, ethical, and 
moral conduct possible. 
 These standards require personal integrity, a commitment to honesty without compromise, as 
well as truth without equivocation and a willingness to place the good of the community above 
the good of the self. Obligations once undertaken must be met, commitments kept.
Answer: "
"According to the paper PROMPT2MODEL: Generating Deployable Models from Natural Language Instructions, what is the exact match achieved by gpt-3.5-turbo on the Squad dataset?
","['graham neubig_e69684fb06a7b1fe621d7ef0c97fc2ca0e122c43_metadata.txt', 'graham neubig_e69684fb06a7b1fe621d7ef0c97fc2ca0e122c43_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: According to the paper PROMPT2MODEL: Generating Deployable Models from Natural Language Instructions, what is the exact match achieved by gpt-3.5-turbo on the Squad dataset?

Context: Faculty Name: graham neubig
Paperid: e69684fb06a7b1fe621d7ef0c97fc2ca0e122c43
Title: Prompt2Model: Generating Deployable Models from Natural Language Instructions
Year: 2023
Abstract: Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment. This is done through a multi-step process of retrieval of existing datasets and pretrained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains models that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model reliability before deployment. Prompt2Model is available open-source at https://github.com/neulab/prompt2model.
Authors: Vijay Viswanathan, Chenyang Zhao, Amanda Bertsch, Tongshuang Sherry Wu, Graham Neubig
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment.'}
Url: https://arxiv.org/pdf/2308.12261
Title: PROMPT2MODEL: Generating Deployable Models from Natural Language Instructions
Authors: Vijay Viswanathan, Chenyang Zhao, Amanda Bertsch, Tongshuang Wu, Graham Neubig
Section: 6 Discussion and Conclusion
reference implementation. This model is believed to be similar to GPT-3 (Brown et al., 2020), which was trained on 93% English documents, 1% German documents, 1% French documents, and <5% documents in any other language. Our use of this model may exacerbate existing disparities in language technologies between highresource languages and low-resource languages. One potential limitation is that we have only tested our approach on 3 tasks, each with a single dataset and a single evaluation metric. We justify this decision because our focus is on providing an extensible software system rather than establishing state-of-the-art results on many datasets, but we believe that our results suggest broader applicability. Ethics Statement Any system which makes powerful technology more accessible to the public has ethical implications. Widder et al. (2022) discuss ethical issues with open-source packages in relation to software libraries for deepfaking, including the possibility of enabling malicious actors to use technology that they would otherwise not have the technical skills to leverage. This is also a risk for an AutoML system such as Prompt2Model; however, we believe this risk is outweighed by the benefits of greater accessibility, especially given that a low barrier to entry for generating harmful data already exists in the form of prompted, web-interface models. While Prompt2Model could, if given harmful inputs, generate toxic, offensive, or inaccurate synthetic data, this is no more of a risk with Prompt2Model than it is with the underlying prompted model (Bender et al., 2021); indeed, the use of models and supplementary datasets retrieved from Hugging Face may lessen the likelihood of a downstream model replicating harms from the prompted model’s outputs, though more investigation is needed. Like all ML models, the models that Prompt2Model returns can make mistakes, and we aim to be transparent in our documentation about potential limitations of the system. We hope that Prompt2Model will be broadly useful.
Answer: "
"In fall 2023, What is the location for unit 02700?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, What is the location for unit 02700?

Context: In Semester Fall 2023, from the department of Computational Biology, the subject titled 'M.S. Thesis Research' with Course ID 02700 and Section A offers 1-12 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Garrand located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'CPCB Course - Current Topics in Computational Biology' with Course ID 02701 and Section A offers 3.0 units. The Class meets Monday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ma located in Building WEH, Room 5403.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Biology Seminar' with Course ID 02702 and Section A offers 3.0 units. The Class meets Friday between 10:30AM and 12:00PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Carja located in Building OFF, Room PITT.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Methods for Biological Modeling and Simulation' with Course ID 02712 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Schwartz, Carja located in Building HH, Room B131.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Medicine' with Course ID 02718 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Wu located in Building WEH, Room 5403.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Laboratory Methods for Computational Biologists' with Course ID 02760 and Section A offers 9.0 units. The Class meets Tuesday between 11:00AM and 12:50PM ET.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'M.S. Thesis Research' with Course ID 02700 and Section A offers 1-12 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Garrand located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'CPCB Course - Current Topics in Computational Biology' with Course ID 02701 and Section A offers 3.0 units. The Class meets Monday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ma located in Building WEH, Room 5403.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Biology Seminar' with Course ID 02702 and Section A offers 3.0 units. The Class meets Friday between 10:30AM and 12:00PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Carja located in Building OFF, Room PITT.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Methods for Biological Modeling and Simulation' with Course ID 02712 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Schwartz, Carja located in Building HH, Room B131.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Medicine' with Course ID 02718 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Wu located in Building WEH, Room 5403.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Laboratory Methods for Computational Biologists' with Course ID 02760 and Section A offers 9.0 units. The Class meets Tuesday between 11:00AM and 12:50PM ET.
Answer: "
"By whom was Kevlar fiber invented?
","['fact_sheet_d407.txt', 'lei li_6d91a5ce236d4f97a69eb326296133e7f0a352ba_content_2.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: By whom was Kevlar fiber invented?

Context: 13%  International
SCHOOLS AND COLLEGES
College of Engineering 
College of Fine Arts
Dietrich College of Humanities  
and Social Sciences
Heinz College of Information Systems 
and Public Policy
Mellon College of Science
School of Computer Science
Tepper School of Business
cmu.edu/assets/pdfs/cmu-fact-sheet.pdf    |   cmu.edu/about/awards.html
March 2020
BIG IDEAS THAT SHAPE YOUR WORLD START HERE
1,000+ companies greenlighted 
across the U.S. and internationally 
Cognitive tutors, which improve 
the math skills of 500,000+ 
students each year 
The first U.S. school to award  
a degree in drama in 1914
James Gosling (SCS 1983) 
invented the Java computer 
programming language.
The first exclusive higher 
education partner of the Tony 
Awards®, co-creating the first, 
national recognition program 
honoring K-12 theatre educators 
The first smile in an email, created 
in 1982 by professor Scott Fahlman
Kevlar Fiber, invented by alumna 
Stephanie Kwolek (MM 1946)
The Last Lecture, written  
by alumni Randy Pausch (SCS 1988)  
and Jeffrey Zaslow (DC 1980)
In 1919, CMU’s first Ph.D. was 
awarded to Mao Yisheng, father  
of Chinese bridge construction. 
Duolingo, invented by professor  
Luis von Ahn (CS 2003, 2005), had  
10 million downloads in 12 months 
and was named Apple’s 2013 app  
of the year.
:-)
LEADERSHIP
Farnam Jahanian  
President
David Coulter 
Chair, Board of Trustees
James H. Garrett Jr.
Title: flexspline-flexible bearing contact pair in harmonic drive based on macro-micro scale modeling
Authors: Yihao Zheng, Ziling Zhang, Qiushi Hu, Heng Li, Guang Wang, Lei Li
Section: COPYRIGHT
contact model. However, the GW model only considers the elastic deformation stage of the asperities. Zhao et al. (2000) established an elastic-plastic asperity microcontact model, namely, the Z-MC contact model. The ZMC model incorporates the transitional regime from elastic deformation to fully plastic flow of the asperity into consideration. Kogut and Etsion. (2002); Kogut and Etsion. (2003) solved the elasticplastic contact problem of a rigid flat pressing against a sphere through finite element simulation and constructed the KE elasticplastic model, which extends the classical Hertz solution to a fully plastic contact area. However, contact models based on statistical parameters do not provide unique characterization and analysis results for the given rough surfaces. Based on the fractal theory and the Weierstrass-Mandelbrot function (Majumdar and Tien, 1990) characterizing the two-dimensional profile features of isotropic rough surfaces, Majumdar and Bhushan (1991) developed the MB fractal contact model for rough surfaces. It is noteworthy that the MB model indicates that smaller contact spots tend to undergo plastic deformation, while larger contact spots undergo elastic deformation. This prediction is in stark contrast to the classical theories of contact mechanics, which predicts the opposite. In the MB model, the deformation of a single asperity is a complete deformation that is independent of the applied load, and the asperity peak curvature radius is a parameter related to the contact area rather than a constant value. Later, Morag and Etsion (2006) demonstrated that the deformation process of a single asperity on a fractal surface always starts from an elastic state and eventually undergoes an elastic-plastic transition. Liou and Lin (2010) believed that the behaves of a fractal asperity conforms to classical contact mechanics. By combining the generalized Weierstrass-Mandelbrot function with the radius-vector function method, the formulas for the fractal surface profiles of sphere-based and cylinder-based particles were derived.
Answer: "
"In spring 2024, What is the title of course 15151?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, What is the title of course 15151?

Context: In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section T offers 12.0 units. The Class meets Wednesday between 07:00PM and 08:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5210.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section U offers 12.0 units. The Class meets Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5208.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Mathematical Foundations for Computer Science' with Course ID 15151 and Section NA offers 12.0 units. The Class meets Monday Wednesday Friday between 10:00AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Peng located in Building GHC, Room 4307.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Mathematical Foundations for Computer Science' with Course ID 15151 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 01:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Peng located in Building GHC, Room 4301.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Mathematical Foundations for Computer Science' with Course ID 15151 and Section B offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Peng located in Building GHC, Room 4301.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Competition Programming I' with Course ID 15195 and Section A offers 5.0 units. The Class meets Wednesday between 07:00PM and 09:50PM ET.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section T offers 12.0 units. The Class meets Wednesday between 07:00PM and 08:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5210.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section U offers 12.0 units. The Class meets Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5208.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Mathematical Foundations for Computer Science' with Course ID 15151 and Section NA offers 12.0 units. The Class meets Monday Wednesday Friday between 10:00AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Peng located in Building GHC, Room 4307.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Mathematical Foundations for Computer Science' with Course ID 15151 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 01:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Peng located in Building GHC, Room 4301.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Mathematical Foundations for Computer Science' with Course ID 15151 and Section B offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Peng located in Building GHC, Room 4301.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Competition Programming I' with Course ID 15195 and Section A offers 5.0 units. The Class meets Wednesday between 07:00PM and 09:50PM ET.
Answer: "
"When are the Spring 2024 grades due?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When are the Spring 2024 grades due?

Context: From 04 May, 2024,Saturday to 05 May, 2024,Sunday marks the Reading Days for Spring 2024 (S24) semester.
On Monday, 06 May, 2024, during the Spring 2024 (S24) semester, Final Examinations is observed.
On Monday, 06 May, 2024, during the Spring 2024 (S24) semester, Semester & Mini-4 Faculty Course Evaluations close is observed.
On Tuesday, 07 May, 2024, during the Spring 2024 (S24) semester, Make-Up Final Examinations is observed.
On Wednesday, 08 May, 2024, during the Spring 2024 (S24) semester, Graduating Final Grades Due by 4 pm is observed.
On Sunday, 12 May, 2024, during the Spring 2024 (S24) semester, Commencement is observed.
On Tuesday, 14 May, 2024, during the Spring 2024 (S24) semester, Final Grades Due by 4 pm is observed.
On Tuesday, 21 May, 2023, during the Spring 2024 (S24) semester, Spring Deans' Lists Posted is observed.
On Monday, 13 May, 2024, during the Summer One 2024 (M24) semester, Semester & Mini-5 Classes Begin is observed.
On Friday, 17 May, 2024, during the Summer One 2024 (M24) semester, Mini-5 add, audit, & tuition adjustment drop deadline (1) is observed.
On Friday, 24 May, 2024, during the Summer One 2024 (M24) semester, Semester add, audit, & tuition adjustment drop deadline (1) is observed.
On Monday, 27 May, 2024, during the Summer One 2024 (M24) semester, Memorial Day is observed, leading to University Closed & No Classes.
On Tuesday, 28 May, 2024, during the Summer One 2024 (M24) semester, Mini-5 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 07 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 22 January, 2024, during the Spring 2024 (S24) semester, Mini-3 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 29 January, 2024, during the Spring 2024 (S24) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Wednesday, 07 February, 2024, during the Spring 2024 (S24) semester, Mini-3 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 19 February, 2024, during the Spring 2024 (S24) semester, Mini-3 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 19 February, 2024, during the Spring 2024 (S24) semester, Mini-3 Faculty Course Evaluations open is observed.
On Monday, 26 February, 2024, during the Spring 2024 (S24) semester, Semester course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 01 March, 2024, during the Spring 2024 (S24) semester, Mini-3 Last Day of Classes is observed.
On Friday, 01 March, 2024, during the Spring 2024 (S24) semester, Mini-3 voucher deadline (4) is observed.
On Saturday, 02 March, 2024, during the Spring 2024 (S24) semester, Mini-3 Exams is observed.
On Saturday, 02 March, 2024, during the Spring 2024 (S24) semester, Mini-3 Faculty Course Evaluations close is observed.
From 04 March, 2024,Monday to 08 March, 2024,Friday marks the Spring Break for Spring 2024 (S24) semester which leads to No Classes.
On Monday, 11 March, 2024, during the Spring 2024 (S24) semester, First day of Mini-4 Classes is observed.
On Monday, 11 March, 2024, during the Spring 2024 (S24) semester, Mid-Semester & Mini-3 grades due by 4 pm is observed.
On Tuesday, 12 March, 2024, during the Spring 2024 (S24) semester, Summer 2024 Registration Opens is observed.
Answer: "
"What are the course number(s) for the courses on LLMs?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the course number(s) for the courses on LLMs?

Context: In Semester Spring 2024, from the department of Heinz College Wide Courses, the subject titled 'Management Consulting' with Course ID 94808 and Section A offers 12.0 units. The Class meets Wednesday between 06:30PM and 09:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Brussalis located in Building HBH, Room 1005.
In Semester Spring 2024, from the department of Heinz College Wide Courses, the subject titled 'Strategy Development' with Course ID 94811 and Section A3 offers 6.0 units. The Class meets Monday Wednesday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Synnott located in Building HBH, Room 2008.
In Semester Spring 2024, from the department of Heinz College Wide Courses, the subject titled 'Applications of NL(X) and LLM' with Course ID 94812 and Section A3 offers 6.0 units. The Class meets Tuesday Thursday Friday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rao located in Building HBH, Room 1007.
In Semester Spring 2024, from the department of Heinz College Wide Courses, the subject titled 'Project Management' with Course ID 94813 and Section A3 offers 6.0 units. The Class meets Monday Wednesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Synnott located in Building HBH, Room 2009.
In Semester Spring 2024, from the department of Heinz College Wide Courses, the subject titled 'Project Management' with Course ID 94813 and Section B3 offers 6.0 units. The Class meets Monday Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Synnott located in Building HBH, Room 2009.
In Semester Spring 2024, from the department of Heinz College Wide Courses, the subject titled 'Agent-Based Modeling & Digital Twins' with Course ID 94815 and Section A4 offers 6.0 units.
In Semester Spring 2024, from the department of Heinz College Wide Courses, the subject titled 'Management Consulting' with Course ID 94808 and Section A offers 12.0 units. The Class meets Wednesday between 06:30PM and 09:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Brussalis located in Building HBH, Room 1005.
In Semester Spring 2024, from the department of Heinz College Wide Courses, the subject titled 'Strategy Development' with Course ID 94811 and Section A3 offers 6.0 units. The Class meets Monday Wednesday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Synnott located in Building HBH, Room 2008.
In Semester Spring 2024, from the department of Heinz College Wide Courses, the subject titled 'Applications of NL(X) and LLM' with Course ID 94812 and Section A3 offers 6.0 units. The Class meets Tuesday Thursday Friday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rao located in Building HBH, Room 1007.
In Semester Spring 2024, from the department of Heinz College Wide Courses, the subject titled 'Project Management' with Course ID 94813 and Section A3 offers 6.0 units. The Class meets Monday Wednesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Synnott located in Building HBH, Room 2009.
In Semester Spring 2024, from the department of Heinz College Wide Courses, the subject titled 'Project Management' with Course ID 94813 and Section B3 offers 6.0 units. The Class meets Monday Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Synnott located in Building HBH, Room 2009.
In Semester Spring 2024, from the department of Heinz College Wide Courses, the subject titled 'Agent-Based Modeling & Digital Twins' with Course ID 94815 and Section A4 offers 6.0 units.
Answer: "
"In the KALE lexical expansion paper, what three datasets are evaluated?
","['jamie callan_1e0a4ff0c5d2d850ff5907e310ffcedc9cad9718_metadata.txt', 'jamie callan_1e0a4ff0c5d2d850ff5907e310ffcedc9cad9718_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the KALE lexical expansion paper, what three datasets are evaluated?

Context: Faculty Name: jamie callan
Paperid: 1e0a4ff0c5d2d850ff5907e310ffcedc9cad9718
Title: KALE: Using a K-Sparse Projector for Lexical Expansion
Year: 2023
Abstract: Recent research has proposed retrieval approaches based on sparse representations and inverted indexes, with terms produced by neural language models and leveraging the advantages from both neural retrieval and lexical matching. This paper proposes KALE, a new lightweight method of this family that uses a small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary. The KALE vocabulary captures semantic concepts than perform well when used in isolation, and perform better when extending the original lexical vocabulary, this way improving first-stage retrieval accuracy. Experiments with the MSMARCOv1 passage retrieval dataset, the TREC Deep Learning dataset, and BEIR datasets, examined the effectiveness of KALE under varying conditions. Results show that the KALE terms can replace the original lexical vocabulary, with gains in accuracy and efficiency. Combining KALE with the original lexical vocabulary, or with other learned terms, can further improve retrieval accuracy with only a modest increase in computational cost.
Authors: Luís Borges, Bruno Martins, Jamie Callan
Venue: International Conference on the Theory of Information Retrieval
Tldr: {'model': 'tldr@v2.0.0', 'text': 'KALE is a new lightweight method that uses a small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary, which can replace the original lexical vocabulary with gains in accuracy and efficiency.'}
Url: https://dl.acm.org/doi/pdf/10.1145/3578337.3605131
Title: KALE: Using a K-Sparse Projector for Lexical Expansion
Authors: Luís Borges, Bruno Martins, Jamie Callan
Section: 5.4 Assessing Posting List Size Distribution
of the DFs, which aligns with the previous expectation that regularization helped balance the DFs, and consequently, decrease search latency. The previous experiments showed that the equipartitioning component was useful in balancing the posting list sizes of the generated KALE vocabulary, and therefore improve search efficiency. Not considering the regularization component led not only to worse MRR@10, but also to an increase in query latency.
Answer: "
"According to the MSAII handbook, what is David Garlan's office building and number?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: According to the MSAII handbook, what is David Garlan's office building and number?

Context: The Class meets Monday Wednesday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Garlan, Kang located in Building CMU, Room REMOTE.
In Semester Fall 2023, from the department of Software & Societal Systems, the subject titled 'Formal Methods' with Course ID 17614 and Section A1 offers 6.0 units. The Class meets Friday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Garlan, Kang located in Building 3SC, Room 265.
In Semester Fall 2023, from the department of Software & Societal Systems, the subject titled 'Formal Methods' with Course ID 17614 and Section B1 offers 6.0 units. The Class meets Friday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Garlan, Kang located in Building 3SC, Room 265.
In Semester Fall 2023, from the department of Software & Societal Systems, the subject titled 'Formal Methods' with Course ID 17614 and Section D1 offers 6.0 units. The Class meets Monday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Garlan, Kang located in Building CMU, Room REMOTE.
In Semester Fall 2023, from the department of Software & Societal Systems, the subject titled 'Programming Quantum Computers' with Course ID 17617 and Section A1 offers 6.0 units. The Class meets Monday Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Justice located in Building 3SC, Room 265.
In Semester Fall 2023, from the department of Software & Societal Systems, the subject titled 'Quantum Machine Learning' with Course ID 17620 and Section A2 offers 6.0 units. The Class meets Monday Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Justice, Singh located in Building 3SC, Room 265.
The Class meets Monday Wednesday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Garlan, Kang located in Building CMU, Room REMOTE.
In Semester Fall 2023, from the department of Software & Societal Systems, the subject titled 'Formal Methods' with Course ID 17614 and Section A1 offers 6.0 units. The Class meets Friday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Garlan, Kang located in Building 3SC, Room 265.
In Semester Fall 2023, from the department of Software & Societal Systems, the subject titled 'Formal Methods' with Course ID 17614 and Section B1 offers 6.0 units. The Class meets Friday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Garlan, Kang located in Building 3SC, Room 265.
In Semester Fall 2023, from the department of Software & Societal Systems, the subject titled 'Formal Methods' with Course ID 17614 and Section D1 offers 6.0 units. The Class meets Monday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Garlan, Kang located in Building CMU, Room REMOTE.
In Semester Fall 2023, from the department of Software & Societal Systems, the subject titled 'Programming Quantum Computers' with Course ID 17617 and Section A1 offers 6.0 units. The Class meets Monday Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Justice located in Building 3SC, Room 265.
In Semester Fall 2023, from the department of Software & Societal Systems, the subject titled 'Quantum Machine Learning' with Course ID 17620 and Section A2 offers 6.0 units. The Class meets Monday Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Justice, Singh located in Building 3SC, Room 265.
Answer: "
"How many authors are on the paper ""Multimodal Fusion Interactions: A Study of Human and Automatic Quantification""?
","['louis philippe morency_90b09bdb1bd78875ee8d8d324a568a36955e4765_content_0.txt', 'louis philippe morency_90b09bdb1bd78875ee8d8d324a568a36955e4765_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many authors are on the paper ""Multimodal Fusion Interactions: A Study of Human and Automatic Quantification""?

Context: Title: Multimodal Fusion Interactions: A Study of Human and Automatic sQuantification
Authors: Paul Pu Liang, Yun Cheng, Ruslan Salakhutdinov, Louis-Philippe Morency
Section: A HUMAN ANNOTATION DETAILS
Participation in all annotations was fully voluntary and we obtained consent from all participants prior to annotations. The authors manually took anonymous notes on all results and feedback in such a manner that the identities of annotators cannot readily be ascertained directly or through identifers linked to the subjects. Participants were not the authors nor in the same research groups as the authors, but they all hold or are working towards a graduate degree in a STEM feld and have knowledge of machine learning. None of the participants knew about this project before their session and each participant only interacted with the setting they were involved in. We sample 50 datapoints from each of the 5 datasets in Table 1 and give them to a total of 18 diferent annotators: • 3 annotators for direct annotation of interactions, • 3 annotators for partial labeling of �1, �2, and �12, • 3 annotators for counterfactual, labeling �1 frst then �1+2, • 3 annotators for counterfactual, labeling �2 frst then �2+1. All annotations were performed via google spreadsheets. A.1 Annotating partial labels We asked 3 annotators to predict the partial labels in a randomized setting. For each annotator, we asked them to annotate �1 then �2 given only modality 1 or 2 respectively, and fnally � given both modalities. This completion order is designed on purpose to minimize possible memorization of the data so that the annotators can provide completely independent unimodal and multimodal predictions on the label. When annotating the visual modality of the video datasets, we explicitly require the annotators to mute the audio and predict the partial labels based only on the video frames. After that, all annotators are asked to provide a confdence score on a scale of 0 (no confdence) to 5 (high confdence) about their annotations. The confdence scale is applied to all annotation settings below.
Faculty Name: louis philippe morency
Paperid: 90b09bdb1bd78875ee8d8d324a568a36955e4765
Title: Multimodal Fusion Interactions: A Study of Human and Automatic Quantification
Year: 2023
Abstract: In order to perform multimodal fusion of heterogeneous signals, we need to understand their interactions: how each modality individually provides information useful for a task and how this information changes in the presence of other modalities. In this paper, we perform a comparative study of how humans annotate two categorizations of multimodal interactions: (1) partial labels, where different annotators annotate the label given the first, second, and both modalities, and (2) counterfactual labels, where the same annotator annotates the label given the first modality before asking them to explicitly reason about how their answer changes when given the second. We further propose an alternative taxonomy based on (3) information decomposition, where annotators annotate the degrees of redundancy: the extent to which modalities individually and together give the same predictions, uniqueness: the extent to which one modality enables a prediction that the other does not, and synergy: the extent to which both modalities enable one to make a prediction that one would not otherwise make using individual modalities. Through experiments and annotations, we highlight several opportunities and limitations of each approach and propose a method to automatically convert annotations of partial and counterfactual labels to information decomposition, yielding an accurate and efficient method for quantifying multimodal interactions.
Authors: P. Liang, Yun Cheng, R. Salakhutdinov, Louis-Philippe Morency
Venue: International Conference on Multimodal Interaction
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A comparative study of how humans annotate two categorizations of multimodal interactions is performed and a method to automatically convert annotations of partial and counterfactual labels to information decomposition is proposed, yielding an accurate and efficient method for quantifying multimodals interactions.'}
Url: https://dl.acm.org/doi/pdf/10.1145/3577190.3614151
Answer: "
"ICML is the abbreviation for which conference?
","['maarten sap_c2850c897a179c07a25023029306600e0ea82f75_content_1.txt', 'maarten sap_c2850c897a179c07a25023029306600e0ea82f75_content_3.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: ICML is the abbreviation for which conference?

Context: Lips. Queer in AI Workshop at International Conference on Machine Learning 2021 (2021). https://sites.google.com/view/queer-in-ai/icml- 2021#h.lx7wo16mt2ax [44] ELLIS. 2022. ELLIS PhD Program: Call for applications 2022. https://ellis.eu/ news/ellis-phd-program-call-for-applications-2022 [45] Myra Marx Ferree. 2016. The discursive politics of feminist intersectionality. In Framing Intersectionality. Routledge, 55–65. [46] Michelle Fine and María Elena Torre. 2006. Intimate details: Participatory action research in prison. Action Research 4, 3 (2006), 253–269. [47] Aoife Finn, Peter-Lucas Jones, Keoni Mahelona, Suzanne Duncan, and Gianna Leoni. 2022. Developing a Part-Of-Speech tagger for te reo Māori. In Proceedings of the Fifth Workshop on the Use of Computational Methods in the Study of Endangered Languages. 93–98. [48] Luciano Floridi. 2019. Establishing the rules for building trustworthy AI. Nature Machine Intelligence 1, 6 (2019), 261–262. [49] Association for Computational Linguistics. (n.d.). https://www.aclweb.org/ [50] Jon Freeman. 2023. Letter to the NSF Director. https://static1.squarespace. com/static/545d3fabe4b0811b5cc48193/t/63c867aefb89f3761070a5a3/ 1674078140137/Letter+to+NSF+Director+-+LGBTQ%2B+Data_redacted.pdf [51] Jonathan B. Freeman. 2020. Measuring and Resolving LGBTQ Disparities in STEM. Policy Insights from the Behavioral and Brain Sciences 7 (2020), 141 – 148. [52] Paolo Gaudiano. 2021. Exposure doesn’t pay: Why tech conferences should compensate their speakers.
2021. Truth from the machine: artificial intelligence and the materialization of identity. Interdisciplinary Science Reviews 46 (2021), 158 – 175. [73] Khipu. (n.d.). https://khipu.ai/committee-2023/ [74] Andrey Kormilitzin, Nenad Tomasev, Kevin R McKee, and Dan W Joyce. 2023. A participatory initiative to include LGBT+ voices in AI for mental health. Nature Medicine (2023), 1–2. [75] Gary A Kreps and Susan Lovegren Bosworth. 1994. Organizing, role enactment, and disaster: A structural theory. University of Delaware Press. [76] Katie Langin. 2023. NSF still won’t track sexual orientation among scientific workforce, prompting frustration. https://www.science.org/content/article/nsfstill-won-t-track-sexual-orientation-among-scientific-workforce-prompting [77] LatinX in AI (n.d.). https://www.latinxinai.org [78] Neil Lawrence. 2021. Comment on pull request: Fix author name. https: //github.com/mlresearch/v119/pull/4#issuecomment-760081621 [79] Jason Edward Lewis, Angie Abdilla, Noelani Arista, Kaipulaumakaniolono Baker, Scott Benesiinaabandan, Michelle Brown, Melanie Cheung, Meredith Coleman, Ashley Cordes, Joel Davison, et al. 2020. Indigenous protocol and artificial intelligence position paper. (2020). [80] Yanan Long. 2021. Automatic Gender Recognition: Perspectives from Phenomenological Hermeneutics. Queer in AI Workshop at International Conference on Machine Learning 2021 (2021). https://sites.google.com/view/queer-inai/icml-2021#h.lx7wo16mt2ax [81] Christina Lu, Jackie Kay, and Kevin McKee. 2022. Subverting machines, fluctuating identities: Re-learning human categorization. In 2022 ACM Conference on Fairness, Accountability, and Transparency. 1005–1015. [82] Sarah Maiter, Laura Simich, Nora Jacobson, and Julie Wise. 2008.
Answer: "
"What are the two standard benchmarks used to evaluate the performance of FREDOM?
","['shinji watanabe_ef567580e167c3e7c546345df93d644be5d4f66f_content_0.txt', 'shinji watanabe_ef567580e167c3e7c546345df93d644be5d4f66f_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the two standard benchmarks used to evaluate the performance of FREDOM?

Context: Title: AV-SUPERB: A MULTI-TASK EVALUATION BENCHMARK FOR AUDIO-VISUAL REPRESENTATION MODELS
Authors: Yuan Tseng, Layne Berry, Yi-Ting Chen, I-Hsiang Chiu, Hsuan-Hao Lin, Max Liu, Puyuan Peng, Yi-Jen Shih, Hung-Yu Wang, Haibin Wu, Po-Yao Huang, Chun-Mao Lai, Shang-Wen Li, David Harwath, Yu Tsao, Shinji Watanabe, Abdelrahman Mohamed, Chi-Luen Feng, Hung-yi Lee
Section: 4.4. Layer-wise Contribution Analysis
After fine-tuning the learnable weighted-sum over all upstream model layers on a downstream task, we may compare layer utilization by examining the weights of each layer in the weighted-sum. [42] Since the magnitude of representations from each layer may differ, we normalize layer weights for each layer by multiplying the weight with the L2-norm of representation values on the training set. For MAViL, we find the layers that are commonly more dominant are the last three layers in the audio encoder, and the last two layers in the video encoder and fusion layers. Despite this, we observe an exception for emotion recognition on IEMOCAP. For IEMOCAP, the most dominant layer is the 0th layer instead. For AV-HuBERT, the final layer often contributes little. In the audio-only setup, we see that the layer with the most contribution is the penultimate layer for most speech and audio tasks besides ASR. For ASR, the last two layers are highly dominant on all three tracks. For non-ASR tasks, we note that when additional visual inputs are given, prior layers increase in contribution only when audio-visual fusion outperforms audio-only performance for AV-HuBERT (VGGSound, Kinetics-Sound, UCF101, VoxCeleb2), suggesting that prior layers in AV-HuBERT are more related to visual information, while the last few layers contain more audio information. Overall, the variation in layer usage for different tasks, models, and modalities strongly motivates the use of the learnable weightedsum technique for evaluation, instead of suboptimally evaluating the final layer alone. 5. HOW DOES INTERMEDIATE-TASK FINE-TUNING AFFECT PERFORMANCE?
Title: AV-SUPERB: A MULTI-TASK EVALUATION BENCHMARK FOR AUDIO-VISUAL REPRESENTATION MODELS
Authors: Yuan Tseng, Layne Berry, Yi-Ting Chen, I-Hsiang Chiu, Hsuan-Hao Lin, Max Liu, Puyuan Peng, Yi-Jen Shih, Hung-Yu Wang, Haibin Wu, Po-Yao Huang, Chun-Mao Lai, Shang-Wen Li, David Harwath, Yu Tsao, Shinji Watanabe, Abdelrahman Mohamed, Chi-Luen Feng, Hung-yi Lee
Section: 4.4. Layer-wise Contribution Analysis
in Table 2. For AV-HuBERT, we see that visual speech recognition on LRS3-TED is not a suitable intermediate task in general. Videoonly representations obtain small gains in generalizability, at the cost of greatly reducing audio-only and fusion performance. We posit that intermediate-task fine-tuning with (video,text) pairs shifts AV-HuBERT Transformer layers to favor video input alone, reducing usability for audio-only and audio-visual inputs. Contrarily, for audio-visual fusion with MAViL, we see that intermediate-task training on AudioSet-2M not only brings substantial improvements to all AEC and AR datasets, but also improves ASV while maintaining ASR performance. This suggests that finetuning on AudioSet-2M may be sufficiently diverse to improve speaker separability of representations without much loss of content information. 6. CONCLUSIONS We introduce AV-SUPERB, the first benchmark for assessing general-purpose capabilities of audio-visual representations. AVSUPERB includes a suite of 7 speech and audio processing datasets covering 5 audio-visual tasks. The benchmark is split into three tracks: two unimodal audio-only or video-only representations tracks, as well as a bimodal audio-visual fusion track, which enables easy comparison between unimodal and bimodal learning. Despite advances made in recent years, our experiments show that none of the models tested generalize to all tasks, leading us to conclude that further study is required to develop universal audio-visual models. As discussed in Section 3.1, although our benchmark aims to comprehensively evaluate audio-visual models, only a limited set of tasks and datasets are included in its current form.
Answer: "
"Which conference was the paper Cross-Modal Fine-Tuning: Align then Refine published in? 
","['graham neubig_03c7f61b0c6bc4c480ed6da4e9d7dda104ddfec3_metadata.txt', 'graham neubig_papers.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which conference was the paper Cross-Modal Fine-Tuning: Align then Refine published in? 

Context: Faculty Name: graham neubig
Paperid: 03c7f61b0c6bc4c480ed6da4e9d7dda104ddfec3
Title: Cross-Modal Fine-Tuning: Align then Refine
Year: 2023
Abstract: Fine-tuning large-scale pretrained models has led to tremendous progress in well-studied modalities such as vision and NLP. However, similar gains have not been observed in many other modalities due to a lack of relevant pretrained models. In this work, we propose ORCA, a general cross-modal fine-tuning framework that extends the applicability of a single large-scale pretrained model to diverse modalities. ORCA adapts to a target task via an align-then-refine workflow: given the target input, ORCA first learns an embedding network that aligns the embedded feature distribution with the pretraining modality. The pretrained model is then fine-tuned on the embedded data to exploit the knowledge shared across modalities. Through extensive experiments, we show that ORCA obtains state-of-the-art results on 3 benchmarks containing over 60 datasets from 12 modalities, outperforming a wide range of hand-designed, AutoML, general-purpose, and task-specific methods. We highlight the importance of data alignment via a series of ablation studies and demonstrate ORCA's utility in data-limited regimes.
Authors: Junhong Shen, Liam Li, L. Dery, Corey Staten, M. Khodak, Graham Neubig, Ameet Talwalkar
Venue: International Conference on Machine Learning
Tldr: {'model': 'tldr@v2.0.0', 'text': ""This work proposes ORCA, a general cross-modal fine-tuning framework that extends the applicability of a single large-scale pretrained model to diverse modalities and highlights the importance of data alignment via a series of ablation studies and demonstrates ORCA's utility in data-limited regimes.""}
Url: http://arxiv.org/pdf/2302.05738
List of 2023 Open Access papers by graham neubig are:
Cross-Modal Fine-Tuning: Align then Refine
ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages
GlobalBench: A Benchmark for Global Progress in Natural Language Processing
Learning Performance-Improving Code Edits
CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code
Neural Machine Translation for the Indigenous Languages of the Americas: An Introduction
User-Centric Evaluation of OCR Systems for Kwak’wala
Multi-Dimensional Evaluation of Text Summarization with In-Context Learning
A Gold Standard Dataset for the Reviewer Assignment Problem
SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing
Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation
FacTool: Factuality Detection in Generative AI - A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios
Active Retrieval Augmented Generation
Large Language Models Enable Few-Shot Clustering
Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach
Crossing the Threshold: Idiomatic Machine Translation through Retrieval Augmentation and Loss Weighting
Why do Nearest Neighbor Language Models Work?
Syntax and Semantics Meet in the “Middle”: Probing the Syntax-Semantics Interface of LMs Through Agentivity
DataFinder: Scientific Dataset Recommendation from Natural Language Descriptions
Multi-lingual and Multi-cultural Figurative Language Understanding
It’s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk
Unlimiformer: Long-Range Transformers with Unlimited Length Input
WebArena: A Realistic Web Environment for Building Autonomous Agents
Prompt2Model: Generating Deployable Models from Natural Language Instructions
Computational Language Acquisition with Theory of Mind
Improving Factuality of Abstractive Summarization via Contrastive Reward Learning
The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation
Answer: "
"Who taught Advanced Natural Language Processing in Fall 2023?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who taught Advanced Natural Language Processing in Fall 2023?

Context: In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning' with Course ID 11485 and Section B offers 9.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ramakrishnan, Singh located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'LTI Minor Project - Seniors' with Course ID 11490 and Section A offers 12.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'LTI Minor Project - Advanced' with Course ID 11590 and Section A offers 12.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Coding & Algorithms Bootcamp:' with Course ID 11601 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Coding Boot Camp' with Course ID 11601 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Brown located in Building DH, Room 2315.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Natural Language Processing:' with Course ID 11611 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning' with Course ID 11485 and Section B offers 9.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ramakrishnan, Singh located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'LTI Minor Project - Seniors' with Course ID 11490 and Section A offers 12.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'LTI Minor Project - Advanced' with Course ID 11590 and Section A offers 12.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Coding & Algorithms Bootcamp:' with Course ID 11601 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Coding Boot Camp' with Course ID 11601 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Brown located in Building DH, Room 2315.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Natural Language Processing:' with Course ID 11611 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
Answer: "
"Is a valid CMU ID needed to make fitness reservations?
","['handbook_phd_2023-2024.txt', 'mlt-student-handbook-2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Is a valid CMU ID needed to make fitness reservations?

Context: The administrative offices are located in the Cohon University Fitness Center. 
The University Centers recreational facilities include an eight-lane pool, racquetball and squash 
courts, aerobics room, fitness center and gym for basketball and volleyball. With renovations to 
Skibo Gym and the new Highmark Center for Health, Wellness, and Athletics scheduled for 
completion in 2024, the strength and conditioning facility has been temporarily placed on the 
lawn next to the outdoor basketball court close to the Donner locker rooms, Gesling Stadium, and 
Weigand Gymnasium. All users must present a valid CMU ID to use these facilities. 
6.18 CMU Alert 
www.cmu.edu/alert  
CMU Alert sends voice and/or text messages to registered phones in the event of a campus 
emergency that threatens public safety or during tests of the system in the spring and fall 
semesters. Students are automatically registered for CMU-Alert using the current contact 
information that has been entered into the Student Information Online (SIO): 
https://www.cmu.edu/hub/sio/about.html. 
 
 
 
 
LTI Ph.D. Graduate Student Handbook 
Page 40 
 
 
Appendix A 
2023-2024 
Highlighted University Resources for Graduate Students 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Note: The following pages are meant to broadly include only some of the resources available to 
graduate students.  It is not an exhaustive appendix of resources, and students are strongly 
encouraged to visit the various websites linked below for the most up-to-date information.   
LTI Ph.D. Graduate Student Handbook 
Page 41 
 
A.1 Key Resources for Graduate Student Support  
A.1.1 Office of Graduate and Postdoctoral Affairs  
https://www.cmu.edu/graduate  graded@cmu.edu  
The Office of Graduate and Postdoctoral Affairs provides university-wide support for all graduate 
students and academic programs, with a focus on supporting graduate student success at 
Carnegie Mellon.
26 
5.3.5 
Gestational and Parental Accommodation . 26 
MLT Graduate Student Handbook 
Page 4 
 
6 
Financial Issues . 27 
6.1 
Graduate Student Funding . 27 
6.2 
University Financial Aid . 28 
6.3 
Conference Travel Funding . 28 
6.4 
Office of the Dean of Students Emergency Support Funding . 28 
6.5 
Health Insurance . 28 
6.6 
Emergency Loans . 29 
6.7 
Employment Eligibility Verification . 29 
7 
Additional University Resources . 29 
7.1 
The HUB Student Services Center . 29 
7.2 
Student Information Online (SIO) . 30 
7.3 
ID Cards . 30 
7.4 
Transcripts . 30 
7.5 
Pittsburgh Council on Higher Education (PCHE) and Cross-registration . 31 
7.6 
Student Privacy Rights and FERPA . 31 
7.7 
Academic Calendar . 31 
7.8 
Professional Development . 32 
7.9 
University Libraries . 32 
7.10 Computing Services . 33 
7.11 Family and Dependents Resources . 33 
7.12 Domestic Partner Registration . 33 
7.13 Housing . 34 
7.14 Dining . 34 
7.15 Parking and Transportation . 34 
7.16 Copying, Printing and Mailing Services . 35 
7.17 University Center . 35 
7.18 Athletic/Fitness Facilities . 35 
7.19 CMU Alert . 36 
A 
Appendix:  Highlighted University Resources for Graduate Students and The WORD, Student 
Handbook . 36 
A.1 
Key Offices for Graduate Student Support . 36 
A.1.1 
Graduate Education Office . 36 
A.1.2 
Office of the Dean of Students . 37 
MLT Graduate Student Handbook 
Page 5 
 
A.1.3 
Center for Student Diversity & Inclusion . 37 
A.1.4 
Eberly Center for Teaching Excellence & Educational Innovation . 38 
A.1.5 
Carnegie Mellon Ethics Hotline . 38 
A.1.6 
Policy Against Retaliation . 39 
A.1.7 
Graduate Student Assembly .
Answer: "
"When did the Fall Break start in 2023?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When did the Fall Break start in 2023?

Context: From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mini-2 Classes Begin is observed.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mid-Semester & Mini-1 grades due by 4 pm is observed.
On Friday, 27 October, 2023, during the Fall 2023 (F23) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
On Tuesday, 07 November, 2023, during the Fall 2023 (F23) semester, Democracy Day is observed, leading to No Classes, except Evening classes after 5 pm will still meet.
On Saturday, 11 November, 2023, during the Fall 2023 (F23) semester, Homecoming is observed.
From 13 November, 2023,Monday to 17 November, 2023,Friday marks the Spring 2024 Registration Week for Fall 2023 (F23) semester.
On Monday, 13 November, 2023, during the Fall 2023 (F23) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Wednesday, 15 November, 2023, during the Fall 2023 (F23) semester, Mini-2 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 22 November, 2023,Wednesday to 24 November, 2023,Friday marks the Thanksgiving Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Mini-2 pass/no pass & withdrawal deadline (2) is observed.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Faculty Course Evaluations open is observed.
On Friday, 08 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Last Day of Classes is observed.
On Monday, 28 August, 2023, during the Fall 2023 (F23) semester, Semester & Mini-1 Classes Begin is observed.
On Friday, 01 September, 2023, during the Fall 2023 (F23) semester, Mini-1 add, audit & tuition adjustment drop deadline  (1) is observed.
On Monday, 04 September, 2023, during the Fall 2023 (F23) semester, Labor Day is observed, leading to No Classes & University Closed.
On Monday, 11 September, 2023, during the Fall 2023 (F23) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Wednesday, 20 September, 2023, during the Fall 2023 (F23) semester, Mini-1 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 02 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Pass/no pass & withdrawal deadline (3) is observed.
On Monday, 02 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Faculty Course Evaluations open is observed.
On Monday, 09 October, 2023, during the Fall 2023 (F23) semester, Semester drop deadline is observed, leading to withdrawal grade assigned after this date.
On Friday, 13 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Last Day of Classes is observed.
On Friday, 13 October, 2023, during the Fall 2023 (F23) semester, Mini-1 voucher election deadline (4) is observed.
From 13 October, 2023,Friday to 14 October, 2023,Saturday marks the Family Weekend for Fall 2023 (F23) semester.
On Saturday, 14 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Exams is observed.
On Saturday, 14 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Faculty Course Evaluations close is observed.
From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
Answer: "
"What are the course number(s) for the Search Engines course?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the course number(s) for the Search Engines course?

Context: In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Search Engines' with Course ID 11442 and Section A offers 9.0 units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Callan located in Building GHC, Room 4401.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning:' with Course ID 11485 and Section NA offers 9.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning' with Course ID 11485 and Section NA offers 9.0 units. The Class meets Monday Wednesday between 08:00AM and 09:20AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Singh, Ramakrishnan located in Building BH, Room A51.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning:' with Course ID 11485 and Section NA offers 9.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning' with Course ID 11485 and Section A offers 9.0 units. The Class meets Friday between 08:00AM and 09:20AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ramakrishnan, Singh located in Building BH, Room A51.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning' with Course ID 11485 and Section B offers 9.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Singh, Ramakrishnan located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Search Engines' with Course ID 11442 and Section A offers 9.0 units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Callan located in Building GHC, Room 4401.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning:' with Course ID 11485 and Section NA offers 9.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning' with Course ID 11485 and Section NA offers 9.0 units. The Class meets Monday Wednesday between 08:00AM and 09:20AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Singh, Ramakrishnan located in Building BH, Room A51.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning:' with Course ID 11485 and Section NA offers 9.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning' with Course ID 11485 and Section A offers 9.0 units. The Class meets Friday between 08:00AM and 09:20AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ramakrishnan, Singh located in Building BH, Room A51.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning' with Course ID 11485 and Section B offers 9.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Singh, Ramakrishnan located in Building DNM, Room DNM.
Answer: "
"To complete the course requirements for the PhD in Language and Information Technologies degree, how many course units of graduate courses does the student have to pass?
","['program_info_PhDinLanguageandInformationTechnology.txt', 'handbook_phd_2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: To complete the course requirements for the PhD in Language and Information Technologies degree, how many course units of graduate courses does the student have to pass?

Context: Academic Program Name:
Ph.D. in Language and Information Technology

Website:
https://lti.cs.cmu.edu/academics/phd-programs/phd-lti.html

Overview:
In order to obtain your Ph.D. in Language and Information Technologies, you need to pass 96 units (generally, eight courses) of graduate courses that fulfill these requirements: At least 72 units of LTI courses: Must include one class in each LTI focus area. At least 24 units of SCS courses. At least two lab courses in two different research areas. Here's a sample of what your five-year schedule might look like: Fall Spring Summer Year 1 Grammars and Lexicons Algorithms for NLP Directed Study Search Engines or Machine Learning for Text Mining Machine Translation Directed Study Required Research Year 2 Software Engineering for LT (I) Speech Understanding Self-Paced Lab Directed Study Software Engineering for LT (II) Self-Paced Lab Directed Study Required Research Year 3 Directed Research Directed Research Directed Research Year 4 Directed Research Directed Research Directed Research Year 5 Directed Research Directed Research Directed Research

Requirements:
In order to obtain your Ph.D. in Language and Information Technologies, you need to pass 96 units (generally, eight courses) of graduate courses that fulfill these requirements: At least 72 units of LTI courses: Must include one class in each LTI focus area. At least 24 units of SCS courses. At least two lab courses in two different research areas. Here's a sample of what your five-year schedule might look like: Fall Spring Summer Year 1 Grammars and Lexicons Algorithms for NLP Directed Study Search Engines or Machine Learning for Text Mining Machine Translation Directed Study Required Research Year 2 Software Engineering for LT (I) Speech Understanding Self-Paced Lab Directed Study Software Engineering for LT (II) Self-Paced Lab Directed Study Required Research Year 3 Directed Research Directed Research Directed Research Year 4 Directed Research Directed Research Directed Research Year 5 Directed Research Directed Research Directed Research

Curriculum:
In order to obtain your Ph.D. in Language and Information Technologies, you need to pass 96 units (generally, eight courses) of graduate courses that fulfill these requirements: At least 72 units of LTI courses: Must include one class in each LTI focus area. At least 24 units of SCS courses. At least two lab courses in two different research areas.
CS ids are being phased out very slowly, so it is likely that you 
will need both types of user id. 
LTI Ph.D. Graduate Student Handbook 
Page 14 
 
The School of Computer Science has a Help Center in GHC 4201. It can be contacted at 
help@cs.cmu.edu , extension 8-4231 from a campus phone, or 412-268-4231 from an outside line 
(M-F, 9am-5pm). 
3 
Standard Degree Requirements & Degree Certification  
                                      
3.1 
LTI Ph.D. Degree Requirements 
To complete the Ph.D. in Language and Information Technologies degree, the student must satisfy 
the following requirements:  
 Pass at least 96 units of graduate level courses, with additional requirements detailed 
below; 
 Satisfy proficiencies in Writing, Presentation, Programming, and Teaching; 
 Propose, write, and defend a Ph.D. dissertation (thesis); 
 Attend the LTI Colloquium (11-700) each semester; and 
 Satisfy the Research Speaking Requirement.  
The sections below provide more detail about each of these requirements. 
3.1.1 Course Requirements 
To complete the course requirements for the Ph.D. in Language and Information Technologies 
degree, the student must pass 96 or more course units of graduate courses, and meet the following 
criteria: 
 At least 72 units of LTI courses and 24 units of SCS courses, 
 At least one class in each LTI Focus Area, and  
 At least two labs, in two different research areas. 
For definitions of quoted terms, see the section on Definitions of LTI Terminology. 
Unless approved by the Program Director in advance, the course requirements must be satisfied 
by actual classroom courses, not credit given for research or independent study. 
An LTI course is any 12-unit course with a number of 11-XXX; a 6-unit course with 11-XXX 
counts as one-half of an LTI course. Unless otherwise specified, ""course"" means an actual 
classroom course, not credit given for research or independent study. Note that the LTI allows 
any one MLD (10-XXX) graduate course to count as an ""LTI course"".
Answer: "
"In spring 2024, What is the title of course 10701?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, What is the title of course 10701?

Context: In Semester Spring 2024, from the department of Chemistry, the subject titled 'Graduate Teaching II' with Course ID 09932 and Section A offers 1224 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Stump located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Chemistry, the subject titled 'Internship' with Course ID 09990 and Section A offers 0-99 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Noonan located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Civil & Environmental Engineering, the subject titled 'Study Abroad' with Course ID 12051 and Section A offers 0.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building TBA, Room None.
In Semester Spring 2024, from the department of Civil & Environmental Engineering, the subject titled 'Exploring CEE: Infrastructure and Environment in a Changing World' with Course ID 12100 and Section NA offers 12.0 units. The Class meets Monday Friday between 02:00PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Harper located in Building PH, Room 107E.
In Semester Spring 2024, from the department of Civil & Environmental Engineering, the subject titled 'Exploring CEE: Infrastructure and Environment in a Changing World' with Course ID 12100 and Section A offers 12.0 units. The Class meets Wednesday between 02:00PM and 03:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Harper located in Building PH, Room 107E.
In Semester Spring 2024, from the department of Civil & Environmental Engineering, the subject titled 'Geology' with Course ID 12201 and Section A offers 9.0 units. The Class meets Monday Wednesday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Gregory located in Building POS, Room 151.
In Semester Spring 2024, from the department of Chemistry, the subject titled 'Graduate Teaching II' with Course ID 09932 and Section A offers 1224 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Stump located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Chemistry, the subject titled 'Internship' with Course ID 09990 and Section A offers 0-99 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Noonan located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Civil & Environmental Engineering, the subject titled 'Study Abroad' with Course ID 12051 and Section A offers 0.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building TBA, Room None.
In Semester Spring 2024, from the department of Civil & Environmental Engineering, the subject titled 'Exploring CEE: Infrastructure and Environment in a Changing World' with Course ID 12100 and Section NA offers 12.0 units. The Class meets Monday Friday between 02:00PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Harper located in Building PH, Room 107E.
In Semester Spring 2024, from the department of Civil & Environmental Engineering, the subject titled 'Exploring CEE: Infrastructure and Environment in a Changing World' with Course ID 12100 and Section A offers 12.0 units. The Class meets Wednesday between 02:00PM and 03:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Harper located in Building PH, Room 107E.
In Semester Spring 2024, from the department of Civil & Environmental Engineering, the subject titled 'Geology' with Course ID 12201 and Section A offers 9.0 units. The Class meets Monday Wednesday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Gregory located in Building POS, Room 151.
Answer: "
"In summer 2024, When is the first day of Mini-6 classes?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-2324.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In summer 2024, When is the first day of Mini-6 classes?

Context: On Friday, 07 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 pass/no pass & withdrawal deadline (3) is observed.
On Friday, 14 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Faculty Course Evaluations open is observed.
On Tuesday, 18 June, 2024, during the Summer One 2024 (M24) semester, Semester course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Wednesday, 19 June, 2022, during the Summer One 2024 (M24) semester, Juneteenth is observed, leading to University Closed & No Classes.
On Thursday, 20 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Last Day of Classes is observed.
On Thursday, 20 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 voucher deadline (4) is observed.
On Friday, 21 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Final Exams is observed.
On Friday, 21 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Faculty Course Evaluations close is observed.
On Monday, 24 June, 2024, during the Summer One 2024 (M24) semester, Mini-6 First Day of Classes is observed.
On Tuesday, 25 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Final Grades Due by 4 pm is observed.
On Friday, 28 June, 2024, during the Summer One 2024 (M24) semester, Mini-6 add, audit, & tuition adjustment drop deadline (1) is observed.
On Thursday, 04 July, 2024, during the Summer One 2024 (M24) semester, Independence Day is observed, leading to University Closed & No Classes.
On Friday, 05 July, 2024, during the Summer One 2024 (M24) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
Event: 'Mini-5 course drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-07', Day: 'Friday', Event: 'Mini-5 pass/no pass & withdrawal deadline (3)', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-14', Day: 'Friday', Event: 'Mini-5 Faculty Course Evaluations open', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-18', Day: 'Tuesday', Event: 'Semester course drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Summer One 2024 (M24)'
Date: '2022-06-19', Day: 'Wednesday', Event: 'Juneteenth; University Closed & No Classes', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-20', Day: 'Thursday', Event: 'Mini-5 Last Day of Classes', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-20', Day: 'Thursday', Event: 'Mini-5 voucher deadline (4)', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-21', Day: 'Friday', Event: 'Mini-5 Final Exams', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-21', Day: 'Friday', Event: 'Mini-5 Faculty Course Evaluations close ', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-24', Day: 'Monday', Event: 'Mini-6 First Day of Classes', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-25', Day: 'Tuesday', Event: 'Mini-5 Final Grades Due by 4 pm', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-28', Day: 'Friday', Event: 'Mini-6 add, audit, & tuition adjustment drop deadline (1)', Semester: 'Summer One 2024 (M24)'
Date: '2024-07-04', Day: 'Thursday', Event: 'Independence Day; University Closed & No Classes',
Answer: "
"Who is the first paper on the KALE paper by Jamie Callan's group?
","['jamie callan_1e0a4ff0c5d2d850ff5907e310ffcedc9cad9718_metadata.txt', 'jamie callan_papers.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who is the first paper on the KALE paper by Jamie Callan's group?

Context: Faculty Name: jamie callan
Paperid: 1e0a4ff0c5d2d850ff5907e310ffcedc9cad9718
Title: KALE: Using a K-Sparse Projector for Lexical Expansion
Year: 2023
Abstract: Recent research has proposed retrieval approaches based on sparse representations and inverted indexes, with terms produced by neural language models and leveraging the advantages from both neural retrieval and lexical matching. This paper proposes KALE, a new lightweight method of this family that uses a small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary. The KALE vocabulary captures semantic concepts than perform well when used in isolation, and perform better when extending the original lexical vocabulary, this way improving first-stage retrieval accuracy. Experiments with the MSMARCOv1 passage retrieval dataset, the TREC Deep Learning dataset, and BEIR datasets, examined the effectiveness of KALE under varying conditions. Results show that the KALE terms can replace the original lexical vocabulary, with gains in accuracy and efficiency. Combining KALE with the original lexical vocabulary, or with other learned terms, can further improve retrieval accuracy with only a modest increase in computational cost.
Authors: Luís Borges, Bruno Martins, Jamie Callan
Venue: International Conference on the Theory of Information Retrieval
Tldr: {'model': 'tldr@v2.0.0', 'text': 'KALE is a new lightweight method that uses a small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary, which can replace the original lexical vocabulary with gains in accuracy and efficiency.'}
Url: https://dl.acm.org/doi/pdf/10.1145/3578337.3605131
List of 2023 Open Access papers by jamie callan are:
Conversational Search with Random Walks over Entity Graphs
KALE: Using a K-Sparse Projector for Lexical Expansion
CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms
Active Retrieval Augmented Generation
Multi-Objective Improvement of Android Applications
Answer: "
"How many months is the longer track of the MIIS program?
","['miis-handbook_2023-2024.txt', 'program_info_MasterofScienceinIntelligentInformationSystems.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many months is the longer track of the MIIS program?

Context: Under U.S. Federal Title IV regulations, 
student eligibility for federal financial aid is contingent upon enrollment in and successful 
completion of courses that are counted as credit toward their current degree program. To 
receive the maximum amount of federal financial aid for which they may be eligible, students 
must enroll each semester in at least 36 units that count toward their current degree level. (See 
separate guidance regarding integrated degree completion.) Students should consult with their 
designated college liaison in The HUB regarding billing and financial aid, particularly for early 
completion, longer-than standard completion, or integrated undergraduate and master’s degree 
programs.   
 
International Students 
Immigration status for students in F-1 and J-1 nonimmigrant status is tied to making normal 
progress toward completing degree requirements. Therefore, F-1 and J-1 students who are 
considering completing their degree requirements early, anticipating longer-than-standard 
completion, or moving from an undergraduate to a graduate student classification (integrated 
undergraduate-graduate study) should consult with their designated advisor in the Office of 
International Education (OIE) to ensure compliance with immigration regulations. 
MIIS Graduate Student Handbook 
Page 13 
 
4 MIIS Degree Requirements and Related Policies/Protocols 
4.1 Program Options 
The MIIS degree is offered in two options: 
Option 1. Standard MIIS degree (MIIS-16) - A 16-month track that is completed in three 
academic semesters (fall, spring, fall) and a summer internship.  
Option 2. MIIS: Advanced Study degree (MIIS-21) - A 21-month track that is completed in 
four academic semesters (fall, spring, fall, spring) and a summer internship. 
MIIS: Advanced Study track offers in depth degree in one of the following areas of 
concentration: 
• Human Language for Language Technologies 
• Language Technology Application 
• Machine Learning for Language Technologies 
Part-time options are available in some cases. 
4.2 Required Units for Degree Attainment 
To complete the Master of Science in Intelligent Information Systems, a student must satisfy 
three types of requirements. Curricular requirements ensure that MIIS students receive 
instruction in core intelligent information systems technologies while also allowing an 
opportunity to specialize in areas of personal interest. Practice requirements are opportunities 
to apply and hone new skills while building state-of-the-art systems.
Academic Program Name:
Master of Science in Intelligent Information Systems

Website:
https://lti.cs.cmu.edu/academics/masters-programs/miis.html

Overview:
The Master's in Intelligent Information Systems degree focuses on recognizing and extracting meaning from text, spoken language and video. As an MIIS student, you’ll receive the department’s deepest exposure to content analysis and machine learning. In addition to completing the program’s coursework, you’ll work on directed study projects with your faculty advisor for two semesters; participate in a summer internship; and collaborate with your peers on a semester-long, group-oriented capstone project. This combination of classroom instruction, professional experience, and using new skills in significant projects with world-class colleagues will help prepare you for a successful career in industry or government. Our alumni have gone on to exciting careers at places like Apple, IBM and Google, and most have job offers within six weeks of graduation.

Requirements:
The Intelligent Information Systems degree offers students the flexibility to create their own course of study in consultation with their advisor.
MIIS students gain three types of practical experience: software development supervised by their advisor (24 units equivalent to two courses); a summer internship (which can be waived for students that have sufficient prior professional experience); and a capstone project executed in a group of peers (42 units equivalent to three 12-unit courses and one 6-unit course). This combination is proven to help IIS students to broaden their skills quickly. The MIIS degree is offered in two options:
Option 1. Standard MIIS degree (MIIS-16) - A 16-month track that is completed in three academic semesters (fall, spring, fall) and a summer internship.
Option 2. MIIS: Advanced Study degree (MIIS-21) - A 21-month track that is completed in four academic semesters (fall, spring, fall, spring) and a summer internship.
MIIS: Advanced Study track offers an in-depth degree in one of the following areas of concentration:
Human Language for Language Technologies
Language Technology Application
Machine Learning for Language Technologies
Part-time education option is available in some cases.
MIIS-16 students must take at least 84 units (typically 7 courses) of qualifying and elective courses that satisfy human language, machine learning, and language technology applications breadth requirements.
Answer: "
"What does ICTIR stand for?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What does ICTIR stand for?

Context: The Class meets Friday between 10:00AM and 11:50AM ET. Students attend lectures at the Kigali, Rwanda location,led by experienced instructor NA located in Building CMR, Room F305.
In Semester Fall 2023, from the department of Information & Communication Technology, the subject titled 'ICT in Africa Seminar' with Course ID 04601 and Section A offers 6.0 units. The Class meets Friday between 08:00AM and 09:50AM ET. Students attend lectures at the Kigali, Rwanda location,led by experienced instructor Luhanga located in Building CMR, Room F203.
In Semester Fall 2023, from the department of Information & Communication Technology, the subject titled 'ICT Entrepreneurship Seminar' with Course ID 04603 and Section A offers 6.0 units. The Class meets Tuesday between 12:00PM and 01:50PM ET. Students attend lectures at the Kigali, Rwanda location,led by experienced instructor Thornburg located in Building CMR, Room F203.
In Semester Fall 2023, from the department of Information & Communication Technology, the subject titled 'ICT Professional Development Seminar' with Course ID 04605 and Section A offers 6.0 units. The Class meets Friday between 10:00AM and 11:50AM ET. Students attend lectures at the Kigali, Rwanda location,led by experienced instructor Niyizamwiyitira located in Building CMR, Room F203.
In Semester Fall 2023, from the department of Information & Communication Technology, the subject titled 'Academic Skills for Engineers I' with Course ID 04606 and Section A offers 6.0 units. The Class meets Tuesday Thursday between 12:00PM and 01:50PM ET. Students attend lectures at the Kigali, Rwanda location,led by experienced instructor Thompson located in Building CMR, Room F205.
In Semester Fall 2023, from the department of Information & Communication Technology, the subject titled 'Academic Skills for Engineers I' with Course ID 04606 and Section B offers 6.0 units. The Class meets Monday Wednesday between 12:00PM and 01:50PM ET. Students attend lectures at the Kigali, Rwanda location,led by experienced instructor Thompson located in Building CMR, Room F205.
The Class meets Friday between 10:00AM and 11:50AM ET. Students attend lectures at the Kigali, Rwanda location,led by experienced instructor NA located in Building CMR, Room F305.
In Semester Fall 2023, from the department of Information & Communication Technology, the subject titled 'ICT in Africa Seminar' with Course ID 04601 and Section A offers 6.0 units. The Class meets Friday between 08:00AM and 09:50AM ET. Students attend lectures at the Kigali, Rwanda location,led by experienced instructor Luhanga located in Building CMR, Room F203.
In Semester Fall 2023, from the department of Information & Communication Technology, the subject titled 'ICT Entrepreneurship Seminar' with Course ID 04603 and Section A offers 6.0 units. The Class meets Tuesday between 12:00PM and 01:50PM ET. Students attend lectures at the Kigali, Rwanda location,led by experienced instructor Thornburg located in Building CMR, Room F203.
In Semester Fall 2023, from the department of Information & Communication Technology, the subject titled 'ICT Professional Development Seminar' with Course ID 04605 and Section A offers 6.0 units. The Class meets Friday between 10:00AM and 11:50AM ET. Students attend lectures at the Kigali, Rwanda location,led by experienced instructor Niyizamwiyitira located in Building CMR, Room F203.
In Semester Fall 2023, from the department of Information & Communication Technology, the subject titled 'Academic Skills for Engineers I' with Course ID 04606 and Section A offers 6.0 units. The Class meets Tuesday Thursday between 12:00PM and 01:50PM ET. Students attend lectures at the Kigali, Rwanda location,led by experienced instructor Thompson located in Building CMR, Room F205.
In Semester Fall 2023, from the department of Information & Communication Technology, the subject titled 'Academic Skills for Engineers I' with Course ID 04606 and Section B offers 6.0 units. The Class meets Monday Wednesday between 12:00PM and 01:50PM ET. Students attend lectures at the Kigali, Rwanda location,led by experienced instructor Thompson located in Building CMR, Room F205.
Answer: "
"What is included in the ACL 60/60 evaluation dataset?
","['mona diab_c5849f406e8263806a84e1a407ec0e0fe131bd5c_metadata.txt', 'shinji watanabe_f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b_content_2.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is included in the ACL 60/60 evaluation dataset?

Context: Faculty Name: mona diab
Paperid: c5849f406e8263806a84e1a407ec0e0fe131bd5c
Title: Evaluating Multilingual Speech Translation under Realistic Conditions with Resegmentation and Terminology
Year: 2023
Abstract: We present the ACL 60/60 evaluation sets for multilingual translation of ACL 2022 technical presentations into 10 target languages. This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics.
Authors: Elizabeth Salesky, Kareem Darwish, Mohamed Al-Badrashiny, Mona T. Diab, J. Niehues
Venue: International Workshop on Spoken Language Translation
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics.'}
Url: https://aclanthology.org/2023.iwslt-1.2.pdf
B.2 Simultaneous SLT B.3 Automatic Subtitling B.4 Multilingual Speech Translation Below we show the Multilingual task (§5) results and overall rankings, ordered according to the average chrF across all 10 target languages after resegmentation to the reference translations. We also compare to the Offline submissions on the ACL 60-60 evaluation set on the 3 language pairs used for the Offline task. Finally, we show the scores for each metric (chrF, COMET, BLEU) per language pair for all systems. B.5 Speech-to-Speech Translation
Answer: "
"Which benchmark was used in the study?
","['lei li_56de9c4c63ee74757be1b203d2ea852690087ded_content_3.txt', 'shinji watanabe_786294f4008732a5dac9895a8507bc4c80450075_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which benchmark was used in the study?

Context: Title: Hence, Socrates is mortal: A Benchmark for Natural Language Syllogistic Reasoning
Authors: Yongkang Wu, Meng Han, Yutao Zhu, Lei Li, Xinyu Zhang, Ruofei Lai, Xiaoguang Li, Yuanhang Ren, Zhicheng Dou, Zhao Cao
Section: I Case Study
audience to performing in front of or boost your ego. You can not use an audience to boost your ego. [Therefore], you can use an audience to performing in front of. rewritten sentences: When you’re in front of an audience, you can put on a show or increase your self-esteem. You cannot exaggerate your ego in front of an audience. [Therefore], you can give a performance in front of an audience. pattern: P is true or Q is true. P is not true, [Therefore], Q is true. original sentences: My flowers are ugly or pretty. My flowers are not ugly. [Therefore], My flowers are pretty. rewritten sentences: The blooms in my garden are either comely or unappealing. The blooms in my garden are not unsightly. Therefore, These flowers are indeed attractive. Table 19: GPT-3 rewriting prompts for polysyllogisms. Rewrite the following sentences to standard English. Keep the meaning of the original sentences, but change the expression of the sentences. original sentences: No hypothesis is fact. Some proposition are hypothesis. Some proposition are not fact. All proposition are abstract object. [Therefore], some abstract object are not fact. rewritten sentences: A hypothesis is a proposed explanation that differs from fact. Some propositions are hypotheses. Some propositions are proven not to be facts. Every proposition is an abstract object. [Therefore], some abstract objects do not exist as facts. original sentences: Applied science is science. No Science is art. Human science is science. Some Behavioral genetics are not human science. Behaviour genetics is psychology. Genetics is biology. [Therefore], some applied science are not biology. rewritten sentences: Applied science is science in every sense of the word. Science and art are two distinct forms of scholarship. Human science is a branch of science. Behavioral genetics does not involve any human science. Behavioral genetics is a branch of psychology.
Faculty Name: shinji watanabe
Paperid: 786294f4008732a5dac9895a8507bc4c80450075
Title: Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech
Year: 2023
Abstract: Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primarily focus on limited or specific tasks. Moreover, the lack of standardized benchmarks hinders a fair comparison across different approaches. Thus, we present Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and harness instruction tuning, we invite the community to collaborate and contribute, facilitating the dynamic growth of the benchmark. To initiate, Dynamic-SUPERB features 55 evaluation instances by combining 33 tasks and 22 datasets. This spans a broad spectrum of dimensions, providing a comprehensive platform for evaluation. Additionally, we propose several approaches to establish benchmark baselines. These include the utilization of speech models, text language models, and the multimodal encoder. Evaluation results indicate that while these baselines perform reasonably on seen tasks, they struggle with unseen ones. We also conducted an ablation study to assess the robustness and seek improvements in the performance. We release all materials to the public and welcome researchers to collaborate on the project, advancing technologies in the field together.
Authors: Chien-yu Huang, Ke-Han Lu, Shi Wang, Chi-Yuan Hsiao, Chun-Yi Kuan, Haibin Wu, Siddhant Arora, Kai-Wei Chang, Jiatong Shi, Yifan Peng, Roshan Sharma, Shinji Watanabe, Bhiksha Ramakrishnan, Shady Shehata, Hung-yi Lee
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work presents Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion, and invites the community to collaborate and contribute, facilitating the dynamic growth of the benchmark.'}
Url: https://arxiv.org/pdf/2309.09510
Answer: "
"In fall 2024, When do Semester & Mini-1 Classes begin?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2024, When do Semester & Mini-1 Classes begin?

Context: On Monday, 28 August, 2023, during the Fall 2023 (F23) semester, Semester & Mini-1 Classes Begin is observed.
On Friday, 01 September, 2023, during the Fall 2023 (F23) semester, Mini-1 add, audit & tuition adjustment drop deadline  (1) is observed.
On Monday, 04 September, 2023, during the Fall 2023 (F23) semester, Labor Day is observed, leading to No Classes & University Closed.
On Monday, 11 September, 2023, during the Fall 2023 (F23) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Wednesday, 20 September, 2023, during the Fall 2023 (F23) semester, Mini-1 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 02 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Pass/no pass & withdrawal deadline (3) is observed.
On Monday, 02 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Faculty Course Evaluations open is observed.
On Monday, 09 October, 2023, during the Fall 2023 (F23) semester, Semester drop deadline is observed, leading to withdrawal grade assigned after this date.
On Friday, 13 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Last Day of Classes is observed.
On Friday, 13 October, 2023, during the Fall 2023 (F23) semester, Mini-1 voucher election deadline (4) is observed.
From 13 October, 2023,Friday to 14 October, 2023,Saturday marks the Family Weekend for Fall 2023 (F23) semester.
On Saturday, 14 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Exams is observed.
On Saturday, 14 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Faculty Course Evaluations close is observed.
From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mini-2 Classes Begin is observed.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mid-Semester & Mini-1 grades due by 4 pm is observed.
On Friday, 27 October, 2023, during the Fall 2023 (F23) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
On Tuesday, 07 November, 2023, during the Fall 2023 (F23) semester, Democracy Day is observed, leading to No Classes, except Evening classes after 5 pm will still meet.
On Saturday, 11 November, 2023, during the Fall 2023 (F23) semester, Homecoming is observed.
From 13 November, 2023,Monday to 17 November, 2023,Friday marks the Spring 2024 Registration Week for Fall 2023 (F23) semester.
On Monday, 13 November, 2023, during the Fall 2023 (F23) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Wednesday, 15 November, 2023, during the Fall 2023 (F23) semester, Mini-2 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 22 November, 2023,Wednesday to 24 November, 2023,Friday marks the Thanksgiving Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Mini-2 pass/no pass & withdrawal deadline (2) is observed.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Faculty Course Evaluations open is observed.
On Friday, 08 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Last Day of Classes is observed.
Answer: "
"In spring 2024, What is the title of course 17200?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, What is the title of course 17200?

Context: In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Software Engineering Independent Study' with Course ID 17806 and Section C offers 1-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Garlan located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Software Engineering Independent Study' with Course ID 17806 and Section D offers 1-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Herbsleb located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Software Engineering Independent Study' with Course ID 17806 and Section E offers 1-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kaestner located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Software Engineering Independent Study' with Course ID 17806 and Section F offers 1-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Le Goues located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Software Engineering Independent Study' with Course ID 17806 and Section G offers 1-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Scherlis located in Building TBA, Room None.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Software Engineering Independent Study' with Course ID 17806 and Section H offers 1-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Begel located in Building TBA, Room None.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Software Engineering Independent Study' with Course ID 17806 and Section C offers 1-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Garlan located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Software Engineering Independent Study' with Course ID 17806 and Section D offers 1-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Herbsleb located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Software Engineering Independent Study' with Course ID 17806 and Section E offers 1-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kaestner located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Software Engineering Independent Study' with Course ID 17806 and Section F offers 1-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Le Goues located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Software Engineering Independent Study' with Course ID 17806 and Section G offers 1-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Scherlis located in Building TBA, Room None.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Software Engineering Independent Study' with Course ID 17806 and Section H offers 1-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Begel located in Building TBA, Room None.
Answer: "
"In spring 2024, What is the deadline for adding or dropping a Mini-4 course with tuition adjustment?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, What is the deadline for adding or dropping a Mini-4 course with tuition adjustment?

Context: On Monday, 22 January, 2024, during the Spring 2024 (S24) semester, Mini-3 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 29 January, 2024, during the Spring 2024 (S24) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Wednesday, 07 February, 2024, during the Spring 2024 (S24) semester, Mini-3 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 19 February, 2024, during the Spring 2024 (S24) semester, Mini-3 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 19 February, 2024, during the Spring 2024 (S24) semester, Mini-3 Faculty Course Evaluations open is observed.
On Monday, 26 February, 2024, during the Spring 2024 (S24) semester, Semester course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 01 March, 2024, during the Spring 2024 (S24) semester, Mini-3 Last Day of Classes is observed.
On Friday, 01 March, 2024, during the Spring 2024 (S24) semester, Mini-3 voucher deadline (4) is observed.
On Saturday, 02 March, 2024, during the Spring 2024 (S24) semester, Mini-3 Exams is observed.
On Saturday, 02 March, 2024, during the Spring 2024 (S24) semester, Mini-3 Faculty Course Evaluations close is observed.
From 04 March, 2024,Monday to 08 March, 2024,Friday marks the Spring Break for Spring 2024 (S24) semester which leads to No Classes.
On Monday, 11 March, 2024, during the Spring 2024 (S24) semester, First day of Mini-4 Classes is observed.
On Monday, 11 March, 2024, during the Spring 2024 (S24) semester, Mid-Semester & Mini-3 grades due by 4 pm is observed.
On Tuesday, 12 March, 2024, during the Spring 2024 (S24) semester, Summer 2024 Registration Opens is observed.
On Tuesday, 12 March, 2024, during the Spring 2024 (S24) semester, Summer 2024 Registration Opens is observed.
On Friday, 15 March, 2024, during the Spring 2024 (S24) semester, Mini-4 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 01 April, 2024, during the Spring 2024 (S24) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Wednesday, 03 April, 2024, during the Spring 2024 (S24) semester, Mini-4 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 11 April, 2024,Thursday to 13 April, 2024,Saturday marks the Spring Carnival for Spring 2024 (S24) semester which leads to No Classes.
From 15 April, 2024,Monday to 19 April, 2024,Friday marks the Fall 2024 Registration Week for Spring 2024 (S24) semester.
On Monday, 15 April, 2024, during the Spring 2024 (S24) semester, Mini-4 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 15 April, 2024, during the Spring 2024 (S24) semester, Semester & Mini-4 Faculty Course Evaluations open is observed.
On Friday, 26 April, 2024, during the Spring 2024 (S24) semester, Last Day of Classes is observed.
On Friday, 26 April, 2024, during the Spring 2024 (S24) semester, Semester & Mini-4 voucher deadline (4) is observed.
From 29 April, 2024,Monday to 30 April, 2024,Tuesday marks the Final Examinations for Spring 2024 (S24) semester.
On Wednesday, 01 May, 2024, during the Spring 2024 (S24) semester, Reading Day is observed.
From 02 May, 2024,Thursday to 03 May, 2024,Friday marks the Final Examinations for Spring 2024 (S24) semester.
From 04 May, 2024,Saturday to 05 May, 2024,Sunday marks the Reading Days for Spring 2024 (S24) semester.
Answer: "
"What is the phone number for CMU's office of Title IX initiatives?
","['miis-handbook_2023-2024.txt', 'handbook-msaii-2022-2023.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the phone number for CMU's office of Title IX initiatives?

Context: Any 
questions about the process can be directed to access@andrew.cmu.edu, or call (412) 268-
6121. 
7.2 Sexual Misconduct Policy 
The University prohibits sex-based discrimination, sexual harassment, sexual assault, dating/ 
domestic violence, and stalking. The University also prohibits retaliation against individuals 
who bring forward such concerns or allegations in good faith.   
The University’s Sexual Misconduct Policy is available at 
https://www.cmu.edu/policies/administrative-and-governance/sexual-misconduct/index.html.   
The University’s Policy Against Retaliation is available at  
https://www.cmu.edu/policies/administrative-and-governance/whistleblower.html.  
If you have been impacted by any of these issues, you are encouraged to make contact with 
any of the following resources: 
 Office of Title IX Initiatives  
o  https://www.cmu.edu/title-ix/, 412-268-7125, tix@cmu.edu 
 University Police 
o https://www.cmu.edu/police/, 412-268-2323 
 Additional resources and information can be found at: 
 https://www.cmu.edu/title-ix/resources-and-information/resources.html. 
7.3 Gestational and Parental Accommodations 
https://www.cmu.edu/graduate/programs-services/maternity-accommodation-protocol.html 
Providing holistic student support is a top priority at Carnegie Mellon. The protocols on this 
page are designed to support the parental needs of students and their families. 
Students seeking any of the Parental Accommodations described below must register with 
the Office of the Dean of Students by contacting the office for an appointment by calling 
412-268-2075. 
MIIS Graduate Student Handbook 
Page 29 
 
Students are encouraged to register with the Office of the Dean of Students ninety (90) days in 
advance of the anticipated arrival of the child as applicable in the individual circumstance.  At 
the time of registering, students will have the opportunity to consult about resources, 
procedures, funding options and preparation for discussing academic accommodations with the 
student’s academic department.  Students should also consult with their academic advisors 
either before or in conjunction with registering with the Office of the Dean of Students.
Obtain general information about Carnegie Mellon University 
by calling 412-268-2000. 
Carnegie Mellon University publishes an annual campus security and fire safety report 
describing the university's security, alcohol and drug, sexual assault and fire safety policies, and 
containing statistics about the number and type of crimes committed on the campus, and the 
number and cause of fires in campus residence facilities during the preceding three years. You 
can obtain a copy by contacting the Carnegie Mellon Police Department at 412-268-2323. The 
annual security and fire safety report also is available online at 
www.cmu.edu/police/annualreports.. 
Information regarding the application of Title IX, including to admission and employment 
decisions, the sexual misconduct grievance procedures and process, including how to file a 
report or a complaint of sex discrimination, how to file a report of sexual harassment, and how 
the university responds to such reports is available at www.cmu.edu/title-ix. The Title IX 
coordinator may be reached at 4615 Forbes Ave, Suite 330, Pittsburgh, PA 15213; 412-268-7125; or 
tix@cmu.edu. 
4 Carnegie Mellon Code 
Students at Carnegie Mellon, because they are members of an academic community dedicated to 
the achievement of excellence, are expected to meet the highest standards of personal, ethical 
and moral conduct possible. 
These standards require personal integrity, a commitment to honesty without compromise, as 
well as truth without equivocation and a willingness to place the good of the community above 
the good of the self. Obligations once undertaken must be met, commitments kept. 
As members of the Carnegie Mellon community, individuals are expected to uphold the 
standards of the community in addition to holding others accountable for said standards. It is 
rare that the life of a student in an academic community can be so private that it will not affect 
the community as a whole or that the above standards do not apply. 
The discovery, advancement, and communication of knowledge are not possible without a 
commitment to these standards. Creativity cannot exist without acknowledgment of the 
creativity of others. New knowledge cannot be developed without credit for prior knowledge. 
Without the ability to trust that these principles will be observed, an academic community 
cannot exist.
Answer: "
"In the paper ""Decoder-only Architecture for Speech Recognition with CTC Prompts and Text Data Augmentation, what is the reduction in word error rates achieved by the proposed models on LibriSpeech test-clean?
","['shinji watanabe_06353e1b7e7c8dc701ac76dcd4db5061b24468c9_metadata.txt', 'shinji watanabe_06353e1b7e7c8dc701ac76dcd4db5061b24468c9_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the paper ""Decoder-only Architecture for Speech Recognition with CTC Prompts and Text Data Augmentation, what is the reduction in word error rates achieved by the proposed models on LibriSpeech test-clean?

Context: Faculty Name: shinji watanabe
Paperid: 06353e1b7e7c8dc701ac76dcd4db5061b24468c9
Title: Decoder-only Architecture for Speech Recognition with CTC Prompts and Text Data Augmentation
Year: 2023
Abstract: Collecting audio-text pairs is expensive; however, it is much easier to access text-only data. Unless using shallow fusion, end-to-end automatic speech recognition (ASR) models require architecture modifications or additional training schemes to use text-only data. Inspired by recent advances in decoder-only language models (LMs), such as GPT-3 and PaLM adopted for speech-processing tasks, we propose using a decoder-only architecture for ASR with simple text augmentation. To provide audio information, encoder features compressed by CTC prediction are used as prompts for the decoder, which can be regarded as refining CTC prediction using the decoder-only model. Because the decoder architecture is the same as an autoregressive LM, it is simple to enhance the model by leveraging external text data with LM training. An experimental comparison using LibriSpeech and Switchboard shows that our proposed models with text augmentation training reduced word error rates from ordinary CTC by 0.3% and 1.4% on LibriSpeech test-clean and testother set, respectively, and 2.9% and 5.0% on Switchboard and CallHome. The proposed model had advantage on computational efficiency compared with conventional encoder-decoder ASR models with a similar parameter setup, and outperformed them on the LibriSpeech 100h and Switchboard training scenarios.
Authors: E. Tsunoo, Hayato Futami, Yosuke Kashiwagi, Siddhant Arora, Shinji Watanabe
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work proposes using a decoder-only architecture for ASR with simple text augmentation training that had advantage on computational efficiency compared with conventional encoder-decoder ASR models with a similar parameter setup, and outperformed them on the LibriSpeech 100h and Switchboard training scenarios.'}
Url: https://arxiv.org/pdf/2309.08876
Title: DECODER-ONLY ARCHITECTURE FOR SPEECH RECOGNITION WITH CTC PROMPTS AND TEXT DATA AUGMENTATION
Authors: Emiru Tsunoo, Hayato Futami, Yosuke Kashiwagi, Siddhant Arora, Shinji Watanabe
Section: 5. REFERENCES
[1] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al., “Deep speech: Scaling up endto-end speech recognition,” arXiv preprint arXiv:1412.5567, 2014. [2] Jan K. Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio, “Attention-based models for speech recognition,” in Proc. of NIPS, 2015, pp. 577– 585. [3] Zhong Meng, Sarangarajan Parthasarathy, Eric Sun, Yashesh Gaur, Naoyuki Kanda, Liang Lu, Xie Chen, Rui Zhao, Jinyu Li, and Yifan Gong, “Internal language model estimation for domain-adaptive end-to-end speech recognition,” in 2021 IEEE Spoken Language Technology Workshop (SLT), 2021, pp. 243–250. [4] Albert Zeyer, André Merboldt, Wilfried Michel, Ralf Schlüter, and Hermann Ney, “Librispeech transducer model with internal language model prior correction,” in Proc. of Interspeech, 2021, pp. 2052–2056. [5] Emiru Tsunoo, Yosuke Kashiwagi, Chaitanya Prasad Narisetty, and Shinji Watanabe, “Residual language model for end-toend speech recognition,” in Proc. of Interspeech 2022, 2022, pp. 3899–3903. [6] Adithya Renduchintala, Shuoyang Ding, Matthew Wiesner, and Shinji Watanabe, “Multi-modal data augmentation for endto-end asr,” Proc.
Answer: "
"Which institute did Carnegie Tech merge with in 1967?
","['cmuhistory_d402.txt', 'history_d401.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which institute did Carnegie Tech merge with in 1967?

Context: The merger built upon a long history of support from the Mellons. It allowed Carnegie Mellon to establish the last of its current pillars: the Mellon College of Science and the College of Humanities and Social Sciences, now known as Marianna Brown Dietrich College of Humanities and Social Sciences. In 2017, Carnegie Mellon celebrated the 50th anniversary of the Carnegie Tech-Mellon Institute merger, revisiting the shared vision of the founders and recognizing the impact it has had, and will continue to have, in the world of higher education, research and discovery. A Global Impact In its 115 years, Carnegie Mellon has soared to national and international leadership in higher education and research. A birthplace of innovation since its founding, it continues to be known for innovation, for solving real-world problems and for interdisciplinary collaboration. Its alumni can be found across the globe — from Tony Award winners to Nobel Prize and Turing Award winners, from CEOs to entrepreneurs, from professors to artists. In the 2000s, in response to demand for expanded international educational opportunities, Carnegie Mellon began offering degree programs outside of Pittsburgh. Today its global presence includes campuses in Qatar and Silicon Valley, Calif., more than a dozen degree-granting locations and more than 20 research partnerships such as Los Angeles; New York City; Washington, D.C.; Australia; China; Portugal and Rwanda. The Future CMU is positioned like never before to meet the challenges of the 21st century. In the coming years, the university will see the largest expansion to the Pittsburgh campus since 1900. At the intersection of technology and humanity, CMU research, innovation and creativity will continue to guide our future as a world-class university. As outlined in the Strategic Plan 2025, the university will focus on advancing the individual student experience, the broader Carnegie Mellon community experience, and the social impact of Carnegie Mellon throughout the world. Carnegie Mellon University challenges the curious and passionate to deliver work that matters.
The university’s first computer science Ph.D.s were graduates of this program.Computer Science Department establishedIn 1965, Carnegie Tech established its Computer Science Department, or CSD, with a $5 million grant from the R.K. Mellon Foundation. Perlis was the first department head. There were no undergraduates; only Ph.D. students were admitted, and the department’s focus was on research, much of it funded by the federal government through the Defense Department’s Advanced Research Projects Agency, or DARPA. In 1967, Carnegie Institute of Technology merged with the nearby Mellon Institute of Industrial Research to form Carnegie-Mellon University. The Department of Computer Science moved into the newly created Mellon Institute of Science, later renamed Mellon College of Science, or MCS. Future SCS dean Raj Reddy joined the CSD in 1969 after three years as an assistant professor at Stanford. He brought with him research in speech, language and computer vision. But in 1970 and 1971, the new Computer Science Department faced its first crisis, as half of its tenured faculty members—including department head Perlis—left for other universities. Joe Traub was recruited from Bell Labs to CMU to become the new department head.Multi-processor machines emergeCSD emerged from the brief crisis as a highly agile, interdisciplinary entity, with many new faculty members taking joint appointments with other CMU departments. Several large projects emerged in the Computer Science Department, including C.mmp, the first shared-memory multiprocessor computer, with 16 processing units, and Cm*, a 50-processor computer. These computers were the forerunners of today’s ubiquitous multi-core desktops and laptops.Turing Awards and a Nobel PrizeIn 1975, Simon and Newell were awarded the A.M. Turing Award for their work in artificial intelligence. (As of 2014, 12 CMU alumni or faculty have been awarded Turings, sometimes considered the Nobel Prize of computing.) Three years later, Simon received the Nobel Prize in Economics for his work on decision-making theory. As the 1970s progressed, Newell became interested in human-computer interaction, and began a long relationship with Xerox’s Palo Alto Research Center, or PARC, which released the Xerox Alto in 1973.
Answer: "
"How many papers does Lori S. Levin have on Semantic Scholar?
","['lori levin_papers.txt', 'lori levin_c5207241406586f4263b235667e004b71ea68953_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many papers does Lori S. Levin have on Semantic Scholar?

Context: List of 2023 Open Access papers by lori levin are:
SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing
Construction Grammar Provides Unique Insight into Neural Language Models
Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation
Syntax and Semantics Meet in the “Middle”: Probing the Syntax-Semantics Interface of LMs Through Agentivity
Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient
Identifying Health-Related Quality of Life Domains after Upper Extremity Transplantation.
Assessment of quality of life after upper extremity transplantation: Framework for patient-reported outcome scale domains
Faculty Name: lori levin
Paperid: c5207241406586f4263b235667e004b71ea68953
Title: Syntax and Semantics Meet in the “Middle”: Probing the Syntax-Semantics Interface of LMs Through Agentivity
Year: 2023
Abstract: Recent advances in large language models have prompted researchers to examine their abilities across a variety of linguistic tasks, but little has been done to investigate how models handle the interactions in meaning across words and larger syntactic forms—i.e. phenomena at the intersection of syntax and semantics. We present the semantic notion of agentivity as a case study for probing such interactions. We created a novel evaluation dataset by utilitizing the unique linguistic properties of a subset of optionally transitive English verbs. This dataset was used to prompt varying sizes of three model classes to see if they are sensitive to agentivity at the lexical level, and if they can appropriately employ these word-level priors given a specific syntactic context. Overall, GPT-3 text-davinci-003 performs extremely well across all experiments, outperforming all other models tested by far. In fact, the results are even better correlated with human judgements than both syntactic and semantic corpus statistics. This suggests that LMs may potentially serve as more useful tools for linguistic annotation, theory testing, and discovery than select corpora for certain tasks.
Authors: Lindia Tjuatja, Emmy Liu, L. Levin, Graham Neubig
Venue: STARSEM
Tldr: {'model': 'tldr@v2.0.0', 'text': 'GPT-3 text-davinci-003 performs extremely well across all experiments, outperforming all other models tested by far and suggests that LMs may potentially serve as more useful tools for linguistic annotation, theory testing, and discovery than select corpora for certain tasks.'}
Url: https://arxiv.org/pdf/2305.18185
Answer: "
"What is the language technologies institute's phone number according to the MCDS handbook?
","['mcds-student-handbook-2023_2024.txt', 'mcds-student-handbook-2023_2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the language technologies institute's phone number according to the MCDS handbook?

Context: To impact society in a transformative way — regionally, nationally, and 
globally — by engaging with partners outside the traditional borders of the 
university campus. 
1.4 MCDS Contact Information 
The people responsible for administering the MCDS degree are: 
 
Jennifer M Lucas 
Academic Program Manager 
Master of Computational Data Science  
Language Technologies Institute 
School of Computer Science  
Carnegie Mellon University 
Gates-Hillman Center 6415 
5000 Forbes Avenue, Pgh, PA 15213 
Phone: (412) 268-9870 
Fax: (412) 268-7287 
 
 
Carolyn Penstein Rosé, Director 
Master of Computational Data Science  
Language Technologies Institute 
School of Computer Science 
Carnegie Mellon University 
Gates-Hillman Center 5419 
5000 Forbes Avenue, Pgh, PA 15213 
Phone: (412) 268-4525 
Fax: (412) 268-7287 
Robert Frederking 
Graduate Program Chair 
Language Technologies Institute 
School of Computer Science  
Carnegie Mellon University 
Gates-Hillman Center 6515 
5000 Forbes Avenue, Pgh, PA 15213 
Phone: (412) 268-6656 
Mona Diab, LTI Director 
Language Technologies Institute 
School of Computer Science  
Carnegie Mellon University 
Gates-Hillman Center 5723 
5000 Forbes Avenue, Pgh, PA 15213 
Phone: (412) 268-3669 
 
The Language Technologies Institute is located primarily on the 5 th and 6th floors of the 
Gates Hillman Complex (GHC) on Carnegie Mellon’s Pittsburgh campus: 
 
Language Technologies Institute 
Carnegie Mellon University 
5000 Forbes Avenue 
Gates Hillman Complex 5402, LTI 
Pittsburgh, PA 15241-3891 
 
412-268-6591 (phone) 
412-268-6298 (fax) 
http://www.lti.cs.cmu.edu/ 
 
 
9
1.5 University Policies and Expectations 
Each member of the Carnegie Mellon community must be familiar with university 
policies and guidelines.
Language Technologies Institute / School of Computer Science 
Graduate Student Handbook 
Academic Year 2023-2024  
Master of Computational Data Science Program 
 
Last revision date: July 20, 2023 
 
The information contained in this graduate handbook template focuses on the 
resources and locations available at the Carnegie Mellon Pittsburgh Campus. 
 
 
1
Table of Contents 
1 Welcome . 6 
1.1 The MCDS Degree . 6 
1.2 Vision . 7 
1.3 Mission . 7 
1.4 MCDS Contact Information. 8 
1.5 University Policies and Expectations . 9 
1.6 Carnegie Mellon University Statement of Assurance . 9 
1.7 The Carnegie Mellon Code . 10 
2 The Language Technologies Institute . 10 
2.1 Main Office . 10 
2.2 Photocopies and Printers . 11 
2.3 Office Space for MS Students . 11 
2.4 Computers for MS Students . 11 
3 MCDS Degree Completion and Certification . 11 
3.1 CMU Degree Completion and Statute of Limitations . 11 
Early Completion 
11 
Extended or Longer-than-Standard Completion 
12 
Policy on Master’s Student Statute of Limitations 
12 
Additional Guidance for Students 
12 
3.2 Full-time Status . 13 
3.3 MCDS Degree Enrollment Process and Related Information . 13 
3.3.1 Duration of the degree program 
13 
3.3.2 Residency requirements 
13 
3.3.3 Degree Certification: Course requirements and related policies/protocols 
13 
3.3.4 Prerequisite Core Course 
14 
3.3.5 Plan of study 
14 
3.3.6.1 MCDS Curriculum 
15 
3.3.6.2 Common MCDS Core Courses 
15 
3.3.6.3 Areas of Concentration 
15 
3.3.6.4 MCDS Capstone Courses 
16 
3.3.10 Capstone project 
17 
3.3.11 Elective courses 
17 
3.3.12 Undergraduate courses 
17 
3.3.13 Independent study course 
18 
 
 
2
3.3.14 Double counting courses 
18 
3.3.15 Courses outside of the School of Computer Science 
18 
3.3.16 Grades 
18 
3.3.17 Student Review,
Answer: "
"In the Convoifilter paper, what is the WER achieved by the joint fine-tuning strategy?
","['graham neubig_03c7f61b0c6bc4c480ed6da4e9d7dda104ddfec3_metadata.txt', 'mona diab_5e2f8088647e357bb6440d271ed1fcc4d5ed7e7c_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the Convoifilter paper, what is the WER achieved by the joint fine-tuning strategy?

Context: Faculty Name: graham neubig
Paperid: 03c7f61b0c6bc4c480ed6da4e9d7dda104ddfec3
Title: Cross-Modal Fine-Tuning: Align then Refine
Year: 2023
Abstract: Fine-tuning large-scale pretrained models has led to tremendous progress in well-studied modalities such as vision and NLP. However, similar gains have not been observed in many other modalities due to a lack of relevant pretrained models. In this work, we propose ORCA, a general cross-modal fine-tuning framework that extends the applicability of a single large-scale pretrained model to diverse modalities. ORCA adapts to a target task via an align-then-refine workflow: given the target input, ORCA first learns an embedding network that aligns the embedded feature distribution with the pretraining modality. The pretrained model is then fine-tuned on the embedded data to exploit the knowledge shared across modalities. Through extensive experiments, we show that ORCA obtains state-of-the-art results on 3 benchmarks containing over 60 datasets from 12 modalities, outperforming a wide range of hand-designed, AutoML, general-purpose, and task-specific methods. We highlight the importance of data alignment via a series of ablation studies and demonstrate ORCA's utility in data-limited regimes.
Authors: Junhong Shen, Liam Li, L. Dery, Corey Staten, M. Khodak, Graham Neubig, Ameet Talwalkar
Venue: International Conference on Machine Learning
Tldr: {'model': 'tldr@v2.0.0', 'text': ""This work proposes ORCA, a general cross-modal fine-tuning framework that extends the applicability of a single large-scale pretrained model to diverse modalities and highlights the importance of data alignment via a series of ablation studies and demonstrates ORCA's utility in data-limited regimes.""}
Url: http://arxiv.org/pdf/2302.05738
Faculty Name: mona diab
Paperid: 5e2f8088647e357bb6440d271ed1fcc4d5ed7e7c
Title: Care4Lang at MEDIQA-Chat 2023: Fine-tuning Language Models for Classifying and Summarizing Clinical Dialogues
Year: 2023
Abstract: Summarizing medical conversations is one of the tasks proposed by MEDIQA-Chat to promote research on automatic clinical note generation from doctor-patient conversations. In this paper, we present our submission to this task using fine-tuned language models, including T5, BART and BioGPT models. The fine-tuned models are evaluated using ensemble metrics including ROUGE, BERTScore andBLEURT. Among the fine-tuned models, Flan-T5 achieved the highest aggregated score for dialogue summarization.
Authors: Amal AlQahtani, R. Salama, Mona T. Diab, Abdou Youssef
Venue: Clinical Natural Language Processing Workshop
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper presents their submission to this task using fine-tuned language models, including T5, BART and BioGPT models, and finds Flan-T5 achieved the highest aggregated score for dialogue summarization.'}
Url: https://aclanthology.org/2023.clinicalnlp-1.55.pdf
Answer: "
"In fall 2023, When is the deadline to drop a Mini-2 course with a withdrawal grade assigned?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-2324.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, When is the deadline to drop a Mini-2 course with a withdrawal grade assigned?

Context: On Monday, 02 September, 2024, during the Fall 2024 (F24) semester, Labor Day is observed, leading to No Classes & University Closed.
On Monday, 09 September, 2024, during the Fall 2024 (F24) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 18 September, 2024, during the Fall 2024 (F24) semester, Mini-1 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 30 September, 2024, during the Fall 2024 (F24) semester, Mini-1 Pass/no pass & withdrawal deadline (3) is observed.
On Monday, 30 September, 2024, during the Fall 2024 (F24) semester, Mini-1 Faculty Course Evaluations open is observed.
On Monday, 07 October, 2024, during the Fall 2024 (F24) semester, Semester drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 11 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Last Day of Classes is observed.
On Friday, 11 October, 2024, during the Fall 2024 (F24) semester, Mini-1 voucher election deadline (4) is observed.
On Saturday, 12 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Exams is observed.
On Saturday, 12 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Faculty Course Evaluations close is observed.
From 14 October, 2024,Monday to 18 October, 2023,Friday marks the Fall Break for Fall 2024 (F24) semester which leads to No Classes.
On Monday, 21 October, 2024, during the Fall 2024 (F24) semester, Mini-2 Classes Begin is observed.
On Wednesday, 23 October, 2024, during the Fall 2024 (F24) semester, Mid-Semester & Mini-1 grades due by 4 pm is observed.
On Friday, 25 October, 2024, during the Fall 2024 (F24) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
Day: 'Monday', Event: 'Semester & Mini-2 Faculty Course Evaluations close', Semester: 'Fall 2023 (F23)'
Date: '2023-12-20', Day: 'Wednesday', Event: 'Final Grades Due by 4 pm ', Semester: 'Fall 2023 (F23)'
Start Date: '2023-12-23', End Date: '2024-01-02', Days: 'Saturday to Tuesday', Event: 'Winter Break; University Closed', Semester: 'Fall 2023 (F23)'
Date: '2023-01-08', Day: 'Monday', Event: 'Fall Deans' Lists Posted', Semester: 'Fall 2023 (F23)'
Date: '2024-01-15', Day: 'Monday', Event: 'Martin Luther King Day; No Classes', Semester: 'Spring 2024 (S24)'
Date: '2024-01-16', Day: 'Tuesday', Event: 'First Day of Class', Semester: 'Spring 2024 (S24)'
Date: '2024-01-22', Day: 'Monday', Event: 'Mini-3 add, audit & tuition adjustment drop deadline (1)', Semester: 'Spring 2024 (S24)'
Date: '2024-01-29', Day: 'Monday', Event: 'Semester add, audit & tuition adjustment drop deadline (1)', Semester: 'Spring 2024 (S24)'
Date: '2024-02-07', Day: 'Wednesday', Event: 'Mini-3 course drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Spring 2024 (S24)'
Date: '2024-02-19', Day: 'Monday', Event: 'Mini-3 pass/no pass & withdrawal deadline (3)', Semester: 'Spring 2024 (S24)'
Date: '2024-02-19', Day: 'Monday', Event: 'Mini-3 Faculty Course Evaluations open', Semester: 'Spring 2024 (S24)'
Date: '2024-02-26', Day: 'Monday', Event: 'Semester course drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Spring 2024 (S24)'
Date: '2024-03-01', Day: 'Friday', Event: 'Mini-3 Last Day of Classes ',
Answer: "
"Does LTI offer a course on text mining?
","['program_info_LanguageTechnologiesConcentration.txt', 'carolyn_rose.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Does LTI offer a course on text mining?

Context: Academic Program Name:
Language Technologies Concentration

Website:
https://lti.cs.cmu.edu/academics/lt-concentration.html

Overview:
Human language technologies have become an increasingly central component of computer science. Information retrieval, machine translation and speech technology are used daily by the general public, while text mining, natural language processing and language-based tutoring are common within more specialized professional or educational environments. The LTI prepares students for this world by offering a minor that gives you the opportunity to not only learn about language technologies, but to also apply that knowledge through a directed project.

Requirements:
Students interested in the language technologies minor must complete our prerequisite courses with an average grade of B (3.0) or better before applying to the program. (Students who do not meet this average must submit a letter of explanation along with their application.) Prerequisites include:
Principles of Imperative Computation (15-122)
Principles of Functional Programming (15-150)
We also strongly encourage candidates to take:
Differential and Integral Calculus (21-120) and Integration and Approximation (21-122)
Matrices and Linear Transformations (21-241) or Matrix Theory (21-242)
Probability and Computing (15-259) or Probability (21-325) or Probability Theory for Computer Scientists (36-218) or Introduction to Probability Theory (36-225)

Curriculum:
The Language Technologies Concentration requires that SCS students complete one core course and their choice of three elective courses of at least 9 units each. The electives can be chosen from a specific set of stand-alone courses. In addition to the four courses, students are required to do an undergraduate research project for at least 9 units to complete their concentration.
Course Requirements for Undergraduate Minor
Carolyn Rosé
Professor, Language Technologies Institute
Human-Computer Interaction Institute
Contact
5415 —Gates & Hillman Centers
Email
412-268-7130
Research Area
Computer-Supported Collaborative Learning/MOOCs, Information Retrieval, Text Mining and Analytics, Language Technologies for Education, Natural Language Processing and Computational Linguistics

Education
Ph.D. in Language and Information Technology
1997
Employer Upon Graduation: 
Faculty, LTI CMU
Research
My research focuses on better understanding the social and pragmatic nature of conversation, and using this understanding to build computational systems that improve the efficacy of conversation both between people, and between people and computers. In order to pursue these goals, I invoke approaches from computational discourse analysis and text mining, conversational agents, and computer-supported collaborative learning. I ground my research in the fields of language technologies and human-computer interaction, and am fortunate to work closely with students and post-docs from the LTI and the Human-Computer Interaction Institute, as well as to direct my own lab, TELEDIA. My group’s highly interdisciplinary work, published in 160 peer-reviewed publications, is represented in the top venues in five fields: language technologies, learning sciences, cognitive science, educational technology and human-computer interaction.

An exciting direction of my group's work is spearheading a satellite working group to support social interaction for learning in MOOCs with EdX called DANCE. My research toward this end has birthed and substantially contributed to the growth of two thriving interrelated research areas: automated analysis of collaborative learning processes and dynamic support for collaborative learning. Both areas use intelligent conversational agents to support collaborative learning in a context-sensitive way.

All of my work draws insight from rich theoretical models from sociolinguistics and discourse analysis, and pares them down to precise operationalizations that capture the most important essence for achieving impact. I always start by investigating how conversation works and formalizing this understanding in models that are precise enough to be reproducible and that demonstrate explanatory power in connection with outcomes with real-world value. The next step is to adapt, extend and apply machine learning and text mining technologies that leverage this deep understanding to build computational models capable of automatically applying these constructs to naturally occurring language interactions. Finally, with the technology to automatically monitor naturalistic language communication in place, we can build interventions with real-world benefits.
Answer: "
"Besides Pittsburgh, where else does CMU have physical campuses?
","['mlt-student-handbook-2023-2024.txt', 'miis-handbook_2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Besides Pittsburgh, where else does CMU have physical campuses?

Context: More information regarding these services, locations and 
contact information can be found in The Word at:  
www.cmu.edu/student-affairs/theword//campus_resources/copyprintmail.html.  
7.17 University Center 
www.cmu.edu/university-center 
The University Center is a centerpiece of the campus that provides a space for special events, 
physical fitness, student organizations and various activities, as well as accommodating retail 
and dining services. As the campus crossroads, the University Center functions as a place for 
students to interact, get involved and enjoy new experiences. Visit the University Center 
website for information about campus eateries, ATMs and PNC Bank, fitness rooms and 
schedules, retail stores, scheduling University Center space, the public prayer room, student 
organizations, and the Wright-Rogal Chapel. 
The University Center Information Desk is the location if you want to know about upcoming 
campus events or have questions about Carnegie Mellon in general. You can call the 
Information Desk at 412-268-2107. The Information Desk not only provides information about 
campus events, but also sells postage stamps, makes copies, sends faxes, distributes campus 
maps, manages a lost & found, and has informational brochures about Pittsburgh and the 
campus. 
7.18 Athletic/Fitness Facilities 
 www.cmu.edu/athletics 
For the participant or the spectator, Carnegie Mellon offers intercollegiate athletics, intramural 
sports, physical education classes and club sports. The Athletics Department also offers 
aerobics classes in the University Center, as well as occasional workshops and instruction 
related to fitness and health. The Athletics Office is located in the Skibo Gymnasium. 
MLT Graduate Student Handbook 
Page 36 
 
The University Centers recreational facilities include an eight-lane pool, racquetball and squash 
courts, class studios, a fully-equipped fitness center, and a gym for basketball and volleyball. All 
users must present a current Carnegie Mellon Card to use these facilities. 
7.19 CMU Alert 
www.cmu.edu/alert  
CMU Alert sends voice and/or text messages to registered phones in the event of a campus 
emergency that threatens public safety or during tests of the system in the spring and fall 
semesters. Students can register for CMU Alert through the website.
The University Center Information Desk is the location if you want to know about upcoming 
campus events or have questions about Carnegie Mellon in general, call the Information Desk 
at 412-268-2107. The Information Desk not only provides information about campus events, 
but also sells postage stamps, makes copies, sends faxes, distributes campus maps, manages a 
lost & found, and has information brochures about Pittsburgh and the campus. 
9.16  Athletic/Fitness Facilities 
 www.cmu.edu/athletics 
For the participant or the spectator, Carnegie Mellon offers intercollegiate athletics, 
intramural sports, physical education classes and club sports. The Athletics Department also 
offers aerobics classes in the University Center and Skibo Gym as well as occasional 
workshops and instruction related to fitness and health.  The Athletics Office is located in the 
Skibo Gymnasium. 
MIIS Graduate Student Handbook 
Page 38 
 
Skibo Gym facilities include courts for basketball, volleyball, badminton, as well as weight-
training and aerobic equipment. The University Center’s recreational facilities include an 
eight-lane pool, racquetball and squash courts, aerobics room, fitness center and gym for 
basketball and volleyball. All users must present a current Carnegie Mellon Card to use these 
facilities. 
9.17  CMU Alert 
www.cmu.edu/alert  
CMU Alert sends voice and/or text messages to registered phones in the event of a campus 
emergency that threatens public safety or during tests of the system in the spring and fall 
semesters.  Students can register for CMU Alert through the website.   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
MIIS Graduate Student Handbook 
Page 39 
 
 
A Appendix  
Appendix A 
2023-2024 
Highlighted University Resources for Graduate Students 
 
Note: The following pages are meant to broadly include only some of the resources available to 
graduate students.  It is not an exhaustive appendix of resources, and students are strongly 
encouraged to visit the various websites linked below for the most up-to-date information.
Answer: "
"Which LTI prof co-authored the paper titled ""Judging LLM-as-a-judge with MT-Bench and Chatbot Arena""?
","['eric xing_a0a79dad89857a96f8f71b14238e5237cbfc4787_metadata.txt', 'eric xing_a0a79dad89857a96f8f71b14238e5237cbfc4787_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which LTI prof co-authored the paper titled ""Judging LLM-as-a-judge with MT-Bench and Chatbot Arena""?

Context: Faculty Name: eric xing
Paperid: a0a79dad89857a96f8f71b14238e5237cbfc4787
Title: Judging LLM-as-a-judge with MT-Bench and Chatbot Arena
Year: 2023
Abstract: Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.
Authors: Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, E. Xing, Haotong Zhang, Joseph Gonzalez, Ion Stoica
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'The results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans, and LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain.'}
Url: https://arxiv.org/pdf/2306.05685
Title: Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena
Authors: Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica
Section: F.2 Arena Fine-tuned Vicuna
Training Due to the incapability of the zero-shot Vicuna-13B model, we further finetune the model with human votes from Chatbot Arena. Specifically, we randomly sample 22K single-turn votes from the arena, covering all models supported by the time of this paper submission (GPT-4, GPT-3.5, Claude-v1, Vicuna-13b, Vicuna-7b, Koala-13B, Alpaca-13B,LLaMA-13B, Dolly-12B, FastChat-T5, RWKV-4-Raven, MPT-Chat, OpenAssistant, ChatGLM, and StableLM), to expose the model with a wider range of chatbot outputs and human preferences. We use 20K votes for training, and 2K for validation. To address the aforementioned weak instruction following problem, we formulate the problem as a 3-way sequence classification problem. Thus, the model simply needs to predict which one of the chat-bot outputs is better (or tie), without needing to exactly following the provided answer template. In particular, we construct an input by using the default prompt and the two model answers. The labels are A, B, and tie (including both-bad-vote and tie-vote). We train for 3 epochs with a cosine learning rate scheduler and a 2e-5 maximum learning rate. We use the 2K validation dataset to choose hyper-parameters, and test on the same 3K dataset in the main body of the paper. Position bias results The results for position bias are provided in Table 15. The consistency improves significantly from 16.2% to 65.0%. Due to the classification formulation, every output is recognizable (error rate 0%). In addition, we measure the classification accuracy over the test dataset.
Answer: "
"How many authors are on the SENTECON paper?
","['louis philippe morency_47a4ac301820c3ea7da4efb8e2466cc6468ad631_content_1.txt', 'louis philippe morency_47a4ac301820c3ea7da4efb8e2466cc6468ad631_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many authors are on the SENTECON paper?

Context: Title: SENTECON: Leveraging Lexicons to Learn Human-Interpretable Language Representations
Authors: Victoria Lin, Louis-Philippe Morency
Section: B Experimental Details
• Not expressed: Out of all possible interpretations of the sentence above, you cannot imagine a scenario in which the speaker of the sentence was expressing the topic. • Potentially expressed: You can imagine at least one scenario in which the speaker of the sentence was expressing the topic. • Most likely expressed: The most natural interpretation of the sentence clearly expresses the topic. Category batches. As mentioned in the main paper, the 52 LIWC categories were randomly split into 5 sets of roughly equal size to avoid annotator fatigue. The splits were as follows: • Batch 1: netspeak, differ, cause, nonflu, discrep, drivers, relig, swear, feel, home, family • Batch 2: leisure, sexual, see, bio, certain, money, percept, female, death, anger, cogproc • Batch 3: filler, sad, posemo, friend, relativ, ingest, body, work, time, social, informal • Batch 4: focusfuture, anx, affiliation, motion, power, reward, space, tentat, risk, focuspresent, affect • Batch 5: negemo, hear, male, health, insight, achiev, focuspast, assent Inter-rater reliability. To assess the reliability of our annotations, we calculated intraclass correlation coefficients (ICCs) using the agreement software package (Girard, 2020). For each batch of sentences, we computed the ICC and its 95% confidence interval, then averaged these across category batches (Table 7). We averaged ICCs over all batches to obtain the overall ICC. Annotators. Annotators were required to be fluent in English and to be nationals of one of the following countries: the United States, the United Kingdom, Ireland, Australia, or Canada. Annotators were further required to have a prior approval rating of ≥ 95%, and an attention check question was included in every sentence batch. All annotators passed the attention check. We took care to compensate annotators at a rate above the local minimum wage. Annotators received an average hourly wage of 8.00 USD.
Title: SENTECON: Leveraging Lexicons to Learn Human-Interpretable Language Representations
Authors: Victoria Lin, Louis-Philippe Morency
Section: B Experimental Details
B.1 LIWC categories The full list of non-grammatical LIWC categories used in our experiments is as follows: affect, posemo, negemo, anx, anger, sad, social, family, friend, female, male, cogproc, insight, cause, discrep, tentat, certain, differ, percept, see, hear, feel, bio, body, health, sexual, ingest, drives, affiliation, achiev, power, reward, risk, focuspast, focuspresent, focusfuture, relativ, motion, space, time, work, leisure, home, money, relig, death, informal, swear, netspeak, assent, nonflu, filler. The list of excluded grammatical LIWC categories is as follows: function, pronoun, ppron, i, we, you, shehe, they, ipron, article, prep, auxverb, adverb, conj, negate, verb, adj, compare, interrog, number, quant.
Answer: "
"What are the last names of the professors that taught 11-711 in Fall 2023?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the last names of the professors that taught 11-711 in Fall 2023?

Context: In Semester Fall 2023, from the department of Mathematical Sciences, the subject titled 'Partial Differential Equations I' with Course ID 21732 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 11:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Tice located in Building WEH, Room 7201.
In Semester Fall 2023, from the department of Mathematical Sciences, the subject titled 'Practicum in Mathematical Sciences' with Course ID 21785 and Section A offers 0-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Slepcev located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Mathematical Sciences, the subject titled 'Advanced Topics in Logic' with Course ID 21800 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 11:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Panagiotopoulos located in Building CFA, Room 102.
In Semester Fall 2023, from the department of Mathematical Sciences, the subject titled 'Advanced Topics in Analysis' with Course ID 21820 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 09:00AM and 09:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Slepcev located in Building BH, Room 235B.
In Semester Fall 2023, from the department of Mathematical Sciences, the subject titled 'Special Topics:' with Course ID 21849 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Mathematical Sciences, the subject titled 'Arithmetic Statistics' with Course ID 21849 and Section A offers 12.0 units. The Class meets Monday Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Anderson located in Building WEH, Room 8201.
In Semester Fall 2023, from the department of Mathematical Sciences, the subject titled 'Partial Differential Equations I' with Course ID 21732 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 11:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Tice located in Building WEH, Room 7201.
In Semester Fall 2023, from the department of Mathematical Sciences, the subject titled 'Practicum in Mathematical Sciences' with Course ID 21785 and Section A offers 0-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Slepcev located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Mathematical Sciences, the subject titled 'Advanced Topics in Logic' with Course ID 21800 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 11:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Panagiotopoulos located in Building CFA, Room 102.
In Semester Fall 2023, from the department of Mathematical Sciences, the subject titled 'Advanced Topics in Analysis' with Course ID 21820 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 09:00AM and 09:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Slepcev located in Building BH, Room 235B.
In Semester Fall 2023, from the department of Mathematical Sciences, the subject titled 'Special Topics:' with Course ID 21849 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Mathematical Sciences, the subject titled 'Arithmetic Statistics' with Course ID 21849 and Section A offers 12.0 units. The Class meets Monday Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Anderson located in Building WEH, Room 8201.
Answer: "
"How many test examples are included in the WebArena benchmark?
","['graham neubig_e41482f4ee984f17382f6cdd900df094d928be06_metadata.txt', 'yonatan bisk_e41482f4ee984f17382f6cdd900df094d928be06_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many test examples are included in the WebArena benchmark?

Context: Faculty Name: graham neubig
Paperid: e41482f4ee984f17382f6cdd900df094d928be06
Title: WebArena: A Realistic Web Environment for Building Autonomous Agents
Year: 2023
Abstract: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.
Authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper builds an environment for language-guided agents that is highly realistic and reproducible, and creates an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.'}
Url: https://arxiv.org/pdf/2307.13854
Faculty Name: yonatan bisk
Paperid: e41482f4ee984f17382f6cdd900df094d928be06
Title: WebArena: A Realistic Web Environment for Building Autonomous Agents
Year: 2023
Abstract: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.
Authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper builds an environment for language-guided agents that is highly realistic and reproducible, and creates an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.'}
Url: https://arxiv.org/pdf/2307.13854
Answer: "
"How many Tony Awards have alumni and current/former faculty won so far?
","['fact_sheet_d407.txt', 'fact_sheet_d407.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many Tony Awards have alumni and current/former faculty won so far?

Context: :-)
LEADERSHIP
Farnam Jahanian  
President
David Coulter 
Chair, Board of Trustees
James H. Garrett Jr. 
Provost
2 National Academy of Engineering       3 National Academy of Sciences        4 National Academy of Medicine
Won by alumni and current/former faculty
The leading university center 
for cybersecurity, providing 
support to more than 110 
centers around the world 
The world’s first university 
robotics department,  
founded in 1979
Alumnus Andy Warhol  
(CFA1949), pop artist pioneer  
and cultural icon
One of only 29 universities 
invited to be a member of the 
World Economic Forum’s 
Global University  
Leaders Forum
CMU.EDU
ECONOMIC IMPACT
• Attracting major companies —  
including Google, Intel, Uber and GE —  
to locate operations and create new  
jobs in Pittsburgh
• To date, the CMU community has 
launched more than 400 startups 
and created more than 152 spinoff 
companies.
• Contributing to the cultural and civic life of  
the city with performances, exhibitions 
and research collaborations
13
ACADEMY  
AWARDS
65
MEMBERS  
OF NAE2
146
EMMY 
AWARDS
20
MEMBERS 
OF NAS3
6
MEMBERS 
OF NAM4
13
TURING 
AWARDS
20
NOBEL 
LAUREATES
58
TONY 
AWARDS
May 2023
13%  International
SCHOOLS AND COLLEGES
College of Engineering 
College of Fine Arts
Dietrich College of Humanities  
and Social Sciences
Heinz College of Information Systems 
and Public Policy
Mellon College of Science
School of Computer Science
Tepper School of Business
cmu.edu/assets/pdfs/cmu-fact-sheet.pdf    |   cmu.edu/about/awards.html
March 2020
BIG IDEAS THAT SHAPE YOUR WORLD START HERE
1,000+ companies greenlighted 
across the U.S. and internationally 
Cognitive tutors, which improve 
the math skills of 500,000+ 
students each year 
The first U.S. school to award  
a degree in drama in 1914
James Gosling (SCS 1983) 
invented the Java computer 
programming language.
The first exclusive higher 
education partner of the Tony 
Awards®, co-creating the first, 
national recognition program 
honoring K-12 theatre educators 
The first smile in an email, created 
in 1982 by professor Scott Fahlman
Kevlar Fiber, invented by alumna 
Stephanie Kwolek (MM 1946)
The Last Lecture, written  
by alumni Randy Pausch (SCS 1988)  
and Jeffrey Zaslow (DC 1980)
In 1919, CMU’s first Ph.D. was 
awarded to Mao Yisheng, father  
of Chinese bridge construction. 
Duolingo, invented by professor  
Luis von Ahn (CS 2003, 2005), had  
10 million downloads in 12 months 
and was named Apple’s 2013 app  
of the year.
:-)
LEADERSHIP
Farnam Jahanian  
President
David Coulter 
Chair, Board of Trustees
James H. Garrett Jr.
Answer: "
"Does SCS Interdisciplinary offer more than 1 course in Summer 2024?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Does SCS Interdisciplinary offer more than 1 course in Summer 2024?

Context: In Semester Spring 2024, from the department of SCS Interdisciplinary, the subject titled 'Teaching Techniques for Computer Science:' with Course ID 07070 and Section NA offers 2.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of SCS Interdisciplinary, the subject titled 'Teaching Techniques for Computer Science' with Course ID 07070 and Section A offers 2.0 units. The Class meets Friday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Garrod, Rivers located in Building GHC, Room 4215.
In Semester Spring 2024, from the department of SCS Interdisciplinary, the subject titled 'Artificial Intelligence Practicum' with Course ID 07090 and Section A offers 3.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Simmons located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of SCS Interdisciplinary, the subject titled 'Concepts in Artificial Intelligence:' with Course ID 07180 and Section NA offers 5.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of SCS Interdisciplinary, the subject titled 'Concepts in Artificial Intelligence' with Course ID 07180 and Section A3 offers 5.0 units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Simmons located in Building HOA, Room 160.
In Semester Spring 2024, from the department of SCS Interdisciplinary, the subject titled 'Research Practicum in Computer Science' with Course ID 07400 and Section A offers 12.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Martins located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of SCS Interdisciplinary, the subject titled 'Teaching Techniques for Computer Science:' with Course ID 07070 and Section NA offers 2.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of SCS Interdisciplinary, the subject titled 'Teaching Techniques for Computer Science' with Course ID 07070 and Section A offers 2.0 units. The Class meets Friday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Garrod, Rivers located in Building GHC, Room 4215.
In Semester Spring 2024, from the department of SCS Interdisciplinary, the subject titled 'Artificial Intelligence Practicum' with Course ID 07090 and Section A offers 3.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Simmons located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of SCS Interdisciplinary, the subject titled 'Concepts in Artificial Intelligence:' with Course ID 07180 and Section NA offers 5.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of SCS Interdisciplinary, the subject titled 'Concepts in Artificial Intelligence' with Course ID 07180 and Section A3 offers 5.0 units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Simmons located in Building HOA, Room 160.
In Semester Spring 2024, from the department of SCS Interdisciplinary, the subject titled 'Research Practicum in Computer Science' with Course ID 07400 and Section A offers 12.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Martins located in Building DNM, Room DNM.
Answer: "
"What does A-LoL use to filter negative advantage (low-quality) data points during training?
","['maarten sap_9d2dc57903e99f33b9cf727c3903718751d82663_metadata.txt', 'maarten sap_9d2dc57903e99f33b9cf727c3903718751d82663_content_2.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What does A-LoL use to filter negative advantage (low-quality) data points during training?

Context: Faculty Name: maarten sap
Paperid: 9d2dc57903e99f33b9cf727c3903718751d82663
Title: Improving Language Models with Advantage-based Offline Policy Gradients
Year: 2023
Abstract: Language Models (LMs) achieve substantial language capabilities when finetuned using Reinforcement Learning with Human Feedback (RLHF). However, RLHF is an unstable and data-hungry process that continually requires new high-quality LM-generated data for finetuning. We introduce Advantage-Leftover Lunch RL (A-LoL), a new class of offline policy gradient algorithms that enable RL training on any pre-existing data. By assuming the entire LM output sequence as a single action, A-LoL allows incorporating sequence-level classifiers or human-designed scoring functions as rewards. Subsequently, by using LM's internal sequence-level value estimate, A-LoL filters negative advantage (low-quality) data points during training, making it resilient to noise. Overall, A-LoL is an easy-to-implement LM training recipe that is sample-efficient and stable. We demonstrate the effectiveness of A-LoL and its variants with a set of four different language generation tasks. We compare against both online RL (PPO) and recent preference-based (DPO, PRO) and reward-based (GOLD) offline RL baselines. On the commonly-used RLHF benchmark, Helpful and Harmless Assistant (HHA), LMs trained with A-LoL methods achieve the highest diversity while also being rated more safe and helpful than baselines according to humans. Additionally, in the remaining three tasks, A-LoL could optimize multiple distinct reward functions even when using noisy or suboptimal training data. We also release our experimental code. https://github.com/abaheti95/LoL-RL
Authors: Ashutosh Baheti, Ximing Lu, Faeze Brahman, Ronan Le Bras, Maarten Sap, Mark O. Riedl
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'Advantage-Leftover Lunch RL (A-LoL), a new class of offline policy gradient algorithms that enable RL training on any pre-existing data that assumes the entire LM output sequence as a single action, and allows incorporating sequence-level classifiers or human-designed scoring functions as rewards.'}
Url: https://arxiv.org/pdf/2305.14718
Title: ADVANTAGE-BASED OFFLINE POLICY GRADIENTS
Authors: Ashutosh Baheti, Ximing Lu, Faeze Brahman, Ronan Le Bras, Maarten Sap, Mark Riedl
Section: C ADDITIONAL EXPERIMENTS AND RESULTS
fluent, engaging, and diverse. Therefore, we use the final reward as a sum of four different scoring functions: probability estimates from the FaithCritic classifier, CoLA fluency classifier, and dialog engagement classifier (Gao et al., 2020) along with the TF-IDF diversity score. We evaluate all LMs using the rewards obtained on Dtest. Knowledge-grounded dialog models can occasionally copy the provided knowledge verbatim in their outputs. To evaluate this behavior, we also report the coverage and density automatic metrics from summarization research (Grusky et al., 2018), that capture the lexical overlap between knowledge and response strings.17 Similar to our previous experiments, we also calculate the average response length and corpus-level distinct-n-gram diversity metrics (Li et al., 2016). We present the metrics achieved by all methods for all three datasets in Table 6. Results We again observe A-LOL models outperform reference LM and all other LMs trained with NLL and reward-based baselines in all three dataset settings. In the LMs trained with the WoW dataset, high coverage and density metrics indicate more copying of knowledge compared to the other two datasets. Interestingly, A-LOL models decrease the average density compared to models 17Coverage is the average lexical overlap between knowledge and response, whereas, Density is the average length of extractive spans in response that are copied from the knowledge. trained with NLL and reward-based objectives. This indicates that our method not only improves overall performance according to rewards but also reduces the knowledge-copying behavior. Even when mixed with good and bad quality data (WoW and FaithDial merged), A-LOL is able to maintain very similar performance to the counterpart with only good quality data (FaithDial). We find that A-LOL identified a negative advantage in 39% of WoW’s training data, 10% of FaithDial’s training data, and 55% of merged training instances. Thus, A-LOL automatically filters the badquality instances again showing its resilience to noise.
Answer: "
"In spring 2024, When do classes start after the winter break?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-2425.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, When do classes start after the winter break?

Context: From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mini-2 Classes Begin is observed.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mid-Semester & Mini-1 grades due by 4 pm is observed.
On Friday, 27 October, 2023, during the Fall 2023 (F23) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
On Tuesday, 07 November, 2023, during the Fall 2023 (F23) semester, Democracy Day is observed, leading to No Classes, except Evening classes after 5 pm will still meet.
On Saturday, 11 November, 2023, during the Fall 2023 (F23) semester, Homecoming is observed.
From 13 November, 2023,Monday to 17 November, 2023,Friday marks the Spring 2024 Registration Week for Fall 2023 (F23) semester.
On Monday, 13 November, 2023, during the Fall 2023 (F23) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Wednesday, 15 November, 2023, during the Fall 2023 (F23) semester, Mini-2 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 22 November, 2023,Wednesday to 24 November, 2023,Friday marks the Thanksgiving Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Mini-2 pass/no pass & withdrawal deadline (2) is observed.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Faculty Course Evaluations open is observed.
On Friday, 08 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Last Day of Classes is observed.
Day: 'Friday', Event: 'Mini-1 voucher election deadline (4)', Semester: 'Fall 2024 (F24)'
Date: '2024-10-12', Day: 'Saturday', Event: 'Mini-1 Exams', Semester: 'Fall 2024 (F24)'
Date: '2024-10-12', Day: 'Saturday', Event: 'Mini-1 Faculty Course Evaluations close', Semester: 'Fall 2024 (F24)'
Start Date: '2024-10-14', End Date: '2023-10-18', Days: 'Monday to Friday', Event: 'Fall Break; No Classes', Semester: 'Fall 2024 (F24)'
Date: '2024-10-21', Day: 'Monday', Event: 'Mini-2 Classes Begin ', Semester: 'Fall 2024 (F24)'
Date: '2024-10-23', Day: 'Wednesday', Event: 'Mid-Semester & Mini-1 grades due by 4 pm', Semester: 'Fall 2024 (F24)'
Date: '2024-10-25', Day: 'Friday', Event: 'Mini-2 add, audit & tuition adjustment drop deadline (1)', Semester: 'Fall 2024 (F24)'
Date: '2024-11-05', Day: 'Tuesday', Event: 'Democracy Day; No Classes, except Evening classes after 5 pm will still meet', Semester: 'Fall 2024 (F24)'
Date: '2024-11-11', Day: 'Monday', Event: 'Semester pass/no pass & withdrawal deadline (3)', Semester: 'Fall 2024 (F24)'
Date: '2024-11-13', Day: 'Wednesday', Event: 'Mini-2 drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Fall 2024 (F24)'
Start Date: '2024-11-18', End Date: '2024-11-22', Days: 'Monday to Friday', Event: 'Spring 2025 Registration Week', Semester: 'Fall 2024 (F24)'
Date: '2024-11-25', Day: 'Monday', Event: 'Mini-2 pass/no pass & withdrawal deadline (3)', Semester: 'Fall 2024 (F24)'
Date: '2024-11-25', Day: 'Monday',
Answer: "
"What is the task success rate of the GPT-4-based agent in WebArena?
","['yonatan bisk_e41482f4ee984f17382f6cdd900df094d928be06_metadata.txt', 'graham neubig_e41482f4ee984f17382f6cdd900df094d928be06_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the task success rate of the GPT-4-based agent in WebArena?

Context: Faculty Name: yonatan bisk
Paperid: e41482f4ee984f17382f6cdd900df094d928be06
Title: WebArena: A Realistic Web Environment for Building Autonomous Agents
Year: 2023
Abstract: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.
Authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper builds an environment for language-guided agents that is highly realistic and reproducible, and creates an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.'}
Url: https://arxiv.org/pdf/2307.13854
Faculty Name: graham neubig
Paperid: e41482f4ee984f17382f6cdd900df094d928be06
Title: WebArena: A Realistic Web Environment for Building Autonomous Agents
Year: 2023
Abstract: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.
Authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper builds an environment for language-guided agents that is highly realistic and reproducible, and creates an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.'}
Url: https://arxiv.org/pdf/2307.13854
Answer: "
"Is there a limit on the number of guests who can attend the main commencement ceremony?
","['Commencement.txt', 'Commencement.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Is there a limit on the number of guests who can attend the main commencement ceremony?

Context: Main Commencement Ceremony
Bachelor’s, master’s and doctoral degree candidates and their guests are invited to join the main commencement ceremony on Sunday, May 12, for the conferral of all degrees.
The main ceremony will include remarks from the president, keynote speaker, student speaker and academic deans, in addition to recognition of the honorary degree recipients.
There is no limit on number of guests who can attend the main commencement ceremony and tickets are not needed.
The ceremony will take place on CMU’s campus beginning at 10 a.m. and will be approximately 1.5 hours long. All guests must be seated by 9:15 a.m. for the start of the student procession. Access to guest seating will be restricted once the student procession begins.

Diploma Ceremonies
In addition to the main commencement ceremony, each college/school/department will host a diploma ceremony to recognize all graduating students. Diploma ceremonies will be held over the course of the weekend (Friday, May 10–Sunday, May 12).
Diploma ceremonies will include the presentation of diplomas to graduates, hooding of doctoral candidates and remarks from their college/school/department leadership. Each ceremony is organized and customized by their college/school/department.
Diploma ceremonies will take place both on and off campus. Locations and times for diploma ceremonies will be provided soon.
Diploma ceremonies typically include a reception and the length of time for each ceremony varies based on the number of graduates.
More details on the weekend schedule, including a diploma ceremony schedule, will be provided in the coming weeks. 

Thursday, May 9
Phi Beta Kappa Initiation Ceremony
Ceremony: 2–3 p.m.
McConomy Auditorium, Cohon University Center
Reception: 3–4 p.m.
Connan Room, Cohon University Center

Contact:
Joseph Devine
jd0x@andrew.cmu.edu 

Joanne Ursenbach
joanneu@andrew.cmu.edu 

President's Graduates Toast (bachelor's students)
3:30–4:30 p.m.
Location TBD
Registration required. Invitation, along with registration details, will be sent in late April.

First Gen Graduation Recognition
Reception: 5-5:30 p.m.
Alumni Concert Hall, College of Fine Arts
Ceremony: 5:30-6:30p.m.
Invitation, along with registration details, will be sent in late April.

First Gen Graduation Recognition
Reception: 5-5:30 p.m.
Alumni Concert Hall, College of Fine Arts
Ceremony: 5:30-6:30p.m.
Kresge Theater, College of Fine Arts

Contact:
M. Shernell Smith
mssmith@andrew.cmu.edu 

Sam Colavecchio
scolavec@andrew.cmu.edu 
412-268-7733


Friday, May 10
Diploma Ceremonies
Various times

Senior Leadership Recognition Ceremony
4–5:30 p.m.
Wiegand Gym, Cohon University Center
Undergraduate students and their guests by invitation only. This ceremony recognizes nominated seniors who have reflected upon their specific leadership contributions during their time at CMU.


Saturday, May 11
Diploma Ceremonies
Various times

Center for Student Diversity and Inclusion Ceremony
Reception: 12:30 p.m.
Program: 1-2:15 p.m.
Simmons Auditorium, Tepper Building

Contact:
M. Shernell Smith
mssmith@andrew.cmu.edu 

Rowshan Lang
rowshan@andrew.cmu.edu
Naval ROTC Commissioning
Ceremony: 1:30-2:30 p.m.
Auditorium, Soldiers & Sailors Memorial Hall & Museum *
4141 Fifth Avenue, Pittsburgh, PA 15213
Contact:
Mike Danko
mdanko@andrew.cmu.edu 

The President’s Reception in honor of CMU’s Doctoral Candidates
4–6 p.m.
Tepper Building Atrium


Sunday, May 12
Gesling Stadium opens to guests
8 a.m.

Robing and procession for graduates
9–10 a.m.
Various locations across campus

Student procession begins
9:15 a.m.
All guests in stadium must be seated. Access to guest seating will be restricted once the procession begins.

Commencement Ceremony
10–11:30 a.m.
Gesling Stadium, CMU’s campus

Diploma Ceremonies
Various times
Answer: "
"What is the full name of the conference where the paper The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Linkbetween Phonemes and Facial Features, got published?
","['bhiksha raj_a6e3a10a6286967413e3406374bbeea533640030_content_1.txt', 'rita singh_a6e3a10a6286967413e3406374bbeea533640030_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the full name of the conference where the paper The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Linkbetween Phonemes and Facial Features, got published?

Context: Title: The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features
Authors: Liao Qu, Xianwei Zou, Xiang Li, Yandong Wen, Rita Singh, Bhiksha Raj
Section: 6. References
Z.-Q. Wang and I. Tashev, “Learning utterance-level representations for speech emotion and age/gender recognition using deep neural networks,” 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5150–5154, 2017. [18] T. Riede, E. Bronson, H. Hatzikirou, and K. Zuberbühler, “Vocal production mechanisms in a non-human primate: morphological data and a model,” Journal of Human Evolution, vol. 48, no. 1, pp. 85–96, 2005. [19] R. Singh, B. Raj, and D. Gençaga, “Forensic anthropometry from voice: An articulatory-phonetic approach,” 2016 39th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO), pp. 1375–1380, 2016. [20] M. T. Ghiselin, P. Ekman, and H. E. Gruber, “Darwin and facial expression: A century of research in review.@@@darwin on man: A psychological study of scientific creativity.” Systematic Biology, vol. 23, p. 562, 1974. [21] M. Tan, B. Chen, R. Pang, V. Vasudevan, and Q. V. Le, “Mnasnet: Platform-aware neural architecture search for mobile,” 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2815–2823, 2018. [22] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778, 2015.
Title: The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features
Authors: Liao Qu, Xianwei Zou, Xiang Li, Yandong Wen, Rita Singh, Bhiksha Raj
Section: 6. References
Z.-Q. Wang and I. Tashev, “Learning utterance-level representations for speech emotion and age/gender recognition using deep neural networks,” 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5150–5154, 2017. [18] T. Riede, E. Bronson, H. Hatzikirou, and K. Zuberbühler, “Vocal production mechanisms in a non-human primate: morphological data and a model,” Journal of Human Evolution, vol. 48, no. 1, pp. 85–96, 2005. [19] R. Singh, B. Raj, and D. Gençaga, “Forensic anthropometry from voice: An articulatory-phonetic approach,” 2016 39th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO), pp. 1375–1380, 2016. [20] M. T. Ghiselin, P. Ekman, and H. E. Gruber, “Darwin and facial expression: A century of research in review.@@@darwin on man: A psychological study of scientific creativity.” Systematic Biology, vol. 23, p. 562, 1974. [21] M. Tan, B. Chen, R. Pang, V. Vasudevan, and Q. V. Le, “Mnasnet: Platform-aware neural architecture search for mobile,” 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2815–2823, 2018. [22] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778, 2015.
Answer: "
"In the ""Surveying (Dis)Parities and Concerns of Compute Hungry NLP Research,"" how many participants were in the survey from the NLP community? 
","['emma strubell_667ba2e8f1933b6c32e9672012526904b4c5dc31_metadata.txt', 'emma strubell_667ba2e8f1933b6c32e9672012526904b4c5dc31_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the ""Surveying (Dis)Parities and Concerns of Compute Hungry NLP Research,"" how many participants were in the survey from the NLP community? 

Context: Faculty Name: emma strubell
Paperid: 667ba2e8f1933b6c32e9672012526904b4c5dc31
Title: Surveying (Dis)Parities and Concerns of Compute Hungry NLP Research
Year: 2023
Abstract: Many recent improvements in NLP stem from the development and use of large pre-trained language models (PLMs) with billions of parameters. Large model sizes makes computational cost one of the main limiting factors for training and evaluating such models; and has raised severe concerns about the sustainability, reproducibility, and inclusiveness for researching PLMs. These concerns are often based on personal experiences and observations. However, there had not been any large-scale surveys that investigate them. In this work, we provide a first attempt to quantify these concerns regarding three topics, namely, environmental impact, equity, and impact on peer reviewing. By conducting a survey with 312 participants from the NLP community, we capture existing (dis)parities between different and within groups with respect to seniority, academia, and industry; and their impact on the peer reviewing process. For each topic, we provide an analysis and devise recommendations to mitigate found disparities, some of which already successfully implemented. Finally, we discuss additional concerns raised by many participants in free-text responses.
Authors: Ji-Ung Lee, Haritz Puerto, Betty van Aken, Yuki Arase, J. Forde, Leon Derczynski, Andreas Ruckl'e, Iryna Gurevych, Roy Schwartz, Emma Strubell, Jesse Dodge
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work captures existing (dis)parities between different and within groups with respect to seniority, academia, and industry; and their impact on the peer reviewing process; and provides an analysis and devise recommendations to mitigate found disparities.'}
Url: http://arxiv.org/pdf/2306.16900
Title: Surveying (Dis)Parities and Concerns of Compute Hungry NLP Research
Authors: Ji-Ung Lee, Haritz Puerto, Betty van Aken, Yuki Arase, Jessica Zosa Forde, Leon Derczynski, Andreas Rücklé, Iryna Gurevych, Roy Schwartz, Emma Strubell, Jesse Dodge
Section: 5.1 Analysis
well as instructing reviewers than senior researchers (11– 15 years with a p-value of 0.089 and 16+ years with a p-value of 0.085). Job Sector. In terms of the job sector, we again find no significant differences with respect to reviewers asking for too expensive experiments (𝑄10) or critique being justified (𝑄12). Interestingly, respondents from small industry received fewer such requests (𝑄11) compared to post-docs (p-value = 0.024), PIs (p-value= 0.061), and larger industry (p-value = 0.087). The most concerning trend can be observed when comparing the different groups with respect to their lack of compute resources to 4https://2022.naacl.org/blog/ reproducibility-track/ reproduce experiments (𝑄13, Fig. 8); where we find significant differences and conduct pairwise analyses.5 In general, students suffered most, with a significant difference compared to the large industry sector with a p-value of 0.002 < 0.005 = 𝛼 (Bonferroni-corrected). We further find substantial differences between students and academic PIs (pvalue = 0.026) and between academic post-docs and large industry labs (p-value = 0.088). We find no substantial differences when it comes to actionable items for the *CL community (𝑄14– 𝑄17), indicating that implementing popular ideas would be welcomed by all groups. However, we find some differences when it comes to encouraging the release of models (𝑄18). For instance, Figure 9d shows that academic post-docs had a higher preference towards reviewers rewarding papers that promise to release models than academic PIs. Also, participants from small industry would prefer visual branding over awards in contrast to large industry.
Answer: "
"What is the name of the benchmark that extends SUPERB to multiple languages?
","['shinji watanabe_bdf9ea3a67691e1b6a362f4019bf80c9cf31cecd_metadata.txt', 'shinji watanabe_090b284b2f8fc93ac3e7a92fc9f91bf4965ba75c_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the name of the benchmark that extends SUPERB to multiple languages?

Context: Faculty Name: shinji watanabe
Paperid: bdf9ea3a67691e1b6a362f4019bf80c9cf31cecd
Title: Findings of the 2023 ML-Superb Challenge: Pre-Training And Evaluation Over More Languages And Beyond
Year: 2023
Abstract: The 2023 Multilingual Speech Universal Performance Benchmark (ML-SUPERB) Challenge expands upon the acclaimed SUPERB framework, emphasizing self-supervised models in multilingual speech recognition and language identification. The challenge comprises a research track focused on applying ML-SUPERB to specific multilingual subjects, a Challenge Track for model submissions, and a New Language Track where language resource researchers can contribute and evaluate their low-resource language data in the context of the latest progress in multilingual speech recognition. The challenge garnered 12 model submissions and 54 language corpora, resulting in a comprehensive benchmark encompassing 154 languages. The findings indicate that merely scaling models is not the definitive solution for multilingual speech tasks, and a variety of speech/voice types present significant challenges in multilingual speech processing.
Authors: Jiatong Shi, William Chen, Dan Berrebbi, Hsiu-Hsuan Wang, Wei-Ping Huang, En-Pei Hu, Ho-Lam Chuang, Xuankai Chang, Yuxun Tang, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Shinji Watanabe
Venue: Automatic Speech Recognition & Understanding
Tldr: {'model': 'tldr@v2.0.0', 'text': 'The 2023 Multilingual Speech Universal Performance Benchmark (ML-SUPERB) Challenge expands upon the acclaimed SUPERB framework, emphasizing self-supervised models in multilingual speech recognition and language identification, resulting in a comprehensive benchmark encompassing 154 languages.'}
Url: https://arxiv.org/pdf/2310.05513
Faculty Name: shinji watanabe
Paperid: 090b284b2f8fc93ac3e7a92fc9f91bf4965ba75c
Title: ML-SUPERB: Multilingual Speech Universal PERformance Benchmark
Year: 2023
Abstract: Speech processing Universal PERformance Benchmark (SUPERB) is a leaderboard to benchmark the performance of Self-Supervised Learning (SSL) models on various speech processing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from high-resource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB benchmark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research.
Authors: Jiatong Shi, Dan Berrebbi, William Chen, Ho-Lam Chung, En-Pei Hu, Wei Huang, Xuankai Chang, Shang-Wen Li, Abdel-rahman Mohamed, Hung-yi Lee, Shinji Watanabe
Venue: Interspeech
Tldr: {'model': 'tldr@v2.0.0', 'text': None}
Url: https://arxiv.org/pdf/2305.10615
Answer: "
"What model does SYNTACC use for multi-accent speech synthesis?
","['alexander waibel_papers.txt', 'shinji watanabe_bdf9ea3a67691e1b6a362f4019bf80c9cf31cecd_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What model does SYNTACC use for multi-accent speech synthesis?

Context: List of 2023 Open Access papers by alexander waibel are:
AdapITN: A Fast, Reliable, and Dynamic Adaptive Inverse Text Normalization
Train Global, Tailor Local: Minimalist Multilingual Translation into Endangered Languages
Towards Efficient Simultaneous Speech Translation: CUNI-KIT System for Simultaneous Track at IWSLT 2023
SYNTACC : Synthesizing Multi-Accent Speech By Weight Factorization
KIT’s Multilingual Speech Translation System for IWSLT 2023
Convoifilter: A case study of doing cocktail party speech recognition
Continually learning new languages
Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff
End-to-End Evaluation for Low-Latency Simultaneous Speech Translation
FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN
Incremental Learning of Humanoid Robot Behavior from Natural Interaction and Large Language Models
ICASSP, 2017, pp. 5295–5299. [20] Wenxin Hou, Yue Dong, Bairong Zhuang, Longfei Yang, Jiatong Shi, and Takahiro Shinozaki, “Large-Scale End-to-End Multilingual Speech Recognition and Language Identification with Multi-Task Learning,” in Proc. Interspeech, 2020, pp. 1037–1041. [21] Vineel Pratap, Anuroop Sriram, Paden Tomasello, Awni Hannun, Vitaliy Liptchinsky, Gabriel Synnaeve, and Ronan Collobert, “Massively Multilingual ASR: 50 Languages, 1 Model, 1 Billion Parameters,” in Proc. Interspeech, 2020, pp. 4751– 4755. [22] Bo Li, Ruoming Pang, Tara N Sainath, Anmol Gulati, Yu Zhang, James Qin, Parisa Haghani, et al., “Scaling end-toend models for large-scale multilingual asr,” in Proc. ASRU, 2021, pp. 1011–1018. [23] Kazuya Kawakami, Luyu Wang, Chris Dyer, Phil Blunsom, and Aaron van den Oord, “Learning robust and multilingual speech representations,” in Findings of EMNLP, 2020, pp. 1182–1192. [24] William Chen, Brian Yan, Jiatong Shi, Yifan Peng, Soumi Maiti, and Shinji Watanabe, “Improving massively multilingual ASR with auxiliary CTC objectives,” Proc. Interspeech, 2023. [25] Jinyi Yang, Amir Hussein, Matthew Wiesner, and Sanjeev Khudanpur, “Jhu iwslt 2022 dialect speech translation system description,” in Proc. IWSLT, 2022, pp. 319–326. [26] Siddhant Arora, Siddharth Dalmia, Pavel Denisov, Xuankai Chang, Yushi Ueda, Yifan Peng, Yuekai Zhang, Sujay Kumar, Karthik Ganesan, Brian Yan, et al., “ESPnet-SLU: Advancing spoken language understanding through espnet,” in Proc.
Answer: "
"What are the units for linguistics lab?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the units for linguistics lab?

Context: In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Linguistics Lab' with Course ID 11723 and Section A offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin, Mortensen located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Human Language for Artificial Intelligence' with Course ID 11724 and Section A offers 12.0 units. The Class meets Monday Wednesday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin located in Building WEH, Room 5409.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Meaning in Language Lab (Self Paced)' with Course ID 11726 and Section A offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rose located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Multilingual Natural Language Processing.' with Course ID 11737 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Li located in Building DH, Room 1212.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Machine Learning for Text and Graph-based Mining' with Course ID 11741 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Yang located in Building HH, Room B131.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Machine Learning for Text and Graph-based Mining' with Course ID 11741 and Section PP offers 12.0 units. The Class meets Schedule will be added between NA and NA ET.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Linguistics Lab' with Course ID 11723 and Section A offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin, Mortensen located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Human Language for Artificial Intelligence' with Course ID 11724 and Section A offers 12.0 units. The Class meets Monday Wednesday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin located in Building WEH, Room 5409.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Meaning in Language Lab (Self Paced)' with Course ID 11726 and Section A offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rose located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Multilingual Natural Language Processing.' with Course ID 11737 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Li located in Building DH, Room 1212.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Machine Learning for Text and Graph-based Mining' with Course ID 11741 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Yang located in Building HH, Room B131.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Machine Learning for Text and Graph-based Mining' with Course ID 11741 and Section PP offers 12.0 units. The Class meets Schedule will be added between NA and NA ET.
Answer: "
"How many courses does Abdelghany teach in Summer 2024?
","['metadata_course_summer_one_all_24.txt', 'combined_metadata_final.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many courses does Abdelghany teach in Summer 2024?

Context: In Semester Summer One(All) 2024, from the department of Mathematical Sciences, the subject titled 'Integration and Approximation' with Course ID 21122 and Section S offers 10.0 units. The Class meets Monday Tuesday Wednesday Thursday Friday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Flaherty located in Building HH, Room B103.
In Semester Summer One(All) 2024, from the department of Mathematical Sciences, the subject titled 'Integration and Approximation' with Course ID 21122 and Section W offers 10.0 units. The Class meets Sunday Monday Tuesday Wednesday Thursday between 11:00AM and 12:30PM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Instructor TBA located in Building TBA, Room None.
In Semester Summer One(All) 2024, from the department of Mathematical Sciences, the subject titled 'Concepts of Mathematics' with Course ID 21127 and Section S offers 12.0 units. The Class meets Monday Tuesday Wednesday Thursday Friday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Abdelghany located in Building SH, Room 105.
In Semester Summer One(All) 2024, from the department of Mathematical Sciences, the subject titled 'Matrices and Linear Transformations' with Course ID 21241 and Section S offers 12.0 units. The Class meets Monday Tuesday Wednesday Thursday Friday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Handron located in Building BH, Room A36.
In Semester Summer One(All) 2024, from the department of Mathematical Sciences, the subject titled 'Calculus in Three Dimensions' with Course ID 21259 and Section S offers 10.0 units. The Class meets Monday Tuesday Wednesday Thursday Friday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Johnson located in Building DH, Room 1212.
In Semester Summer One(All) 2024, from the department of Mathematical Sciences, the subject titled 'Differential Equations' with Course ID 21260 and Section S offers 9.0 units.
In Semester Summer One(All) 2024, from the department of Mathematical Sciences, the subject titled 'Integration and Approximation' with Course ID 21122 and Section W offers 10.0 units. The Class meets Sunday Monday Tuesday Wednesday Thursday between 11:00AM and 12:30PM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Instructor TBA located in Building TBA, Room None.
In Semester Summer One(All) 2024, from the department of Mathematical Sciences, the subject titled 'Concepts of Mathematics' with Course ID 21127 and Section S offers 12.0 units. The Class meets Monday Tuesday Wednesday Thursday Friday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Abdelghany located in Building SH, Room 105.
In Semester Summer One(All) 2024, from the department of Mathematical Sciences, the subject titled 'Matrices and Linear Transformations' with Course ID 21241 and Section S offers 12.0 units. The Class meets Monday Tuesday Wednesday Thursday Friday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Handron located in Building BH, Room A36.
In Semester Summer One(All) 2024, from the department of Mathematical Sciences, the subject titled 'Calculus in Three Dimensions' with Course ID 21259 and Section S offers 10.0 units. The Class meets Monday Tuesday Wednesday Thursday Friday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Johnson located in Building DH, Room 1212.
In Semester Summer One(All) 2024, from the department of Mathematical Sciences, the subject titled 'Differential Equations' with Course ID 21260 and Section S offers 9.0 units. The Class meets Monday Tuesday Wednesday Thursday Friday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Handron located in Building HH, Room B103.
In Semester Summer One(All) 2024, from the department of Mathematical Sciences, the subject titled 'Probability' with Course ID 21325 and Section S offers 9.0 units.
Answer: "
"Which LTI faculty member works on recommender systems?
","['mlt-student-handbook-2023-2024.txt', 'fernando diaz_567f6bc975deb3d728feec9bfcf7d4036ceabb12_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which LTI faculty member works on recommender systems?

Context: 4.5 Recommended Electives outside of SCS  
Students are free to take elective courses outside the SCS, at Carnegie Mellon or cross-
registered at the University of Pittsburgh, as long as the student fulfills the requirements of 
their program as described above. The student should discuss any such electives in advance 
with their advisor; typical choices might include ECE courses for Speech students, Pitt Linguistics 
courses, or Statistics courses. 
Note: recommended electives outside of the SCS count towards the SCS course requirement. 
Please see the Program Director for approval of electives as SCS. 
Note also that students need advance approval for any courses not covered by their normal 
tuition (e.g., summer courses). The grading of outside courses is the responsibility of the 
department offering the course; however, the LTI's Minimum Course Grade Policy described 
above still applies (B- is the minimum for PhD, C for MLT). 
 
MLT Graduate Student Handbook 
Page 18 
 
4.6 LTI Orientation 
At the beginning of each Fall semester, the LTI provides a set of lectures and talks to help 
students learn about the work done by CMU faculty and to provide an opportunity for advisors 
to recruit new students.  Students are expected to attend them and treat them seriously, 
because they provide a good introduction to the broad range of research done at the LTI. 
Students do not register for the LTI Orientation, nor do they receive a grade; however, the 
department is serious about its expectation that new students will attend these talks. 
For many years, these talks were called ""the Immigration Course (IC)"", which caused confusion.  
The older name has been retired; however, people who have been at CMU a long time may 
occasionally use the older name.   
4.7 End of Semester Evaluation 
Near the end of the spring and fall semesters, the student must prepare a statement that 
describes their achievements in the current semester, and plans for the next semester. At the 
end of the semester, the faculty evaluates each student's academic progress. The student's 
advisor serves as the student's advocate in this process. The result of the evaluation is a letter 
from the faculty to the student that indicates whether the student is making satisfactory 
progress towards completing the degree. 
A good letter typically indicates that the student is making satisfactory progress.
Faculty Name: fernando diaz
Paperid: 567f6bc975deb3d728feec9bfcf7d4036ceabb12
Title: Fairness Through Domain Awareness: Mitigating Popularity Bias For Music Discovery
Year: 2023
Abstract: As online music platforms grow, music recommender systems play a vital role in helping users navigate and discover content within their vast musical databases. At odds with this larger goal, is the presence of popularity bias, which causes algorithmic systems to favor mainstream content over, potentially more relevant, but niche items. In this work we explore the intrinsic relationship between music discovery and popularity bias. To mitigate this issue we propose a domain-aware, individual fairness-based approach which addresses popularity bias in graph neural network (GNNs) based recommender systems. Our approach uses individual fairness to reflect a ground truth listening experience, i.e., if two songs sound similar, this similarity should be reflected in their representations. In doing so, we facilitate meaningful music discovery that is robust to popularity bias and grounded in the music domain. We apply our BOOST methodology to two discovery based tasks, performing recommendations at both the playlist level and user level. Then, we ground our evaluation in the cold start setting, showing that our approach outperforms existing fairness benchmarks in both performance and recommendation of lesser-known content. Finally, our analysis explains why our proposed methodology is a novel and promising approach to mitigating popularity bias and improving the discovery of new and niche content in music recommender systems.
Authors: Rebecca Salganik, Fernando Diaz, G. Farnadi
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work proposes a domain-aware, individual fairness-based approach which addresses popularity bias in graph neural network (GNNs) based recommender systems and applies the BOOST methodology to two discovery based tasks, performing recommendations at both the playlist level and user level.'}
Url: https://arxiv.org/pdf/2308.14601
Answer: "
"What does the Plan module in the PET framework do?
","['yonatan bisk_5ce2f1dff23a5620f77f9b11f1e534422ab8ff3f_metadata.txt', 'yonatan bisk_5ce2f1dff23a5620f77f9b11f1e534422ab8ff3f_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What does the Plan module in the PET framework do?

Context: Faculty Name: yonatan bisk
Paperid: 5ce2f1dff23a5620f77f9b11f1e534422ab8ff3f
Title: Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents
Year: 2023
Abstract: Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accomplished each sub-task. On the AlfWorld instruction following benchmark, the PET framework leads to a significant 15% improvement over SOTA for generalization to human goal specifications.
Authors: Yue Wu, So Yeon Min, Yonatan Bisk, R. Salakhutdinov, A. Azaria, Yuan-Fang Li, Tom M. Mitchell, Shrimai Prabhumoye
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A framework to use the knowledge in LLMs to simplify the control problem, rather than solving it is proposed, which leads to a significant 15% improvement over SOTA for generalization to human goal specifications.'}
Url: http://arxiv.org/pdf/2305.02412
Title: Plan, Eliminate, and Track — Language Models are Good Teachers for Embodied Agents
Authors: Yue Wu, So Yeon Min, Yonatan Bisk, Ruslan Salakhutdinov, Amos Azaria, Yuanzhi Li, Tom M. Mitchell, Shrimai Prabhumoye
Section: 5. Conclusion, Limitations, and Future Work
In this work, we propose the Plan, Eliminate, and Track (PET) framework that uses pre-trained LLMs to assist an embodied agent in three steps. Our PET framework requires no fine-tuning and is designed to be compatible with any goal-conditional embodied agents. In our experiments, we combine PET with a novel Action Attention agent that handles the dynamic action space in AlfWorld. Our Action Attention agent greatly outperforms the BUTLER baseline. In addition, since the PET framework is not trained to fit the training set tasks, it demonstrates better generalization to unseen human goal specification tasks. Finally, our ablation studies show the Plan and Track modules together improve the performance of Eliminate module to achieve the best performance. Our results show that LLMs can be a good source of common sense and procedural knowledge for embodied agents, and multiple LLMs may be used in coordination with each other to further improve effectiveness. One of the major limitations of our current system design is that the Track module (progress tracker) does not re-visit finished sub-tasks. If for example, the agent is executing sub-tasks [picked up a pan, put the pan on countertop], and it picked up a pan but put it in the fridge (undo pickup action). Since the progress tracker does not take into consideration previous progress being undone, the system may break in this situation. Future work can focus on adding sub-task-level dynamic replanning to address this limitation or explore other ways in which LLMs can assist the learning of the policy (i.e., reading an instruction manual about the environment).
Answer: "
"What is the term for the discrepancies between increases in computational throughput and reductions in floating point operations, and improvements in wall-clock inference latency?
","['yonatan bisk_b777aa86b5a1d49ce8eababc5c2ee56d3562801e_metadata.txt', 'emma strubell_b777aa86b5a1d49ce8eababc5c2ee56d3562801e_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the term for the discrepancies between increases in computational throughput and reductions in floating point operations, and improvements in wall-clock inference latency?

Context: Faculty Name: yonatan bisk
Paperid: b777aa86b5a1d49ce8eababc5c2ee56d3562801e
Title: The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment
Year: 2023
Abstract: Increased focus on the computational efficiency of NLP systems has motivated the design of efficient model architectures and improvements to underlying hardware accelerators. However, the resulting increases in computational throughput and reductions in floating point operations have not directly translated to improvements in wall-clock inference latency. We demonstrate that these discrepancies can be largely attributed to bottlenecks introduced by deep learning frameworks. We denote this phenomenon as the \textit{framework tax}, and observe that the disparity is growing as hardware speed increases over time. In this work, we examine this phenomenon through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency. Code is available at https://github.com/JaredFern/Framework-Tax.
Authors: Jared Fernandez, Jacob Kahn, Clara Na, Yonatan Bisk, Emma Strubell
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work examines the effects of model design decisions, framework paradigms, and hardware platforms on total model latency through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency.'}
Url: http://arxiv.org/pdf/2302.06117
Faculty Name: emma strubell
Paperid: b777aa86b5a1d49ce8eababc5c2ee56d3562801e
Title: The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment
Year: 2023
Abstract: Increased focus on the computational efficiency of NLP systems has motivated the design of efficient model architectures and improvements to underlying hardware accelerators. However, the resulting increases in computational throughput and reductions in floating point operations have not directly translated to improvements in wall-clock inference latency. We demonstrate that these discrepancies can be largely attributed to bottlenecks introduced by deep learning frameworks. We denote this phenomenon as the \textit{framework tax}, and observe that the disparity is growing as hardware speed increases over time. In this work, we examine this phenomenon through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency. Code is available at https://github.com/JaredFern/Framework-Tax.
Authors: Jared Fernandez, Jacob Kahn, Clara Na, Yonatan Bisk, Emma Strubell
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work examines the effects of model design decisions, framework paradigms, and hardware platforms on total model latency through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency.'}
Url: http://arxiv.org/pdf/2302.06117
Answer: "
"In spring 2024, What is the course number for Game Theoretic Probability, Statistics and Learning?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, What is the course number for Game Theoretic Probability, Statistics and Learning?

Context: The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Poczos located in Building TEP, Room 1403.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Representation and Generation in Neuroscience and AI' with Course ID 10733 and Section A offers 12.0 units. The Class meets Monday Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Wehbe located in Building GHC, Room 4102.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Responsible AI' with Course ID 10735 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Heidari, London located in Building POS, Room 151.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Game Theoretic Probability, Statistics and Learning' with Course ID 10880 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ramdas located in Building DH, Room 1211.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Graduate Reading and Research' with Course ID 10920 and Section A offers 12-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Mitchell located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Dissertation Research' with Course ID 10930 and Section A offers 5-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Mitchell located in Building DNM, Room DNM.
The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Poczos located in Building TEP, Room 1403.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Representation and Generation in Neuroscience and AI' with Course ID 10733 and Section A offers 12.0 units. The Class meets Monday Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Wehbe located in Building GHC, Room 4102.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Responsible AI' with Course ID 10735 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Heidari, London located in Building POS, Room 151.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Game Theoretic Probability, Statistics and Learning' with Course ID 10880 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ramdas located in Building DH, Room 1211.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Graduate Reading and Research' with Course ID 10920 and Section A offers 12-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Mitchell located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Dissertation Research' with Course ID 10930 and Section A offers 5-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Mitchell located in Building DNM, Room DNM.
Answer: "
"Which Faculty from LTI Co-authored the paper Transformed Protoform Reconstruction?
","['david mortensen_c5c6d006e399386c99068daba138021a62d6cc17_metadata.txt', 'david mortensen_c5c6d006e399386c99068daba138021a62d6cc17_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which Faculty from LTI Co-authored the paper Transformed Protoform Reconstruction?

Context: Faculty Name: david mortensen
Paperid: c5c6d006e399386c99068daba138021a62d6cc17
Title: Transformed Protoform Reconstruction
Year: 2023
Abstract: Protoform reconstruction is the task of inferring what morphemes or words appeared like in the ancestral languages of a set of daughter languages. Meloni et al (2021) achieved the state-of-the-art on Latin protoform reconstruction with an RNN-based encoder-decoder with attention model. We update their model with the state-of-the-art seq2seq model: the Transformer. Our model outperforms their model on a suite of different metrics on two different datasets: their Romance data of 8,000 cognates spanning 5 languages and a Chinese dataset (Hou 2004) of 800+ cognates spanning 39 varieties. We also probe our model for potential phylogenetic signal contained in the model. Our code is publicly available at https://github.com/cmu-llab/acl-2023.
Authors: Young Min Kim, Kalvin Chang, Chenxuan Cui, David R. Mortensen
Venue: Annual Meeting of the Association for Computational Linguistics
Tldr: {'model': 'tldr@v2.0.0', 'text': 'The Meloni et al (2021) model is updated with the state-of-the-art seq2seq model: the Transformer, which outperforms their model on a suite of different metrics on two different datasets: their Romance data of 8,000 cognate spanning 5 languages and a Chinese dataset (Hou 2004) of 800+ cognates spanning 39 varieties.'}
Url: https://arxiv.org/pdf/2307.01896
Title: Transformed Protoform Reconstruction
Authors: Young Min Kim, Kalvin Chang, Chenxuan Cui, David Mortensen
Section: C Supplementary Results
In order to compare our model to earlier work, we used the Rom-phon and Rom-orth datasets from Meloni et al. (2021). However, this set includes a subset from Ciobanu and Dinu (2018) which is not freely redistributable. So that our results can be reproduced, we also computed them on the publicly available subset of Meloni et al. (2021)’s dataset, which is presented in Table 7. Phylogenetic trees for Chinese were also extracted from the RNN and Transformer models. These are shown in Figures 3 and 4. We also plot the dendrograms derived from the Rom-orto dataset in Figure 5.
Answer: "
"What is the full name of the conference where the paper ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages, got published? 
","['david mortensen_11a571eaab42a6ffb1d938635a093315e392756d_metadata.txt', 'graham neubig_11a571eaab42a6ffb1d938635a093315e392756d_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the full name of the conference where the paper ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages, got published? 

Context: Faculty Name: david mortensen
Paperid: 11a571eaab42a6ffb1d938635a093315e392756d
Title: ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages
Year: 2023
Abstract: Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs’ MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world’s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language’s resource level is the most important feature in determining ChatGPT’s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.
Authors: Nathaniel R. Robinson, Perez Ogayo, David R. Mortensen, Graham Neubig
Venue: Conference on Machine Translation
Tldr: {'model': 'tldr@v2.0.0', 'text': ""This work presents the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark, and reveals that a language’s resource level is the most important feature in determining ChatGPT's relative ability to translate it, and suggests thatChatGPT is especially disadvantaged for LRLs and African languages.""}
Url: https://arxiv.org/pdf/2309.07423
Faculty Name: graham neubig
Paperid: 11a571eaab42a6ffb1d938635a093315e392756d
Title: ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages
Year: 2023
Abstract: Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs’ MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world’s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language’s resource level is the most important feature in determining ChatGPT’s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.
Authors: Nathaniel R. Robinson, Perez Ogayo, David R. Mortensen, Graham Neubig
Venue: Conference on Machine Translation
Tldr: {'model': 'tldr@v2.0.0', 'text': ""This work presents the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark, and reveals that a language’s resource level is the most important feature in determining ChatGPT's relative ability to translate it, and suggests thatChatGPT is especially disadvantaged for LRLs and African languages.""}
Url: https://arxiv.org/pdf/2309.07423
Answer: "
"In the KALE paper, what evaluation metrics were reported on TREC DL 19?
","['bhiksha raj_255bad49d29202e2d255926ab0983c125dcce835_content_2.txt', 'rita singh_255bad49d29202e2d255926ab0983c125dcce835_content_2.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the KALE paper, what evaluation metrics were reported on TREC DL 19?

Context: Title: EVALUATING SPEECH SYNTHESIS BY TRAINING RECOGNIZERS ON SYNTHETIC SPEECH
Authors: Dareen Alharthi, Roshan Sharma, Hira Dhamyal, Soumi Maiti, Bhiksha Raj, Rita Singh
Section: 4.4. Evaluation Metrics
of synthetic speech using speech recognition,” in Procs. of the 16th International Congress on Sound and Vibration (ICSV16), Kraków, Poland, 2009, pp. 5–9. [2] S. Maiti, Y. Peng, T. Saeki, and S. Watanabe, “Speechlmscore: Evaluating speech generation using speech language model,” in Proc. ICASSP, 2023, pp. 1–5. [3] T. Sellam, A. Bapna, J. Camp, D. Mackinnon, A. P. Parikh, and J. Riesa, “Squid: Measuring speech naturalness in many languages,” in Proc. ICASSP, 2023, pp. 1–5. [4] M. Le, A. Vyas, B. Shi, B. Karrer, L. Sari, R. Moritz, M. Williamson, V. Manohar, Y. Adi, J. Mahadeokar, et al., “Voicebox: Text-guided multilingual universal speech generation at scale,” arXiv preprint arXiv:2306.15687, 2023. [5] M. Bińkowski, J. Donahue, S. Dieleman, A. Clark, E. Elsen, N. Casagrande, L. C. Cobo, and K. Simonyan, “High fidelity speech synthesis with adversarial networks,” in Proc. ICLR, 2019. [6] C. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li, et al., “Neural codec language models are zero-shot text to speech synthesizers,” arXiv preprint arXiv:2301.02111, 2023. [7] C.-C. Lo, S.-W.
Title: EVALUATING SPEECH SYNTHESIS BY TRAINING RECOGNIZERS ON SYNTHETIC SPEECH
Authors: Dareen Alharthi, Roshan Sharma, Hira Dhamyal, Soumi Maiti, Bhiksha Raj, Rita Singh
Section: 4.4. Evaluation Metrics
of synthetic speech using speech recognition,” in Procs. of the 16th International Congress on Sound and Vibration (ICSV16), Kraków, Poland, 2009, pp. 5–9. [2] S. Maiti, Y. Peng, T. Saeki, and S. Watanabe, “Speechlmscore: Evaluating speech generation using speech language model,” in Proc. ICASSP, 2023, pp. 1–5. [3] T. Sellam, A. Bapna, J. Camp, D. Mackinnon, A. P. Parikh, and J. Riesa, “Squid: Measuring speech naturalness in many languages,” in Proc. ICASSP, 2023, pp. 1–5. [4] M. Le, A. Vyas, B. Shi, B. Karrer, L. Sari, R. Moritz, M. Williamson, V. Manohar, Y. Adi, J. Mahadeokar, et al., “Voicebox: Text-guided multilingual universal speech generation at scale,” arXiv preprint arXiv:2306.15687, 2023. [5] M. Bińkowski, J. Donahue, S. Dieleman, A. Clark, E. Elsen, N. Casagrande, L. C. Cobo, and K. Simonyan, “High fidelity speech synthesis with adversarial networks,” in Proc. ICLR, 2019. [6] C. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li, et al., “Neural codec language models are zero-shot text to speech synthesizers,” arXiv preprint arXiv:2301.02111, 2023. [7] C.-C. Lo, S.-W.
Answer: "
"When are the sweepstakes finals at Spring Carnival?
","['Apr-13_Eventno_49_SpringCarnivalBoothSweepstakesAwardCeremony.txt', 'cmubuggy_d403.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When are the sweepstakes finals at Spring Carnival?

Context: Event: Spring Carnival Booth & Sweepstakes Award Ceremony
Date: 4/13/24
Time: 4:30 PM-5:30 PM ET
Participants/Audience: Open to entire CMU community 
Event Details: 
Note: No registration required. No event fee.
Buggy Races Keep Rolling at Carnegie Mellon - News - Carnegie Mellon University Carnegie Mellon University ——— Search Search Search this site only News News › Stories › Archives › 2019 › April › Buggy Races Keep Rolling at Carnegie Mellon April 10, 2019 Buggy Races Keep Rolling at Carnegie Mellon In its 99th year, the tradition is a Spring Carnival treat By Heidi Opdyke opdyke(through)andrew.cmu.edu Media Inquiries Julie Mattera Marketing and Communications jmattera(through)cmu.edu 412-268-2902 Sweepstakes, also known as the Buggy Races, has come a long way at Carnegie Mellon University. The slick, torpedo-like vessels carrying drivers with nerves of steel are a far cry from the two-man teams that once changed places halfway through a race and rode in everything from rain barrels with bicycle wheels to three-wheeled ash cans 99 years ago. Today, it takes six people to maneuver the .84 -mile course around Schenley Park's Flagstaff Hill. But while five pushers and a driver navigate the course's hills, dozens of people are needed to make a successful race happen. A year of planning goes into just over two minutes of racing. The Machine The basics of a buggy are straightforward, but teams are often secretive in how they build the machines, in particular the way they brake, steer and what types of wheels are used. Each has a body, pushbar for runners to move the machine up the hills, wheels, a safety harness and driving and braking mechanisms. Some also include fairings, a type of housing around the wheels that help reduce drag, make the vehicle quieter and just looks cool. Fairings have been a key feature for the Fringe team in recent years, which is celebrating its 50th anniversary of Buggy. ""We have a reputation of being the quietest on the course,"" said Diya Nuxoll, who wrapped up her bachelor's degree in mechanical engineering in December and is working on an advanced studies master's degree in design and manufacturing. Nuxoll leads the Fringe team as one of two head mechanics. An mural celebrating Buggy can be found in CMU's Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions. Sweepstakes Slang Buggy: Vehicle being raced and also a nickname for the competition.
Answer: "
"Where is the SENTECON paper published at?
","['louis philippe morency_papers.txt', 'louis philippe morency_47a4ac301820c3ea7da4efb8e2466cc6468ad631_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Where is the SENTECON paper published at?

Context: List of 2023 Open Access papers by louis philippe morency are:
Quantifying & Modeling Feature Interactions: An Information Decomposition Framework
Reconstructing the neurodynamics of face perception during real world vision in humans using intracranial EEG recordings
MultiZoo & MultiBench: A Standardized Toolkit for Multimodal Deep Learning
SenteCon: Leveraging Lexicons to Learn Human-Interpretable Language Representations
Neural Mixed Effects for Nonlinear Personalized Predictions
SHAP-based Prediction of Mother's History of Depression to Understand the Influence on Child Behavior
Counterfactual Augmentation for Multimodal Learning Under Presentation Bias
Difference-Masking: Choosing What to Mask in Continued Pretraining
Expanding the Role of Affective Phenomena in Multimodal Interaction Research
Multimodal Fusion Interactions: A Study of Human and Automatic Quantification
Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications
Representation Learning for Interpersonal and Multimodal Behavior Dynamics: A Multiview Extension of Latent Change Score Models
MultiViz: Towards User-Centric Visualizations and Interpretations of Multimodal Models
Understanding Masked Autoencoders via Hierarchical Latent Variable Models
Factorized Contrastive Learning: Going Beyond Multi-view Redundancy
Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions
Faculty Name: louis philippe morency
Paperid: 47a4ac301820c3ea7da4efb8e2466cc6468ad631
Title: SenteCon: Leveraging Lexicons to Learn Human-Interpretable Language Representations
Year: 2023
Abstract: Although deep language representations have become the dominant form of language featurization in recent years, in many settings it is important to understand a model's decision-making process. This necessitates not only an interpretable model but also interpretable features. In particular, language must be featurized in a way that is interpretable while still characterizing the original text well. We present SenteCon, a method for introducing human interpretability in deep language representations. Given a passage of text, SenteCon encodes the text as a layer of interpretable categories in which each dimension corresponds to the relevance of a specific category. Our empirical evaluations indicate that encoding language with SenteCon provides high-level interpretability at little to no cost to predictive performance on downstream tasks. Moreover, we find that SenteCon outperforms existing interpretable language representations with respect to both its downstream performance and its agreement with human characterizations of the text.
Authors: Victoria Lin, Louis-Philippe Morency
Venue: Annual Meeting of the Association for Computational Linguistics
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work presents SenteCon, a method for introducing human interpretability in deep language representations that outperforms existing interpretable language representations with respect to both its downstream performance and its agreement with human characterizations of the text.'}
Url: http://arxiv.org/pdf/2305.14728
Answer: "
"What is the framework proposed to simplify the control problem of embodied agents using LLMs?
","['yonatan bisk_5ce2f1dff23a5620f77f9b11f1e534422ab8ff3f_metadata.txt', 'yonatan bisk_5ce2f1dff23a5620f77f9b11f1e534422ab8ff3f_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the framework proposed to simplify the control problem of embodied agents using LLMs?

Context: Faculty Name: yonatan bisk
Paperid: 5ce2f1dff23a5620f77f9b11f1e534422ab8ff3f
Title: Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents
Year: 2023
Abstract: Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accomplished each sub-task. On the AlfWorld instruction following benchmark, the PET framework leads to a significant 15% improvement over SOTA for generalization to human goal specifications.
Authors: Yue Wu, So Yeon Min, Yonatan Bisk, R. Salakhutdinov, A. Azaria, Yuan-Fang Li, Tom M. Mitchell, Shrimai Prabhumoye
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A framework to use the knowledge in LLMs to simplify the control problem, rather than solving it is proposed, which leads to a significant 15% improvement over SOTA for generalization to human goal specifications.'}
Url: http://arxiv.org/pdf/2305.02412
Title: Plan, Eliminate, and Track — Language Models are Good Teachers for Embodied Agents
Authors: Yue Wu, So Yeon Min, Yonatan Bisk, Ruslan Salakhutdinov, Amos Azaria, Yuanzhi Li, Tom M. Mitchell, Shrimai Prabhumoye
Section: 5. Conclusion, Limitations, and Future Work
In this work, we propose the Plan, Eliminate, and Track (PET) framework that uses pre-trained LLMs to assist an embodied agent in three steps. Our PET framework requires no fine-tuning and is designed to be compatible with any goal-conditional embodied agents. In our experiments, we combine PET with a novel Action Attention agent that handles the dynamic action space in AlfWorld. Our Action Attention agent greatly outperforms the BUTLER baseline. In addition, since the PET framework is not trained to fit the training set tasks, it demonstrates better generalization to unseen human goal specification tasks. Finally, our ablation studies show the Plan and Track modules together improve the performance of Eliminate module to achieve the best performance. Our results show that LLMs can be a good source of common sense and procedural knowledge for embodied agents, and multiple LLMs may be used in coordination with each other to further improve effectiveness. One of the major limitations of our current system design is that the Track module (progress tracker) does not re-visit finished sub-tasks. If for example, the agent is executing sub-tasks [picked up a pan, put the pan on countertop], and it picked up a pan but put it in the fridge (undo pickup action). Since the progress tracker does not take into consideration previous progress being undone, the system may break in this situation. Future work can focus on adding sub-task-level dynamic replanning to address this limitation or explore other ways in which LLMs can assist the learning of the policy (i.e., reading an instruction manual about the environment).
Answer: "
"In spring carnival, Scotch'n'Soda's theatre carnival shows are on what days of the week?
","['Apr-13_Eventno_46_ScotchnSodaTheatreCarnivalShowTheLittleMermaid.txt', 'Apr-13_Eventno_56_ScotchnSodaTheatreCarnivalShowTheLittleMermaid.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring carnival, Scotch'n'Soda's theatre carnival shows are on what days of the week?

Context: Event: Scotch'n'Soda Theatre Carnival Show: The Little Mermaid
Date: 4/13/24
Time: 3:00 PM-5:00 PM ET
Participants/Audience: Open to entire CMU community 
Event Details: 
Come celebrate Carnival with Scotch'n'Soda Theatre, and join us for a performance under the sea! This year, Scotch'n'Soda is thrilled to present our production of Disney's The Little Mermaid! Adapted from the wildly popular animated film and Hans Christian Anderson's original fairytale, the musical follows Ariel, King Triton's youngest daughter, who wishes to explore the world up above and pursue the human Prince Eric. But the bargains and sea witches aren't all that they seem, and Ariel needs the help of her colorful friends, Flounder the fish, Scuttle the seagull and Sebastian the crab to restore order under the sea. Sing along with all of your favorite childhood classics, and come be a part of our world! Funded in part by the Student Activities Fee. Note: Disney's The Little Mermaid is presented through special arrangement with Music Theatre International (MTI). All authorized performance materials are also supplied by MTI.  

Cost
Tickets will be available online in March and at the door. Cost: $5 for students/faculty/staff; $10 for alumni and guests. 

Weekend show times

Thursday: 7-9:30 p.m.
Friday: 6-8:30 p.m. and 10 p.m.-12:30 a.m.
Saturday: 3-5:30 p.m. and 7-9:30 p.m.
Event: Scotch'n'Soda Theatre Carnival Show: The Little Mermaid
Date: 4/13/24
Time: 7:00 PM-9:00 PM ET
Participants/Audience: Open to entire CMU community 
Event Details: 
Come celebrate Carnival with Scotch'n'Soda Theatre, and join us for a performance under the sea! This year, Scotch'n'Soda is thrilled to present our production of Disney's The Little Mermaid! Adapted from the wildly popular animated film and Hans Christian Anderson's original fairytale, the musical follows Ariel, King Triton's youngest daughter, who wishes to explore the world up above and pursue the human Prince Eric. But the bargains and sea witches aren't all that they seem, and Ariel needs the help of her colorful friends, Flounder the fish, Scuttle the seagull and Sebastian the crab to restore order under the sea. Sing along with all of your favorite childhood classics, and come be a part of our world! Funded in part by the Student Activities Fee. Note: Disney's The Little Mermaid is presented through special arrangement with Music Theatre International (MTI). All authorized performance materials are also supplied by MTI.  

Cost
Tickets will be available online in March and at the door. Cost: $5 for students/faculty/staff; $10 for alumni and guests. 

Weekend show times

Thursday: 7-9:30 p.m.
Friday: 6-8:30 p.m. and 10 p.m.-12:30 a.m.
Saturday: 3-5:30 p.m. and 7-9:30 p.m.
Answer: "
"How many credits is the MIIS Capstone Planning Seminar worth?
","['miis-handbook_2023-2024.txt', 'combined_metadata_final.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many credits is the MIIS Capstone Planning Seminar worth?

Context: Students with prior professional experience may petition the MIIS 
Program Director to waive this requirement.  
MIIS students that do an internship during the summer semester are required to present 
their internship at a poster session at the beginning of the following Fall semester.  The 
poster and the student’s participation in the poster session are part of the internship 
requirement.  Participation is required unless waived in writing by the MIIS Program 
Director. 
3. Capstone requirements: Students must complete a capstone project (36 units) and a 
capstone planning seminar (6 units). The capstone requirement gives students 
experience with collaborative, team-oriented software development; significant hands-
on experience with the techniques studied in the classroom; and an opportunity to work 
on a large software application.  
a. The capstone project (36 units) is a large, group-oriented demonstration of 
student skill in one or more areas covered by the degree. Typically, the result of 
the capstone project is a major software application. The capstone project is 
supervised by a member of the faculty who meets with students on a weekly 
basis to monitor progress and provide guidance. 
MIIS Graduate Student Handbook 
Page 17 
 
b. The capstone planning seminar (6 units) organizes students into groups; defines 
capstone project goals, requirements, success metrics, and deliverables; and 
identifies and acquires data, software, and other resources required for successful 
completion of the project. The planning seminar must be completed in the 
semester prior to taking the capstone project. 
4.6 Registration Process/Procedures 
Students are responsible for registering for their courses. Use the Stellic Degree Audit 
Application to monitor your progress and plan your degree. Students can search for relevant 
courses in a number of ways and share your plan with your advisor directly through the 
application.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Software Engineering for AI-Enabled Systems' with Course ID 11695 and Section E offers 12.0 units. The Class meets Friday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kaestner, Le Goues located in Building PH, Room 226C.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Software Engineering for AI-Enabled Systems' with Course ID 11695 and Section F offers 12.0 units. The Class meets Friday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kaestner, Le Goues located in Building GHC, Room 4215.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'MIIS Capstone Planning Seminar' with Course ID 11696 and Section A offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Brown located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'MSAII Program Capstone' with Course ID 11699 and Section A offers 36.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Shamos located in Building TBA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'LTI Colloquium' with Course ID 11700 and Section A offers 6.0 units. The Class meets Friday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Bisk, Diaz located in Building SH, Room 105.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'LTI Colloquium' with Course ID 11700 and Section P offers 6.0 units. The Class meets Schedule will be added between NA and NA ET.
Answer: "
"What are the attention dot-product scores in the Unlimiformer approach?
","['graham neubig_dbc368bc8b49347dd27679894524fa62f88492c9_metadata.txt', 'chenyan xiong_275da3802142fc42f6fab2ce2104223b2e0ef40d_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the attention dot-product scores in the Unlimiformer approach?

Context: Faculty Name: graham neubig
Paperid: dbc368bc8b49347dd27679894524fa62f88492c9
Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input
Year: 2023
Abstract: Since the proposal of transformers, these models have been limited to bounded input lengths, because of their need to attend to every token in the input. In this work, we propose Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores. This kNN index can be kept on either the GPU or CPU memory and queried in sub-linear time; this way, we can index practically unlimited input sequences, while every attention head in every decoder layer retrieves its top-k keys, instead of attending to every key. We evaluate Unlimiformer on several long-document and book-summarization benchmarks, showing that it can process even 500k token-long inputs from the BookSum dataset, without any input truncation at test time. We demonstrate that Unlimiformer improves pretrained models such as BART and Longformer by extending them to unlimited inputs without additional learned weights and without modifying their code. We make our code and models publicly available at https://github.com/abertsch72/unlimiformer .
Authors: Amanda Bertsch, Uri Alon, Graham Neubig, Matthew R. Gormley
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work proposes Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores.'}
Url: http://arxiv.org/pdf/2305.01625
Title: Fusion-in-T5: Unifying Document Ranking Signals for Improved Information Retrieval
Authors: Shi Yu, Chenghao Fan, Chenyan Xiong, David Jin, Zhiyuan Liu, Zhenghao Liu
Section: D Experiment Details
In the experiment analyzing attention distribution in §5.4, we compute attention values using the fol- 1https://sourceforge.net/p/lemur/code/HEAD/tree/RankLib/ lowing method. We assume that the global attention similarity between the i-th and k-th samples in the j-th layer of transformers is denoted by Aji,k: Aji,k = ĥji,[CLS] · ĥ j k,[CLS] ||ĥji,[CLS]||||ĥ j k,[CLS]|| (5) Assuming the i-th sample is associated with a relevance label li for query q, we compute the mean value of global attention similarity Ajq(R1, R2) in the j-th layer between samples with relevance scores R1 and R2,which indicate the model’s ability to distinguish between similar documents. Ajq(R1, R2) = ∑n i=1,li=R1 ∑n k=1,rk=R2 Aji,k∑n i=1,li=R1 ∑n k=1,lk=R2 1 (6) To facilitate smoother visualization of the results for all queries, we perform min-max normalization on the those scores in the same layer j. {Ajq(R1, R2)} = Min-Max({Ajq(R1, R2)}) (7) For j equal to 10, 11, and 12, with R1 and R2 ranging from 0 to 3, the outcomes are presented in Figure 2a. Additionally, for j equal to 12, with R1 at 3 and R2 ranging from 0 to 3, the outcomes are shown in Figure 2b.
Answer: "
"Which LTI faculty member is an author on the COBRA Frames paper?
","['maarten sap_185ace5661963e2e1eb998e739e4110272a6bb43_metadata.txt', 'maarten sap_185ace5661963e2e1eb998e739e4110272a6bb43_content_2.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which LTI faculty member is an author on the COBRA Frames paper?

Context: Faculty Name: maarten sap
Paperid: 185ace5661963e2e1eb998e739e4110272a6bb43
Title: COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive Statements
Year: 2023
Abstract: Warning: This paper contains content that may be offensive or upsetting. Understanding the harms and offensiveness of statements requires reasoning about the social and situational context in which statements are made. For example, the utterance""your English is very good""may implicitly signal an insult when uttered by a white man to a non-white colleague, but uttered by an ESL teacher to their student would be interpreted as a genuine compliment. Such contextual factors have been largely ignored by previous approaches to toxic language detection. We introduce COBRA frames, the first context-aware formalism for explaining the intents, reactions, and harms of offensive or biased statements grounded in their social and situational context. We create COBRACORPUS, a dataset of 33k potentially offensive statements paired with machine-generated contexts and free-text explanations of offensiveness, implied biases, speaker intents, and listener reactions. To study the contextual dynamics of offensiveness, we train models to generate COBRA explanations, with and without access to the context. We find that explanations by context-agnostic models are significantly worse than by context-aware ones, especially in situations where the context inverts the statement's offensiveness (29% accuracy drop). Our work highlights the importance and feasibility of contextualized NLP by modeling social factors.
Authors: Xuhui Zhou, Haojie Zhu, Akhila Yerukola, Thomas Davidson, Jena D. Hwang, Swabha Swayamdipta, Maarten Sap
Venue: Annual Meeting of the Association for Computational Linguistics
Tldr: {'model': 'tldr@v2.0.0', 'text': 'COBRA frames are introduced, the first context-aware formalism for explaining the intents, reactions, and harms of offensive or biased statements grounded in their social and situational context, and the importance and feasibility of contextualized NLP by modeling social factors are highlighted.'}
Url: http://arxiv.org/pdf/2306.01985
Title: COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive Statements
Authors: Xuhui Zhou, Hao Zhu, Akhila Yerukola, Thomas Davidson, Jena D. Hwang, Swabha Swayamdipta, Maarten Sap
Section: 6 Conclusion & Discussion
approach where we would examine different interpretations. We leave that up for future work. Dual Use We aim to combat the negative effects and harms of discriminatory language on already marginalized people (Sap et al., 2019b; Davidson et al., 2019). It is possible however that our frames, dataset, and models could be used to perpetuate harm against those very people. We do not endorse the use of our data for those purposes. Risk of Suppressing Speech Our frames, dataset, and models are built with content moderation in mind, as online spaces are increasingly riddled with hate and abuse and content moderators are struggling to sift through all of the content. We hope future work will examine frameworks for using our frames to help content moderators. We do not endorse the use of our system to suppress speech without human oversight and encourage practitioners to take non-censorship-oriented approaches to content moderation (e.g., counterspeech (Tekiroğlu et al., 2022)). Harms of Exposing Workers to Toxic Content The verification process of COBRACORPUS and COBRACORPUS-CF is performed by human annotators. Exposure to such offensive content can be harmful to the annotators (Liu et al., 2016). We mitigated these by designing minimum annotation workload, paying workers above minimum wage ($7-12), and providing them with crisis management resources. Our annotation work is also supervised by an Institutional Review Board (IRB).
Answer: "
"Have Professors Bhiksha Raj and Rita Singh co-authored a paper?
","['rita singh_37e8e07d3ecfa43a1e64d48202c73f597e6f9fee_metadata.txt', 'bhiksha raj_37e8e07d3ecfa43a1e64d48202c73f597e6f9fee_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Have Professors Bhiksha Raj and Rita Singh co-authored a paper?

Context: Faculty Name: rita singh
Paperid: 37e8e07d3ecfa43a1e64d48202c73f597e6f9fee
Title: Towards Noise-Tolerant Speech-Referring Video Object Segmentation: Bridging Speech and Text
Year: 2023
Abstract: None
Authors: Xiang Li, Jinglu Wang, Xiaohao Xu, Muqiao Yang, Fan Yang, Yizhou Zhao, Rita Singh, Bhiksha Raj
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: None
Url: https://aclanthology.org/2023.emnlp-main.140.pdf
Faculty Name: bhiksha raj
Paperid: 37e8e07d3ecfa43a1e64d48202c73f597e6f9fee
Title: Towards Noise-Tolerant Speech-Referring Video Object Segmentation: Bridging Speech and Text
Year: 2023
Abstract: None
Authors: Xiang Li, Jinglu Wang, Xiaohao Xu, Muqiao Yang, Fan Yang, Yizhou Zhao, Rita Singh, Bhiksha Raj
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: None
Url: https://aclanthology.org/2023.emnlp-main.140.pdf
Answer: "
"Who is the Director of the MSAII program?
","['handbook-msaii-2022-2023.txt', 'handbook-msaii-2022-2023.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who is the Director of the MSAII program?

Context: 6 Academic Calendar 
The Academic Calendar can be found at https://www.cmu.edu/hub/calendar/index.html and 
provides information on all deadlines including registration dates, class start dates, add/drop 
deadlines, exam dates and more. 
7 School and Departmental Information 
The following are key personnel with whom you may interact during your time at Carnegie 
Mellon: 
 
Martial Hebert 
 
 
 
 
 
Dean, School of Computer Science 
9 
 
 
University Professor 
 
GHC 6105 
 
412-268-5704 
 
hebert@cs.cmu.edu  
https://www.cs.cmu.edu/~hebert/  
 
David Garlan 
Associate Dean for Master’s Programs, SCS 
Professor 
TCS 420 
garlan@cs.cmu.edu 
http://www.cs.cmu.edu/~garlan/academics.htm  
 
Carolyn Rosé  
 
 
 
 
Interim Director, Language Technologies Institute 
 
Professor 
 
GHC 5415 
 
412-268-7130 
 
cprose@cs.cmu.edu 
8 MSAII Policies 
The Language Technologies Institute (LTI) has prepared this statement of policies, program 
requirements, guidance, process and procedures for students in the M.S. in Artificial Intelligence 
and Innovation (MSAII) program. A copy of this handbook is also available online at the 
program website.   
The University Student guide, The Word, which details university-wide policies, is also 
available online at www.cmu.edu/student-affairs/theword.     
Additional resources specific to graduate students can be found at 
www.cmu.edu/graduate/policies/    
8.1 The MSAII Degree 
The M.S. in Artificial Intelligence and Innovation (MSAII) is a professional master's program 
offered by the Language Technologies Institute (LTI) in the School of Computer Science at 
Carnegie Mellon University. This program seeks to educate students in the innovative use of 
artificial intelligence to create practical solutions in a wide variety of application areas. As 
artificial intelligence matures, there is a great need for students who can envision, design, plan 
and deliver solutions that integrate AI technologies such as deep learning, natural language 
processing, robotics and big data analytics into new applications.  The program stresses both 
10 
 
intrapreneurship and entrepreneurship and encourages students to develop ideas for startups 
and for introduction into established companies.
See also the ‘Duration of Study’ policy. 
11.7 Residency Requirement 
The MSAII is a full-time, in-residence program conducted only on the Pittsburgh campus.  In 
exceptional circumstances, such as visa complications or medical exigencies, permission may be 
granted by the Director allowing a student to participate in short portions of the program 
remotely.  This is not possible for first-year students. 
12 Financial Issues 
12.1 Graduate Student Funding  
34 
 
The LTI does not provide financial aid or support to students in the professional MS programs.  
Students are encouraged to seek financial aid and support from other sources.  The HUB website 
(https://www.cmu.edu/sfs/financial-aid/graduate/index.html) provides the Graduate Financial 
Aid Guide, information about funding options and how to apply for financial aid and other 
helpful links.  Additional information on financial issues for graduate students can be found on 
the web at www.cmu.edu/hub/new-grad/. 
Students in the professional MS programs are not prohibited from seeking support as Teaching 
Assistants and Research Assistants.  However, typically full-time MS students do not have time 
for these activities.  Typically Research Assistantships are most likely to be awarded to students 
in Carnegie Mellon’s research-oriented degree programs. 
12.2 University Financial Aid 
Graduate students should consult the graduate student financial aid information found on The 
HUB website: www.cmu.edu/sfs/financial-aid/graduate/index.html.  Students will find the 
Graduate Financial Aid Guide, information about funding options and how to apply for financial 
aid and other helpful links. 
12.3 Health Insurance  
Carnegie Mellon has a Student Health Insurance policy requiring full-time, degree-seeking 
students to carry adequate medical insurance. Students must either purchase the plan offered by 
the University or an application for a waiver can be made if the student is “enrolled as the 
dependent, partner/spouse or principal in an employer or government-sponsored insurance 
plan” (see the Carnegie Mellon University Student Health Insurance Policy at 
https://www.cmu.edu/policies/student-and-student-life/student-health-insurance.html).  
It is the responsibility of each student to make arrangements with Student Health Services to 
either pay for their insurance at the beginning of the semester, or elect a payment plan over the 
course of the academic year.
Answer: "
"What is the full name of the conference where the paper Why do Nearest Neighbor Language Models Work?, got published?
","['graham neubig_c432aff446d55e72a28394a1508e760cc9a25c08_metadata.txt', 'graham neubig_papers.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the full name of the conference where the paper Why do Nearest Neighbor Language Models Work?, got published?

Context: Faculty Name: graham neubig
Paperid: c432aff446d55e72a28394a1508e760cc9a25c08
Title: Why do Nearest Neighbor Language Models Work?
Year: 2023
Abstract: Language models (LMs) compute the probability of a text by sequentially computing a representation of an already-seen context and using this representation to predict the next word. Currently, most LMs calculate these representations through a neural network consuming the immediate previous context. However recently, retrieval-augmented LMs have shown to improve over standard neural LMs, by accessing information retrieved from a large datastore, in addition to their standard, parametric, next-word prediction. In this paper, we set out to understand why retrieval-augmented language models, and specifically why k-nearest neighbor language models (kNN-LMs) perform better than standard parametric LMs, even when the k-nearest neighbor component retrieves examples from the same training set that the LM was originally trained on. To this end, we perform a careful analysis of the various dimensions over which kNN-LM diverges from standard LMs, and investigate these dimensions one by one. Empirically, we identify three main reasons why kNN-LM performs better than standard LMs: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature for the kNN distribution. Further, we incorporate these insights into the model architecture or the training procedure of the standard parametric LM, improving its results without the need for an explicit retrieval component. The code is available at https://github.com/frankxu2004/knnlm-why.
Authors: Frank F. Xu, Uri Alon, Graham Neubig
Venue: International Conference on Machine Learning
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper identifies three main reasons why k-nearest neighbor language models (kNN-LM) perform better than standard LMs: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature for the kNN distribution.'}
Url: http://arxiv.org/pdf/2301.02828
List of 2023 Open Access papers by graham neubig are:
Cross-Modal Fine-Tuning: Align then Refine
ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages
GlobalBench: A Benchmark for Global Progress in Natural Language Processing
Learning Performance-Improving Code Edits
CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code
Neural Machine Translation for the Indigenous Languages of the Americas: An Introduction
User-Centric Evaluation of OCR Systems for Kwak’wala
Multi-Dimensional Evaluation of Text Summarization with In-Context Learning
A Gold Standard Dataset for the Reviewer Assignment Problem
SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing
Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation
FacTool: Factuality Detection in Generative AI - A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios
Active Retrieval Augmented Generation
Large Language Models Enable Few-Shot Clustering
Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach
Crossing the Threshold: Idiomatic Machine Translation through Retrieval Augmentation and Loss Weighting
Why do Nearest Neighbor Language Models Work?
Syntax and Semantics Meet in the “Middle”: Probing the Syntax-Semantics Interface of LMs Through Agentivity
DataFinder: Scientific Dataset Recommendation from Natural Language Descriptions
Multi-lingual and Multi-cultural Figurative Language Understanding
It’s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk
Unlimiformer: Long-Range Transformers with Unlimited Length Input
WebArena: A Realistic Web Environment for Building Autonomous Agents
Prompt2Model: Generating Deployable Models from Natural Language Instructions
Computational Language Acquisition with Theory of Mind
Improving Factuality of Abstractive Summarization via Contrastive Reward Learning
The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation
Answer: "
"What is the mean confidence difference for the ""he, she"" gender-word pair in the paper ""Language Models Get a Gender Makeover""?
","['louis philippe morency_f891e9eeedbf20cdc54429ffcc0402a10f48494e_content_1.txt', 'louis philippe morency_f891e9eeedbf20cdc54429ffcc0402a10f48494e_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the mean confidence difference for the ""he, she"" gender-word pair in the paper ""Language Models Get a Gender Makeover""?

Context: Title: Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions
Authors: Himanshu Thakur, Atishay Jain, Praneetha Vaddamanu, Paul Pu Liang, Louis-Philippe Morency
Section: A Appendix
StereoSet Gender SS score out of all our intervention methods. Similarily, naive-masking obtains the lowest Crow-S pair score.
Title: Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions
Authors: Himanshu Thakur, Atishay Jain, Praneetha Vaddamanu, Paul Pu Liang, Louis-Philippe Morency
Section: A Appendix
A.1 Dataset Bias Analysis To gauge the feasibility of using a wordlist based intervention approach, we first analyze our datasets for occurrences of gender words. As shown in the word cloud 4, gender pronouns are the mostfrequent word in our datasets. Moreover, as per Figure 1, ""she,"" ""he,"" and ""her"" are the top three most frequently occurring words in our dataset. This suggests that we can definitely detect gender words in our corpus and apply our interventions. A.2 Sensitivity to Choice of Dataset To understand the effectiveness of our proposed data-interventions, we study apply our methods to two datasets under varying number of training samples (10, 50 and 100) and selection strategies (most biased first and random) as per Table 6. Our methods obtain better results on StereoSet (dev) dataset. One reason this could happen is due to the fact that StereoSet has explicit gender bias, thus it would be less likely for a sentence like ""She needs a gynaecologist"" to appear on it. Because our interventions perform blunt substitutions, this sentence might become incorrect due to our method - ""Either he or she needs a gynaecologist"". A.3 Sensitivity to Number of Training Samples and Sampling Strategy As per Figure 5, When we vary the number of training samples, we observe that the difference in performance is not huge when we transition from 10 to 100 samples, thus suggesting that our method is capable of few-shot fine-tuning. Moreover, sampling the most biased data points helps our methods achieve better performance consistently, as shown in Figure 5 and Table 6. Table ?? shows some top three most gender biased entries found in the StereoSet dataset. A.4 Ablations of interventions We study the effects of choosing different ways of replacement for name and non-name words. In addition to our three interventions proposed previously, we also experimented with a couple of others. In female-first-random-phrase-masking, we always keep the female gendered word before a male word.
Answer: "
"In spring 2024, When do Mini-3 faculty course evaluations open?
","['Meta-Data-Calendar-2425.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, When do Mini-3 faculty course evaluations open?

Context: Day: 'Monday', Event: 'Mini-3 Faculty Course Evaluations open', Semester: 'Spring 2025 (S25)'
Date: '2025-02-24', Day: 'Monday', Event: 'Semester course drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Spring 2025 (S25)'
Date: '2025-02-28', Day: 'Friday', Event: 'Mini-3 Last Day of Classes ', Semester: 'Spring 2025 (S25)'
Date: '2025-02-28', Day: 'Friday', Event: 'Mini-3 voucher deadline (4)', Semester: 'Spring 2025 (S25)'
Date: '2025-03-01', Day: 'Saturday', Event: 'Mini-3 Exams', Semester: 'Spring 2025 (S25)'
Date: '2025-03-01', Day: 'Saturday', Event: 'Mini-3 Faculty Course Evaluations close', Semester: 'Spring 2025 (S25)'
Start Date: '2025-03-03', End Date: '2023-03-07', Days: 'Monday to Friday', Event: 'Spring Break; No Classes', Semester: 'Spring 2025 (S25)'
Date: '2025-03-10', Day: 'Monday', Event: 'First day of Mini-4 Classes', Semester: 'Spring 2025 (S25)'
Date: '2025-03-11', Day: 'Tuesday', Event: 'Summer 2025 Registration Opens', Semester: 'Spring 2025 (S25)'
Date: '2025-03-12', Day: 'Wednesday', Event: 'Mid-Semester & Mini-3 grades due by 4 pm', Semester: 'Spring 2025 (S25)'
Date: '2025-03-14', Day: 'Friday', Event: 'Mini-4 add, audit & tuition adjustment drop deadline (1)', Semester: 'Spring 2025 (S25)'
Date: '2025-03-31', Day: 'Monday', Event: 'Semester pass/no pass & withdrawal deadline', Semester: 'Spring 2025 (S25)'
Date: '2025-04-02', Day: 'Wednesday', Event: 'Mini-4 course drop deadline; withdrawal grade assigned after this date (2)',
On Wednesday, 05 February, 2025, during the Spring 2025 (S25) semester, Mini-3 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 17 February, 2025, during the Spring 2025 (S25) semester, Mini-3 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 17 February, 2025, during the Spring 2025 (S25) semester, Mini-3 Faculty Course Evaluations open is observed.
On Monday, 24 February, 2025, during the Spring 2025 (S25) semester, Semester course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 28 February, 2025, during the Spring 2025 (S25) semester, Mini-3 Last Day of Classes is observed.
On Friday, 28 February, 2025, during the Spring 2025 (S25) semester, Mini-3 voucher deadline (4) is observed.
On Saturday, 01 March, 2025, during the Spring 2025 (S25) semester, Mini-3 Exams is observed.
On Saturday, 01 March, 2025, during the Spring 2025 (S25) semester, Mini-3 Faculty Course Evaluations close is observed.
From 03 March, 2025,Monday to 07 March, 2023,Friday marks the Spring Break for Spring 2025 (S25) semester which leads to No Classes.
On Monday, 10 March, 2025, during the Spring 2025 (S25) semester, First day of Mini-4 Classes is observed.
On Tuesday, 11 March, 2025, during the Spring 2025 (S25) semester, Summer 2025 Registration Opens is observed.
On Wednesday, 12 March, 2025, during the Spring 2025 (S25) semester, Mid-Semester & Mini-3 grades due by 4 pm is observed.
On Friday, 14 March, 2025, during the Spring 2025 (S25) semester, Mini-4 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 31 March, 2025, during the Spring 2025 (S25) semester, Semester pass/no pass & withdrawal deadline is observed.
Answer: "
"Which ranker outperformed BM25 consistently in the InPars study?
","['eric nyberg_3a30217c4115777fb30c182c97cc77d34d065556_metadata.txt', 'eric nyberg_3a30217c4115777fb30c182c97cc77d34d065556_content_2.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which ranker outperformed BM25 consistently in the InPars study?

Context: Faculty Name: eric nyberg
Paperid: 3a30217c4115777fb30c182c97cc77d34d065556
Title: InPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers
Year: 2023
Abstract: We carried out a reproducibility study of InPars, which is a method for unsupervised training of neural rankers (Bonifacio et al., 2022). As a by-product, we developed InPars-light, which is a simple-yet-effective modification of InPars. Unlike InPars, InPars-light uses 7x-100x smaller ranking models and only a freely available language model BLOOM, which -- as we found out -- produced more accurate rankers compared to a proprietary GPT-3 model. On all five English retrieval collections (used in the original InPars study) we obtained substantial (7%-30%) and statistically significant improvements over BM25 (in nDCG and MRR) using only a 30M parameter six-layer MiniLM-30M ranker and a single three-shot prompt. In contrast, in the InPars study only a 100x larger monoT5-3B model consistently outperformed BM25, whereas their smaller monoT5-220M model (which is still 7x larger than our MiniLM ranker) outperformed BM25 only on MS MARCO and TREC DL 2020. In the same three-shot prompting scenario, our 435M parameter DeBERTA v3 ranker was at par with the 7x larger monoT5-3B (average gain over BM25 of 1.3 vs 1.32): In fact, on three out of five datasets, DeBERTA slightly outperformed monoT5-3B. Finally, these good results were achieved by re-ranking only 100 candidate documents compared to 1000 used by Bonifacio et al. (2022). We believe that InPars-light is the first truly cost-effective prompt-based unsupervised recipe to train and deploy neural ranking models that outperform BM25. Our code and data is publicly available.
Title: InPars-Light: Cost-Effective Unsupervised Training of Effi- cient Rankers
Authors: Leonid Boytsov, Preksha Patel, Vivek Sourabh, Riddhi Nisar, Sayani Kundu, Ramya Ramanathan, Eric Nyberg
Section: 5 Results
the best-seed outcomes which are presented in § A.3 Table 5. For MiniLM-L6-30M, the all-domain pre-training improves the best-seed accuracy in all cases. For DeBERTA-v3-435M, there is either a substantial degradation or a small decrease/increase that is not statistically significant (denoted by super-script label “c”). Thus, our biggest model—unlike a 15x smaller MiniLM-L6-30M—does not benefit from all-domain pretraining. However, there is no substantial degradation either. Supervised transfer learning with optional unsupervised fine-tuning. We found that our ranking models trained on MS MARCO (both MiniLM-L6-30M and DeBERTA-v3-435M) transferred well to other collections in almost all the cases. However, monoT5 models trained on MS MARCO are still substantially more accurate. According to Table 1, the average gains over BM25 are (1) 1.21 for MiniLM-30M vs. 1.46 for monoT5-200M and (2) 1.42 for DeBERTA-v3-435M vs. 1.59 for monoT5-3B. In that, this gap is not reduced by fine-tuning using synthetically generated data. This is different from the fully unsupervised scenario described above, where MiniLM-L6-30M often outperforms monoT5-220M while DeBERTA-v3-435M is at par with monoT5-3B. This is in line with prior findings that large ranking models have better zero-shot transferring effectiveness (Ni et al., 2021; Rosa et al., 2022). However, using multi-billion parameter models pre-trained on MS MARCO in a commercial setting is problematic from both efficiency and legal standpoints. In particular, MS MARCO has a research-only license.12. Model-type ablation.
Answer: "
"For additional information about the MIIS program, who should you contact?
","['miis-handbook_2023-2024.txt', 'miis-handbook_2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: For additional information about the MIIS program, who should you contact?

Context: Information about The Word, the Student Handbook, 
the Office of Graduate and Postdoc Affairs, the Office of the Dean of Student Affairs and others 
are included in Appendix A of this handbook. 
All policies not explicitly described in this document conform to School of Computer Science 
(SCS) policies and university policies described in The Word, the Student Handbook and at 
the University Policies website.  It is the responsibility of each member of the Carnegie Mellon 
community to be familiar with university policies and guidelines. 
1.4 MIIS Contact Information 
The people responsible for administering the MIIS degree are: 
 
Brianna Eriksen 
 
 
 
Teruko Mitamura 
 
Academic Program Manager  
 
Program Director, MIIS 
 
GHC 6415 
 
 
 
 
Professor, LTI  
 
412-268-4277  
 
 
 
GHC 6711 
 
bfreema2@andrew.cmu.edu   
 
412-268-6596 
 
  
 
 
 
 
 
teruko@andrew.cmu.edu 
 
 
Kate Schaich  
 
 
 
Robert Frederking 
Administrative Manager 
 
 
Chair of Graduate Programs, LTI 
GHC 6415 
 
 
 
 
Principal Systems Scientist 
412-268-4788  
 
 
 
412-268-6656 
kschaich@andrew.cmu.edu  
 
ref@cs.cmu.edu 
 
 
 
Mona Diab 
 
 
 
 
 
Director, LTI 
 
 
 
 
 
GHC 5415 
 
 
 
 
 
412-268-3669 
 
 
 
 
 
mdiab@andrew.cmu.edu 
 
In addition, students may confer with the Graduate Education Office 
(graded@andrew.cmu.edu) regarding issues of process or other concerns as they navigate 
conflicts. 
MIIS Graduate Student Handbook 
Page 8 
 
The Language Technologies Institute is located primarily on the 5th and 6th floors of the Gates 
Hillman Complex (GHC) on Carnegie Mellon’s Pittsburgh campus.
Students 
can log on to SIO by going to www.cmu.edu/hub/sio and entering their Andrew User ID and 
password. 
On SIO, students will designate an emergency contact address of a relative or family friend to 
be contacted in the case of an emergency.   If students do not want their name and address 
published in the campus directory, they must notify the HUB in writing.  
MIIS students are responsible for notifying the University of all address changes in a timely 
manner. Students will be held responsible for any failure to receive official college notices due 
to not having a correct address on file; F-1 students may jeopardize their status if address 
information is not kept current. 
 
Students can change their address using SIO, which is available via the HUB website 
(http://www.cmu.edu/hub/index.html). 
9.3 ID Cards 
Graduate students can obtain their ID card from The HUB once they have been entered into 
SIO for the semester.  These cards identify their holders as members of the campus 
community.  Student cards are deactivated upon the cardholder’s separation from the 
university. 
 
Affiliate ID Cards are available for spouses and partners of graduate students that allow them 
to access Carnegie Mellon’s campus.  These cards are available through The HUB to spouses 
and partners of graduate students who are enrolled for the current academic year in a full-time 
graduate degree program.  The card is valid for one year.   For information about domestic 
partner registration, visit the Office of the Dean of Student Affairs webpage: 
https://www.cmu.edu/student-affairs/dean/domestic-partner/index.html. 
For more information about student and affiliate ID cards (spouse, domestic partners and 
dependent children), please visit: https://www.cmu.edu/idplus/idcards/index.html.  
9.4 Transcripts 
Information about and instructions for ordering transcripts are available at: 
https://www.cmu.edu/hub/registrar/student-records/transcripts/.  Transcript questions may be 
directed to uro-transcripts@andrew.cmu.edu. 
9.5 Student Privacy Rights and FERPA 
https://www.cmu.edu/policies/student-and-student-life/privacy-rights-students.html 
MIIS Graduate Student Handbook 
Page 34 
 
This university policy notifies students of their rights under the federal Family Educational 
Rights and Privacy Act (FERPA).
Answer: "
"According to the framework tax paper, what is observed to be growing as hardware speed increases over time?
","['yonatan bisk_b777aa86b5a1d49ce8eababc5c2ee56d3562801e_metadata.txt', 'emma strubell_b777aa86b5a1d49ce8eababc5c2ee56d3562801e_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: According to the framework tax paper, what is observed to be growing as hardware speed increases over time?

Context: Faculty Name: yonatan bisk
Paperid: b777aa86b5a1d49ce8eababc5c2ee56d3562801e
Title: The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment
Year: 2023
Abstract: Increased focus on the computational efficiency of NLP systems has motivated the design of efficient model architectures and improvements to underlying hardware accelerators. However, the resulting increases in computational throughput and reductions in floating point operations have not directly translated to improvements in wall-clock inference latency. We demonstrate that these discrepancies can be largely attributed to bottlenecks introduced by deep learning frameworks. We denote this phenomenon as the \textit{framework tax}, and observe that the disparity is growing as hardware speed increases over time. In this work, we examine this phenomenon through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency. Code is available at https://github.com/JaredFern/Framework-Tax.
Authors: Jared Fernandez, Jacob Kahn, Clara Na, Yonatan Bisk, Emma Strubell
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work examines the effects of model design decisions, framework paradigms, and hardware platforms on total model latency through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency.'}
Url: http://arxiv.org/pdf/2302.06117
Faculty Name: emma strubell
Paperid: b777aa86b5a1d49ce8eababc5c2ee56d3562801e
Title: The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment
Year: 2023
Abstract: Increased focus on the computational efficiency of NLP systems has motivated the design of efficient model architectures and improvements to underlying hardware accelerators. However, the resulting increases in computational throughput and reductions in floating point operations have not directly translated to improvements in wall-clock inference latency. We demonstrate that these discrepancies can be largely attributed to bottlenecks introduced by deep learning frameworks. We denote this phenomenon as the \textit{framework tax}, and observe that the disparity is growing as hardware speed increases over time. In this work, we examine this phenomenon through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency. Code is available at https://github.com/JaredFern/Framework-Tax.
Authors: Jared Fernandez, Jacob Kahn, Clara Na, Yonatan Bisk, Emma Strubell
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work examines the effects of model design decisions, framework paradigms, and hardware platforms on total model latency through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency.'}
Url: http://arxiv.org/pdf/2302.06117
Answer: "
"In spring 2025, What is the deadline for adding, auditing, and tuition adjustment drop for Mini-3?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2025, What is the deadline for adding, auditing, and tuition adjustment drop for Mini-3?

Context: On Monday, 22 January, 2024, during the Spring 2024 (S24) semester, Mini-3 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 29 January, 2024, during the Spring 2024 (S24) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Wednesday, 07 February, 2024, during the Spring 2024 (S24) semester, Mini-3 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 19 February, 2024, during the Spring 2024 (S24) semester, Mini-3 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 19 February, 2024, during the Spring 2024 (S24) semester, Mini-3 Faculty Course Evaluations open is observed.
On Monday, 26 February, 2024, during the Spring 2024 (S24) semester, Semester course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 01 March, 2024, during the Spring 2024 (S24) semester, Mini-3 Last Day of Classes is observed.
On Friday, 01 March, 2024, during the Spring 2024 (S24) semester, Mini-3 voucher deadline (4) is observed.
On Saturday, 02 March, 2024, during the Spring 2024 (S24) semester, Mini-3 Exams is observed.
On Saturday, 02 March, 2024, during the Spring 2024 (S24) semester, Mini-3 Faculty Course Evaluations close is observed.
From 04 March, 2024,Monday to 08 March, 2024,Friday marks the Spring Break for Spring 2024 (S24) semester which leads to No Classes.
On Monday, 11 March, 2024, during the Spring 2024 (S24) semester, First day of Mini-4 Classes is observed.
On Monday, 11 March, 2024, during the Spring 2024 (S24) semester, Mid-Semester & Mini-3 grades due by 4 pm is observed.
On Tuesday, 12 March, 2024, during the Spring 2024 (S24) semester, Summer 2024 Registration Opens is observed.
On Wednesday, 05 February, 2025, during the Spring 2025 (S25) semester, Mini-3 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 17 February, 2025, during the Spring 2025 (S25) semester, Mini-3 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 17 February, 2025, during the Spring 2025 (S25) semester, Mini-3 Faculty Course Evaluations open is observed.
On Monday, 24 February, 2025, during the Spring 2025 (S25) semester, Semester course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 28 February, 2025, during the Spring 2025 (S25) semester, Mini-3 Last Day of Classes is observed.
On Friday, 28 February, 2025, during the Spring 2025 (S25) semester, Mini-3 voucher deadline (4) is observed.
On Saturday, 01 March, 2025, during the Spring 2025 (S25) semester, Mini-3 Exams is observed.
On Saturday, 01 March, 2025, during the Spring 2025 (S25) semester, Mini-3 Faculty Course Evaluations close is observed.
From 03 March, 2025,Monday to 07 March, 2023,Friday marks the Spring Break for Spring 2025 (S25) semester which leads to No Classes.
On Monday, 10 March, 2025, during the Spring 2025 (S25) semester, First day of Mini-4 Classes is observed.
On Tuesday, 11 March, 2025, during the Spring 2025 (S25) semester, Summer 2025 Registration Opens is observed.
On Wednesday, 12 March, 2025, during the Spring 2025 (S25) semester, Mid-Semester & Mini-3 grades due by 4 pm is observed.
On Friday, 14 March, 2025, during the Spring 2025 (S25) semester, Mini-4 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 31 March, 2025, during the Spring 2025 (S25) semester, Semester pass/no pass & withdrawal deadline is observed.
Answer: "
"In ""Aligning Large Multimodal Models with Factually Augmented RLHF,"" what is the name of the method that they propose for alignment?
","['bhiksha raj_f5a7a4fda49c657742072a2758f43b1cbcde3886_metadata.txt', 'yang yiming_papers.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In ""Aligning Large Multimodal Models with Factually Augmented RLHF,"" what is the name of the method that they propose for alignment?

Context: Faculty Name: bhiksha raj
Paperid: f5a7a4fda49c657742072a2758f43b1cbcde3886
Title: Continual Contrastive Spoken Language Understanding
Year: 2023
Abstract: Recently, neural networks have shown impressive progress across diverse fields, with speech processing being no exception. However, recent breakthroughs in this area require extensive offline training using large datasets and tremendous computing resources. Unfortunately, these models struggle to retain their previously acquired knowledge when learning new tasks continually, and retraining from scratch is almost always impractical. In this paper, we investigate the problem of learning sequence-to-sequence models for spoken language understanding in a class-incremental learning (CIL) setting and we propose COCONUT, a CIL method that relies on the combination of experience replay and contrastive learning. Through a modified version of the standard supervised contrastive loss applied only to the rehearsal samples, COCONUT preserves the learned representations by pulling closer samples from the same class and pushing away the others. Moreover, we leverage a multimodal contrastive loss that helps the model learn more discriminative representations of the new data by aligning audio and text features. We also investigate different contrastive designs to combine the strengths of the contrastive loss with teacher-student architectures used for distillation. Experiments on two established SLU datasets reveal the effectiveness of our proposed approach and significant improvements over the baselines. We also show that COCONUT can be combined with methods that operate on the decoder side of the model, resulting in further metrics improvements.
Authors: Umberto Cappellazzo, Enrico Fini, Muqiao Yang, Daniele Falavigna, A. Brutti, Bhiksha Raj
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper investigates the problem of learning sequence-to-sequence models for spoken language understanding in a class-incremental learning (CIL) setting and proposes COCONUT, a CIL method that relies on the combination of experience replay and contrastive learning.'}
Url: https://arxiv.org/pdf/2310.02699
List of 2023 Open Access papers by yang yiming are:
Functional targeted therapy for glioma based on platelet membrane-coated nanogels
Dual Responsive Magnetic Drug Delivery Nanomicelles with Tumor Targeting for Enhanced Cancer Chemo/Magnetothermal Synergistic Therapy
Expression of ALCAM in Clinical Colon Cancer and Relationship With Patients’ Treatment Responses
Claudin-10 in the Blood Brain Barrier Function of Cerebral Endothelial Cells and Transendothelial Invasion of Breast Cancer Cells
Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation
Automatic synchrotron tomographic alignment schemes based on genetic algorithms and human-in-the-loop software
High CD8+tumor-infiltrating lymphocytes indicate severe exhaustion and poor prognosis in angioimmunoblastic T-cell lymphoma
Numerical Investigation of Fatigue Crack Propagation Behaviour of 550E High-Performance Steel
Association between albumin-to-globulin ratio and the risk of overall survival in advanced non-small cell lung cancer patients with anlotinib treatment: a retrospective cohort study
Analysis of Volatile Components in Dried Fruits and Branch Exudates of Schisandra chinensis with Different Fruit Colors Using GC-IMS Technology
Secreted endogenous macrosomes reduce Aβ burden and ameliorate Alzheimer’s disease
DIFUSCO: Graph-based Diffusion Solvers for Combinatorial Optimization
Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs
Aligning Large Multimodal Models with Factually Augmented RLHF
An Experimental Study on Secondary Transfer Performances of Prestress after Anchoring Failure of Steel Wire Strands
MRI Features for Predicting Microvascular Invasion and Postoperative Recurrence in Hepatocellular Carcinoma Without Peritumoral Hypointensity
Impact of local governments’ construction land allocation strategies on innovation-driven development of China
16p11.2 CNV gene Doc2α functions in neurodevelopment and social behaviors through interaction with Secretagogin.
Chinese EFL learners different from English natives in cataphora resolution: Evidence from eye-tracking studies
Strain-driven Kovacs-like memory effect in glasses
The Relationship between the Serum NLRP1 Level and Coronary Lesions in Patients with Coronary Artery Disease
Matrine induces ferroptosis in cervical cancer through activation of piezo1 channel.
Answer: "
"What is the role of a chute flagger in the sweepstakes competition?
","['cmubuggy_d403.txt', 'cmubuggy_d403.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the role of a chute flagger in the sweepstakes competition?

Context: An mural celebrating Buggy can be found in CMU's Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions. Sweepstakes Slang Buggy: Vehicle being raced and also a nickname for the competition. Chute: A section of the freeroll portion of the buggy course (near the southwestern end of Frew Street at its intersection with Schenley Drive) where buggies make the sharp righthand turn from Schenley Drive onto Frew Street. Chute Flagger: Team member who provides a signal for buggy drivers to know when to start the right-hand turn from Schenley Drive onto Frew Street. Driver: Person who travels with a buggy and controls the vehicles via steering and braking systems. Pushbar: Structure attached to a buggy that a person pushes to propel that buggy forward. Pusher: Person who propels a buggy via a pushbar along one of the five hills of the buggy course. Shell: Entire outer structure or covering of a buggy that determines that buggy's aerodynamic characteristics. Transition: Procedure whereby one pusher finishes pushing a buggy and the next pusher in sequence starts to push that same buggy. Members of Fringe, celebrating their 50th year of Buggy racing, allowed a rare visit into its shop for a behind-the-scenes tour. This year Fringe is planning to roll four different vehicles, built and maintained in the Fringe workshop in the basement of the East Campus Garage, known as the ""Froom."" ""We're used to saying everything with 'fr' in front of it but when we say something in front of other people, it gets them confused,"" said Fringe head mechanic Dave Singh, who will graduate in May with a bachelor's degree in mechanical engineering and biomedical engineering. Fringe vehicles often are named with the letter ""B,"" like Boson, Blueshift, Bissa and Bumper. Other teams, such as Apex, often use names that connotate fire, while the SDC (Student Dormitory Committee) team, uses names such as Vice, Bane, Avarice and Malice. This year's buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It's one of the few times spectators can see buggies up close.
Buggy Races Keep Rolling at Carnegie Mellon - News - Carnegie Mellon University Carnegie Mellon University ——— Search Search Search this site only News News › Stories › Archives › 2019 › April › Buggy Races Keep Rolling at Carnegie Mellon April 10, 2019 Buggy Races Keep Rolling at Carnegie Mellon In its 99th year, the tradition is a Spring Carnival treat By Heidi Opdyke opdyke(through)andrew.cmu.edu Media Inquiries Julie Mattera Marketing and Communications jmattera(through)cmu.edu 412-268-2902 Sweepstakes, also known as the Buggy Races, has come a long way at Carnegie Mellon University. The slick, torpedo-like vessels carrying drivers with nerves of steel are a far cry from the two-man teams that once changed places halfway through a race and rode in everything from rain barrels with bicycle wheels to three-wheeled ash cans 99 years ago. Today, it takes six people to maneuver the .84 -mile course around Schenley Park's Flagstaff Hill. But while five pushers and a driver navigate the course's hills, dozens of people are needed to make a successful race happen. A year of planning goes into just over two minutes of racing. The Machine The basics of a buggy are straightforward, but teams are often secretive in how they build the machines, in particular the way they brake, steer and what types of wheels are used. Each has a body, pushbar for runners to move the machine up the hills, wheels, a safety harness and driving and braking mechanisms. Some also include fairings, a type of housing around the wheels that help reduce drag, make the vehicle quieter and just looks cool. Fairings have been a key feature for the Fringe team in recent years, which is celebrating its 50th anniversary of Buggy. ""We have a reputation of being the quietest on the course,"" said Diya Nuxoll, who wrapped up her bachelor's degree in mechanical engineering in December and is working on an advanced studies master's degree in design and manufacturing. Nuxoll leads the Fringe team as one of two head mechanics. An mural celebrating Buggy can be found in CMU's Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions. Sweepstakes Slang Buggy: Vehicle being raced and also a nickname for the competition.
Answer: "
"What is the improvement achieved by the PET framework on the AlfWorld instruction following benchmark?
","['yonatan bisk_5ce2f1dff23a5620f77f9b11f1e534422ab8ff3f_metadata.txt', 'yonatan bisk_5ce2f1dff23a5620f77f9b11f1e534422ab8ff3f_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the improvement achieved by the PET framework on the AlfWorld instruction following benchmark?

Context: Faculty Name: yonatan bisk
Paperid: 5ce2f1dff23a5620f77f9b11f1e534422ab8ff3f
Title: Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents
Year: 2023
Abstract: Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accomplished each sub-task. On the AlfWorld instruction following benchmark, the PET framework leads to a significant 15% improvement over SOTA for generalization to human goal specifications.
Authors: Yue Wu, So Yeon Min, Yonatan Bisk, R. Salakhutdinov, A. Azaria, Yuan-Fang Li, Tom M. Mitchell, Shrimai Prabhumoye
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A framework to use the knowledge in LLMs to simplify the control problem, rather than solving it is proposed, which leads to a significant 15% improvement over SOTA for generalization to human goal specifications.'}
Url: http://arxiv.org/pdf/2305.02412
Title: Plan, Eliminate, and Track — Language Models are Good Teachers for Embodied Agents
Authors: Yue Wu, So Yeon Min, Yonatan Bisk, Ruslan Salakhutdinov, Amos Azaria, Yuanzhi Li, Tom M. Mitchell, Shrimai Prabhumoye
Section: 5. Conclusion, Limitations, and Future Work
In this work, we propose the Plan, Eliminate, and Track (PET) framework that uses pre-trained LLMs to assist an embodied agent in three steps. Our PET framework requires no fine-tuning and is designed to be compatible with any goal-conditional embodied agents. In our experiments, we combine PET with a novel Action Attention agent that handles the dynamic action space in AlfWorld. Our Action Attention agent greatly outperforms the BUTLER baseline. In addition, since the PET framework is not trained to fit the training set tasks, it demonstrates better generalization to unseen human goal specification tasks. Finally, our ablation studies show the Plan and Track modules together improve the performance of Eliminate module to achieve the best performance. Our results show that LLMs can be a good source of common sense and procedural knowledge for embodied agents, and multiple LLMs may be used in coordination with each other to further improve effectiveness. One of the major limitations of our current system design is that the Track module (progress tracker) does not re-visit finished sub-tasks. If for example, the agent is executing sub-tasks [picked up a pan, put the pan on countertop], and it picked up a pan but put it in the fridge (undo pickup action). Since the progress tracker does not take into consideration previous progress being undone, the system may break in this situation. Future work can focus on adding sub-task-level dynamic replanning to address this limitation or explore other ways in which LLMs can assist the learning of the policy (i.e., reading an instruction manual about the environment).
Answer: "
"In the Plan, Eliminate and Track paper, which benchmark was used in the experiments?
","['yonatan bisk_5ce2f1dff23a5620f77f9b11f1e534422ab8ff3f_metadata.txt', 'shinji watanabe_b524ec331cd9708b125fad70d95d36189fa0d7b6_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the Plan, Eliminate and Track paper, which benchmark was used in the experiments?

Context: Faculty Name: yonatan bisk
Paperid: 5ce2f1dff23a5620f77f9b11f1e534422ab8ff3f
Title: Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents
Year: 2023
Abstract: Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accomplished each sub-task. On the AlfWorld instruction following benchmark, the PET framework leads to a significant 15% improvement over SOTA for generalization to human goal specifications.
Authors: Yue Wu, So Yeon Min, Yonatan Bisk, R. Salakhutdinov, A. Azaria, Yuan-Fang Li, Tom M. Mitchell, Shrimai Prabhumoye
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A framework to use the knowledge in LLMs to simplify the control problem, rather than solving it is proposed, which leads to a significant 15% improvement over SOTA for generalization to human goal specifications.'}
Url: http://arxiv.org/pdf/2305.02412
Faculty Name: shinji watanabe
Paperid: b524ec331cd9708b125fad70d95d36189fa0d7b6
Title: A Study on the Integration of Pipeline and E2E SLU Systems for Spoken Semantic Parsing Toward Stop Quality Challenge
Year: 2023
Abstract: Recently there have been efforts to introduce new benchmark tasks for spoken language understanding (SLU), like semantic parsing. In this paper, we describe our proposed spoken semantic parsing system for the quality track (Track 1) in Spoken Language Understanding Grand Challenge which is part of ICASSP Signal Processing Grand Challenge 2023. We experiment with both end-to-end and pipeline systems for this task. Strong automatic speech recognition (ASR) models like Whisper and pretrained Language models (LM) like BART are utilized inside our SLU framework to boost performance. We also investigate the output level combination of various models to get an exact match accuracy of 80.8, which won the 1st place at the challenge.
Authors: Siddhant Arora, Hayato Futami, Shih-Lun Wu, Jessica Huynh, Yifan Peng, Yosuke Kashiwagi, E. Tsunoo, Brian Yan, Shinji Watanabe
Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper describes the proposed spoken semantic parsing system for the quality track (Track 1) in Spoken Language Understanding Grand Challenge which is part of ICASSP Signal Processing Grand Challenge 2023.'}
Url: https://arxiv.org/pdf/2305.01620
Answer: "
"Who should LTI PhD students contact if they have a question about their offices?
","['handbook_phd_2023-2024.txt', 'handbook_phd_2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who should LTI PhD students contact if they have a question about their offices?

Context: students in residence on the Pittsburgh campus are given an office in which to study 
and do research. Typically, offices are shared with other Ph.D. students, but they may also be 
shared with staff, visitors, or other members of the LTI. 
Offices are assigned by the LTIs Office Manager (see Section 1.2, Department Personnel, for 
contact information). 
2.3 
Mailboxes and Office Supplies 
Mailboxes and office supplies are in GHC 5404. 
2.4 
Photocopies and Printers 
Printers and photocopies are available to LTI students. The use of a photocopier or printer 
requires you to log in with your CMU ID card. LTI students may use printers/photocopiers 
scattered throughout the School of Computer Science buildings, but the machines in GHC 5404 
and GHC 6604 are the most convenient. The SCS Computing Facilities publishes a list of printers 
online at https://computing.cs.cmu.edu/desktop/printer-list. 
2.5 
Computers for LTI Ph.D. Students 
Ph.D. students are responsible for having their own laptop computers to support their education 
and research. Students are free to choose their own operating system (e.g., Linux, MacOs, 
Windows). 
Many Ph.D. advisors also provide access to computer clusters, cloud computing, or other 
resources to support computationally-intense research. 
Ph.D. students are given access to the LTIs computer cluster on an as-needed basis, to be used for 
course assignments, directed study projects, and/or capstone projects.  The LTI cluster provides 
storage and computation for projects involving large datasets and/or lengthy computation. 
Ph.D. students receive two types of user ids: An Andrew id and a CS id. All CMU students have 
an Andrew id. Computer Science students also have a CS id that provides access to SCS-specific 
resources (e.g., computer clusters). CS ids are being phased out very slowly, so it is likely that you 
will need both types of user id. 
LTI Ph.D. Graduate Student Handbook 
Page 14 
 
The School of Computer Science has a Help Center in GHC 4201.
4.1.2 External Internships  
The LTI provides summer support for its Ph.D. students, so Ph.D. students are expected to do 
research at Carnegie Mellon during the summer. However, outside experience can be a valuable 
educational experience, Ph.D. students in good standing are allowed to do an external internship. 
Some students may benefit from more than one internship, especially if they require access to 
proprietary data for their work, thus, with the approval of the students advisor, a student can intern up 
to four (4) times during their Ph.D. degree. Interning more than four (4) times requires approval 
from the Ph.D. Program Director. 
Any internship must be planned in consultation with the students advisor and the LTI Ph.D. 
Coordinator. If an internship is part of a fully-funded external fellowship (e.g., Microsoft Research 
Fellowship), the student does not need prior LTI approval, but should still coordinate with their 
advisor. International students must consult with Office of International Education (OIE) for 
eligibility before seeking an internship or signing an offer contract.  
Internships are typically scheduled during the summer. In certain cases, it is possible to schedule 
an internship during the fall or spring semester; the details are complex, especially for 
international students, so the student should discuss this as early as possible with the Ph.D. 
Program Director. International students must coordinate carefully with the LTI in any event, due 
to visa restrictions.  
LTI Ph.D. Graduate Student Handbook 
Page 25 
 
Note that self-funded Ph.D. students (e.g., those registered for five units while on semester 
internship) are not permitted to receive a partial stipend for the semester of their internship, while 
full-time LTI-sponsored students are eligible for a partial stipend.  
All students are cautioned to be aware of potential intellectual property (IP) problems with 
internships, and to review any IP agreements with their advisors before signing them. It is possible 
to lose ownership of your ideas.  
If the student is to receive academic credit for the internship, it must have deliverables from the 
student commensurate with the number of units, they are taking.
Answer: "
"Which populations were found to be predominantly aligned with by the datasets and models in the NLPositionality study?
","['maarten sap_a66ff335f5934fe7503a99d3eb3abed493994df1_metadata.txt', 'maarten sap_a66ff335f5934fe7503a99d3eb3abed493994df1_content_2.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which populations were found to be predominantly aligned with by the datasets and models in the NLPositionality study?

Context: Faculty Name: maarten sap
Paperid: a66ff335f5934fe7503a99d3eb3abed493994df1
Title: NLPositionality: Characterizing Design Biases of Datasets and Models
Year: 2023
Abstract: Design biases in NLP systems, such as performance differences for different populations, often stem from their creator’s positionality, i.e., views and lived experiences shaped by identity and background. Despite the prevalence and risks of design biases, they are hard to quantify because researcher, system, and dataset positionality is often unobserved. We introduce NLPositionality, a framework for characterizing design biases and quantifying the positionality of NLP datasets and models. Our framework continuously collects annotations from a diverse pool of volunteer participants on LabintheWild, and statistically quantifies alignment with dataset labels and model predictions. We apply NLPositionality to existing datasets and models for two tasks—social acceptability and hate speech detection. To date, we have collected 16,299 annotations in over a year for 600 instances from 1,096 annotators across 87 countries.We find that datasets and models align predominantly with Western, White, college-educated, and younger populations. Additionally, certain groups, such as non-binary people and non-native English speakers, are further marginalized by datasets and models as they rank least in alignment across all tasks. Finally, we draw from prior literature to discuss how researchers can examine their own positionality and that of their datasets and models, opening the door for more inclusive NLP systems.
Authors: Sebastin Santy, Jenny T Liang, Ronan Le Bras, Katharina Reinecke, Maarten Sap
Venue: Annual Meeting of the Association for Computational Linguistics
Tldr: {'model': 'tldr@v2.0.0', 'text': 'NLPositionality, a framework for characterizing design biases and quantifying the positionality of NLP datasets and models, is introduced and it is found that dataset and models align predominantly with Western, White, college-educated, and younger populations.'}
Url: http://arxiv.org/pdf/2306.01943
Title: NLPositionality: Characterizing Design Biases of Datasets and Models
Authors: Sebastin Santy, Jenny T. Liang, Ronan Le Bras, Katharina Reinecke, Maarten Sap, Aditya Sharma
Section: 5 Discussion
Hanna et al., 2020; Bender et al., 2021). This can be done using approaches such as participatory design (Spinuzzi, 2005), including interactive storyboarding (Madsen and Aiken, 1993), as well as value-sensitive design (Friedman, 1996), including panels of experiential experts (Madsen and Aiken, 1993). Building datasets and models with large global teams such as BigBench (Srivastava et al., 2022) and NL-Augmenter (Dhole et al., 2021) could also reduce design biases by having diverse teams (Li, 2020). To account for annotator subjectivity (Aroyo and Welty, 2015), researchers should make concerted efforts to recruit annotators from diverse backgrounds. Websites like LabintheWild can be platforms where these annotators are recruited. Since new design biases could be introduced in this process, we recommend following the practice of documenting the demographics of annotators as in prior works (e.g., Forbes et al., 2020; Vidgen et al., 2021) to record a dataset’s positionality. We urge considering research through the lens of perspectivism (Basile et al., 2021), i.e. being mindful of different perspectives by sharing datasets with disaggregated annotations and finding modeling techniques that can handle inherent disagreements or distributions (Plank, 2022), instead of forcing a single answer in the data (e.g., by majority vote; Davani et al., 2022) or model (e.g., by classification to one label; Costanza-Chock, 2018). Researchers also should carefully consider how they aggregate labels from diverse annotators during modeling so their perspectives are represented, such as not averaging annotations to avoid the “tyranny of the mean” (Talat et al., 2022). Finally, we argue that the notion of “inclusive NLP” does not mean that all language technologies have to work for everyone.
Answer: "
"In fall 2023, Who is the instructor for unit 02701?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, Who is the instructor for unit 02701?

Context: In Semester Fall 2023, from the department of Computational Biology, the subject titled 'M.S. Thesis Research' with Course ID 02700 and Section A offers 1-12 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Garrand located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'CPCB Course - Current Topics in Computational Biology' with Course ID 02701 and Section A offers 3.0 units. The Class meets Monday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ma located in Building WEH, Room 5403.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Biology Seminar' with Course ID 02702 and Section A offers 3.0 units. The Class meets Friday between 10:30AM and 12:00PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Carja located in Building OFF, Room PITT.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Methods for Biological Modeling and Simulation' with Course ID 02712 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Schwartz, Carja located in Building HH, Room B131.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Medicine' with Course ID 02718 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Wu located in Building WEH, Room 5403.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Laboratory Methods for Computational Biologists' with Course ID 02760 and Section A offers 9.0 units. The Class meets Tuesday between 11:00AM and 12:50PM ET.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'M.S. Thesis Research' with Course ID 02700 and Section A offers 1-12 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Garrand located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'CPCB Course - Current Topics in Computational Biology' with Course ID 02701 and Section A offers 3.0 units. The Class meets Monday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ma located in Building WEH, Room 5403.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Biology Seminar' with Course ID 02702 and Section A offers 3.0 units. The Class meets Friday between 10:30AM and 12:00PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Carja located in Building OFF, Room PITT.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Methods for Biological Modeling and Simulation' with Course ID 02712 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Schwartz, Carja located in Building HH, Room B131.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Medicine' with Course ID 02718 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Wu located in Building WEH, Room 5403.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Laboratory Methods for Computational Biologists' with Course ID 02760 and Section A offers 9.0 units. The Class meets Tuesday between 11:00AM and 12:50PM ET.
Answer: "
"In the paper ""Decoder-only Architecture for Speech Recognition with CTC Prompts and Text Data Augmentation, what is the reduction in word error rates achieved by the proposed models on LibriSpeech testother?
","['shinji watanabe_06353e1b7e7c8dc701ac76dcd4db5061b24468c9_metadata.txt', 'shinji watanabe_06353e1b7e7c8dc701ac76dcd4db5061b24468c9_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the paper ""Decoder-only Architecture for Speech Recognition with CTC Prompts and Text Data Augmentation, what is the reduction in word error rates achieved by the proposed models on LibriSpeech testother?

Context: Faculty Name: shinji watanabe
Paperid: 06353e1b7e7c8dc701ac76dcd4db5061b24468c9
Title: Decoder-only Architecture for Speech Recognition with CTC Prompts and Text Data Augmentation
Year: 2023
Abstract: Collecting audio-text pairs is expensive; however, it is much easier to access text-only data. Unless using shallow fusion, end-to-end automatic speech recognition (ASR) models require architecture modifications or additional training schemes to use text-only data. Inspired by recent advances in decoder-only language models (LMs), such as GPT-3 and PaLM adopted for speech-processing tasks, we propose using a decoder-only architecture for ASR with simple text augmentation. To provide audio information, encoder features compressed by CTC prediction are used as prompts for the decoder, which can be regarded as refining CTC prediction using the decoder-only model. Because the decoder architecture is the same as an autoregressive LM, it is simple to enhance the model by leveraging external text data with LM training. An experimental comparison using LibriSpeech and Switchboard shows that our proposed models with text augmentation training reduced word error rates from ordinary CTC by 0.3% and 1.4% on LibriSpeech test-clean and testother set, respectively, and 2.9% and 5.0% on Switchboard and CallHome. The proposed model had advantage on computational efficiency compared with conventional encoder-decoder ASR models with a similar parameter setup, and outperformed them on the LibriSpeech 100h and Switchboard training scenarios.
Authors: E. Tsunoo, Hayato Futami, Yosuke Kashiwagi, Siddhant Arora, Shinji Watanabe
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work proposes using a decoder-only architecture for ASR with simple text augmentation training that had advantage on computational efficiency compared with conventional encoder-decoder ASR models with a similar parameter setup, and outperformed them on the LibriSpeech 100h and Switchboard training scenarios.'}
Url: https://arxiv.org/pdf/2309.08876
Title: DECODER-ONLY ARCHITECTURE FOR SPEECH RECOGNITION WITH CTC PROMPTS AND TEXT DATA AUGMENTATION
Authors: Emiru Tsunoo, Hayato Futami, Yosuke Kashiwagi, Siddhant Arora, Shinji Watanabe
Section: 5. REFERENCES
[1] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al., “Deep speech: Scaling up endto-end speech recognition,” arXiv preprint arXiv:1412.5567, 2014. [2] Jan K. Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio, “Attention-based models for speech recognition,” in Proc. of NIPS, 2015, pp. 577– 585. [3] Zhong Meng, Sarangarajan Parthasarathy, Eric Sun, Yashesh Gaur, Naoyuki Kanda, Liang Lu, Xie Chen, Rui Zhao, Jinyu Li, and Yifan Gong, “Internal language model estimation for domain-adaptive end-to-end speech recognition,” in 2021 IEEE Spoken Language Technology Workshop (SLT), 2021, pp. 243–250. [4] Albert Zeyer, André Merboldt, Wilfried Michel, Ralf Schlüter, and Hermann Ney, “Librispeech transducer model with internal language model prior correction,” in Proc. of Interspeech, 2021, pp. 2052–2056. [5] Emiru Tsunoo, Yosuke Kashiwagi, Chaitanya Prasad Narisetty, and Shinji Watanabe, “Residual language model for end-toend speech recognition,” in Proc. of Interspeech 2022, 2022, pp. 3899–3903. [6] Adithya Renduchintala, Shuoyang Ding, Matthew Wiesner, and Shinji Watanabe, “Multi-modal data augmentation for endto-end asr,” Proc.
Answer: "
"Which LTI faculty is involved in the work Improving Factuality of Abstractive Summarization via Contrastive Reward Learning?
","['graham neubig_f640e89fcede075b4bde3b2fa0dc78f591589ba3_metadata.txt', 'graham neubig_f640e89fcede075b4bde3b2fa0dc78f591589ba3_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which LTI faculty is involved in the work Improving Factuality of Abstractive Summarization via Contrastive Reward Learning?

Context: Faculty Name: graham neubig
Paperid: f640e89fcede075b4bde3b2fa0dc78f591589ba3
Title: Improving Factuality of Abstractive Summarization via Contrastive Reward Learning
Year: 2023
Abstract: Modern abstractive summarization models often generate summaries that contain hallucinated or contradictory information. In this paper, we propose a simple but effective contrastive learning framework that incorporates recent developments in reward learning and factuality metrics. Empirical studies demonstrate that the proposed framework enables summarization models to learn from feedback of factuality metrics using contrastive reward learning, leading to more factual summaries by human evaluations. This suggests that further advances in learning and evaluation algorithms can feed directly into providing more factual summaries. Code and human evaluation results will be publicly available at \url{https://github.com/EthanC111/factuality_summarization}.
Authors: Ethan Chern, Zhiruo Wang, Sanjan Das, Bhavuk Sharma, Pengfei Liu, Graham Neubig
Venue: TRUSTNLP
Tldr: {'model': 'tldr@v2.0.0', 'text': 'Empirical studies demonstrate that the proposed framework enables summarization models to learn from feedback of factuality metrics using contrastive reward learning, leading to more factual summaries by human evaluations, suggesting that further advances in learning and evaluation algorithms can feed directly into providing morefactuality summaries.'}
Url: https://arxiv.org/pdf/2307.04507
Title: Improving Factuality of Abstractive Summarization via Contrastive Reward Learning
Authors: I-Chun Chern, Zhiruo Wang, Sanjan Das, Bhavuk Sharma, Pengfei Liu, Graham Neubig
Section: 3.1 Experimental Setup
guidelines provided to the annotators are listed in Table 1. An expert annotator is involved in the human evaluation studies.
Answer: "
"Is there a YouTube channel for The Kiltie Band?
","['kiltieband_d406.txt', 'kiltieband_d406_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Is there a YouTube channel for The Kiltie Band?

Context: in the CUC Studio Theater. If you have questions, please email the Direcator at
jolisar@andrew.cmu.edu
.
Q: Can I borrow a school instrument?
A: There are several Kiltie-owned instruments available for your use. Loans begin at 4:30 p.m. before the first rehearsal.
Q: What do they wear under those kilts?
A: Join and you’ll find out!
For more information, visit the
Kiltie Band website
.
Interested in Joining?
Please email the following information to Kiltie Band Director
Jeremy Olisar.
Name
High School
Address at Carnegie Mellon (if known)
Home address
Cell number
Home number
Whether you plan on being in the band or colorguard
If in the band what instrument(s) you play
Whether you need to borrow an instrument or equipment
To see and hear the band in action, visit our
YouTube channel
.
{
  ""description"": ""table border=\""0\"" cellspacing=\""5\"">\n<tbody>\n<tr>\n<td style=\""padding: 0px;\"">\n<p><img style=\""border: 1px solid black; float: right;\"" src=\""/athletics/kilt ..."",
  ""viewport"": ""width=device-width, initial-scale=1"",
  ""msapplication-TileColor"": ""#c41230"",
  ""msapplication-config"": ""/assets/favicons/browserconfig.xml"",
  ""theme-color"": ""#c41230"",
  ""fb:app_id"": ""280467664480"",
  ""og:locale"": ""en_US"",
  ""og:title"": ""The Kiltie Band"",
  ""dcterms.title"": ""The Kiltie Band"",
  ""og:description"": ""The Kiltie Band"",
  ""dcterms.description"": ""The Kiltie Band"",
  ""og:image"": ""https://athletics.cmu.edu/images/setup/thumbnail_default.jpg?max_width=1200&max_height=675"",
  "" og:image:alt"": ""Carnegie Mellon University Athletics thumbnail"",
  ""og:site_name"": ""Carnegie Mellon University Athletics"",
  ""og:url"": ""https://athletics.cmu.edu/athletics/kiltieband/index"",
  ""dcterms.identifier"": ""https://athletics.cmu.edu/athletics/kiltieband/index"",
  ""og:type"": ""article"",
  ""dcterms.type"": ""article""
}
Topic category is kiltieband
Answer: "
"Where is the Senior Leadership Recognition Ceremony held on May 10 2024?
","['Commencement.txt', 'Commencement.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Where is the Senior Leadership Recognition Ceremony held on May 10 2024?

Context: Invitation, along with registration details, will be sent in late April.

First Gen Graduation Recognition
Reception: 5-5:30 p.m.
Alumni Concert Hall, College of Fine Arts
Ceremony: 5:30-6:30p.m.
Kresge Theater, College of Fine Arts

Contact:
M. Shernell Smith
mssmith@andrew.cmu.edu 

Sam Colavecchio
scolavec@andrew.cmu.edu 
412-268-7733


Friday, May 10
Diploma Ceremonies
Various times

Senior Leadership Recognition Ceremony
4–5:30 p.m.
Wiegand Gym, Cohon University Center
Undergraduate students and their guests by invitation only. This ceremony recognizes nominated seniors who have reflected upon their specific leadership contributions during their time at CMU.


Saturday, May 11
Diploma Ceremonies
Various times

Center for Student Diversity and Inclusion Ceremony
Reception: 12:30 p.m.
Program: 1-2:15 p.m.
Simmons Auditorium, Tepper Building

Contact:
M. Shernell Smith
mssmith@andrew.cmu.edu 

Rowshan Lang
rowshan@andrew.cmu.edu
Naval ROTC Commissioning
Ceremony: 1:30-2:30 p.m.
Auditorium, Soldiers & Sailors Memorial Hall & Museum *
4141 Fifth Avenue, Pittsburgh, PA 15213
Contact:
Mike Danko
mdanko@andrew.cmu.edu 

The President’s Reception in honor of CMU’s Doctoral Candidates
4–6 p.m.
Tepper Building Atrium


Sunday, May 12
Gesling Stadium opens to guests
8 a.m.

Robing and procession for graduates
9–10 a.m.
Various locations across campus

Student procession begins
9:15 a.m.
All guests in stadium must be seated. Access to guest seating will be restricted once the procession begins.

Commencement Ceremony
10–11:30 a.m.
Gesling Stadium, CMU’s campus

Diploma Ceremonies
Various times
Main Commencement Ceremony
Bachelor’s, master’s and doctoral degree candidates and their guests are invited to join the main commencement ceremony on Sunday, May 12, for the conferral of all degrees.
The main ceremony will include remarks from the president, keynote speaker, student speaker and academic deans, in addition to recognition of the honorary degree recipients.
There is no limit on number of guests who can attend the main commencement ceremony and tickets are not needed.
The ceremony will take place on CMU’s campus beginning at 10 a.m. and will be approximately 1.5 hours long. All guests must be seated by 9:15 a.m. for the start of the student procession. Access to guest seating will be restricted once the student procession begins.

Diploma Ceremonies
In addition to the main commencement ceremony, each college/school/department will host a diploma ceremony to recognize all graduating students. Diploma ceremonies will be held over the course of the weekend (Friday, May 10–Sunday, May 12).
Diploma ceremonies will include the presentation of diplomas to graduates, hooding of doctoral candidates and remarks from their college/school/department leadership. Each ceremony is organized and customized by their college/school/department.
Diploma ceremonies will take place both on and off campus. Locations and times for diploma ceremonies will be provided soon.
Diploma ceremonies typically include a reception and the length of time for each ceremony varies based on the number of graduates.
More details on the weekend schedule, including a diploma ceremony schedule, will be provided in the coming weeks. 

Thursday, May 9
Phi Beta Kappa Initiation Ceremony
Ceremony: 2–3 p.m.
McConomy Auditorium, Cohon University Center
Reception: 3–4 p.m.
Connan Room, Cohon University Center

Contact:
Joseph Devine
jd0x@andrew.cmu.edu 

Joanne Ursenbach
joanneu@andrew.cmu.edu 

President's Graduates Toast (bachelor's students)
3:30–4:30 p.m.
Location TBD
Registration required. Invitation, along with registration details, will be sent in late April.

First Gen Graduation Recognition
Reception: 5-5:30 p.m.
Alumni Concert Hall, College of Fine Arts
Ceremony: 5:30-6:30p.m.
Answer: "
"What is the name of Yonatan Bisk's lab?
","['yonatan bisk_3b0c02955e88f5862e61b560c7f70ba8cf235b1d_metadata.txt', 'yonatan bisk_e41482f4ee984f17382f6cdd900df094d928be06_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the name of Yonatan Bisk's lab?

Context: Faculty Name: yonatan bisk
Paperid: 3b0c02955e88f5862e61b560c7f70ba8cf235b1d
Title: HomeRobot: Open-Vocabulary Mobile Manipulation
Year: 2023
Abstract: HomeRobot (noun): An affordable compliant robot that navigates homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future research work improve performance. See videos on our website: https://ovmm.github.io/.
Faculty Name: yonatan bisk
Paperid: e41482f4ee984f17382f6cdd900df094d928be06
Title: WebArena: A Realistic Web Environment for Building Autonomous Agents
Year: 2023
Abstract: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.
Authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper builds an environment for language-guided agents that is highly realistic and reproducible, and creates an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.'}
Url: https://arxiv.org/pdf/2307.13854
Answer: "
"How many months does the advanced study MIIS degree typically take?
","['program_info_MasterofScienceinIntelligentInformationSystems.txt', 'miis-handbook_2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many months does the advanced study MIIS degree typically take?

Context: Academic Program Name:
Master of Science in Intelligent Information Systems

Website:
https://lti.cs.cmu.edu/academics/masters-programs/miis.html

Overview:
The Master's in Intelligent Information Systems degree focuses on recognizing and extracting meaning from text, spoken language and video. As an MIIS student, you’ll receive the department’s deepest exposure to content analysis and machine learning. In addition to completing the program’s coursework, you’ll work on directed study projects with your faculty advisor for two semesters; participate in a summer internship; and collaborate with your peers on a semester-long, group-oriented capstone project. This combination of classroom instruction, professional experience, and using new skills in significant projects with world-class colleagues will help prepare you for a successful career in industry or government. Our alumni have gone on to exciting careers at places like Apple, IBM and Google, and most have job offers within six weeks of graduation.

Requirements:
The Intelligent Information Systems degree offers students the flexibility to create their own course of study in consultation with their advisor.
MIIS students gain three types of practical experience: software development supervised by their advisor (24 units equivalent to two courses); a summer internship (which can be waived for students that have sufficient prior professional experience); and a capstone project executed in a group of peers (42 units equivalent to three 12-unit courses and one 6-unit course). This combination is proven to help IIS students to broaden their skills quickly. The MIIS degree is offered in two options:
Option 1. Standard MIIS degree (MIIS-16) - A 16-month track that is completed in three academic semesters (fall, spring, fall) and a summer internship.
Option 2. MIIS: Advanced Study degree (MIIS-21) - A 21-month track that is completed in four academic semesters (fall, spring, fall, spring) and a summer internship.
MIIS: Advanced Study track offers an in-depth degree in one of the following areas of concentration:
Human Language for Language Technologies
Language Technology Application
Machine Learning for Language Technologies
Part-time education option is available in some cases.
MIIS-16 students must take at least 84 units (typically 7 courses) of qualifying and elective courses that satisfy human language, machine learning, and language technology applications breadth requirements.
Under U.S. Federal Title IV regulations, 
student eligibility for federal financial aid is contingent upon enrollment in and successful 
completion of courses that are counted as credit toward their current degree program. To 
receive the maximum amount of federal financial aid for which they may be eligible, students 
must enroll each semester in at least 36 units that count toward their current degree level. (See 
separate guidance regarding integrated degree completion.) Students should consult with their 
designated college liaison in The HUB regarding billing and financial aid, particularly for early 
completion, longer-than standard completion, or integrated undergraduate and master’s degree 
programs.   
 
International Students 
Immigration status for students in F-1 and J-1 nonimmigrant status is tied to making normal 
progress toward completing degree requirements. Therefore, F-1 and J-1 students who are 
considering completing their degree requirements early, anticipating longer-than-standard 
completion, or moving from an undergraduate to a graduate student classification (integrated 
undergraduate-graduate study) should consult with their designated advisor in the Office of 
International Education (OIE) to ensure compliance with immigration regulations. 
MIIS Graduate Student Handbook 
Page 13 
 
4 MIIS Degree Requirements and Related Policies/Protocols 
4.1 Program Options 
The MIIS degree is offered in two options: 
Option 1. Standard MIIS degree (MIIS-16) - A 16-month track that is completed in three 
academic semesters (fall, spring, fall) and a summer internship.  
Option 2. MIIS: Advanced Study degree (MIIS-21) - A 21-month track that is completed in 
four academic semesters (fall, spring, fall, spring) and a summer internship. 
MIIS: Advanced Study track offers in depth degree in one of the following areas of 
concentration: 
• Human Language for Language Technologies 
• Language Technology Application 
• Machine Learning for Language Technologies 
Part-time options are available in some cases. 
4.2 Required Units for Degree Attainment 
To complete the Master of Science in Intelligent Information Systems, a student must satisfy 
three types of requirements. Curricular requirements ensure that MIIS students receive 
instruction in core intelligent information systems technologies while also allowing an 
opportunity to specialize in areas of personal interest. Practice requirements are opportunities 
to apply and hone new skills while building state-of-the-art systems.
Answer: "
"Carnegie Mellon University is home to how many members of the National Academy of Engineering (NAE)? 
","['fact_sheet_d407.txt', 'history_d401.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Carnegie Mellon University is home to how many members of the National Academy of Engineering (NAE)? 

Context: Carnegie Mellon University has been a birthplace 
of innovation since its founding in 1900.
Today, CMU is a global leader bringing groundbreaking 
ideas to market and creating successful startup 
businesses. Our award-winning faculty are renowned for 
working closely with students to solve major scientific, 
technological and societal challenges. We put a strong 
emphasis on creating things — from art to robots. We 
have become a model for economic development in 
forming partnerships with companies such as Uber, 
Google and Disney. Our students are recruited by some 
of the world’s most innovative companies.
MANAGEMENT 
INFORMATION 
SYSTEMS
U.S. News & World Report, 2022
#1
ARTIFICIAL 
INTELLIGENCE
U.S. News & World Report, 2022
#1
AMONG U.S. 
UNIVERSITIES
Times Higher Education  
World University 
Ranking, 2023
#19
SOFTWARE 
ENGINEERING
U.S. News & World Report, 2022
COLLEGE OF 
ENGINEERING
U.S. News & World Report, 2022
#1
#4
#1
SCHOOL OF 
COMPUTER 
SCIENCE
U.S. News & World Report, 2022
UNIVERSITY  
IN THE WORLD
Times Higher Education  
World University  
Ranking, 2023
#28
TIME-BASED/ 
NEW MEDIA
U.S. News & World Report, 2019
#1
1,529  
FACULTY
16,779 
STUDENTS
Graduate
Undergraduate
117,257 
ALUMNI (LIVING)
85%  U.S.
15%  International
35%  U.S. 
65%  International
76%  U.S. 
24%  International
GLOBAL COMMUNITY
Students representing
126 countries
Faculty representing
56 countries
Alumni representing 
148 countries
OF COMPUTER 
SCIENCE’S FIRST-
YEAR STUDENTS  
WERE WOMEN  
IN 2019
Nearly triple the  
national average
“ My Heart is in the Work.” 
 
Andrew Carnegie, Founder 
 
November 15, 1900
49.8%
87%  U.S.
Drawing on Shaw’s plan and also on the work of other faculty members, an undergraduate program in computer science was created during the 1989-90 academic year. Seven CS majors were admitted to the program as sophomores. Another 73 undergraduates were admitted in 1990–91. By 1995, there were 401 undergraduates in the School of Computer Science; in fall 2013, more than 600 undergraduates made up about 37 percent of student enrollment at SCS, along with more than 600 master’s degree students.New departments, new areas of studyAlong the way, the Center for Machine Translation became the Language Technologies Institute, and other new departments were formed, including the Human-Computer Interaction Institute (1993), the Institute for Software Research (1999), the Machine Learning Department (2006) and the Ray and Stephanie Lane Center for Computational Biology (2009). SCS’s seven degree-granting departments draw faculty and students from a wide variety of disciplines, including engineering, mathematics, social sciences, linguistics and design.Committed to extending our founders’ visionThe School of Computer Science at Carnegie Mellon University enters its second quarter century as a world-leading educational and research institution, embracing all facets of computing. Its graduate programs are consisted ranked with the best in the world by a leading U.S. magazine, while its undergraduate programs are also rated the best in the U.S. by corporate recruiters. In 2013, SCS had 284 faculty members and a total student enrollment of nearly 1,700, including undergraduate, master’s and Ph.D. students, and conducted $124 million in research. Indeed, by itself, the Robotics Institute is the largest university robotics research group in the world, with more than 500 people and more than 100 ongoing research projects. A half-century ago, Perlis, Simon and Newell outlined a vision for computer science. The School of Computer Science at CMU remains committed to continuing and extending their vision in the context of big data and connected computing in the 21st century.
Answer: "
"In fall 2024, When is Labor Day?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2024, When is Labor Day?

Context: On Monday, 02 September, 2024, during the Fall 2024 (F24) semester, Labor Day is observed, leading to No Classes & University Closed.
On Monday, 09 September, 2024, during the Fall 2024 (F24) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 18 September, 2024, during the Fall 2024 (F24) semester, Mini-1 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 30 September, 2024, during the Fall 2024 (F24) semester, Mini-1 Pass/no pass & withdrawal deadline (3) is observed.
On Monday, 30 September, 2024, during the Fall 2024 (F24) semester, Mini-1 Faculty Course Evaluations open is observed.
On Monday, 07 October, 2024, during the Fall 2024 (F24) semester, Semester drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 11 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Last Day of Classes is observed.
On Friday, 11 October, 2024, during the Fall 2024 (F24) semester, Mini-1 voucher election deadline (4) is observed.
On Saturday, 12 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Exams is observed.
On Saturday, 12 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Faculty Course Evaluations close is observed.
From 14 October, 2024,Monday to 18 October, 2023,Friday marks the Fall Break for Fall 2024 (F24) semester which leads to No Classes.
On Monday, 21 October, 2024, during the Fall 2024 (F24) semester, Mini-2 Classes Begin is observed.
On Wednesday, 23 October, 2024, during the Fall 2024 (F24) semester, Mid-Semester & Mini-1 grades due by 4 pm is observed.
On Friday, 25 October, 2024, during the Fall 2024 (F24) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 28 August, 2023, during the Fall 2023 (F23) semester, Semester & Mini-1 Classes Begin is observed.
On Friday, 01 September, 2023, during the Fall 2023 (F23) semester, Mini-1 add, audit & tuition adjustment drop deadline  (1) is observed.
On Monday, 04 September, 2023, during the Fall 2023 (F23) semester, Labor Day is observed, leading to No Classes & University Closed.
On Monday, 11 September, 2023, during the Fall 2023 (F23) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Wednesday, 20 September, 2023, during the Fall 2023 (F23) semester, Mini-1 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 02 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Pass/no pass & withdrawal deadline (3) is observed.
On Monday, 02 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Faculty Course Evaluations open is observed.
On Monday, 09 October, 2023, during the Fall 2023 (F23) semester, Semester drop deadline is observed, leading to withdrawal grade assigned after this date.
On Friday, 13 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Last Day of Classes is observed.
On Friday, 13 October, 2023, during the Fall 2023 (F23) semester, Mini-1 voucher election deadline (4) is observed.
From 13 October, 2023,Friday to 14 October, 2023,Saturday marks the Family Weekend for Fall 2023 (F23) semester.
On Saturday, 14 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Exams is observed.
On Saturday, 14 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Faculty Course Evaluations close is observed.
From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
Answer: "
"Which two faculty are co-teaching the neural code generation course?
","['graham neubig_31366ff634fc905affd78dbd8ddc9a872c006a87_metadata.txt', 'combined_metadata_final.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which two faculty are co-teaching the neural code generation course?

Context: Faculty Name: graham neubig
Paperid: 31366ff634fc905affd78dbd8ddc9a872c006a87
Title: CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code
Year: 2023
Abstract: Since the rise of neural natural-language-to-code models (NL->Code) that can generate long expressions and statements rather than a single next-token, one of the major problems has been reliably evaluating their generated output. In this paper, we propose CodeBERTScore: an evaluation metric for code generation, which builds on BERTScore (Zhang et al., 2020). Instead of encoding only the generated tokens as in BERTScore, CodeBERTScore also encodes the natural language input preceding the generated code, thus modeling the consistency between the generated code and its given natural language context as well. We perform an extensive evaluation of CodeBERTScore across four programming languages. We find that CodeBERTScore achieves a higher correlation with human preference and with functional correctness than all existing metrics. That is, generated code that receives a higher score by CodeBERTScore is more likely to be preferred by humans, as well as to function correctly when executed. We release five language-specific pretrained models to use with our publicly available code. Our language-specific models have been downloaded more than 1,000,000 times from the Huggingface Hub. Our code and data are available at https://github.com/neulab/code-bert-score
Authors: Shuyan Zhou, Uri Alon, Sumit Agarwal, Graham Neubig
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'It is found that generated code that receives a higher score by CodeBERTScore is more likely to be preferred by humans, as well as to function correctly when executed, than all existing metrics.'}
Url: http://arxiv.org/pdf/2302.05527
The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Sap located in Building POS, Room 153.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Quantum Computing, Cryptography and Machine Learning Lab' with Course ID 11860 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Justice, Singh, Ramakrishnan located in Building DH, Room 1211.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Large Language Model Systems' with Course ID 11868 and Section A offers 12.0 units. The Class meets Monday Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Li located in Building POS, Room A35.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Advanced Topics in Multimodal Machine Learning' with Course ID 11877 and Section A offers VAR units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Liang, Fried located in Building WEH, Room 4709.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Neural Code Generation:' with Course ID 11891 and Section NA offers VAR units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Neural Code Generation' with Course ID 11891 and Section A offers VAR units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Fried, Welleck located in Building WEH, Room 4625.
Answer: "
"What are the course numbers for question answering courses at LTI?
","['mlt-student-handbook-2023-2024.txt', 'combined_metadata_final.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the course numbers for question answering courses at LTI?

Context: International Students: Immigration status for students in F-1 and J-1 non-immigrant 
status is tied to making normal progress toward completing degree requirements. 
Therefore, F-1 and J-1 students who are considering completing their degree 
requirements early, anticipating longer-than-standard completion, or moving from an 
undergraduate to a graduate student classification (integrated undergraduate-graduate 
study) should consult with their designated  advisor in the Office of International 
Education (OIE) to ensure compliance with  immigration regulations. 
 
MLT Graduate Student Handbook 
Page 13 
 
4 MLT Degree Attainment 
4.1 Course Requirements 
In order to complete the Master of Language Technologies degree, the student must pass 120 
or more course units of senior-to-graduate courses, and meet the following criteria: 
 within those 120 units, at least 72 units of LTI courses and 24 units of SCS courses, 
 within those 72 units, 11-711, 11-791 (or an equivalent, see below), and one ``Task 
Orientation Focus'' class, and 
 within those 72 units, at least one of the following: 
o an LTI lab course, 
o 11-792, or 
o project-oriented Masters thesis; 
 Of the remaining 24 units, 12 must be 11-910 Directed Research; 
 The final 12 units are an Open Elective. 
The student must also complete two summers of full-time directed research, attend the LTI 
Colloquium (11-700) each semester, and satisfy the Research Speaking Requirement described 
elsewhere. 
Since 11-791 is not being offered currently, the faculty have defined a list of acceptable 
substitute courses: 
 11-727: Computational Semantics for NLP (only if the course project was done as a 
group project)  
 11-731: Machine Translation  
 11-747: Neural Networks for NLP  
 11-751: Speech Recognition  
 11-775: Large-Scale Multimedia  
 11-776: Multimodal Affective Computing 
 11-777: Multimodal Machine Learning  
 11-785: Deep Learning  
 11-797: Question Answering  
 
Students may request to have other LTI courses with a group engineering project component to 
be added to this list.  
For definitions of quoted terms, see the section on Definitions of LTI Terminology.
Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Introduction to Question Answering' with Course ID 11697 and Section A offers 12.0 units. The Class meets Monday Wednesday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Nyberg, Mitamura located in Building PH, Room A18A.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'LTI Colloquium' with Course ID 11700 and Section A offers 6.0 units. The Class meets Friday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Bisk, Diaz located in Building POS, Room A35.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'LTI Colloquium' with Course ID 11700 and Section PP offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Lisbon, Portugal location,led by experienced instructor Diaz, Bisk located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Advanced Natural Language Processing' with Course ID 11711 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Frederking, Fried located in Building TEP, Room 1403.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Graduate Seminar on Dialog Processing' with Course ID 11716 and Section A offers 6.0 units. The Class meets Tuesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rudnicky located in Building PH, Room A21A.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Linguistics Lab' with Course ID 11723 and Section A offers 6.0 units.
Answer: "
"What number do all of the Architecture classes start with?
","['combined_metadata_final.txt', 'metadata_course_summer_one_all_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What number do all of the Architecture classes start with?

Context: In Semester Fall 2023, from the department of Architecture, the subject titled 'Architecture Design Studio: Praxis Studio 3' with Course ID 48400 and Section E offers 18.0 units. The Class meets Tuesday Thursday between 01:00PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building MM, Room 312.
In Semester Fall 2023, from the department of Architecture, the subject titled 'History and Future of Interaction Design' with Course ID 48409 and Section A offers 9.0 units. The Class meets Monday Wednesday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Pangaro located in Building MM, Room 107.
In Semester Fall 2023, from the department of Architecture, the subject titled 'Environment II: Design Integration of Active Building Systems' with Course ID 48432 and Section A offers 9.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Baird located in Building MM, Room 103.
In Semester Fall 2023, from the department of Architecture, the subject titled 'Aztec to Zacatecas: Mesoamerican & Spanish Colonial Arch of Mexico & Guatemala' with Course ID 48434 and Section A offers 9.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Shaw located in Building CFA, Room 102.
In Semester Fall 2023, from the department of Architecture, the subject titled 'Undergraduate Internship' with Course ID 48490 and Section A offers 3.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Architecture, the subject titled 'Advanced Synthesis Options Studio' with Course ID 48500 and Section NA offers 018 units. The Class meets Tuesday Thursday between 01:00PM and 03:20PM ET.
In Semester Summer One(All) 2024, from the department of Architecture, the subject titled 'Undergraduate Internship' with Course ID 48490 and Section A offers 3.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Arscott located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Architecture, the subject titled 'Architecture Design Studio: Thesis II/ Independent Project' with Course ID 48519 and Section A offers VAR units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building TBA, Room None.
In Semester Summer One(All) 2024, from the department of Architecture, the subject titled 'Independent Study' with Course ID 48599 and Section A offers 3-9 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building TBA, Room None.
In Semester Summer One(All) 2024, from the department of Architecture, the subject titled 'ASO Studio General' with Course ID 48600 and Section A offers 18.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Arscott located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Architecture, the subject titled 'Internship (Studio-Based Graduate Programs):' with Course ID 48695 and Section NA offers 3.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Summer One(All) 2024, from the department of Architecture, the subject titled 'Master of Architecture' with Course ID 48695 and Section A offers 3.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Bard, Gutschow, Anklesaria located in Building DNM, Room DNM.
Answer: "
"What is the full name of the conference where the paper A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech, got published?
","['alexander rudnicky_4b8d3ede673ddeab9dfb5184da6b748d7a526754_metadata.txt', 'shinji watanabe_4b8d3ede673ddeab9dfb5184da6b748d7a526754_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the full name of the conference where the paper A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech, got published?

Context: Faculty Name: alexander rudnicky
Paperid: 4b8d3ede673ddeab9dfb5184da6b748d7a526754
Title: A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech
Year: 2023
Abstract: Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human-level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregressive models, leading to unintelligible synthesis, and demonstrate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ablation analyses to identify the efficacy of our methods. We show that MQTTS outperforms existing TTS systems in several objective and subjective measures.
Authors: Li-Wei Chen, Shinji Watanabe, Alexander I. Rudnicky
Venue: AAAI Conference on Artificial Intelligence
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work introduces the MQTTS system, a Text-to-Speech system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality, and shows that MqTTS outperforms existing TTS systems in several objective and subjective measures.'}
Url: http://arxiv.org/pdf/2302.04215
Faculty Name: shinji watanabe
Paperid: 4b8d3ede673ddeab9dfb5184da6b748d7a526754
Title: A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech
Year: 2023
Abstract: Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human-level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregressive models, leading to unintelligible synthesis, and demonstrate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ablation analyses to identify the efficacy of our methods. We show that MQTTS outperforms existing TTS systems in several objective and subjective measures.
Authors: Li-Wei Chen, Shinji Watanabe, Alexander I. Rudnicky
Venue: AAAI Conference on Artificial Intelligence
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work introduces the MQTTS system, a Text-to-Speech system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality, and shows that MqTTS outperforms existing TTS systems in several objective and subjective measures.'}
Url: http://arxiv.org/pdf/2302.04215
Answer: "
"In fall 2023, What is the course number for Undergraduate Research in Computational Biology?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, What is the course number for Undergraduate Research in Computational Biology?

Context: In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Undergraduate Research in Computational Biology' with Course ID 02500 and Section A offers 6-24 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Compeau located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Methods for Biological Modeling and Simulation' with Course ID 02512 and Section A offers 9.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Carja, Schwartz located in Building HH, Room B131.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Medicine' with Course ID 02518 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Wu located in Building WEH, Room 5403.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Programming for Scientists' with Course ID 02601 and Section NA offers 12.0 units. The Class meets Monday Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Compeau located in Building POS, Room 152.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Programming for Scientists' with Course ID 02601 and Section A offers 12.0 units. The Class meets Friday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Compeau located in Building POS, Room 152.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Professional Issues for Computational and Automated Scientists' with Course ID 02602 and Section A offers 3.0 units. The Class meets Thursday between 11:00AM and 12:20PM ET.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Undergraduate Research in Computational Biology' with Course ID 02500 and Section A offers 6-24 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Compeau located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Methods for Biological Modeling and Simulation' with Course ID 02512 and Section A offers 9.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Carja, Schwartz located in Building HH, Room B131.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Medicine' with Course ID 02518 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Wu located in Building WEH, Room 5403.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Programming for Scientists' with Course ID 02601 and Section NA offers 12.0 units. The Class meets Monday Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Compeau located in Building POS, Room 152.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Programming for Scientists' with Course ID 02601 and Section A offers 12.0 units. The Class meets Friday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Compeau located in Building POS, Room 152.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Professional Issues for Computational and Automated Scientists' with Course ID 02602 and Section A offers 3.0 units. The Class meets Thursday between 11:00AM and 12:20PM ET.
Answer: "
"In the BASS paper from Interspeech 2023, what is the improvement in ROUGE-L score demonstrated by the proposed block-wise training method?
","['bhiksha raj_3bd320ddb25886417ae90011b00f13f5d558097b_metadata.txt', 'rita singh_3bd320ddb25886417ae90011b00f13f5d558097b_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the BASS paper from Interspeech 2023, what is the improvement in ROUGE-L score demonstrated by the proposed block-wise training method?

Context: Faculty Name: bhiksha raj
Paperid: 3bd320ddb25886417ae90011b00f13f5d558097b
Title: BASS: Block-wise Adaptation for Speech Summarization
Year: 2023
Abstract: End-to-end speech summarization has been shown to improve performance over cascade baselines. However, such models are difficult to train on very large inputs (dozens of minutes or hours) owing to compute restrictions and are hence trained with truncated model inputs. Truncation leads to poorer models, and a solution to this problem rests in block-wise modeling, i.e., processing a portion of the input frames at a time. In this paper, we develop a method that allows one to train summarization models on very long sequences in an incremental manner. Speech summarization is realized as a streaming process, where hypothesis summaries are updated every block based on new acoustic information. We devise and test strategies to pass semantic context across the blocks. Experiments on the How2 dataset demonstrate that the proposed block-wise training method improves by 3 points absolute on ROUGE-L over a truncated input baseline.
Authors: Roshan Sharma, Kenneth Zheng, Siddhant Arora, Shinji Watanabe, Rita Singh, B. Raj
Venue: Interspeech
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper develops a method that allows one to train summarization models on very long sequences in an incremental manner and devise and test strategies to pass semantic context across the blocks.'}
Url: https://arxiv.org/pdf/2307.08217
Faculty Name: rita singh
Paperid: 3bd320ddb25886417ae90011b00f13f5d558097b
Title: BASS: Block-wise Adaptation for Speech Summarization
Year: 2023
Abstract: End-to-end speech summarization has been shown to improve performance over cascade baselines. However, such models are difficult to train on very large inputs (dozens of minutes or hours) owing to compute restrictions and are hence trained with truncated model inputs. Truncation leads to poorer models, and a solution to this problem rests in block-wise modeling, i.e., processing a portion of the input frames at a time. In this paper, we develop a method that allows one to train summarization models on very long sequences in an incremental manner. Speech summarization is realized as a streaming process, where hypothesis summaries are updated every block based on new acoustic information. We devise and test strategies to pass semantic context across the blocks. Experiments on the How2 dataset demonstrate that the proposed block-wise training method improves by 3 points absolute on ROUGE-L over a truncated input baseline.
Authors: Roshan Sharma, Kenneth Zheng, Siddhant Arora, Shinji Watanabe, Rita Singh, B. Raj
Venue: Interspeech
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper develops a method that allows one to train summarization models on very long sequences in an incremental manner and devise and test strategies to pass semantic context across the blocks.'}
Url: https://arxiv.org/pdf/2307.08217
Answer: "
"What is the accuracy using SHAP reduction?
","['louis philippe morency_6838c43e702a3f995967ba2e3edd5f65ff5f5511_metadata.txt', 'eric xing_640e1bcc472a71d36c6b9261403b60c680d93917_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the accuracy using SHAP reduction?

Context: Faculty Name: louis philippe morency
Paperid: 6838c43e702a3f995967ba2e3edd5f65ff5f5511
Title: SHAP-based Prediction of Mother's History of Depression to Understand the Influence on Child Behavior
Year: 2023
Abstract: Depression strongly impacts parents’ behavior. Does parents’ depression strongly affect the behavior of their children as well? To investigate this question, we compared dyadic interactions between 73 depressed and 75 non-depressed mothers and their adolescent child. Families were of low income and 84% were white. Child behavior was measured from audio-video recordings using manual annotation of verbal and nonverbal behavior by expert coders and by multimodal computational measures of facial expression, face and head dynamics, prosody, speech behavior, and linguistics. For both sets of measures, we used Support Vector Machines. For computational measures, we investigated the relative contribution of single versus multiple modalities using a novel approach to SHapley Additive exPlanations (SHAP). Computational measures outperformed manual ratings by human experts. Among individual computational measures, prosody was the most informative. SHAP reduction resulted in a four-fold decrease in the number of features and highest performance (77% accuracy; positive and negative agreements at 75% and 76%, respectively). These findings suggest that maternal depression strongly impacts the behavior of adolescent children; differences are most revealed in prosody; multimodal features together with SHAP reduction are most powerful.
Authors: Maneesh Bilalpur, Saurabh Hinduja, Laura Cariola, Lisa B. Sheeber, Nicholas B Allen, Louis-Philippe Morency, Jeffrey F. Cohn
Venue: International Conference on Multimodal Interaction
Tldr: None
Url: https://dl.acm.org/doi/pdf/10.1145/3577190.3614136
Faculty Name: eric xing
Paperid: 640e1bcc472a71d36c6b9261403b60c680d93917
Title: GET: a foundation model of transcription across human cell types
Year: 2023
Abstract: Transcriptional regulation, involving the complex interplay between regulatory sequences and proteins, directs all biological processes. Computational models of transcriptions lack generalizability to accurately extrapolate in unseen cell types and conditions. Here, we introduce GET, an interpretable foundation model, designed to uncover regulatory grammars across 213 human fetal and adult cell types. Relying exclusively on chromatin accessibility data and sequence information, GET achieves experimental-level accuracy in predicting gene expression even in previously unseen cell types. GET showcases remarkable adaptability across new sequencing platforms and assays, enabling regulatory inference across a broad range of cell types and conditions, and uncovering universal and cell type specific transcription factor interaction networks. We evaluated its performance on prediction of regulatory activity, inference of regulatory elements and regulators, and identification of physical interactions between transcription factors. Specifically, we show GET outperforms current models in predicting lentivirus-based massive parallel reporter assay readout with reduced input data. In Fetal erythroblast, we identify distal (>1Mbp) regulatory regions that were missed by previous models. In B cell, we identified a lymphocyte-specific transcription factor-transcription factor interaction that explains the functional significance of a lymphoma-risk predisposing germline mutation. In sum, we provide a generalizable and accurate model for transcription together with catalogs of gene regulation and transcription factor interactions, all with cell type specificity.
Authors: Xi Fu, Shentong Mo, Anqi Shao, Anouchka P. Laurent, Alejandro Buendia, Adolfo A. Ferrando, Alberto Ciccia, Yanyan Lan, Teresa Palomero, David M. Owens, Eric P. Xing, Raúl Rabadán
Venue: bioRxiv
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work introduces GET, an interpretable foundation model, designed to uncover regulatory grammars across 213 human fetal and adult cell types, and provides a generalizable and accurate model for transcription together with catalogs of gene regulation and transcription factor interactions, all with cell type specificity.'}
Answer: "
"What are the two key factors addressed by CSurF?
","['jamie callan_6b7eefa15c0a461afeab4fa13cf862c5340fdc2a_metadata.txt', 'jamie callan_6b7eefa15c0a461afeab4fa13cf862c5340fdc2a_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the two key factors addressed by CSurF?

Context: Faculty Name: jamie callan
Paperid: 6b7eefa15c0a461afeab4fa13cf862c5340fdc2a
Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms
Year: 2023
Abstract: Lexical exact-match systems perform text retrieval efficiently with sparse matching signals and fast retrieval through inverted lists, but naturally suffer from the mismatch between lexical surface form and implicit term semantics. This paper proposes to directly bridge the surface form space and the term semantics space in lexical exact-match retrieval via contextualized surface forms (CSF). Each CSF pairs a lexical surface form with a context source, and is represented by a lexical form weight and a contextualized semantic vector representation. This framework is able to perform sparse lexicon-based retrieval by learning to represent each query and document as a ""bag-of-CSFs"", simultaneously addressing two key factors in sparse retrieval: vocabulary expansion of surface form and semantic representation of term meaning. At retrieval time, it efficiently matches CSFs through exact-match of learned surface forms, and effectively scores each CSF pair via contextual semantic representations, leading to joint improvement in both term match and term scoring. Multiple experiments show that this approach successfully resolves the main mismatch issues in lexical exact-match retrieval and outperforms state-of-the-art lexical exact-match systems, reaching comparable accuracy as lexical all-to-all soft match systems as an efficient exact-match-based system.
Authors: Zhen Fan, Luyu Gao, Jamie Callan
Venue: International Conference on the Theory of Information Retrieval
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes to directly bridge the surface form space and the term semantics space in lexical exact-match retrieval via contextualized surface forms (CSF), reaching comparable accuracy as lexical all-to-all soft match systems as an efficient exact- match-based system.'}
Url: https://dl.acm.org/doi/pdf/10.1145/3578337.3605126
Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms
Authors: Zhen Fan, Luyu Gao, Jamie Callan
Section: 6 CONCLUSION
This paper proposes CSurF, which performs sparse lexicon-based retrieval through constructing and matching Contextualized Surface Forms. Its retrieval process combines efficient surface form exact match and fine-grained contextualized semantic scoring, which leads to maximized model capacity while maintaining the simplicity and efficiency of exact-match-based retrieval systems. CSurF extends current term-weight based learned sparse retrieval approaches with vector term representations. On experiments across multiple datasets and retrieval settings, CSurF is able to simultaneously bridge the vocabulary and semantic mismatch in exact-match retrieval, and achieve state-of-the-art retrieval performance for lexical exact-match systems. Ablation studies and analysis further demonstrate CSurF’s ability to jointly expandmeaningful surface forms and ground surface forms to underlying semantics, which leads to increased model capacity. We also propose a simple interpolation approach in out-of-domain retrieval settings, to analyze the effect of original text vs. expanded surface forms as well as the quality of lexical form expansion on different retrieval tasks. Compared to all-to-all soft-match retrievers, CSurF achieves comparable performance across all retrieval tasks as an exact-matchbased retrieval system. CSurF is able to learn sparse connections of the original query and document terms, resolving the key efficiency issue of lexical soft-match. The retrieval efficiency of CSurF can also be further optimized with different approaches including training regularization adjustment, post-hoc index pruning, and vector representation approximation or dimension control, without significantly affecting retrieval accuracy. We hope this work encourages more research on building effective, efficient, robust and knowledge-enhanced sparse retrieval systems in the real world, as well as exploring the connection and distinction among current retrieval frameworks and systems.
Answer: "
"In spring 2024, What is the title of course 15110?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, What is the title of course 15110?

Context: In Semester Spring 2024, from the department of Computer Science, the subject titled 'TBA' with Course ID 15110 and Section E offers 10.0 units. The Class meets Thursday between 01:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rivers located in Building GHC, Room 5208.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'TBA' with Course ID 15110 and Section F offers 10.0 units. The Class meets Thursday between 02:00PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rivers located in Building GHC, Room 5208.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'TBA' with Course ID 15110 and Section G offers 10.0 units. The Class meets Thursday between 03:00PM and 03:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rivers located in Building GHC, Room 5208.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'TBA' with Course ID 15110 and Section H offers 10.0 units. The Class meets Thursday between 04:00PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rivers located in Building GHC, Room 5208.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'TBA' with Course ID 15110 and Section I offers 10.0 units. The Class meets Thursday between 10:00AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Xhakaj located in Building GHC, Room 5210.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'TBA' with Course ID 15110 and Section J offers 10.0 units. The Class meets Thursday between 11:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Xhakaj located in Building GHC, Room 5210.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'TBA' with Course ID 15110 and Section E offers 10.0 units. The Class meets Thursday between 01:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rivers located in Building GHC, Room 5208.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'TBA' with Course ID 15110 and Section F offers 10.0 units. The Class meets Thursday between 02:00PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rivers located in Building GHC, Room 5208.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'TBA' with Course ID 15110 and Section G offers 10.0 units. The Class meets Thursday between 03:00PM and 03:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rivers located in Building GHC, Room 5208.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'TBA' with Course ID 15110 and Section H offers 10.0 units. The Class meets Thursday between 04:00PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rivers located in Building GHC, Room 5208.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'TBA' with Course ID 15110 and Section I offers 10.0 units. The Class meets Thursday between 10:00AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Xhakaj located in Building GHC, Room 5210.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'TBA' with Course ID 15110 and Section J offers 10.0 units. The Class meets Thursday between 11:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Xhakaj located in Building GHC, Room 5210.
Answer: "
"What is the name of the proposed approach for fairness domain adaptation in semantic scene segmentation?
","['bhiksha raj_078f86c6a691806cc71bbef1e734f75690db0ffc_metadata.txt', 'bhiksha raj_078f86c6a691806cc71bbef1e734f75690db0ffc_content_2.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the name of the proposed approach for fairness domain adaptation in semantic scene segmentation?

Context: Faculty Name: bhiksha raj
Paperid: 078f86c6a691806cc71bbef1e734f75690db0ffc
Title: FREDOM: Fairness Domain Adaptation Approach to Semantic Scene Understanding
Year: 2023
Abstract: Although Domain Adaptation in Semantic Scene Segmentation has shown impressive improvement in recent years, the fairness concerns in the domain adaptation have yet to be well defined and addressed. In addition, fairness is one of the most critical aspects when deploying the segmentation models into human-related real-world applications, e.g., autonomous driving, as any unfair predictions could influence human safety. In this paper, we propose a novel Fairness Domain Adaptation (FREDOM) approach to semantic scene segmentation. In particular, from the proposed formulated fairness objective, a new adaptation framework will be introduced based on the fair treatment of class distributions. Moreover, to generally model the context of structural dependency, a new conditional structural constraint is introduced to impose the consistency of predicted segmentation. Thanks to the proposed Conditional Structure Network, the self-attention mechanism has sufficiently modeled the structural information of segmentation. Through the ablation studies, the proposed method has shown the performance improvement of the segmentation models and promoted fairness in the model predictions. The experimental results on the two standard benchmarks, i.e., SYNTHIA $\rightarrow$ Cityscapes and GTA5 $\rightarrow$ Cityscapes, have shown that our method achieved State-of-the-Art (SOTA) performance11The implementation of FREDOM is available at https://github.com/uark-cviu/FREDOM
Authors: Thanh-Dat Truong, Ngan T. H. Le, B. Raj, J. Cothren, Khoa Luu
Venue: Computer Vision and Pattern Recognition
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes a novel Fairness Domain Adaptation (FREDOM) approach to semantic scene segmentation, where a new adaptation framework will be introduced based on the fair treatment of class distributions to generally model the context of structural dependency.'}
Url: https://arxiv.org/pdf/2304.02135
Title: FREDOM: Fairness Domain Adaptation Approach to Semantic Scene Understanding
Authors: Thanh-Dat Truong, Ngan Le, Bhiksha Raj, Jackson Cothren, Khoa Luu
Section: 3.2. The Proposed Fairness Adaptation Approach
segmentation maps is highly dependent on the window size used in Markovian approaches (the number of neighbor pixels being selected). In our work, to sufficiently capture the conditional structural constraint, instead of modeling only neighborhood dependencies as Markovian approaches, we generalize it by modeling ps(y \k s |yks ) via a conditional structure network (detailed in Sec. 4) to consider the correlation between all pixels in the segmentation. Relaxation of Ideal Data Distribution One of the key challenging problems in optimizing Eqn. (8) is that the conditional ideal data distributions p′s(y \k s |yks ) and p′s(y \k t |ykt ) are not available. Therefore, instead of directly optimizing these terms, let us consider the tight bound as in Eqn. (9). Exs∼ps(xs) log ( p′s(y \k s |yks ) ps(y \k s |yks ) ) + Ext∼pt(xt) log ( p′s(y \k t |ykt ) ps(y \k t |ykt ) ) ≤ − [ Exs∼ps(xs) log ps(y \k s |yks ) + Ext∼pt(xt) log ps(y \k t |y k t ) ] (9) With any form of ideal distribution p′s(·), Eqn. (9) always hold due to log p′s(·) ≤ 0. Hence, optimizing Eqn. (9) also ensure the conditional structural constraint in Eqn. (8) imposed due to the upper bound of Eqn. (9). Therefore, the demand for ideal data distribution is relaxed. Fig. 3 illustrates our proposed fairness domain adaptation framework.
Answer: "
"What is the full name of the conference where the paper BASS: Block-wise Adaptation for Speech Summarization, got published?
","['bhiksha raj_3bd320ddb25886417ae90011b00f13f5d558097b_content_1.txt', 'rita singh_3bd320ddb25886417ae90011b00f13f5d558097b_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the full name of the conference where the paper BASS: Block-wise Adaptation for Speech Summarization, got published?

Context: Title: BASS: Block-wise Adaptation for Speech Summarization
Authors: Roshan Sharma, Kenneth Zheng, Rita Singh, Bhiksha Raj
Section: 7. References
Workshop (ASRU), 2019, pp. 920–927. [15] E. Tsunoo, Y. Kashiwagi, and S. Watanabe, “Streaming transformer asr with blockwise synchronous beam search,” in 2021 IEEE Spoken Language Technology Workshop (SLT), 2021, pp. 22–29. [16] Y. Shi, Y. Wang, C. Wu, C.-F. Yeh, J. Chan, F. Zhang, D. Le, and M. Seltzer, “Emformer: Efficient memory transformer based acoustic model for low latency streaming speech recognition,” in ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021, pp. 6783– 6787. [17] X. Ma, Y. Wang, M. J. Dousti, P. Koehn, and J. Pino, “Streaming simultaneous speech translation with augmented memory transformer,” in ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021, pp. 7523–7527. [18] Y. Wang, H. Lv, D. Povey, L. Xie, and S. Khudanpur, “Wake word detection with streaming transformers,” in ICASSP 2021- 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 5864–5868. [19] S. Kim and F. Metze, “Dialog-context aware end-to-end speech recognition,” in 2018 IEEE Spoken Language Technology Workshop, SLT 2018, Athens, Greece, December 18-21, 2018. IEEE, 2018, pp. 434–440. [Online].
Title: BASS: Block-wise Adaptation for Speech Summarization
Authors: Roshan Sharma, Kenneth Zheng, Rita Singh, Bhiksha Raj
Section: 7. References
Workshop (ASRU), 2019, pp. 920–927. [15] E. Tsunoo, Y. Kashiwagi, and S. Watanabe, “Streaming transformer asr with blockwise synchronous beam search,” in 2021 IEEE Spoken Language Technology Workshop (SLT), 2021, pp. 22–29. [16] Y. Shi, Y. Wang, C. Wu, C.-F. Yeh, J. Chan, F. Zhang, D. Le, and M. Seltzer, “Emformer: Efficient memory transformer based acoustic model for low latency streaming speech recognition,” in ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021, pp. 6783– 6787. [17] X. Ma, Y. Wang, M. J. Dousti, P. Koehn, and J. Pino, “Streaming simultaneous speech translation with augmented memory transformer,” in ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021, pp. 7523–7527. [18] Y. Wang, H. Lv, D. Povey, L. Xie, and S. Khudanpur, “Wake word detection with streaming transformers,” in ICASSP 2021- 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 5864–5868. [19] S. Kim and F. Metze, “Dialog-context aware end-to-end speech recognition,” in 2018 IEEE Spoken Language Technology Workshop, SLT 2018, Athens, Greece, December 18-21, 2018. IEEE, 2018, pp. 434–440. [Online].
Answer: "
"In spring 2024, How many units is course 15090 worth?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, How many units is course 15090 worth?

Context: In Semester Spring 2024, from the department of Computational Biology, the subject titled 'Automated Science Internship' with Course ID 02802 and Section A offers VAR units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kangas located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Computational Biology, the subject titled 'Ph.D. Thesis Research' with Course ID 02900 and Section A offers 1-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Stenger located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Computer Science Practicum' with Course ID 15090 and Section A offers 3.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Stehlik located in Building TBA, Room None.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Computing' with Course ID 15110 and Section Lec 1 offers 10.0 units. The Class meets Monday Wednesday Friday between 02:00PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rivers located in Building GHC, Room 4401.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Computing' with Course ID 15110 and Section Lec 2 offers 10.0 units. The Class meets Monday Wednesday Friday between 03:00PM and 03:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Xhakaj located in Building POS, Room 153.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Computing:' with Course ID 15110 and Section NA offers 10.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Computational Biology, the subject titled 'Automated Science Internship' with Course ID 02802 and Section A offers VAR units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kangas located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Computational Biology, the subject titled 'Ph.D. Thesis Research' with Course ID 02900 and Section A offers 1-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Stenger located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Computer Science Practicum' with Course ID 15090 and Section A offers 3.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Stehlik located in Building TBA, Room None.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Computing' with Course ID 15110 and Section Lec 1 offers 10.0 units. The Class meets Monday Wednesday Friday between 02:00PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rivers located in Building GHC, Room 4401.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Computing' with Course ID 15110 and Section Lec 2 offers 10.0 units. The Class meets Monday Wednesday Friday between 03:00PM and 03:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Xhakaj located in Building POS, Room 153.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Computing:' with Course ID 15110 and Section NA offers 10.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
Answer: "
"How much does it cost to apply for the MLT program if an application is submitted on December 4th, 2023?
","['mlt-student-handbook-2023-2024.txt', 'mlt-student-handbook-2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How much does it cost to apply for the MLT program if an application is submitted on December 4th, 2023?

Context: It is to the student's advantage to avoid switching advisors, especially late in 
their graduate studies, because forging a strong student-advisor relationship takes time. 
4.3 Optional Masters Thesis 
MLT students may optionally elect to complete a Masters Thesis; it is not required. Students 
who intend to apply for the PhD program should consider the MLT Thesis as one of several 
ways to demonstrate their research potential. Other possibilities include innovative research in 
a research project or class project work. 
The MLT Thesis must be completed according to the following guidelines. Note in particular the 
October 31 proposal deadline! 
 There are two course components required for students who elect to complete the M.S. 
Thesis: 11-928  Masters Thesis I (with the chosen thesis advisor, typically for 12 units, 
typically in the Fall of their second year) and 11-929  Masters Thesis II in the following 
Spring. 
 Each of these two courses counts towards 6 units of MLT-level LTI course credit; if the 
student goes on to an LTI PhD, the second course (11-929) counts towards one PhD lab 
requirement.   
 The MLT thesis may focus on either a significant research result or a significant 
implementation of a new technology. The student should work with their advisor to 
define an interesting but bounded thesis research project. The MLT thesis culminates in 
MLT Graduate Student Handbook 
Page 16 
 
a publication-quality paper that clearly describes the work in terms of its research 
significance. 
 The MLT thesis committee will be composed of a thesis advisor and two additional 
readers, to be chosen by the student in consultation with the advisor, during the Fall 
Masters Thesis I course. At least 2 of the 3 committee members must be SCS faculty 
working in Language Technologies. 
 To receive a final grade for the first semester's course, the student must have prepared 
a written proposal of at least five pages, describing the scientific thesis to be 
investigated, any relevant research already completed by the student, a comparison 
with related work by others, and a detailed description of the work to be done in the 
next semester. The proposal MUST be accepted by the committee by the end of 
October, or the student will not be allowed to enroll in 11-929  Masters Thesis II!
5.1.4 Independent Study 
For an Independent Study to satisfy an MLT students coursework requirements, it must be 
approved by the MLT Program Director in advance.  In consultation with the CMU faculty 
member who will be supervising the Independent Study, the student should produce a short 
(half-page to one page) description of the goals of the Independent Study, and how the results 
will be evaluated, and submit it to the Program Director before the end of the Add period of the 
semester of the proposed study. The study should be planned so that it is finished in time for 
the supervising faculty member to give a grade at the end of the semester.  Independent 
studies may be undertaken for 6 or 12 units. Normally only one Independent Study would be 
approved during a students MLT coursework. 
5.1.5 Transfer Credit 
An equivalent graduate course previously completed at another institution may be permitted to 
satisfy one of the MLT course requirements. The decision on whether a course may be 
transferred is made by the MLT Program Director. Typically, the student will provide the 
Program Director with the syllabus of the external course, and the Program Director will use 
that and the students transcript to make the decision.  
See the section on Definition of transfer credit versus course exemption. 
All MLT students are required to take a minimum of 96 units of coursework at CMU.  
5.1.6 External Internships 
MLT students may only engage in internships if their advisors recommend it, since the program 
is 24 months, including summers. International students are required to consult with the Office 
of International Education for eligibility before seeking an internship/co-op or signing an offer 
contract. 
We caution all students to be aware of potential intellectual property (IP) problems with 
internships, and to review any IP agreements with their advisors before signing them. It is 
possible to lose ownership of your ideas. 
Important: See also section 4.2.3, External Employment/Consulting. 
MLT Graduate Student Handbook 
Page 22 
 
5.1.7 Transferring Into the MLT Program 
Transfers into the MLT program are not permitted during a students first semester at CMU. 
Students must begin their study at CMU in the program that admitted them; this is a university 
policy.
Answer: "
"What is the proposed learning objective to improve perceptual quality of speech?
","['bhiksha raj_611f9ee6eef0936462cd78f371798d0699951c59_metadata.txt', 'shinji watanabe_611f9ee6eef0936462cd78f371798d0699951c59_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the proposed learning objective to improve perceptual quality of speech?

Context: Faculty Name: bhiksha raj
Paperid: 611f9ee6eef0936462cd78f371798d0699951c59
Title: Paaploss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement
Year: 2023
Abstract: Despite rapid advancement in recent years, current speech enhancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics. We identify temporal acoustic parameters – such as spectral tilt, spectral flux, shimmer, etc. – that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-specific weights for each feature, as the acoustic parameters are known to show different behavior in different phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement workflows in both time-domain and time-frequency domain, as measured by standard evaluation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement.
Authors: Muqiao Yang, Joseph Konan, David Bick, YUNYANG ZENG, Shuo Han, Anurag Kumar, Shinji Watanabe, B. Raj
Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A learning objective is proposed that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics, and a neural network estimator is developed that can accurately predict their time-series values across an utterance.'}
Url: https://arxiv.org/pdf/2302.08095
Faculty Name: shinji watanabe
Paperid: 611f9ee6eef0936462cd78f371798d0699951c59
Title: Paaploss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement
Year: 2023
Abstract: Despite rapid advancement in recent years, current speech enhancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics. We identify temporal acoustic parameters – such as spectral tilt, spectral flux, shimmer, etc. – that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-specific weights for each feature, as the acoustic parameters are known to show different behavior in different phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement workflows in both time-domain and time-frequency domain, as measured by standard evaluation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement.
Authors: Muqiao Yang, Joseph Konan, David Bick, YUNYANG ZENG, Shuo Han, Anurag Kumar, Shinji Watanabe, B. Raj
Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A learning objective is proposed that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics, and a neural network estimator is developed that can accurately predict their time-series values across an utterance.'}
Url: https://arxiv.org/pdf/2302.08095
Answer: "
"Where does the sharp right-hand turn of the buggy course occur?
","['cmubuggy_d403.txt', 'cmubuggy_d403.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Where does the sharp right-hand turn of the buggy course occur?

Context: An mural celebrating Buggy can be found in CMU's Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions. Sweepstakes Slang Buggy: Vehicle being raced and also a nickname for the competition. Chute: A section of the freeroll portion of the buggy course (near the southwestern end of Frew Street at its intersection with Schenley Drive) where buggies make the sharp righthand turn from Schenley Drive onto Frew Street. Chute Flagger: Team member who provides a signal for buggy drivers to know when to start the right-hand turn from Schenley Drive onto Frew Street. Driver: Person who travels with a buggy and controls the vehicles via steering and braking systems. Pushbar: Structure attached to a buggy that a person pushes to propel that buggy forward. Pusher: Person who propels a buggy via a pushbar along one of the five hills of the buggy course. Shell: Entire outer structure or covering of a buggy that determines that buggy's aerodynamic characteristics. Transition: Procedure whereby one pusher finishes pushing a buggy and the next pusher in sequence starts to push that same buggy. Members of Fringe, celebrating their 50th year of Buggy racing, allowed a rare visit into its shop for a behind-the-scenes tour. This year Fringe is planning to roll four different vehicles, built and maintained in the Fringe workshop in the basement of the East Campus Garage, known as the ""Froom."" ""We're used to saying everything with 'fr' in front of it but when we say something in front of other people, it gets them confused,"" said Fringe head mechanic Dave Singh, who will graduate in May with a bachelor's degree in mechanical engineering and biomedical engineering. Fringe vehicles often are named with the letter ""B,"" like Boson, Blueshift, Bissa and Bumper. Other teams, such as Apex, often use names that connotate fire, while the SDC (Student Dormitory Committee) team, uses names such as Vice, Bane, Avarice and Malice. This year's buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It's one of the few times spectators can see buggies up close.
This year's buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It's one of the few times spectators can see buggies up close. Dave Singh explains how Fringe builds its buggies. Each team keeps its actual processes a secret, but each buggy has certain features such as a body, pushbar, wheels and driving and braking mechanisms. Behind Singh are other buggies built by Fringe. Behind the Wheel Drivers, have the closest connection to the vehicles, aside from the mechanics who spent countless hours building and maintaining individual buggies. While races are brief, each driver must log a number of practice runs to qualify to race. Teams practice nights and weekends throughout the fall and spring semesters as weather permits. Buggies are often built around drivers, so the fit can be snug, and drivers are often less than 5 feet, 3 inches in height. Most — but not all — are women. Tishya Girdhar, a junior in neuroscience, is social chair for Fringe. She came to CMU wanting to drive, but first she had to get over being claustrophobic. She started as a mechanic and got behind the wheel for last year's Sweepstakes. ""Our team's philosophy is to teach everyone how to do everything,"" Girdhar said. ""But I came to CMU wanting to drive. I wanted to drive so badly."" Amy Chen demonstrates how a driver is positioned inside of Boson, a Fringe buggy built in 2016. Safety is a top priority for teams. All drivers are required to have five pieces of safety gear: mouth guard, goggles, a harness that includes three points of contact to the body of the buggy, gloves and a helmet. And during practice runs and races, flaggers man the course to let drivers know if it's safe to proceed. The streets of the race course haven't changed, but the condition of the course varies due to potholes, which can make or break a fast run. The last course records were set in 2017, but this year potholes are making for some challenging conditions, said Amy Chen, a senior in psychology who has been driving for four years.
Answer: "
"In spring 2024, Who is the instructor for course 15151?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, Who is the instructor for course 15151?

Context: In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section T offers 12.0 units. The Class meets Wednesday between 07:00PM and 08:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5210.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section U offers 12.0 units. The Class meets Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5208.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Mathematical Foundations for Computer Science' with Course ID 15151 and Section NA offers 12.0 units. The Class meets Monday Wednesday Friday between 10:00AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Peng located in Building GHC, Room 4307.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Mathematical Foundations for Computer Science' with Course ID 15151 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 01:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Peng located in Building GHC, Room 4301.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Mathematical Foundations for Computer Science' with Course ID 15151 and Section B offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Peng located in Building GHC, Room 4301.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Competition Programming I' with Course ID 15195 and Section A offers 5.0 units. The Class meets Wednesday between 07:00PM and 09:50PM ET.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section T offers 12.0 units. The Class meets Wednesday between 07:00PM and 08:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5210.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section U offers 12.0 units. The Class meets Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5208.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Mathematical Foundations for Computer Science' with Course ID 15151 and Section NA offers 12.0 units. The Class meets Monday Wednesday Friday between 10:00AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Peng located in Building GHC, Room 4307.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Mathematical Foundations for Computer Science' with Course ID 15151 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 01:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Peng located in Building GHC, Room 4301.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Mathematical Foundations for Computer Science' with Course ID 15151 and Section B offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Peng located in Building GHC, Room 4301.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Competition Programming I' with Course ID 15195 and Section A offers 5.0 units. The Class meets Wednesday between 07:00PM and 09:50PM ET.
Answer: "
"What LTI professor was the last author in ""To Build Our Future, We Must Know Our Past: Contextualizing Paradigm Shifts in Natural Language Processing""?
","['emma strubell_1433b8d43d446fcc7f3e1370b22f744a4dd7c8e4_content_3.txt', 'emma strubell_1433b8d43d446fcc7f3e1370b22f744a4dd7c8e4_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What LTI professor was the last author in ""To Build Our Future, We Must Know Our Past: Contextualizing Paradigm Shifts in Natural Language Processing""?

Context: Title: To Build Our Future, We Must Know Our Past: Contextualizing Paradigm Shifts in Natural Language Processing
Authors: Sireesh Gururaja, Amanda Bertsch, Clara Na, David Gray Widder, Emma Strubell
Section: 3 Exploit-explore cycles of work
that is farther from current norms/methodologies requires higher upfront time investment. This competitiveness can manifest in harsher reviews, and one participant described a “deadly combination” (19) of higher standards for papers and lower quality of reviews. Some participants described this as a reason they were choosing to engage less with NLP conferences; one industry researcher stated that “I just find it difficult to publish papers in *CL that have ideas in them.” (22).
Faculty Name: emma strubell
Paperid: 1433b8d43d446fcc7f3e1370b22f744a4dd7c8e4
Title: To Build Our Future, We Must Know Our Past: Contextualizing Paradigm Shifts in Natural Language Processing
Year: 2023
Abstract: NLP is in a period of disruptive change that is impacting our methodologies, funding sources, and public perception. In this work, we seek to understand how to shape our future by better understanding our past. We study factors that shape NLP as a field, including culture, incentives, and infrastructure by conducting long-form interviews with 26 NLP researchers of varying seniority, research area, institution, and social identity. Our interviewees identify cyclical patterns in the field, as well as new shifts without historical parallel, including changes in benchmark culture and software infrastructure. We complement this discussion with quantitative analysis of citation, authorship, and language use in the ACL Anthology over time. We conclude by discussing shared visions, concerns, and hopes for the future of NLP. We hope that this study of our field's past and present can prompt informed discussion of our community's implicit norms and more deliberate action to consciously shape the future.
Authors: Sireesh Gururaja, Amanda Bertsch, Clara Na, D. Widder, Emma Strubell
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work conducts long-form interviews with 26 NLP researchers of varying seniority, research area, institution, and social identity to study factors that shape NLP as a field, including culture, incentives, and infrastructure.'}
Url: https://arxiv.org/pdf/2310.07715
Answer: "
"What is the BartScore achieved by the CRL-COM (R) system from the paper Improving Factuality of Abstractive Summarization via Contrastive Reward Learning, on the XSUM dataset?
","['graham neubig_f640e89fcede075b4bde3b2fa0dc78f591589ba3_metadata.txt', 'graham neubig_papers.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the BartScore achieved by the CRL-COM (R) system from the paper Improving Factuality of Abstractive Summarization via Contrastive Reward Learning, on the XSUM dataset?

Context: Faculty Name: graham neubig
Paperid: f640e89fcede075b4bde3b2fa0dc78f591589ba3
Title: Improving Factuality of Abstractive Summarization via Contrastive Reward Learning
Year: 2023
Abstract: Modern abstractive summarization models often generate summaries that contain hallucinated or contradictory information. In this paper, we propose a simple but effective contrastive learning framework that incorporates recent developments in reward learning and factuality metrics. Empirical studies demonstrate that the proposed framework enables summarization models to learn from feedback of factuality metrics using contrastive reward learning, leading to more factual summaries by human evaluations. This suggests that further advances in learning and evaluation algorithms can feed directly into providing more factual summaries. Code and human evaluation results will be publicly available at \url{https://github.com/EthanC111/factuality_summarization}.
Authors: Ethan Chern, Zhiruo Wang, Sanjan Das, Bhavuk Sharma, Pengfei Liu, Graham Neubig
Venue: TRUSTNLP
Tldr: {'model': 'tldr@v2.0.0', 'text': 'Empirical studies demonstrate that the proposed framework enables summarization models to learn from feedback of factuality metrics using contrastive reward learning, leading to more factual summaries by human evaluations, suggesting that further advances in learning and evaluation algorithms can feed directly into providing morefactuality summaries.'}
Url: https://arxiv.org/pdf/2307.04507
List of 2023 Open Access papers by graham neubig are:
Cross-Modal Fine-Tuning: Align then Refine
ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages
GlobalBench: A Benchmark for Global Progress in Natural Language Processing
Learning Performance-Improving Code Edits
CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code
Neural Machine Translation for the Indigenous Languages of the Americas: An Introduction
User-Centric Evaluation of OCR Systems for Kwak’wala
Multi-Dimensional Evaluation of Text Summarization with In-Context Learning
A Gold Standard Dataset for the Reviewer Assignment Problem
SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing
Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation
FacTool: Factuality Detection in Generative AI - A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios
Active Retrieval Augmented Generation
Large Language Models Enable Few-Shot Clustering
Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach
Crossing the Threshold: Idiomatic Machine Translation through Retrieval Augmentation and Loss Weighting
Why do Nearest Neighbor Language Models Work?
Syntax and Semantics Meet in the “Middle”: Probing the Syntax-Semantics Interface of LMs Through Agentivity
DataFinder: Scientific Dataset Recommendation from Natural Language Descriptions
Multi-lingual and Multi-cultural Figurative Language Understanding
It’s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk
Unlimiformer: Long-Range Transformers with Unlimited Length Input
WebArena: A Realistic Web Environment for Building Autonomous Agents
Prompt2Model: Generating Deployable Models from Natural Language Instructions
Computational Language Acquisition with Theory of Mind
Improving Factuality of Abstractive Summarization via Contrastive Reward Learning
The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation
Answer: "
"What types of information does FiT5 integrate into a single unified model?
","['chenyan xiong_275da3802142fc42f6fab2ce2104223b2e0ef40d_metadata.txt', 'eric xing_e6dbe34d154591618ef78d56d5e8a50583b5f9d1_content_4.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What types of information does FiT5 integrate into a single unified model?

Context: Faculty Name: chenyan xiong
Paperid: 275da3802142fc42f6fab2ce2104223b2e0ef40d
Title: Fusion-in-T5: Unifying Document Ranking Signals for Improved Information Retrieval
Year: 2023
Abstract: Common IR pipelines are typically cascade systems that may involve multiple rankers and/or fusion models to integrate different information step-by-step. In this paper, we propose a novel re-ranker named Fusion-in-T5 (FiT5), which integrates document text information, retrieval features, and global document information into a single unified model using templated-based input and global attention. Experiments on passage ranking benchmarks MS MARCO and TREC DL show that FiT5 significantly improves ranking performance over prior pipelines. Analyses find that through global attention, FiT5 is able to jointly utilize the ranking features via gradually attending to related documents, and thus improve the detection of subtle nuances between them. Our code will be open-sourced.
Authors: S. Yu, Cheng-Chung Fan, Chenyan Xiong, David Jin, Zhiyuan Liu, Zhenghao Liu Tsinghua University, Huazhong University of Science, Technology, Microsoft Research, M. I. O. Technology, N. University
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A novel re-ranker named Fusion-in-T5 (FiT5), which integrates document text information, retrieval features, and global document information into a single unified model using templated-based input and global attention, is proposed.'}
Url: http://arxiv.org/pdf/2305.14685
Linear parameterization provides a differentiable ob-138 jective for optimizing each model and the linear residual errors are proportional to the negative log likelihood of139 each network model under the data. Our unifying linearization of these models allows us to apply contextualization140 uniformly to each network class, and further enables us to benchmark and test the effects of common model personal-141 ization paradigms against contextualization in terms of model likelihood and modeling errors.142 4 Markov Neighborhood Correlation Population 0.985 ± 0.006 0.984 ± 0.004 0.963 ± 0.000 Cluster-specific 0.365 ± 0.014 0.349 ± 0.012 0.683 ± 0.052 Disease-specific 0.368 ± 0.003 0.351 ± 0.003 0.673 ± 0.002 Contextualized 0.322 ± 0.014 0.296 ± 0.013 0.529 ± 0.019 Contextualized Networks Improve Likelihood of Held-Out Expression Profiles143 Contextualization improves the fit of networks models to gene expression data (Table 1). We benchmark the contextu-144 alized networks by comparing against several granularities of partition-based models: (1) a population network model145 which estimates the same network for all samples, (2) cluster-specific networks that are estimated independently for146 each cluster of contextual information, and (3) disease-specific networks that are estimated independently for each147 cancer type (Fig. 7). For all three network models, we evaluate the fit of the network model to actual expression148 data by measuring the predictive performance of the network graphical model. These predictive performances are149 measured as mean-squared errors between predicted and observed expression data which are inversely proportional to150 the model likelihood under the probabilistic interpretation of the network graphical
Answer: "
"In spring 2024, How many units is course 17214?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, How many units is course 17214?

Context: In Semester Spring 2024, from the department of Mathematical Sciences, the subject titled 'Concepts of Mathematics' with Course ID 21127 and Section P offers 12.0 units. The Class meets Tuesday Thursday between 04:00PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hsieh located in Building BH, Room 235A.
In Semester Spring 2024, from the department of Mathematical Sciences, the subject titled 'Concepts of Mathematics' with Course ID 21127 and Section W offers 12.0 units. The Class meets Sunday Monday Tuesday Wednesday Thursday between 11:30AM and 12:20PM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Khare located in Building CMB, Room 2147.
In Semester Spring 2024, from the department of Mathematical Sciences, the subject titled 'Mathematics and the Arts' with Course ID 21150 and Section W offers 9.0 units. The Class meets Sunday Tuesday between 02:30PM and 03:45PM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Yilma located in Building CMB, Room 2049.
In Semester Spring 2024, from the department of Mathematical Sciences, the subject titled 'Undergraduate Colloquium' with Course ID 21201 and Section A3 offers 1.0 units. The Class meets Tuesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Offner located in Building SH, Room 105.
In Semester Spring 2024, from the department of Mathematical Sciences, the subject titled 'Discrete Mathematics' with Course ID 21228 and Section NA offers 9.0 units. The Class meets Monday Wednesday Friday between 10:00AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Bohman located in Building HH, Room B103.
In Semester Spring 2024, from the department of Mathematical Sciences, the subject titled 'Discrete Mathematics' with Course ID 21228 and Section A offers 9.0 units. The Class meets Tuesday between 09:00AM and 09:50AM ET.
In Semester Spring 2024, from the department of Mathematical Sciences, the subject titled 'Concepts of Mathematics' with Course ID 21127 and Section P offers 12.0 units. The Class meets Tuesday Thursday between 04:00PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hsieh located in Building BH, Room 235A.
In Semester Spring 2024, from the department of Mathematical Sciences, the subject titled 'Concepts of Mathematics' with Course ID 21127 and Section W offers 12.0 units. The Class meets Sunday Monday Tuesday Wednesday Thursday between 11:30AM and 12:20PM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Khare located in Building CMB, Room 2147.
In Semester Spring 2024, from the department of Mathematical Sciences, the subject titled 'Mathematics and the Arts' with Course ID 21150 and Section W offers 9.0 units. The Class meets Sunday Tuesday between 02:30PM and 03:45PM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Yilma located in Building CMB, Room 2049.
In Semester Spring 2024, from the department of Mathematical Sciences, the subject titled 'Undergraduate Colloquium' with Course ID 21201 and Section A3 offers 1.0 units. The Class meets Tuesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Offner located in Building SH, Room 105.
In Semester Spring 2024, from the department of Mathematical Sciences, the subject titled 'Discrete Mathematics' with Course ID 21228 and Section NA offers 9.0 units. The Class meets Monday Wednesday Friday between 10:00AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Bohman located in Building HH, Room B103.
In Semester Spring 2024, from the department of Mathematical Sciences, the subject titled 'Discrete Mathematics' with Course ID 21228 and Section A offers 9.0 units. The Class meets Tuesday between 09:00AM and 09:50AM ET.
Answer: "
"What time in the day does SafeWalk start?
","['mcds-student-handbook-2023_2024.txt', 'handbook-msaii-2022-2023.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What time in the day does SafeWalk start?

Context: The Parking and Transportation Services office is located in the lower 
level of the University Center, LL#8. There is limited parking on campus 
and the varying permit rates can be found on the website. All parking 
areas of campus are either by permit, metered or by the hour in the 
garage. Parking and Transportation Services will ticket any car parked in a 
permit area without a permit or at an expired meter. The city monitors the 
metered parking along Margaret Morrison, Frew and Tech Streets and will 
ticket at expired meters as well. 
 
More information can be found at: 
http://www.cmu.edu/parking/ 
 
The University offers shuttle and escort services operated through 
University Police. The Shuttle Service operates several routes within 
Oakland, Squirrel Hill and Shadyside areas, as well as to university sites 
located outside of the main campus. The Escort Service offers vehicle 
routes within a radius of campus between 6:30 pm-6 am daily. Information 
regarding up-to-date shuttle and escort schedules, pick-up/drop-off 
locations, routes and usage policies can be found at: 
www.cmu.edu/police/shuttleandescort/. 
 
SafeWalk provides another option to campus community members 
walking across and around campus during late-night hours. SafeWalk is a 
student volunteer organization that provides campus escorts for all 
members of the Carnegie Mellon community. SafeWalk operates nightly 
during the regular academic year (except certain holidays and break 
periods) from 10pm until 2am. Students, faculty and staff may request an 
escort by calling 412-268-SAFE (8-7233 from a campus phone), by 
approaching an escort team, or by stopping by the SafeWalk dispatch area 
in the University Center, Lower Level near the Post Office Package Pick-
 
 
42
Up window between 10pm-2am. SafeWalk will escort to locations 
approximately one mile from campus. Additional SafeWalk information 
can be found at: 
www.studentaffairs.cmu.edu/safewalk. 
6.16 Copying, Printing and Mailing Services 
Carnegie Mellon offers community members easy access to FedEx, copy 
centers, printing and mailing services, and postal services. More 
information regarding these services, locations and contact information 
can be found at the provided link.
The Parking and Transportation Services office is 
located in the East Campus Garage by the Forbes Ave entrance.  There is limited parking on 
campus, and the varying permit rates can be found on the website.  All parking areas of campus 
are either by permit, metered or by the hour in the garage.  Parking and Transportation Services 
will ticket any car parked in a permit area without a permit or at an expired meter.  The city 
monitors the metered parking along Margaret Morrison, Frew and Tech Streets and will ticket 
at expired meters as well. 
The university offers shuttle and escort services operated by University Police.  The Shuttle 
Service operates several routes within Oakland, Squirrel Hill and Shadyside areas, as well as to 
University sites located outside of the main campus.  The Escort Service offers vehicle routes 
within a radius of campus between 6:30 pm-4:15 am daily.  Information regarding up-to-date 
shuttle and escort schedules, pick-up/drop-off locations, routes and usage policies can be found 
at www.cmu.edu/parking/shuttle/index.html.  
SafeWalk provides another option to campus community members walking across and around 
campus during late-night hours. SafeWalk is a student volunteer organization that provides 
campus escorts for all members of the Carnegie Mellon community. SafeWalk operates nightly 
during the regular academic year (except certain holidays and break periods) from 10pm until 
2am. Students, faculty, and staff may request an escort by calling 412-268-SAFE (8-7233 from a 
campus phone), by approaching an escort team, or by stopping by the SafeWalk dispatch area in 
the University Center, Lower Level near the Post Office Package Pick-Up window between 
10pm-2am. SafeWalk will escort to locations approximately one mile from campus. Additional 
40 
 
SafeWalk information can be found at https://www.cmu.edu/admission/campus-
experience/student-services. 
13.16  Copying, Printing and Mailing Services 
Carnegie Mellon offers community members easy access to FedEx, copy centers, printing and 
mailing services, and postal services.
Answer: "
"In fall 2023, Where is unit 02518 held?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, Where is unit 02518 held?

Context: In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Undergraduate Research in Computational Biology' with Course ID 02500 and Section A offers 6-24 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Compeau located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Methods for Biological Modeling and Simulation' with Course ID 02512 and Section A offers 9.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Carja, Schwartz located in Building HH, Room B131.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Medicine' with Course ID 02518 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Wu located in Building WEH, Room 5403.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Programming for Scientists' with Course ID 02601 and Section NA offers 12.0 units. The Class meets Monday Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Compeau located in Building POS, Room 152.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Programming for Scientists' with Course ID 02601 and Section A offers 12.0 units. The Class meets Friday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Compeau located in Building POS, Room 152.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Professional Issues for Computational and Automated Scientists' with Course ID 02602 and Section A offers 3.0 units. The Class meets Thursday between 11:00AM and 12:20PM ET.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Undergraduate Research in Computational Biology' with Course ID 02500 and Section A offers 6-24 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Compeau located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Methods for Biological Modeling and Simulation' with Course ID 02512 and Section A offers 9.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Carja, Schwartz located in Building HH, Room B131.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Medicine' with Course ID 02518 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Wu located in Building WEH, Room 5403.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Programming for Scientists' with Course ID 02601 and Section NA offers 12.0 units. The Class meets Monday Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Compeau located in Building POS, Room 152.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Programming for Scientists' with Course ID 02601 and Section A offers 12.0 units. The Class meets Friday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Compeau located in Building POS, Room 152.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Professional Issues for Computational and Automated Scientists' with Course ID 02602 and Section A offers 3.0 units. The Class meets Thursday between 11:00AM and 12:20PM ET.
Answer: "
"What is the name of the novel framework introduced for learning unified multi-sensory object property representations?
","['yonatan bisk_69b8cd15966c4c9c3e44e71769e557f1c87fb3f9_metadata.txt', 'yonatan bisk_69b8cd15966c4c9c3e44e71769e557f1c87fb3f9_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the name of the novel framework introduced for learning unified multi-sensory object property representations?

Context: Faculty Name: yonatan bisk
Paperid: 69b8cd15966c4c9c3e44e71769e557f1c87fb3f9
Title: MOSAIC: Learning Unified Multi-Sensory Object Property Representations for Robot Perception
Year: 2023
Abstract: A holistic understanding of object properties across diverse sensory modalities (e.g., visual, audio, and haptic) is essential for tasks ranging from object categorization to complex manipulation. Drawing inspiration from cognitive science studies that emphasize the significance of multi-sensory integration in human perception, we introduce MOSAIC (Multimodal Object property learning with Self-Attention and Interactive Comprehension), a novel framework designed to facilitate the learning of unified multi-sensory object property representations. While it is undeniable that visual information plays a prominent role, we acknowledge that many fundamental object properties extend beyond the visual domain to encompass attributes like texture, mass distribution, or sounds, which significantly influence how we interact with objects. In MOSAIC, we leverage this profound insight by distilling knowledge from multimodal foundation models and aligning these representations not only across vision but also haptic and auditory sensory modalities. Through extensive experiments on a dataset where a humanoid robot interacts with 100 objects across 10 exploratory behaviors, we demonstrate the versatility of MOSAIC in two task families: object categorization and object-fetching tasks. Our results underscore the efficacy of MOSAIC's unified representations, showing competitive performance in category recognition through a simple linear probe setup and excelling in the fetch object task under zero-shot transfer conditions. This work pioneers the application of sensory grounding in foundation models for robotics, promising a significant leap in multi-sensory perception capabilities for autonomous systems. We have released the code, datasets, and additional results: https://github.com/gtatiya/MOSAIC.
Authors: Gyan Tatiya, Jonathan M Francis, Ho-Hsiang Wu, Yonatan Bisk, Jivko Sinapov
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work pioneers the application of sensory grounding in foundation models for robotics, promising a significant leap in multi-sensory perception capabilities for autonomous systems.'}
Url: https://arxiv.org/pdf/2309.08508
Title: MOSAIC: Learning Unified Multi-Sensory Object Property Representations for Robot Learning via Interactive Perception
Authors: Gyan Tatiya, Jonathan Francis, Ho-Hsiang Wu, Yonatan Bisk, Jivko Sinapov
Section: VI. CONCLUSION AND FUTURE WORK
We introduced the MOSAIC framework to enable robots to generate versatile, multimodal representations through interactive object perception and to leverage these unified representations across various downstream robot learning tasks. Through extensive performance evaluation, we have showcased the effectiveness of these unified representations in tasks such as category recognition, using a simple linear probe setup, and the fetch object task under zero-shot conditions. Moving forward, there are several exciting directions for future research. Firstly, we plan to consider the transfer of unified representations across different robot morphologies, enabling a broader range of robots to benefit from this technology. Furthermore, we envision settings where interactive behaviors are learned and composed, alongside the tasks we considered in this paper, thereby further increasing the efficacy of object exploration. These future endeavors hold the potential to further enhance the utility of unified representations in robotics and expand their applications across a multitude of scenarios and environments. One limitation in our current study is that, for the fetch object task, we evaluated using a zero-shot transfer condition rather than a learning-based approach to find the target object. For future work, it would be important to explore learningbased policies for solving the fetch object task, potentially increasing the versatility and adaptability of our framework.
Answer: "
"According to the paper ChatGPT MT, which languages does the study suggest ChatGPT is especially disadvantaged for?
","['david mortensen_11a571eaab42a6ffb1d938635a093315e392756d_metadata.txt', 'graham neubig_11a571eaab42a6ffb1d938635a093315e392756d_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: According to the paper ChatGPT MT, which languages does the study suggest ChatGPT is especially disadvantaged for?

Context: Faculty Name: david mortensen
Paperid: 11a571eaab42a6ffb1d938635a093315e392756d
Title: ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages
Year: 2023
Abstract: Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs’ MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world’s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language’s resource level is the most important feature in determining ChatGPT’s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.
Authors: Nathaniel R. Robinson, Perez Ogayo, David R. Mortensen, Graham Neubig
Venue: Conference on Machine Translation
Tldr: {'model': 'tldr@v2.0.0', 'text': ""This work presents the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark, and reveals that a language’s resource level is the most important feature in determining ChatGPT's relative ability to translate it, and suggests thatChatGPT is especially disadvantaged for LRLs and African languages.""}
Url: https://arxiv.org/pdf/2309.07423
Faculty Name: graham neubig
Paperid: 11a571eaab42a6ffb1d938635a093315e392756d
Title: ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages
Year: 2023
Abstract: Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs’ MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world’s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language’s resource level is the most important feature in determining ChatGPT’s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.
Authors: Nathaniel R. Robinson, Perez Ogayo, David R. Mortensen, Graham Neubig
Venue: Conference on Machine Translation
Tldr: {'model': 'tldr@v2.0.0', 'text': ""This work presents the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark, and reveals that a language’s resource level is the most important feature in determining ChatGPT's relative ability to translate it, and suggests thatChatGPT is especially disadvantaged for LRLs and African languages.""}
Url: https://arxiv.org/pdf/2309.07423
Answer: "
"What can be used to attack multimodal models that allow users to provide images?
","['daniel fried_2b2d9ee18507aa89a7f1ecba0bf68844c7d9aa75_content_1.txt', 'daphne ippolito_8724579d3f126e753a0451d98ff57b165f722e72_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What can be used to attack multimodal models that allow users to provide images?

Context: Title: Grounding Language Models to Images for Multimodal Inputs and Outputs
Authors: Jing Yu Koh, Ruslan Salakhutdinov, Daniel Fried
Section: D. Current Limitations and Broader Impacts
it is essential to test and analyze data used to mitigate the risk of training large multimodal models (Birhane et al., 2021). This will involve filtering of images, rigorous testing of model biases (for both image and text content), and more.
Faculty Name: daphne ippolito
Paperid: 8724579d3f126e753a0451d98ff57b165f722e72
Title: Are aligned neural networks adversarially aligned?
Year: 2023
Abstract: Large language models are now tuned to align with the goals of their creators, namely to be""helpful and harmless.""These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study to what extent these models remain aligned, even when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs. However the recent trend in large-scale ML models is multimodal models that allow users to provide images that influence the text that is generated. We show these models can be easily attacked, i.e., induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image. We conjecture that improved NLP attacks may demonstrate this same level of adversarial control over text-only models.
Authors: Nicholas Carlini, Milad Nasr, Christopher A. Choquette-Choo, Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tramèr, Ludwig Schmidt
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'It is shown that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models, and conjecture that improved NLP attacks may demonstrate this same level of adversarial control over text-only models.'}
Url: http://arxiv.org/pdf/2306.15447
Answer: "
"In fall 2023, What are the units for unit 02614?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, What are the units for unit 02614?

Context: In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Professional Issues for Computational and Automated Scientists' with Course ID 02602 and Section A offers 3.0 units. The Class meets Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Brasier, DeBlasio located in Building POS, Room 152.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Algorithms and Advanced Data Structures' with Course ID 02613 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 09:00AM and 09:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Yu located in Building DH, Room 2315.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'String Algorithms' with Course ID 02614 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kingsford located in Building CFA, Room 102.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Essential Mathematics and Statistics for Scientists' with Course ID 02680 and Section NA offers 9.0 units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Lugo-Martinez, DeBlasio located in Building POS, Room 153.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Essential Mathematics and Statistics for Scientists' with Course ID 02680 and Section A offers 9.0 units. The Class meets Friday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor DeBlasio, Lugo-Martinez located in Building SH, Room 105.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'M.S. Thesis Research' with Course ID 02700 and Section A offers 1-12 units. The Class meets Schedule will be added between NA and NA ET.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Professional Issues for Computational and Automated Scientists' with Course ID 02602 and Section A offers 3.0 units. The Class meets Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Brasier, DeBlasio located in Building POS, Room 152.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Algorithms and Advanced Data Structures' with Course ID 02613 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 09:00AM and 09:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Yu located in Building DH, Room 2315.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'String Algorithms' with Course ID 02614 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kingsford located in Building CFA, Room 102.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Essential Mathematics and Statistics for Scientists' with Course ID 02680 and Section NA offers 9.0 units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Lugo-Martinez, DeBlasio located in Building POS, Room 153.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Essential Mathematics and Statistics for Scientists' with Course ID 02680 and Section A offers 9.0 units. The Class meets Friday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor DeBlasio, Lugo-Martinez located in Building SH, Room 105.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'M.S. Thesis Research' with Course ID 02700 and Section A offers 1-12 units. The Class meets Schedule will be added between NA and NA ET.
Answer: "
"Where was the paper titled ""Computational Language Acquisition with Theory of Mind"" published?
","['graham neubig_e7b3b692b0816821aafc0d354749bc3802cbf6ac_metadata.txt', 'yonatan bisk_e7b3b692b0816821aafc0d354749bc3802cbf6ac_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Where was the paper titled ""Computational Language Acquisition with Theory of Mind"" published?

Context: Faculty Name: graham neubig
Paperid: e7b3b692b0816821aafc0d354749bc3802cbf6ac
Title: Computational Language Acquisition with Theory of Mind
Year: 2023
Abstract: Unlike current state-of-the-art language models, young children actively acquire language through interactions with their surrounding environment and caretakers. One mechanism that has been argued to be critical to language learning is the ability to infer the mental states of other agents in social environments, coined Theory of Mind (ToM) by Premack&Woodruff (1978). Drawing inspiration from the modern operationalized versions of ToM implemented in Rabinowitz et al. (2018) and Zhu et al. (2021), we build language-learning agents equipped with ToM, and measure its effects on the learning process. We model ToM by giving the speaker agent an internal listener model that is trained alongside the speaker and used to rerank potential utterances. We experiment with varying task difficulty, hypothesizing that models will acquire more complex language to adapt to stronger environmental pressures. We find that training speakers with a highly weighted ToM listener component leads to performance gains in our image referential game setting. We also find some evidence that increasing task difficulty in the training process results in more fluent and precise utterances in evaluation. This suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition.
Authors: Andy T. Liu, Hao Zhu, Emmy Liu, Yonatan Bisk, Graham Neubig
Venue: International Conference on Learning Representations
Tldr: {'model': 'tldr@v2.0.0', 'text': ""It is found that training speakers with a highly weighted ToM listener component leads to performance gains in the authors' image referential game setting, and suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition.""}
Url: http://arxiv.org/pdf/2303.01502
Faculty Name: yonatan bisk
Paperid: e7b3b692b0816821aafc0d354749bc3802cbf6ac
Title: Computational Language Acquisition with Theory of Mind
Year: 2023
Abstract: Unlike current state-of-the-art language models, young children actively acquire language through interactions with their surrounding environment and caretakers. One mechanism that has been argued to be critical to language learning is the ability to infer the mental states of other agents in social environments, coined Theory of Mind (ToM) by Premack&Woodruff (1978). Drawing inspiration from the modern operationalized versions of ToM implemented in Rabinowitz et al. (2018) and Zhu et al. (2021), we build language-learning agents equipped with ToM, and measure its effects on the learning process. We model ToM by giving the speaker agent an internal listener model that is trained alongside the speaker and used to rerank potential utterances. We experiment with varying task difficulty, hypothesizing that models will acquire more complex language to adapt to stronger environmental pressures. We find that training speakers with a highly weighted ToM listener component leads to performance gains in our image referential game setting. We also find some evidence that increasing task difficulty in the training process results in more fluent and precise utterances in evaluation. This suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition.
Authors: Andy T. Liu, Hao Zhu, Emmy Liu, Yonatan Bisk, Graham Neubig
Venue: International Conference on Learning Representations
Tldr: {'model': 'tldr@v2.0.0', 'text': ""It is found that training speakers with a highly weighted ToM listener component leads to performance gains in the authors' image referential game setting, and suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition.""}
Url: http://arxiv.org/pdf/2303.01502
Answer: "
"In fall 2023, When is unit 02761 on Tuesdays and Thursdays?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, When is unit 02761 on Tuesdays and Thursdays?

Context: In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Professional Issues for Computational and Automated Scientists' with Course ID 02602 and Section A offers 3.0 units. The Class meets Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Brasier, DeBlasio located in Building POS, Room 152.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Algorithms and Advanced Data Structures' with Course ID 02613 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 09:00AM and 09:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Yu located in Building DH, Room 2315.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'String Algorithms' with Course ID 02614 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kingsford located in Building CFA, Room 102.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Essential Mathematics and Statistics for Scientists' with Course ID 02680 and Section NA offers 9.0 units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Lugo-Martinez, DeBlasio located in Building POS, Room 153.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Essential Mathematics and Statistics for Scientists' with Course ID 02680 and Section A offers 9.0 units. The Class meets Friday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor DeBlasio, Lugo-Martinez located in Building SH, Room 105.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'M.S. Thesis Research' with Course ID 02700 and Section A offers 1-12 units. The Class meets Schedule will be added between NA and NA ET.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Professional Issues for Computational and Automated Scientists' with Course ID 02602 and Section A offers 3.0 units. The Class meets Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Brasier, DeBlasio located in Building POS, Room 152.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Algorithms and Advanced Data Structures' with Course ID 02613 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 09:00AM and 09:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Yu located in Building DH, Room 2315.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'String Algorithms' with Course ID 02614 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kingsford located in Building CFA, Room 102.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Essential Mathematics and Statistics for Scientists' with Course ID 02680 and Section NA offers 9.0 units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Lugo-Martinez, DeBlasio located in Building POS, Room 153.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Essential Mathematics and Statistics for Scientists' with Course ID 02680 and Section A offers 9.0 units. The Class meets Friday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor DeBlasio, Lugo-Martinez located in Building SH, Room 105.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'M.S. Thesis Research' with Course ID 02700 and Section A offers 1-12 units. The Class meets Schedule will be added between NA and NA ET.
Answer: "
"What class is taught by Eric Nyberg and Teruko Mitamura?
","['teruko mitamura_444737639aeea4e1e616509e368afb0bae8f89d6_metadata.txt', 'combined_metadata_final.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What class is taught by Eric Nyberg and Teruko Mitamura?

Context: Faculty Name: teruko mitamura
Paperid: 444737639aeea4e1e616509e368afb0bae8f89d6
Title: Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting for Multilingual Document-Grounded QA
Year: 2023
Abstract: The DialDoc 2023 shared task has expanded the document-grounded dialogue task to encompass multiple languages, despite having limited annotated data. This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-agnostic approach is superior. Additionally, the study investigates the impact of query rewriting techniques using large language models, such as ChatGPT, on multilingual, document-grounded question-answering systems. The experiments conducted demonstrate that, for the examples examined, query rewriting does not enhance performance compared to the original queries. This failure is due to topic switching in final dialogue turns and irrelevant topics being considered for query rewriting.
Authors: Srinivas Gowriraj, Soham Dinesh Tiwari, Mitali Potnis, Srijan Bansal, T. Mitamura, Eric Nyberg
Venue: Workshop on Document-grounded Dialogue and Conversational Question Answering
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-gnostic approach is superior.'}
Url: https://aclanthology.org/2023.dialdoc-1.11.pdf
Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Introduction to Question Answering' with Course ID 11697 and Section A offers 12.0 units. The Class meets Monday Wednesday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Nyberg, Mitamura located in Building PH, Room A18A.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'LTI Colloquium' with Course ID 11700 and Section A offers 6.0 units. The Class meets Friday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Bisk, Diaz located in Building POS, Room A35.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'LTI Colloquium' with Course ID 11700 and Section PP offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Lisbon, Portugal location,led by experienced instructor Diaz, Bisk located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Advanced Natural Language Processing' with Course ID 11711 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Frederking, Fried located in Building TEP, Room 1403.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Graduate Seminar on Dialog Processing' with Course ID 11716 and Section A offers 6.0 units. The Class meets Tuesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rudnicky located in Building PH, Room A21A.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Linguistics Lab' with Course ID 11723 and Section A offers 6.0 units.
Answer: "
"When is the Holi celebration at the Spring Carnival?
","['Apr-14_Eventno_3_Holi.txt', 'Feb-5_Eventno_1_FacesofCarnivalPhotoMosaic.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When is the Holi celebration at the Spring Carnival?

Context: Event: Holi
Date: 4/14/24
Time: 2:30 PM-6:30 PM ET
Participants/Audience: Open to entire CMU community 
Event Details: 
Join us for a delightful celebration of Holi at CMU! Embrace the start of spring as we celebrate the festival of colors during Carnival Weekend. Experience the charm of traditional South Asian dance teams and a special guest performer who will add a touch of magic to our color-throwing ceremony. Best of all, everyone is welcome to join in the festivities!

Note: Time will be confirmed in the coming weeks. Ticket information will be added here when live (likely late March).
Event: Faces of Carnival Photo Mosaic
Date: 2/5/24 – 4/15/24
Time: 8:00 AM-9:00 PM ET
Participants/Audience: Open to entire CMU community 
Event Details: 
Submit your photo by April 16 to be included in the Faces of Carnival. Look for your image — along with hundreds of others from the worldwide CMU community — when we share the mosaic on social media and in email after Spring Carnival 2024.
Answer: "
"At what conference was ""BASS: Block-wise Adaptation for Speech Summarization"" published?
","['bhiksha raj_3bd320ddb25886417ae90011b00f13f5d558097b_content_1.txt', 'rita singh_3bd320ddb25886417ae90011b00f13f5d558097b_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: At what conference was ""BASS: Block-wise Adaptation for Speech Summarization"" published?

Context: Title: BASS: Block-wise Adaptation for Speech Summarization
Authors: Roshan Sharma, Kenneth Zheng, Rita Singh, Bhiksha Raj
Section: 7. References
Workshop (ASRU), 2019, pp. 920–927. [15] E. Tsunoo, Y. Kashiwagi, and S. Watanabe, “Streaming transformer asr with blockwise synchronous beam search,” in 2021 IEEE Spoken Language Technology Workshop (SLT), 2021, pp. 22–29. [16] Y. Shi, Y. Wang, C. Wu, C.-F. Yeh, J. Chan, F. Zhang, D. Le, and M. Seltzer, “Emformer: Efficient memory transformer based acoustic model for low latency streaming speech recognition,” in ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021, pp. 6783– 6787. [17] X. Ma, Y. Wang, M. J. Dousti, P. Koehn, and J. Pino, “Streaming simultaneous speech translation with augmented memory transformer,” in ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021, pp. 7523–7527. [18] Y. Wang, H. Lv, D. Povey, L. Xie, and S. Khudanpur, “Wake word detection with streaming transformers,” in ICASSP 2021- 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 5864–5868. [19] S. Kim and F. Metze, “Dialog-context aware end-to-end speech recognition,” in 2018 IEEE Spoken Language Technology Workshop, SLT 2018, Athens, Greece, December 18-21, 2018. IEEE, 2018, pp. 434–440. [Online].
Title: BASS: Block-wise Adaptation for Speech Summarization
Authors: Roshan Sharma, Kenneth Zheng, Rita Singh, Bhiksha Raj
Section: 7. References
Workshop (ASRU), 2019, pp. 920–927. [15] E. Tsunoo, Y. Kashiwagi, and S. Watanabe, “Streaming transformer asr with blockwise synchronous beam search,” in 2021 IEEE Spoken Language Technology Workshop (SLT), 2021, pp. 22–29. [16] Y. Shi, Y. Wang, C. Wu, C.-F. Yeh, J. Chan, F. Zhang, D. Le, and M. Seltzer, “Emformer: Efficient memory transformer based acoustic model for low latency streaming speech recognition,” in ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021, pp. 6783– 6787. [17] X. Ma, Y. Wang, M. J. Dousti, P. Koehn, and J. Pino, “Streaming simultaneous speech translation with augmented memory transformer,” in ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021, pp. 7523–7527. [18] Y. Wang, H. Lv, D. Povey, L. Xie, and S. Khudanpur, “Wake word detection with streaming transformers,” in ICASSP 2021- 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 5864–5868. [19] S. Kim and F. Metze, “Dialog-context aware end-to-end speech recognition,” in 2018 IEEE Spoken Language Technology Workshop, SLT 2018, Athens, Greece, December 18-21, 2018. IEEE, 2018, pp. 434–440. [Online].
Answer: "
"What year did Andrew Carnegie die?
","['cmuhistory_d402.txt', 'kiltieband_d406.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What year did Andrew Carnegie die?

Context: History - CMU - Carnegie Mellon University Carnegie Mellon University ——— Search Search CMU › About › History Andrew Carnegie A self-educated ""working boy"" who loved books, Andrew Carnegie emigrated from Scotland in 1848 and settled in Pittsburgh, Pa. Attending night school and borrowing books, Carnegie went from factory worker in a textile mill to successful entrepreneur and industrialist. He rose to prominence by founding what became the world's largest steel producing company by the end of the 19th century. Carnegie Technical Schools At one point the richest man in the world, Carnegie believed that ""to die rich is to die disgraced."" He turned his attention to writing, social activism and philanthropy, determined to establish educational opportunities for the general public where few existed. In 1900, he donated $1 million for the creation of a technical institute for the city of Pittsburgh, envisioning a school where working-class men and women of Pittsburgh could learn practical skills, trades and crafts that would enhance their careers, lives and communities. ""My heart is in the work,"" he stated, which would become part of the school's official motto. The Carnegie Technical Schools offered two- and three-year certificates in the arts as well as in engineering disciplines and included a college for women, Margaret Morrison Carnegie College. Carnegie Tech – Early Years Soon faced with the demand for baccalaureate programs, Carnegie Technical Schools began offering bachelor's degrees through its College of Engineering and College of Fine Arts, becoming the Carnegie Institute of Technology, or ""Carnegie Tech."" During the first half of the 20th century, with support from Andrew Carnegie and other funders, Carnegie Tech laid the foundation for a school on the cutting edge. Some key developments were: It expanded from two buildings into an elegant 20th century campus designed in the beaux arts architectural style, housing a wealth of machine shops, studios and laboratories — the hands-on center of learning that persists today. It pioneered conservatory degree programs in music and drama, in addition to visual art and design programs. The first U.S. drama degree was awarded in 1914 at Carnegie Tech. It began offering graduate degrees. In 1919, the first doctorate (in civil engineering) was awarded to Mao Yisheng, a student from China.
The Kiltie Band
The Kiltie Band began in 1908 with a group of just seven students dedicated to supporting Carnegie Tech football, and today's Kiltie Band continues a tradition of excellence originated over a century ago. The Band took the field for its first official performance on November 25th, 1922, on what would have been Andrew Carnegie's 87th birthday.
The Kiltie Marching Band consists of musicians and colorguard and plays at all Carnegie Mellon home football games. There is a new show every week with music ranging from traditional marches and oldies to current pop tunes and jazz standards.
After football season, the Kiltie Concert Band plays a holiday concert and two spring concerts, including a performance at Carnegie Mellon’s Spring Carnival. Additionally, a part of the band gets together and acts as a pep band for the basketball teams.
The Kiltie Band is a campus organization recognized by Student Senate and administered by the Department of Athletics, Physical Education and Recreation. Membership is open to all members of the campus/community without audition. Participants must do their best to promote the success of the ensemble through attendance, attitude, and dependability.
FAQs
Q: When are rehearsals?
A: Football season: Mondays and Thursdays from 5:30 p.m.-7:30 p.m. and Game Day 9:30-11:00a.m. Concert Season: Mondays and Thursdays from 5:30-6:30 p.m.
Location is the CUC Studio Theater.
Q: When are performances?
A: The Kiltie Band performs at all home football games, a holiday concert, and two spring concerts. Times and dates for all performances are announced at the first practice.
Q: Are there auditions?
A: No, any member of the campus community with music experience is able to join the Kiltie Band!
Q: Do I have to memorize music?
A: No, the music is changed for every show. You should invest in a good and trusty lyre.
Q: When is the first rehearsal?
A: On the first day of class (Monday) at 5:30 p.m. in the CUC Studio Theater. If you have questions, please email the Direcator at
jolisar@andrew.cmu.edu
.
Q: Can I borrow a school instrument?
A: There are several Kiltie-owned instruments available for your use.
Answer: "
"In spring 2024, What is the location of course 10500?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, What is the location of course 10500?

Context: In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Deep Reinforcement Learning & Control' with Course ID 10403 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Fragkiadaki located in Building GHC, Room 4215.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Machine Learning with Large Datasets (Undergraduate)' with Course ID 10405 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Gordon, Talwalkar located in Building DH, Room 2210.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Foundations of Learning, Game Theory, and Their Connections' with Course ID 10422 and Section A offers 12.0 units. The Class meets Monday Wednesday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Balcan located in Building GHC, Room 4211.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Generative AI' with Course ID 10423 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Gormley, Li, Lipton located in Building GHC, Room 4401.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Senior Research Project' with Course ID 10500 and Section A offers 612 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Gormley located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Independent Study' with Course ID 10520 and Section A offers VAR units. The Class meets Schedule will be added between NA and NA ET.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Deep Reinforcement Learning & Control' with Course ID 10403 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Fragkiadaki located in Building GHC, Room 4215.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Machine Learning with Large Datasets (Undergraduate)' with Course ID 10405 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Gordon, Talwalkar located in Building DH, Room 2210.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Foundations of Learning, Game Theory, and Their Connections' with Course ID 10422 and Section A offers 12.0 units. The Class meets Monday Wednesday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Balcan located in Building GHC, Room 4211.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Generative AI' with Course ID 10423 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Gormley, Li, Lipton located in Building GHC, Room 4401.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Senior Research Project' with Course ID 10500 and Section A offers 612 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Gormley located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Independent Study' with Course ID 10520 and Section A offers VAR units. The Class meets Schedule will be added between NA and NA ET.
Answer: "
"Which LTI prof co-authored the paper titled ""AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual Representation Models""?
","['shinji watanabe_ef567580e167c3e7c546345df93d644be5d4f66f_metadata.txt', 'shinji watanabe_ef567580e167c3e7c546345df93d644be5d4f66f_content_3.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which LTI prof co-authored the paper titled ""AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual Representation Models""?

Context: Faculty Name: shinji watanabe
Paperid: ef567580e167c3e7c546345df93d644be5d4f66f
Title: AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual Representation Models
Year: 2023
Abstract: Audio-visual representation learning aims to develop systems with human-like perception by utilizing correlation between auditory and visual information. However, current models often focus on a limited set of tasks, and generalization abilities of learned representations are unclear. To this end, we propose the AV-SUPERB benchmark that enables general-purpose evaluation of unimodal audio/visual and bimodal fusion representations on 7 datasets covering 5 audio-visual tasks in speech and audio processing. We evaluate 5 recent self-supervised models and show that none of these models generalize to all tasks, emphasizing the need for future study on improving universal model performance. In addition, we show that representations may be improved with intermediate-task fine-tuning and audio event classification with AudioSet serves as a strong intermediate task. We release our benchmark with evaluation code and a model submission platform to encourage further research in audio-visual learning.
Authors: Yuan Tseng, Layne Berry, Yi-Ting Chen, I-Hsiang Chiu, Hsuan-Hao Lin, Max Liu, Puyuan Peng, Yi-Jen Shih, Hung-Yu Wang, Haibin Wu, Po-Yao Huang, Chun-Mao Lai, Shang-Wen Li, David F. Harwath, Yu Tsao, Shinji Watanabe, Abdel-rahman Mohamed, Chi-Luen Feng, Hung-yi Lee
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'The AV-SUPERB benchmark is proposed that enables general-purpose evaluation of unimodal audio/visual and bimodal fusion representations on 7 datasets covering 5 audio-visual tasks in speech and audio processing and shows that representations may be improved with intermediate-task fine-tuning and audio event classification with AudioSet serves as a strong intermediate task.'}
Url: https://arxiv.org/pdf/2309.10787
Title: AV-SUPERB: A MULTI-TASK EVALUATION BENCHMARK FOR AUDIO-VISUAL REPRESENTATION MODELS
Authors: Yuan Tseng, Layne Berry, Yi-Ting Chen, I-Hsiang Chiu, Hsuan-Hao Lin, Max Liu, Puyuan Peng, Yi-Jen Shih, Hung-Yu Wang, Haibin Wu, Po-Yao Huang, Chun-Mao Lai, Shang-Wen Li, David Harwath, Yu Tsao, Shinji Watanabe, Abdelrahman Mohamed, Chi-Luen Feng, Hung-yi Lee
Section: 4.4. Layer-wise Contribution Analysis
Shi et al., “Learning audio-visual speech representation by masked multimodal cluster prediction,” in ICLR, 2022. [36] Himangi Mittal et al., “Learning state-aware visual representations from audible interactions,” in NeurIPS, 2022. [37] Sangho Lee et al., “Parameter efficient multimodal transformers for video representation learning,” in ICLR, 2021. [38] Po-Yao Huang et al., “Mavil: Masked audio-video learners,” arXiv preprint arXiv:2212.08071, 2022. [39] Ankita Pasad et al., “Layer-wise analysis of a self-supervised speech representation model,” in ASRU, 2021. [40] Jort F. Gemmeke et al., “Audio set: An ontology and humanlabeled dataset for audio events,” in ICASSP, 2017. [41] Honglie Chen et al., “Vggsound: A large-scale audio-visual dataset,” in ICASSP, 2020. [42] Sanyuan Chen et al., “Wavlm: Large-scale self-supervised pretraining for full stack speech processing,” IEEE Journal of Selected Topics in Signal Processing, 2022. [43] Jason Phang et al., “Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks,” arXiv preprint arXiv:1811.01088, 2018. [44] Alex Wang et al., “Can you tell me how to get past sesame street? sentence-level pretraining beyond language modeling,” in ACL, 2019.
Answer: "
"In fall 2023, Who are the instructors for unit 02512?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, Who are the instructors for unit 02512?

Context: In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Undergraduate Research in Computational Biology' with Course ID 02500 and Section A offers 6-24 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Compeau located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Methods for Biological Modeling and Simulation' with Course ID 02512 and Section A offers 9.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Carja, Schwartz located in Building HH, Room B131.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Medicine' with Course ID 02518 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Wu located in Building WEH, Room 5403.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Programming for Scientists' with Course ID 02601 and Section NA offers 12.0 units. The Class meets Monday Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Compeau located in Building POS, Room 152.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Programming for Scientists' with Course ID 02601 and Section A offers 12.0 units. The Class meets Friday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Compeau located in Building POS, Room 152.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Professional Issues for Computational and Automated Scientists' with Course ID 02602 and Section A offers 3.0 units. The Class meets Thursday between 11:00AM and 12:20PM ET.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Undergraduate Research in Computational Biology' with Course ID 02500 and Section A offers 6-24 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Compeau located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Methods for Biological Modeling and Simulation' with Course ID 02512 and Section A offers 9.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Carja, Schwartz located in Building HH, Room B131.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Medicine' with Course ID 02518 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Wu located in Building WEH, Room 5403.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Programming for Scientists' with Course ID 02601 and Section NA offers 12.0 units. The Class meets Monday Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Compeau located in Building POS, Room 152.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Programming for Scientists' with Course ID 02601 and Section A offers 12.0 units. The Class meets Friday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Compeau located in Building POS, Room 152.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Professional Issues for Computational and Automated Scientists' with Course ID 02602 and Section A offers 3.0 units. The Class meets Thursday between 11:00AM and 12:20PM ET.
Answer: "
"Who taught the first freshman-level computer programming course at CMU?
","['history_d401.txt', 'fact_sheet_d407.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who taught the first freshman-level computer programming course at CMU?

Context: A history of SCS | SCS25 - Carnegie Mellon University School of Computer Science Skip to main content Legal Events Register 25 Things History Video A history of SCS For an expanded history of the School of Computer Science and its predecessors at CMU, read ""Institutional Memories"" in the Summer 2014 issue of The Link magazine.In 2014, the School of Computer Science celebrated its 25th year as a stand-alone college within Carnegie Mellon University. It was arguably the first college devoted solely to computer science in the United States, and a model for others that followed. But CMU’s computer science era begins much earlier—in 1956, with the arrival of an IBM 650 computer on the campus of what was then known as Carnegie Institute of Technology. The IBM 650 had magnetic-drum memory and a processing speed of approximately 60 instructions per second. Herb Simon (H’90), associate dean of the Graduate School of Industrial Administration—now known as CMU’s Tepper School of Business—established Carnegie Tech’s first Computation Center with the help of its first director, Alan Perlis (S’42).First freshman-level computer science courseIn 1956 and 1957, Simon, Allen Newell (IA’57) and Cliff Shaw of RAND designed the Logic Theorist, a computer program that could develop proofs for theorems in much the same way a human would work. They also developed linked-list data structures, the foundation of computer programming. Perlis, Simon and Newell are credited with defining the term “computer science” as “the theory and design of computers,” as well as (in Newell’s words) “the study of the phenomena arising from them.” In 1958, Perlis began teaching the first freshman-level computer programming course in the United States at Carnegie Tech.Computer science Ph.D. program createdIn 1961, the Computation Center and its newest computer, a Bendix G-20, were moved to recently completed Scaife Hall. That same year, Carnegie Tech created an interdisciplinary Ph.D. program called Systems and Communications Sciences, combining elements of computer science, mathematics, psychology, business and electrical engineering. The university’s first computer science Ph.D.s were graduates of this program.Computer Science Department establishedIn 1965, Carnegie Tech established its Computer Science Department, or CSD, with a $5 million grant from the R.K. Mellon Foundation.
13%  International
SCHOOLS AND COLLEGES
College of Engineering 
College of Fine Arts
Dietrich College of Humanities  
and Social Sciences
Heinz College of Information Systems 
and Public Policy
Mellon College of Science
School of Computer Science
Tepper School of Business
cmu.edu/assets/pdfs/cmu-fact-sheet.pdf    |   cmu.edu/about/awards.html
March 2020
BIG IDEAS THAT SHAPE YOUR WORLD START HERE
1,000+ companies greenlighted 
across the U.S. and internationally 
Cognitive tutors, which improve 
the math skills of 500,000+ 
students each year 
The first U.S. school to award  
a degree in drama in 1914
James Gosling (SCS 1983) 
invented the Java computer 
programming language.
The first exclusive higher 
education partner of the Tony 
Awards®, co-creating the first, 
national recognition program 
honoring K-12 theatre educators 
The first smile in an email, created 
in 1982 by professor Scott Fahlman
Kevlar Fiber, invented by alumna 
Stephanie Kwolek (MM 1946)
The Last Lecture, written  
by alumni Randy Pausch (SCS 1988)  
and Jeffrey Zaslow (DC 1980)
In 1919, CMU’s first Ph.D. was 
awarded to Mao Yisheng, father  
of Chinese bridge construction. 
Duolingo, invented by professor  
Luis von Ahn (CS 2003, 2005), had  
10 million downloads in 12 months 
and was named Apple’s 2013 app  
of the year.
:-)
LEADERSHIP
Farnam Jahanian  
President
David Coulter 
Chair, Board of Trustees
James H. Garrett Jr.
Answer: "
"What is the publication venue of ""Assessment of quality of life after upper extremity transplantation: Framework for patient-reported outcome scale domains""?
","['lori levin_52a97ad16605c18e23c9750a388a26a9cdf12200_content_0.txt', 'lori levin_52a97ad16605c18e23c9750a388a26a9cdf12200_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the publication venue of ""Assessment of quality of life after upper extremity transplantation: Framework for patient-reported outcome scale domains""?

Context: Title: Assessment of quality of life after upper extremity transplantation: Framework for patient-reported outcome scale domains
Authors: Paul D. Marasco, Callie E. Tyner
Section: Publisher’s note
All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.
Faculty Name: lori levin
Paperid: 52a97ad16605c18e23c9750a388a26a9cdf12200
Title: Assessment of quality of life after upper extremity transplantation: Framework for patient-reported outcome scale domains
Year: 2023
Abstract: Upper extremity transplantation offers the promise of restored function and regained quality of life (QOL) for individuals who have sustained hand or arm amputation. However, a major challenge for this procedure becoming an accessible treatment option for patients is the lack of standard measures to document benefits to QOL. Patient-reported outcomes (PRO) measures are well-suited for this kind of intervention, where the perspective of the patient is central to defining treatment success. To date, qualitative work with experts, clinicians, and patients has been used to identify the most important domains of QOL for PRO item development. Specifically, our group’s qualitative work has identified several domains of QOL that are unique to individuals who have received upper extremity transplants, which are distinct from topics covered by existing PRO measures. These include emotional and social aspects of upper extremity transplant, such as Expectations and Perceived Outcomes, Integration and Assimilation of Transplant, Fitting in, and Post-Surgical Challenges and Complications. The broad topic of Satisfaction with Transplant was subdivided into three subtopics: Function, Sensation, and Aesthetics. Satisfaction with Sensation was also identified as a unique domain not evaluated by existing PRO measures. This report operationalizes these eight QOL domains by presenting scoping definitions. This manuscript describes the work that has been completed for domain characterization as an early step toward developing standardized PRO measures to evaluate these important outcomes specific to upper extremity transplantation.
Authors: Callie E Tyner, J. Slotkin, Pamela A. Kisala, Lori S. Levin, Scott M. Tintle, D. Tulsky
Venue: Frontiers in Psychology
Tldr: {'model': 'tldr@v2.0.0', 'text': 'Qualitative work with experts, clinicians, and patients has identified several domains of QOL that are unique to individuals who have received upper extremity transplants, which are distinct from topics covered by existing PRO measures.'}
Url: https://www.frontiersin.org/articles/10.3389/fpsyg.2022.989593/pdf
Answer: "
"What are the results of training 1.1B parameter models on Java, JavaScript, and Python subsets of The Stack and evaluating them on MultiPL-E?
","['daniel fried_1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a_metadata.txt', 'louis philippe morency_40fb36ee67fdde99b196b4d1772de114aa821698_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the results of training 1.1B parameter models on Java, JavaScript, and Python subsets of The Stack and evaluating them on MultiPL-E?

Context: Faculty Name: daniel fried
Paperid: 1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a
Title: SantaCoder: don't reach for the stars!
Year: 2023
Abstract: The BigCode project is an open-scientific collaboration working on the responsible development of large language models for code. This tech report describes the progress of the collaboration until December 2022, outlining the current state of the Personally Identifiable Information (PII) redaction pipeline, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data. We train 1.1B parameter models on the Java, JavaScript, and Python subsets of The Stack and evaluate them on the MultiPL-E text-to-code benchmark. We find that more aggressive filtering of near-duplicates can further boost performance and, surprisingly, that selecting files from repositories with 5+ GitHub stars deteriorates performance significantly. Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and infilling on the Java, JavaScript, and Python portions of MultiPL-E, despite being a substantially smaller model. All models are released under an OpenRAIL license at https://hf.co/bigcode.
Title: MULTIZOO & MULTIBENCH: A Standardized Toolkit for Multimodal Deep Learning
Authors: Paul Pu Liang, Yiwei Lyu, Xiang Fan, Arav Agarwal, Yun Cheng, Louis-Philippe Morency, Ruslan Salakhutdinov, Antti Honkela
Section: 2 MULTIBENCH and MULTIZOO
Actions workflows, which automatically runs the test builds and is triggered every time new changes are incorporated. After making the desired changes and making sure all tests pass, users can create a pull request and the authors will merge these changes into the main branch. Together: In Algorithm 1, we show a sample code snippet in Python that loads a dataset, defines the unimodal and multimodal architectures, optimization objective, and training procedures, before running the evaluation protocol. Our toolkit is easy to use and trains models in less than 10 lines of code. By standardizing the implementation of each module and disentangling individual modules, optimizations, and training, MULTIZOO ensures accessibility and reproducibility of its algorithms. Algorithm 1 PyTorch code integrating MULTIBENCH datasets and MULTIZOO models. from datasets.get_data import get_dataloader from unimodals.common_models import ResNet, Transformer from fusions.common_fusions import MultInteractions from training_structures.gradient_blend import train, test # load Multimodal IMDB dataset traindata, validdata, testdata = get_dataloader(’multimodal_imdb’) out_channels = 3 # define ResNet and Transformer unimodal encoders encoders = [ResNet(in_channels=1, out_channels=3, layers=5), Transformer(in_channels=1, out_channels=3, layers=3)] # define a Multiplicative Interactions fusion layer fusion = MultInteractions([out_channels*8, out_channels*32], out_channels*32, ’matrix’) classifier = MLP(out_channels*32, 100, labels=23) # train using Gradient Blend algorithm model = train(encoders, fusion, classifier, traindata, validdata, epochs=100, optimtype=torch.optim.SGD, lr=0.01, weight_decay=0.0001) # test performance, complexity, robustness = test(model, testdata)
Answer: "
"In what year was Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning published?
","['sean welleck_papers.txt', 'eric xing_papers.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In what year was Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning published?

Context: List of 2023 Open Access papers by sean welleck are:
Self-Refine: Iterative Refinement with Self-Feedback
Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning
List of 2023 Open Access papers by eric xing are:
SlimPajama-DC: Understanding Data Combinations for LLM Training
Fusing Models with Complementary Expertise
Making Scalable Meta Learning Practical
3D Open-vocabulary Segmentation with Foundation Models
Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective
One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning
Defending Against Malicious Behaviors in Federated Learning with Blockchain
Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming
Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models
KD-DLGAN: Data Limited Image Generation via Knowledge Distillation
3D Semantic Segmentation in the Wild: Learning Generalized Models for Adverse-Condition Point Clouds
Cuttlefish: Low-Rank Model Training without All the Tuning
Does compressing activations help model parallel training?
Federated Learning as Variational Inference: A Scalable Expectation Propagation Approach
Memory-adaptive Depth-wise Heterogenous Federated Learning
Identification of Nonlinear Latent Hierarchical Models
StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields
Judging LLM-as-a-judge with MT-Bench and Chatbot Arena
Understanding Masked Autoencoders via Hierarchical Latent Variable Models
LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers
LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset
FedNAR: Federated Optimization with Normalized Annealing Regularization
US residents' preferences for sharing of electronic health record and genetic information: a discrete choice experiment.
Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning
GET: a foundation model of transcription across human cell types
Contextualized Networks Reveal Heterogeneous Transcriptomic Regulation in Tumors at Sample-Specific Resolution
Research on the Training Path of Live E-commerce Talents Oriented by Industry Development
Recent progresses on the gamma-ray observations of DAMPE
Effective Multi-Agent Deep Reinforcement Learning Control with Relative Entropy Regularization
Convolutional Neural Network Measurement of Non-Fiducial Electrons Cosmic-Rays Using the DAMPE Experiment.
Answer: "
"How much does it cost to apply for the MLT program if an application is submitted a week before the deadline?
","['mlt-student-handbook-2023-2024.txt', 'mlt-student-handbook-2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How much does it cost to apply for the MLT program if an application is submitted a week before the deadline?

Context: It is to the student's advantage to avoid switching advisors, especially late in 
their graduate studies, because forging a strong student-advisor relationship takes time. 
4.3 Optional Masters Thesis 
MLT students may optionally elect to complete a Masters Thesis; it is not required. Students 
who intend to apply for the PhD program should consider the MLT Thesis as one of several 
ways to demonstrate their research potential. Other possibilities include innovative research in 
a research project or class project work. 
The MLT Thesis must be completed according to the following guidelines. Note in particular the 
October 31 proposal deadline! 
 There are two course components required for students who elect to complete the M.S. 
Thesis: 11-928  Masters Thesis I (with the chosen thesis advisor, typically for 12 units, 
typically in the Fall of their second year) and 11-929  Masters Thesis II in the following 
Spring. 
 Each of these two courses counts towards 6 units of MLT-level LTI course credit; if the 
student goes on to an LTI PhD, the second course (11-929) counts towards one PhD lab 
requirement.   
 The MLT thesis may focus on either a significant research result or a significant 
implementation of a new technology. The student should work with their advisor to 
define an interesting but bounded thesis research project. The MLT thesis culminates in 
MLT Graduate Student Handbook 
Page 16 
 
a publication-quality paper that clearly describes the work in terms of its research 
significance. 
 The MLT thesis committee will be composed of a thesis advisor and two additional 
readers, to be chosen by the student in consultation with the advisor, during the Fall 
Masters Thesis I course. At least 2 of the 3 committee members must be SCS faculty 
working in Language Technologies. 
 To receive a final grade for the first semester's course, the student must have prepared 
a written proposal of at least five pages, describing the scientific thesis to be 
investigated, any relevant research already completed by the student, a comparison 
with related work by others, and a detailed description of the work to be done in the 
next semester. The proposal MUST be accepted by the committee by the end of 
October, or the student will not be allowed to enroll in 11-929  Masters Thesis II!
5.1.4 Independent Study 
For an Independent Study to satisfy an MLT students coursework requirements, it must be 
approved by the MLT Program Director in advance.  In consultation with the CMU faculty 
member who will be supervising the Independent Study, the student should produce a short 
(half-page to one page) description of the goals of the Independent Study, and how the results 
will be evaluated, and submit it to the Program Director before the end of the Add period of the 
semester of the proposed study. The study should be planned so that it is finished in time for 
the supervising faculty member to give a grade at the end of the semester.  Independent 
studies may be undertaken for 6 or 12 units. Normally only one Independent Study would be 
approved during a students MLT coursework. 
5.1.5 Transfer Credit 
An equivalent graduate course previously completed at another institution may be permitted to 
satisfy one of the MLT course requirements. The decision on whether a course may be 
transferred is made by the MLT Program Director. Typically, the student will provide the 
Program Director with the syllabus of the external course, and the Program Director will use 
that and the students transcript to make the decision.  
See the section on Definition of transfer credit versus course exemption. 
All MLT students are required to take a minimum of 96 units of coursework at CMU.  
5.1.6 External Internships 
MLT students may only engage in internships if their advisors recommend it, since the program 
is 24 months, including summers. International students are required to consult with the Office 
of International Education for eligibility before seeking an internship/co-op or signing an offer 
contract. 
We caution all students to be aware of potential intellectual property (IP) problems with 
internships, and to review any IP agreements with their advisors before signing them. It is 
possible to lose ownership of your ideas. 
Important: See also section 4.2.3, External Employment/Consulting. 
MLT Graduate Student Handbook 
Page 22 
 
5.1.7 Transferring Into the MLT Program 
Transfers into the MLT program are not permitted during a students first semester at CMU. 
Students must begin their study at CMU in the program that admitted them; this is a university 
policy.
Answer: "
"Tartan Athletics Club was launched in which year?
","['tartanfacts_d404_metadata.txt', 'tartanfacts_d404.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Tartan Athletics Club was launched in which year?

Context: {
  ""description"": ""p><strong>Who founded Carnegie Mellon University?<br>\n\n</strong> Carnegie Technical Schools was founded in 1900 by Andrew Carnegie. Twelve years late ..."",
  ""viewport"": ""width=device-width, initial-scale=1"",
  ""msapplication-TileColor"": ""#c41230"",
  ""msapplication-config"": ""/assets/favicons/browserconfig.xml"",
  ""theme-color"": ""#c41230"",
  ""fb:app_id"": ""280467664480"",
  ""og:locale"": ""en_US"",
  ""og:title"": ""Tartan Facts"",
  ""dcterms.title"": ""Tartan Facts"",
  ""og:description"": ""Tartan Facts"",
  ""dcterms.description"": ""Tartan Facts"",
  ""og:image"": ""https://athletics.cmu.edu/images/setup/thumbnail_default.jpg?max_width=1200&max_height=675"",
  "" og:image:alt"": ""Carnegie Mellon University Athletics thumbnail"",
  ""og:site_name"": ""Carnegie Mellon University Athletics"",
  ""og:url"": ""https://athletics.cmu.edu/athletics/tartanfacts"",
  ""dcterms.identifier"": ""https://athletics.cmu.edu/athletics/tartanfacts"",
  ""og:type"": ""article"",
  ""dcterms.type"": ""article""
}
Topic category is tartanfacts
Tartan Facts
Who founded Carnegie Mellon University?
Carnegie Technical Schools was founded in 1900 by Andrew Carnegie. Twelve years later it became known as the Carnegie Institute of Technology. In 1967, the school merged with Mellon Institute and became what is known today as Carnegie Mellon University.
What is a Tartan?
The Carnegie Mellon athletic teams are nicknamed the ""Tartans"" as a nod to Andrew Carnegie's Scottish heritage. A tartan is often misrepresented as a fierce warrior from either the Asian tundra or Scottish highlands. In actuality, a Tartan is a twilled woolen fabric with a plaid design. It is of Scottish origin and consists of stripes of various colors and widths against a solid ground, denoting a particular family lineage. The school's founder, Andrew Carnegie, was born in Dunfermline, Scotland, in 1835. Carnegie came to the United States in 1848 and founded Carnegie Technical Schools in Pittsburgh in 1900.
The Scottish terrier mascot performer sports Carnegie tartan attire, while the graphic mascot is wearing a plaid scarf around its neck. So what's the difference between tartan and plaid?
You'll know it's a tartan if...
• ""It's a check or pattern in a variety of colours in woven fabric in which bands of colour are repeated in equal proportion in warp (running lengthwise) and weft (running across).""
• ""Each stripe of the warp crosses every stripe of the weft, so when vertical and horizontal stripes of the same color cross, the result is solid color at the point of intersection.""
• ""The arrangement of colored threads is the same in the warp as in the weft.""
You can find our official tartan on various items in the
University Store
.
Source: ""Tartan: Romancing the Plaid,"" by Jeffrey Banks and Doria De La Chapelle
Official Mascot?
More than a century after Carnegie Mellon University opened its doors, an official mascot finally made its mark. Although students have dressed as a Scottish terrier — typically referred to as Scotty — for 50 years, it wasn't until 2007 that Carnegie Mellon officially welcomed the Scottish terrier as the university's first mascot.
In keeping with tradition, students, faculty, staff and alumni voted to name the new official mascot Scotty. But it's not just the costumed mascot that voters named.
Answer: "
"What is the full name of the conference where the paper Using counterfactual contrast to improve compositional generalization for multi-step quantitative reasonin, got published?
","['carolyn rose_117e1323677cb5d78ece0fd07b5cfa81618f4866_metadata.txt', 'carolyn rose_117e1323677cb5d78ece0fd07b5cfa81618f4866_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the full name of the conference where the paper Using counterfactual contrast to improve compositional generalization for multi-step quantitative reasonin, got published?

Context: Faculty Name: carolyn rose
Paperid: 117e1323677cb5d78ece0fd07b5cfa81618f4866
Title: Using counterfactual contrast to improve compositional generalization for multi-step quantitative reasoning
Year: 2023
Abstract: In quantitative question answering, compositional generalization is one of the main challenges of state of the art models, especially when longer sequences of reasoning steps are required. In this paper we propose CounterComp, a method that uses counterfactual scenarios to generate samples with compositional contrast. Instead of a data augmentation approach, CounterComp is based on metric learning, which allows for direct sampling from the training set and circumvents the need for additional human labels. Our proposed auxiliary metric learning loss improves the performance of three state of the art models on four recently released datasets. We also show how the approach can improve OOD performance on unseen domains, as well as unseen compositions. Lastly, we demonstrate how the method can lead to better compositional attention patterns during training.
Authors: Armineh Nourbakhsh, Sameena Shah, C. Rosé
Venue: Annual Meeting of the Association for Computational Linguistics
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes CounterComp, a method that uses counterfactual scenarios to generate samples with compositional contrast based on metric learning, which allows for direct sampling from the training set and circumvents the need for additional human labels.'}
Url: https://aclanthology.org/2023.acl-long.834.pdf
Title: Using counterfactual contrast to improve compositional generalization for multi-step quantitative reasoning
Authors: Armineh Nourbakhsh, Sameena Shah, Carolyn Rosé
Section: A For every submission:
run? 6 7 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)? No packages used. D 7 Did you use human annotators (e.g., crowdworkers) or research with human participants? Left blank. D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? No response. D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants’ demographic (e.g., country of residence)? No response. D3. Did you discuss whether and how consent was obtained from people whose data you’re using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? No response. D4. Was the data collection protocol approved (or determined exempt) by an ethics review board? No response. D5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? No response.
Answer: "
"Which was the the first U.S. school to award a degree in drama?
","['cmuhistory_d402.txt', 'fact_sheet_d407.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which was the the first U.S. school to award a degree in drama?

Context: The first U.S. drama degree was awarded in 1914 at Carnegie Tech. It began offering graduate degrees. In 1919, the first doctorate (in civil engineering) was awarded to Mao Yisheng, a student from China. It laid the groundwork for a research institution, recruiting leading scientists, offering sponsored fellowships with government and industry leaders and pioneering nontraditional interdisciplinary research, which brought together physicists, chemists and metallurgists, for example. Interdisciplinary research would become the hallmark of Carnegie Mellon research. It initiated the 'Carnegie Plan' in 1938, a new curriculum that required science and engineer students to take courses in humanities and social sciences in order to better understand the needs of society. Carnegie died in 1919, but his vision for an educated public lived on after him. Carnegie Tech - Post-war Years With the end of World War II, the latter half of the 20th century brought unprecedented growth to Carnegie Tech. In 1956, the arrival of the first IBM computer to campus was revolutionary, initiating a university culture where information technology pervaded virtually all areas of study. University culture also changed in 1973 when Margaret Morrison closed and women joined their male peers in classrooms and dorms. The times were changing, and Tech positioned itself at the forefront, opening three new schools: 1948: The Graduate School of Industrial Administration, later renamed the David A. Tepper School of Business, focusing on quantitative analysis and pioneering the field of management science. 1968: School of Urban and Public Affairs, later renamed the H. John Heinz III College, providing graduate training for work in the public sector. 1986: School of Computer Science, pioneering computing and artificial intelligence, led by interdisciplinary efforts of Allen Newell and Herbert Simon. Carnegie Mellon University In 1967, Carnegie Tech merged with the Mellon Institute, a science research center founded by the Mellon family of Pittsburgh, to become known as Carnegie Mellon University. The merger built upon a long history of support from the Mellons. It allowed Carnegie Mellon to establish the last of its current pillars: the Mellon College of Science and the College of Humanities and Social Sciences, now known as Marianna Brown Dietrich College of Humanities and Social Sciences.
13%  International
SCHOOLS AND COLLEGES
College of Engineering 
College of Fine Arts
Dietrich College of Humanities  
and Social Sciences
Heinz College of Information Systems 
and Public Policy
Mellon College of Science
School of Computer Science
Tepper School of Business
cmu.edu/assets/pdfs/cmu-fact-sheet.pdf    |   cmu.edu/about/awards.html
March 2020
BIG IDEAS THAT SHAPE YOUR WORLD START HERE
1,000+ companies greenlighted 
across the U.S. and internationally 
Cognitive tutors, which improve 
the math skills of 500,000+ 
students each year 
The first U.S. school to award  
a degree in drama in 1914
James Gosling (SCS 1983) 
invented the Java computer 
programming language.
The first exclusive higher 
education partner of the Tony 
Awards®, co-creating the first, 
national recognition program 
honoring K-12 theatre educators 
The first smile in an email, created 
in 1982 by professor Scott Fahlman
Kevlar Fiber, invented by alumna 
Stephanie Kwolek (MM 1946)
The Last Lecture, written  
by alumni Randy Pausch (SCS 1988)  
and Jeffrey Zaslow (DC 1980)
In 1919, CMU’s first Ph.D. was 
awarded to Mao Yisheng, father  
of Chinese bridge construction. 
Duolingo, invented by professor  
Luis von Ahn (CS 2003, 2005), had  
10 million downloads in 12 months 
and was named Apple’s 2013 app  
of the year.
:-)
LEADERSHIP
Farnam Jahanian  
President
David Coulter 
Chair, Board of Trustees
James H. Garrett Jr.
Answer: "
"In the paper ""Modeling Empathic Similarity in Personal Narratives"", what is the name of the dataset created for this task?
","['maarten sap_14ddefae2be4b5bb50b9fcb4a085e45fbecb5c5c_metadata.txt', 'maarten sap_14ddefae2be4b5bb50b9fcb4a085e45fbecb5c5c_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the paper ""Modeling Empathic Similarity in Personal Narratives"", what is the name of the dataset created for this task?

Context: Faculty Name: maarten sap
Paperid: 14ddefae2be4b5bb50b9fcb4a085e45fbecb5c5c
Title: Modeling Empathic Similarity in Personal Narratives
Year: 2023
Abstract: The most meaningful connections between people are often fostered through expression of shared vulnerability and emotional experiences in personal narratives. We introduce a new task of identifying similarity in personal stories based on empathic resonance, i.e., the extent to which two people empathize with each others' experiences, as opposed to raw semantic or lexical similarity, as has predominantly been studied in NLP. Using insights from social psychology, we craft a framework that operationalizes empathic similarity in terms of three key features of stories: main events, emotional trajectories, and overall morals or takeaways. We create EmpathicStories, a dataset of 1,500 personal stories annotated with our empathic similarity features, and 2,000 pairs of stories annotated with empathic similarity scores. Using our dataset, we fine-tune a model to compute empathic similarity of story pairs, and show that this outperforms semantic similarity models on automated correlation and retrieval metrics. Through a user study with 150 participants, we also assess the effect our model has on retrieving stories that users empathize with, compared to naive semantic similarity-based retrieval, and find that participants empathized significantly more with stories retrieved by our model. Our work has strong implications for the use of empathy-aware models to foster human connection and empathy between people.
Authors: Jocelyn Shen, Maarten Sap, Pedro Colon-Hernandez, Hae Won Park, C. Breazeal
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': ""A new task of identifying similarity in personal stories based on empathic resonance is introduced, i.e., the extent to which two people empathize with each others' experiences, as opposed to raw semantic or lexical similarity, as has predominantly been studied in NLP.""}
Url: http://arxiv.org/pdf/2305.14246
Title: Modeling Empathic Similarity in Personal Narratives
Authors: Jocelyn Shen, Maarten Sap, Pedro Colon-Hernandez, Hae Won Park, Cynthia Breazeal
Section: H Using LLMs as a Proxy for Human Annotations
Recent works raise the question of whether LLMs can be used to proxy human annotations (Gilardi et al., 2023). The motivation behind this method is that obtaining human labels across many pairs of stories is costly, and this cost only compounds as the number of stories in the corpus increases. As such, we provide additional analyses as to whether or not these models can truly perform at the same level as human annotators for our task, which involves heavy empathy and emotion reasoning. H.1 Individual Story Annotation We prompt ChatGPT (gpt-3.5-turbo) to generate summaries of each story’s main event, emotion, and moral, in addition to a list of reasons why a narrator might empathize with the story. We compare these summaries against human-written summaries using BLEU, ROUGE, METEOR, and BertScore (Table 10), showing that ChatGPT has relatively low performance across all four metrics. H.2 Paired Story Annotation We feed the same prompt given to human annotators into ChatGPT, asking for a Likert score from 1-4 for the empathic similarity between two stories. The Spearman’s correlation between human and ChatGPT generated labels is 0.22 (p < 0.001), indicating weakly positive correlation between human annotations and ChatGPT annotations. In addition, we perform a one-sample t-test on the meansquared error between automatically generated labels and human annotations across all story pairs in the training data, obtaining a p-value < 0.001, indicating that the mean of all the errors is nonzero with statistical significance. Finally, we bin the ChatGPT annotations into agree/disagree categories, and compute the classification precision (0.59), recall (0.40), F1 score (0.48), and accuracy (0.59) as compared to human gold labels. These scores offer insight as to how well ChatGPT predicts the direction of the empathic similarity annotation, but we see that accuracy is low when comparing to human labels.
Answer: "
"How many action types were included in the WebArena benchmark?
","['daniel fried_e41482f4ee984f17382f6cdd900df094d928be06_content_0.txt', 'graham neubig_e41482f4ee984f17382f6cdd900df094d928be06_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many action types were included in the WebArena benchmark?

Context: Title: WEBARENA: A REALISTIC WEB ENVIRONMENT FOR BUILDING AUTONOMOUS AGENTS
Authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig
Section: A.8 ADDITIONAL ERROR ANALYSIS
Observation Bias Realistic websites frequently present information on similar topics across various sections to ensure optimal user accessibility. However, a GPT-4 agent often demonstrates a tendency to latch onto the first related piece of information it encounters without sufficiently verifying its relevance or accuracy. For instance, the homepage of the E-Commerce CMS displays the best-selling items based on recent purchases, while historical best-seller data is typically accessed via a separate report. Presented with the task of “What is the top-1 best-selling product in 2022”, the GPT-4 agent defaults to leveraging the readily available information on the homepage, bypassing the necessary step of generating the report to obtain the accurate data. Failures in Observation Interpretation Interestingly, while GPT-4 is capable of summarizing the observations, it occasionally overlooks more granular information, such as the previously entered input. As in the right-hand example of Figure 11, [5172] StaticText indicates that the search term “DMV area” has already been entered. However, the agent disregards this detail and continuously issues the command type [2430] [DMV area] until it reaches the maximum step limit. Furthermore, the agent often neglects the previous action information that is provided alongside the observation. We hypothesize that these observed failures are related to the current pretraining and supervised fine-tuning on dialogues employed in GPT models Ouyang et al. (2022). These models are primarily trained to execute instructions given immediate observations (i.e.,, the dialogue history); thereby, they may exhibit a lack of explorations. Furthermore, in dialogue scenarios, subtle differences in NL expressions often have less impact on the overall conversation. As a result, models may tend to overlook minor variations in their observations.
Title: WEBARENA: A REALISTIC WEB ENVIRONMENT FOR BUILDING AUTONOMOUS AGENTS
Authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig
Section: A.8 ADDITIONAL ERROR ANALYSIS
Observation Bias Realistic websites frequently present information on similar topics across various sections to ensure optimal user accessibility. However, a GPT-4 agent often demonstrates a tendency to latch onto the first related piece of information it encounters without sufficiently verifying its relevance or accuracy. For instance, the homepage of the E-Commerce CMS displays the best-selling items based on recent purchases, while historical best-seller data is typically accessed via a separate report. Presented with the task of “What is the top-1 best-selling product in 2022”, the GPT-4 agent defaults to leveraging the readily available information on the homepage, bypassing the necessary step of generating the report to obtain the accurate data. Failures in Observation Interpretation Interestingly, while GPT-4 is capable of summarizing the observations, it occasionally overlooks more granular information, such as the previously entered input. As in the right-hand example of Figure 11, [5172] StaticText indicates that the search term “DMV area” has already been entered. However, the agent disregards this detail and continuously issues the command type [2430] [DMV area] until it reaches the maximum step limit. Furthermore, the agent often neglects the previous action information that is provided alongside the observation. We hypothesize that these observed failures are related to the current pretraining and supervised fine-tuning on dialogues employed in GPT models Ouyang et al. (2022). These models are primarily trained to execute instructions given immediate observations (i.e.,, the dialogue history); thereby, they may exhibit a lack of explorations. Furthermore, in dialogue scenarios, subtle differences in NL expressions often have less impact on the overall conversation. As a result, models may tend to overlook minor variations in their observations.
Answer: "
"What is the name of the event where buggies are raced?
","['Apr-12_Eventno_40_HistoryofBuggy.txt', 'Apr-13_Eventno_11_SCSBreakfastBuggy.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the name of the event where buggies are raced?

Context: Event: History of Buggy
Date: 4/12/24
Time: 4:00 PM-5:00 PM ET
Participants/Audience: Open to entire CMU community 
Event Details: 
Learn about the origin and development of the Sweepstakes races and what students are doing to push the boundaries of the sport in new directions. 

Note: Registration required. Walk-ins are welcome as space permits. No event fee. This event is open to the entire CMU community and their guests.
Event: SCS Breakfast & Buggy
Date: 4/13/24
Time: 9:00 AM-12:00 PM ET
Participants/Audience: Open to the SCS community 
Event Details: 
The School of Computer Science (SCS) will host a breakfast buffet for all SCS alumni and livestream the buggy races.

Note: No registration required. No event fee. This event is for SCS community alumni and their guests.
Answer: "
"According to the paper PWESUITE: Phonetic Word Embeddings and Tasks They Facilitate, what is the percentage accuracy for rhymes, achieved by the autoencoder model on the evaluation suite?
","['david mortensen_db14d05b18ec852f8afcd6d2d10bbd9eeaef8325_metadata.txt', 'alexander hauptmann_376f494126d1ea4f571ea0263c43ac2b6331800a_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: According to the paper PWESUITE: Phonetic Word Embeddings and Tasks They Facilitate, what is the percentage accuracy for rhymes, achieved by the autoencoder model on the evaluation suite?

Context: Faculty Name: david mortensen
Paperid: db14d05b18ec852f8afcd6d2d10bbd9eeaef8325
Title: PWESuite: Phonetic Word Embeddings and Tasks They Facilitate
Year: 2023
Abstract: Mapping words into a fixed-dimensional vector space is the backbone of modern NLP. While most word embedding methods successfully encode semantic information, they overlook phonetic information that is crucial for many tasks. We develop three methods that use articulatory features to build phonetically informed word embeddings. To address the inconsistent evaluation of existing phonetic word embedding methods, we also contribute a task suite to fairly evaluate past, current, and future methods. We evaluate both (1) intrinsic aspects of phonetic word embeddings, such as word retrieval and correlation with sound similarity, and (2) extrinsic performance on tasks such as rhyme and cognate detection and sound analogies. We hope our task suite will promote reproducibility and inspire future phonetic embedding research.
Authors: Vilém Zouhar, Kalvin Chang, Chenxuan Cui, Nathaniel Carlson, Nathaniel R. Robinson, Mrinmaya Sachan, David R. Mortensen
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'Three methods that use articulatory features to build phonetically informed word embeddings are developed that address the inconsistent evaluation of existing phonetic word embedding methods and contribute a task suite to fairly evaluate past, current, and future methods.'}
Url: http://arxiv.org/pdf/2304.02541
Faculty Name: alexander hauptmann
Paperid: 376f494126d1ea4f571ea0263c43ac2b6331800a
Title: SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs
Year: 2023
Abstract: In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.
Authors: Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A. Ross, Irfan Essa, Yonatan Bisk, Ming Yang, K. Murphy, A. Hauptmann, Lu Jiang
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.'}
Url: http://arxiv.org/pdf/2306.17842
Answer: "
"What are the number of units for independent study: breadth?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the number of units for independent study: breadth?

Context: In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Independent Study: Breadth' with Course ID 11920 and Section P offers 1-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Waibel located in Building TBA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Independent Study: Breadth' with Course ID 11920 and Section PP offers 1-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Lisbon, Portugal location,led by experienced instructor Instructor TBA located in Building TBA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Independent Study: Breadth' with Course ID 11920 and Section Q offers 1-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Yang located in Building TBA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Independent Study: Breadth' with Course ID 11920 and Section QQ offers 1-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ramakrishnan located in Building TBA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Independent Study: Breadth' with Course ID 11920 and Section R offers 1-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Strubell located in Building TBA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Independent Study: Breadth' with Course ID 11920 and Section S offers 1-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Mostow located in Building TBA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Independent Study: Breadth' with Course ID 11920 and Section P offers 1-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Waibel located in Building TBA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Independent Study: Breadth' with Course ID 11920 and Section PP offers 1-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Lisbon, Portugal location,led by experienced instructor Instructor TBA located in Building TBA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Independent Study: Breadth' with Course ID 11920 and Section Q offers 1-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Yang located in Building TBA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Independent Study: Breadth' with Course ID 11920 and Section QQ offers 1-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ramakrishnan located in Building TBA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Independent Study: Breadth' with Course ID 11920 and Section R offers 1-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Strubell located in Building TBA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Independent Study: Breadth' with Course ID 11920 and Section S offers 1-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Mostow located in Building TBA, Room None.
Answer: "
"Which LTI prof co-authored the paper titled ""Exploration on HuBERT with Multiple Resolutions""?
","['shinji watanabe_fa75ef55e04e3b25b8af56435478c2fd17403ce8_metadata.txt', 'shinji watanabe_fa75ef55e04e3b25b8af56435478c2fd17403ce8_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which LTI prof co-authored the paper titled ""Exploration on HuBERT with Multiple Resolutions""?

Context: Faculty Name: shinji watanabe
Paperid: fa75ef55e04e3b25b8af56435478c2fd17403ce8
Title: Exploration on HuBERT with Multiple Resolutions
Year: 2023
Abstract: Hidden-unit BERT (HuBERT) is a widely-used self-supervised learning (SSL) model in speech processing. However, we argue that its fixed 20ms resolution for hidden representations would not be optimal for various speech-processing tasks since their attributes (e.g., speaker characteristics and semantics) are based on different time scales. To address this limitation, we propose utilizing HuBERT representations at multiple resolutions for downstream tasks. We explore two approaches, namely the parallel and hierarchical approaches, for integrating HuBERT features with different resolutions. Through experiments, we demonstrate that HuBERT with multiple resolutions outperforms the original model. This highlights the potential of utilizing multiple resolutions in SSL models like HuBERT to capture diverse information from speech signals.
Authors: Jiatong Shi, Yun Tang, H. Inaguma, Hongyu Gong, J. Pino, Shinji Watanabe
Venue: Interspeech
Tldr: {'model': 'tldr@v2.0.0', 'text': 'Through experiments, it is demonstrated that HuBERT with multiple resolutions outperforms the original model, highlighting the potential of utilizing multiple resolutions in SSL models like HuberT to capture diverse information from speech signals.'}
Url: http://arxiv.org/pdf/2306.01084
Title: Exploration on HuBERT with Multiple Resolutions
Authors: Jiatong Shi, Yun Tang, Hirofumi Inaguma, Hongyu Gong, Juan Pino, Shinji Watanabe
Section: 6. References
[1] A. Mohamed et al., “Self-supervised speech representation learning: A review,” IEEE Journal of Selected Topics in Signal Processing, 2022. [2] W.-N. Hsu et al., “HuBERT: Self-supervised speech representation learning by masked prediction of hidden units,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451–3460, 2021. [3] S.-w. Yang et al., “SUPERB: Speech Processing Universal PERformance Benchmark,” in Proc. Interspeech, 2021, pp. 1194– 1198. [4] H.-S. Tsai et al., “SUPERB-SG: Enhanced speech processing universal performance benchmark for semantic and generative capabilities,” in Proc. ACL, 2022, pp. 8479–8492. [5] T.-h. Feng et al., “SUPERB@ SLT 2022: Challenge on generalization and efficiency of self-supervised speech representation learning,” in Proc. SLT, 2023, pp. 1096–1103. [6] X. Chang et al., “An exploration of self-supervised pretrained representations for end-to-end speech recognition,” in Proc. ASRU, 2021, pp. 228–235. [7] D. Berrebbi et al., “Combining Spectral and Self-Supervised Features for Low Resource Speech Recognition and Translation,” in Proc. Interspeech, 2022, pp. 3533–3537. [8] S. H. Mallidi and H. Hermansky, “Novel neural network based fusion for multistream asr,” in Proc. ICASSP, 2016, pp. 5680– 5684. [9] S. H. R. Mallidi et al., “A practical and efficient multistream framework for noise robust speech recognition,” Ph.D. dissertation, Johns Hopkins University, 2018.
Answer: "
"What was the name of the CMU project that created its first high-speed computer network?
","['history_d401.txt', 'history_d401.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What was the name of the CMU project that created its first high-speed computer network?

Context: As the 1970s progressed, Newell became interested in human-computer interaction, and began a long relationship with Xerox’s Palo Alto Research Center, or PARC, which released the Xerox Alto in 1973. Considered a forerunner to many of the computing environments that followed, Alto featured a graphical-user interface and was among the first commercially available workstations controlled with a mouse. Inspired by Alto, Reddy launched a drive for development of CMU’s own “three-M” machine—a personal workstation with a megabyte of memory, a megapixel display and at least one million instructions per second of processing power.Launching a Robotics InstituteIn 1979, an executive at Pittsburgh’s Westinghouse Electric Corp., Tom Murrin, collaborated with Jordan and Reddy to create the Robotics Institute, with Reddy as its first director. By 1982, the Computer Science Department included more than 30 faculty members and 100 graduate students.The best-wired campus in the worldWorking with IBM in the early 1980s, the university and the Computer Science Department established another new research frontier: Development of a high-speed computer network that would reach virtually every room on campus, along with a GUI-based computing environment, and providing networked PCs or workstations for 7,000 students, faculty members and employees. Called the Andrew Project, it turned Carnegie Mellon into the best-connected, most-wired university in the world—a process Newell called “greening up the campus with computer science.” CMU also became home to a new Software Engineering Institute, funded by the Defense Department, to study computer security and develop best practices in the design of operating systems. Between 1982 and 1985, the amount of sponsored research in the Computer Science Department doubled, from $7.2 million to $15.3 million—more than the other four departments in the Mellon College of Science combined.A “school of computer science” is proposedFeeling that CSD’s needs were inadequately represented in MCS, CSD head A. Nico Habermann and then-CMU provost Angel Jordan in 1986 wrote a white paper proposing the creation of “a School of Computer Science.” Responding to concerns from the faculty that the change might be taking place too quickly, the university first established a free-floating Department of Computer Science. The experiment, which lasted two years, was an unqualified success.
A history of SCS | SCS25 - Carnegie Mellon University School of Computer Science Skip to main content Legal Events Register 25 Things History Video A history of SCS For an expanded history of the School of Computer Science and its predecessors at CMU, read ""Institutional Memories"" in the Summer 2014 issue of The Link magazine.In 2014, the School of Computer Science celebrated its 25th year as a stand-alone college within Carnegie Mellon University. It was arguably the first college devoted solely to computer science in the United States, and a model for others that followed. But CMU’s computer science era begins much earlier—in 1956, with the arrival of an IBM 650 computer on the campus of what was then known as Carnegie Institute of Technology. The IBM 650 had magnetic-drum memory and a processing speed of approximately 60 instructions per second. Herb Simon (H’90), associate dean of the Graduate School of Industrial Administration—now known as CMU’s Tepper School of Business—established Carnegie Tech’s first Computation Center with the help of its first director, Alan Perlis (S’42).First freshman-level computer science courseIn 1956 and 1957, Simon, Allen Newell (IA’57) and Cliff Shaw of RAND designed the Logic Theorist, a computer program that could develop proofs for theorems in much the same way a human would work. They also developed linked-list data structures, the foundation of computer programming. Perlis, Simon and Newell are credited with defining the term “computer science” as “the theory and design of computers,” as well as (in Newell’s words) “the study of the phenomena arising from them.” In 1958, Perlis began teaching the first freshman-level computer programming course in the United States at Carnegie Tech.Computer science Ph.D. program createdIn 1961, the Computation Center and its newest computer, a Bendix G-20, were moved to recently completed Scaife Hall. That same year, Carnegie Tech created an interdisciplinary Ph.D. program called Systems and Communications Sciences, combining elements of computer science, mathematics, psychology, business and electrical engineering. The university’s first computer science Ph.D.s were graduates of this program.Computer Science Department establishedIn 1965, Carnegie Tech established its Computer Science Department, or CSD, with a $5 million grant from the R.K. Mellon Foundation.
Answer: "
"In spring 2024, what time does the Subword Modeling class start?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, what time does the Subword Modeling class start?

Context: In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Natural Language Processing' with Course ID 11411 and Section W offers 12.0 units. The Class meets Monday Wednesday between 02:30PM and 03:45PM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Oflazer located in Building CMB, Room 2052.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Grammar Formalisims' with Course ID 11422 and Section A offers 12.0 units. The Class meets Monday Wednesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin located in Building WEH, Room 4707.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'ConLanging: Lrng. Ling. & Lang Tech via Constru Artif. Lang.' with Course ID 11423 and Section A offers 12.0 units. The Class meets Monday Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin located in Building GHC, Room 4301.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Subword Modeling' with Course ID 11424 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Mortensen located in Building PH, Room 100.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Machine Learning with Graphs' with Course ID 11441 and Section A offers 9.0 units. The Class meets Tuesday Thursday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Yang located in Building NSH, Room 1305.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Search Engines' with Course ID 11442 and Section A offers 9.0 units.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Natural Language Processing' with Course ID 11411 and Section W offers 12.0 units. The Class meets Monday Wednesday between 02:30PM and 03:45PM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Oflazer located in Building CMB, Room 2052.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Grammar Formalisims' with Course ID 11422 and Section A offers 12.0 units. The Class meets Monday Wednesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin located in Building WEH, Room 4707.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'ConLanging: Lrng. Ling. & Lang Tech via Constru Artif. Lang.' with Course ID 11423 and Section A offers 12.0 units. The Class meets Monday Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin located in Building GHC, Room 4301.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Subword Modeling' with Course ID 11424 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Mortensen located in Building PH, Room 100.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Machine Learning with Graphs' with Course ID 11441 and Section A offers 9.0 units. The Class meets Tuesday Thursday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Yang located in Building NSH, Room 1305.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Search Engines' with Course ID 11442 and Section A offers 9.0 units.
Answer: "
"Who is ther first author of the paper Unlimiformer: Long-Range Transformers with Unlimited Length Input?
","['graham neubig_dbc368bc8b49347dd27679894524fa62f88492c9_metadata.txt', 'graham neubig_dbc368bc8b49347dd27679894524fa62f88492c9_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who is ther first author of the paper Unlimiformer: Long-Range Transformers with Unlimited Length Input?

Context: Faculty Name: graham neubig
Paperid: dbc368bc8b49347dd27679894524fa62f88492c9
Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input
Year: 2023
Abstract: Since the proposal of transformers, these models have been limited to bounded input lengths, because of their need to attend to every token in the input. In this work, we propose Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores. This kNN index can be kept on either the GPU or CPU memory and queried in sub-linear time; this way, we can index practically unlimited input sequences, while every attention head in every decoder layer retrieves its top-k keys, instead of attending to every key. We evaluate Unlimiformer on several long-document and book-summarization benchmarks, showing that it can process even 500k token-long inputs from the BookSum dataset, without any input truncation at test time. We demonstrate that Unlimiformer improves pretrained models such as BART and Longformer by extending them to unlimited inputs without additional learned weights and without modifying their code. We make our code and models publicly available at https://github.com/abertsch72/unlimiformer .
Authors: Amanda Bertsch, Uri Alon, Graham Neubig, Matthew R. Gormley
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work proposes Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores.'}
Url: http://arxiv.org/pdf/2305.01625
Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input
Authors: Amanda Bertsch, Uri Alon, Graham Neubig, Matthew R. Gormley
Section: G Additional Related Work
Long-document summarization Prior work has proposed several strategies for long-document summarization. In particular, many methods select a subsection of input to summarize using TF-IDF (Liu* et al., 2018), smaller retriever models (Liu and Lapata, 2019), or sentence similarity metrics (Bajaj et al., 2021). An orthogonal approach is to summarize chunks of the input, then combine and condense these sub-summaries into a global summary, either using vanilla transformer models (Kryściński et al. (2021), Zhang et al. (2022), (Zhang et al., 2021)) or a specialized architecture (Liu and Lapata (2019), Grail et al. (2021)). Other work has focused on expanding the amount of text that can be processed, by applying long-context transformers or developing new long-context methods (Huang et al., 2021). However, these methods all suffer from cascading errors: if the initial trimming or chunk summarization steps remove important information, there is no way to recover that information in the downstream summary. Retrieval-augmented transformers Interpolating language model probabilities with nearest neighbors retrieval from an external datastore was originally proposed by Khandelwal et al. (2019). Additional work in this space has improved the selection of neighbors (Drozdov et al., 2022) or added structure to the datastore (Alon et al., 2022). Despite the shared use of retrieval, all these works retrieve from an external datastore, while Unlimiformer retrieves from a single input example, independently from external cumbersome sources. Borgeaud et al. (2022) incorporate retrieval from the external datastore into the architecture, which requires pretaining the model from scratch; in contrast, Unlimiformer leverages any already-pretrained model, and thus can be applied to future models as well. Other efficient processing methods Outside of retrieval, many other works have attempted to combine inputs encoded across multiple context windows to process long inputs.
Answer: "
"Is the GRE optional for the Master of Science in Intelligent Information Systems application? Answer yes or no.
","['program_info_MasterofScienceinIntelligentInformationSystems.txt', 'miis-handbook_2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Is the GRE optional for the Master of Science in Intelligent Information Systems application? Answer yes or no.

Context: Academic Program Name:
Master of Science in Intelligent Information Systems

Website:
https://lti.cs.cmu.edu/academics/masters-programs/miis.html

Overview:
The Master's in Intelligent Information Systems degree focuses on recognizing and extracting meaning from text, spoken language and video. As an MIIS student, you’ll receive the department’s deepest exposure to content analysis and machine learning. In addition to completing the program’s coursework, you’ll work on directed study projects with your faculty advisor for two semesters; participate in a summer internship; and collaborate with your peers on a semester-long, group-oriented capstone project. This combination of classroom instruction, professional experience, and using new skills in significant projects with world-class colleagues will help prepare you for a successful career in industry or government. Our alumni have gone on to exciting careers at places like Apple, IBM and Google, and most have job offers within six weeks of graduation.

Requirements:
The Intelligent Information Systems degree offers students the flexibility to create their own course of study in consultation with their advisor.
MIIS students gain three types of practical experience: software development supervised by their advisor (24 units equivalent to two courses); a summer internship (which can be waived for students that have sufficient prior professional experience); and a capstone project executed in a group of peers (42 units equivalent to three 12-unit courses and one 6-unit course). This combination is proven to help IIS students to broaden their skills quickly. The MIIS degree is offered in two options:
Option 1. Standard MIIS degree (MIIS-16) - A 16-month track that is completed in three academic semesters (fall, spring, fall) and a summer internship.
Option 2. MIIS: Advanced Study degree (MIIS-21) - A 21-month track that is completed in four academic semesters (fall, spring, fall, spring) and a summer internship.
MIIS: Advanced Study track offers an in-depth degree in one of the following areas of concentration:
Human Language for Language Technologies
Language Technology Application
Machine Learning for Language Technologies
Part-time education option is available in some cases.
MIIS-16 students must take at least 84 units (typically 7 courses) of qualifying and elective courses that satisfy human language, machine learning, and language technology applications breadth requirements.
Master of Science in 
Intelligent Information Systems 
 
Student Handbook 
2023-2024 
 
 
 
 
 
Revised: August 2023 
 
Last revision date: September 1, 2023 
 
The information contained in this graduate handbook template focuses on the 
resources and locations available at the Carnegie Mellon Pittsburgh Campus. 
MIIS Graduate Student Handbook 
Page 2 
 
Contents 
1 
Welcome 6 
1.1 
Vision . 6 
1.2 
Mission . 6 
1.3 
The MIIS Degree . 6 
1.4 
MIIS Contact Information . 7 
1.5 
Carnegie Mellon University Statement of Assurance . 8 
1.6 
The Carnegie Mellon Code . 9 
1.7 
University Policies and Expectations . 9 
1.8 
Academic Calendar . 10 
2 
The Language Technologies Institute 10 
2.1.1 
Mailboxes & Office Supplies . 10 
2.1.2 
Photocopies and Printers . 10 
2.1.3 
Office Space for MS Students . 10 
2.1.4 
Computers for MS Students . 10 
3 
CMU Degree Completion and Certification 
11 
3.1 
Standard Degree Requirements & Degree Certification . 11 
3.2 
Statute of Limitations . 12 
3.3 
Additional Guidance for Students . 12 
4 
MIIS Degree Requirements and Related Policies/Protocols 
13 
4.1 
Program Options . 13 
4.2 
Required Units for Degree Attainment . 13 
4.3 
Core Requirements . 13 
4.4 
Approved Qualifying Courses . 14 
4.4.1 
Breadth Courses:  Human Language . 15 
4.4.2 
Breadth Courses:  Language Technology Applications . 15 
4.4.3 
Breadth Courses:  Machine Learning . 15 
4.5 
Practice Requirements .16 
4.6 
Registration Process/Procedures . 17 
4.7 
Drop/Add/Withdraw Procedures . 17 
4.8 
Transfer Courses and Pittsburgh Council on Higher Education (PCHE) . 17 
4.9 
Internships . 18 
4.10 
Advising . 18 
MIIS Graduate Student Handbook 
Page 3 
 
4.11 
LTI Orientation .
Answer: "
"How much does it cost to apply for the MLT program if an application is submitted a month before the deadline?
","['mlt-student-handbook-2023-2024.txt', 'mlt-student-handbook-2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How much does it cost to apply for the MLT program if an application is submitted a month before the deadline?

Context: 5.1.4 Independent Study 
For an Independent Study to satisfy an MLT students coursework requirements, it must be 
approved by the MLT Program Director in advance.  In consultation with the CMU faculty 
member who will be supervising the Independent Study, the student should produce a short 
(half-page to one page) description of the goals of the Independent Study, and how the results 
will be evaluated, and submit it to the Program Director before the end of the Add period of the 
semester of the proposed study. The study should be planned so that it is finished in time for 
the supervising faculty member to give a grade at the end of the semester.  Independent 
studies may be undertaken for 6 or 12 units. Normally only one Independent Study would be 
approved during a students MLT coursework. 
5.1.5 Transfer Credit 
An equivalent graduate course previously completed at another institution may be permitted to 
satisfy one of the MLT course requirements. The decision on whether a course may be 
transferred is made by the MLT Program Director. Typically, the student will provide the 
Program Director with the syllabus of the external course, and the Program Director will use 
that and the students transcript to make the decision.  
See the section on Definition of transfer credit versus course exemption. 
All MLT students are required to take a minimum of 96 units of coursework at CMU.  
5.1.6 External Internships 
MLT students may only engage in internships if their advisors recommend it, since the program 
is 24 months, including summers. International students are required to consult with the Office 
of International Education for eligibility before seeking an internship/co-op or signing an offer 
contract. 
We caution all students to be aware of potential intellectual property (IP) problems with 
internships, and to review any IP agreements with their advisors before signing them. It is 
possible to lose ownership of your ideas. 
Important: See also section 4.2.3, External Employment/Consulting. 
MLT Graduate Student Handbook 
Page 22 
 
5.1.7 Transferring Into the MLT Program 
Transfers into the MLT program are not permitted during a students first semester at CMU. 
Students must begin their study at CMU in the program that admitted them; this is a university 
policy.
It is to the student's advantage to avoid switching advisors, especially late in 
their graduate studies, because forging a strong student-advisor relationship takes time. 
4.3 Optional Masters Thesis 
MLT students may optionally elect to complete a Masters Thesis; it is not required. Students 
who intend to apply for the PhD program should consider the MLT Thesis as one of several 
ways to demonstrate their research potential. Other possibilities include innovative research in 
a research project or class project work. 
The MLT Thesis must be completed according to the following guidelines. Note in particular the 
October 31 proposal deadline! 
 There are two course components required for students who elect to complete the M.S. 
Thesis: 11-928  Masters Thesis I (with the chosen thesis advisor, typically for 12 units, 
typically in the Fall of their second year) and 11-929  Masters Thesis II in the following 
Spring. 
 Each of these two courses counts towards 6 units of MLT-level LTI course credit; if the 
student goes on to an LTI PhD, the second course (11-929) counts towards one PhD lab 
requirement.   
 The MLT thesis may focus on either a significant research result or a significant 
implementation of a new technology. The student should work with their advisor to 
define an interesting but bounded thesis research project. The MLT thesis culminates in 
MLT Graduate Student Handbook 
Page 16 
 
a publication-quality paper that clearly describes the work in terms of its research 
significance. 
 The MLT thesis committee will be composed of a thesis advisor and two additional 
readers, to be chosen by the student in consultation with the advisor, during the Fall 
Masters Thesis I course. At least 2 of the 3 committee members must be SCS faculty 
working in Language Technologies. 
 To receive a final grade for the first semester's course, the student must have prepared 
a written proposal of at least five pages, describing the scientific thesis to be 
investigated, any relevant research already completed by the student, a comparison 
with related work by others, and a detailed description of the work to be done in the 
next semester. The proposal MUST be accepted by the committee by the end of 
October, or the student will not be allowed to enroll in 11-929  Masters Thesis II!
Answer: "
"What is the nickname for the sweepstakes competition?
","['cmubuggy_d403.txt', 'cmubuggy_d403.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the nickname for the sweepstakes competition?

Context: An mural celebrating Buggy can be found in CMU's Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions. Sweepstakes Slang Buggy: Vehicle being raced and also a nickname for the competition. Chute: A section of the freeroll portion of the buggy course (near the southwestern end of Frew Street at its intersection with Schenley Drive) where buggies make the sharp righthand turn from Schenley Drive onto Frew Street. Chute Flagger: Team member who provides a signal for buggy drivers to know when to start the right-hand turn from Schenley Drive onto Frew Street. Driver: Person who travels with a buggy and controls the vehicles via steering and braking systems. Pushbar: Structure attached to a buggy that a person pushes to propel that buggy forward. Pusher: Person who propels a buggy via a pushbar along one of the five hills of the buggy course. Shell: Entire outer structure or covering of a buggy that determines that buggy's aerodynamic characteristics. Transition: Procedure whereby one pusher finishes pushing a buggy and the next pusher in sequence starts to push that same buggy. Members of Fringe, celebrating their 50th year of Buggy racing, allowed a rare visit into its shop for a behind-the-scenes tour. This year Fringe is planning to roll four different vehicles, built and maintained in the Fringe workshop in the basement of the East Campus Garage, known as the ""Froom."" ""We're used to saying everything with 'fr' in front of it but when we say something in front of other people, it gets them confused,"" said Fringe head mechanic Dave Singh, who will graduate in May with a bachelor's degree in mechanical engineering and biomedical engineering. Fringe vehicles often are named with the letter ""B,"" like Boson, Blueshift, Bissa and Bumper. Other teams, such as Apex, often use names that connotate fire, while the SDC (Student Dormitory Committee) team, uses names such as Vice, Bane, Avarice and Malice. This year's buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It's one of the few times spectators can see buggies up close.
Buggy Races Keep Rolling at Carnegie Mellon - News - Carnegie Mellon University Carnegie Mellon University ——— Search Search Search this site only News News › Stories › Archives › 2019 › April › Buggy Races Keep Rolling at Carnegie Mellon April 10, 2019 Buggy Races Keep Rolling at Carnegie Mellon In its 99th year, the tradition is a Spring Carnival treat By Heidi Opdyke opdyke(through)andrew.cmu.edu Media Inquiries Julie Mattera Marketing and Communications jmattera(through)cmu.edu 412-268-2902 Sweepstakes, also known as the Buggy Races, has come a long way at Carnegie Mellon University. The slick, torpedo-like vessels carrying drivers with nerves of steel are a far cry from the two-man teams that once changed places halfway through a race and rode in everything from rain barrels with bicycle wheels to three-wheeled ash cans 99 years ago. Today, it takes six people to maneuver the .84 -mile course around Schenley Park's Flagstaff Hill. But while five pushers and a driver navigate the course's hills, dozens of people are needed to make a successful race happen. A year of planning goes into just over two minutes of racing. The Machine The basics of a buggy are straightforward, but teams are often secretive in how they build the machines, in particular the way they brake, steer and what types of wheels are used. Each has a body, pushbar for runners to move the machine up the hills, wheels, a safety harness and driving and braking mechanisms. Some also include fairings, a type of housing around the wheels that help reduce drag, make the vehicle quieter and just looks cool. Fairings have been a key feature for the Fringe team in recent years, which is celebrating its 50th anniversary of Buggy. ""We have a reputation of being the quietest on the course,"" said Diya Nuxoll, who wrapped up her bachelor's degree in mechanical engineering in December and is working on an advanced studies master's degree in design and manufacturing. Nuxoll leads the Fringe team as one of two head mechanics. An mural celebrating Buggy can be found in CMU's Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions. Sweepstakes Slang Buggy: Vehicle being raced and also a nickname for the competition.
Answer: "
"When was the first freshman-level computer programming course offered at CMU?
","['history_d401.txt', 'history_d401.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When was the first freshman-level computer programming course offered at CMU?

Context: A history of SCS | SCS25 - Carnegie Mellon University School of Computer Science Skip to main content Legal Events Register 25 Things History Video A history of SCS For an expanded history of the School of Computer Science and its predecessors at CMU, read ""Institutional Memories"" in the Summer 2014 issue of The Link magazine.In 2014, the School of Computer Science celebrated its 25th year as a stand-alone college within Carnegie Mellon University. It was arguably the first college devoted solely to computer science in the United States, and a model for others that followed. But CMU’s computer science era begins much earlier—in 1956, with the arrival of an IBM 650 computer on the campus of what was then known as Carnegie Institute of Technology. The IBM 650 had magnetic-drum memory and a processing speed of approximately 60 instructions per second. Herb Simon (H’90), associate dean of the Graduate School of Industrial Administration—now known as CMU’s Tepper School of Business—established Carnegie Tech’s first Computation Center with the help of its first director, Alan Perlis (S’42).First freshman-level computer science courseIn 1956 and 1957, Simon, Allen Newell (IA’57) and Cliff Shaw of RAND designed the Logic Theorist, a computer program that could develop proofs for theorems in much the same way a human would work. They also developed linked-list data structures, the foundation of computer programming. Perlis, Simon and Newell are credited with defining the term “computer science” as “the theory and design of computers,” as well as (in Newell’s words) “the study of the phenomena arising from them.” In 1958, Perlis began teaching the first freshman-level computer programming course in the United States at Carnegie Tech.Computer science Ph.D. program createdIn 1961, the Computation Center and its newest computer, a Bendix G-20, were moved to recently completed Scaife Hall. That same year, Carnegie Tech created an interdisciplinary Ph.D. program called Systems and Communications Sciences, combining elements of computer science, mathematics, psychology, business and electrical engineering. The university’s first computer science Ph.D.s were graduates of this program.Computer Science Department establishedIn 1965, Carnegie Tech established its Computer Science Department, or CSD, with a $5 million grant from the R.K. Mellon Foundation.
The experiment, which lasted two years, was an unqualified success. Separately, and also in 1986, the Pittsburgh Supercomputing Center was created as a joint effort between CMU, the University of Pittsburgh and Westinghouse Electric Corp.SCS is officially formedCMU’s Faculty Senate in the fall of 1988 agreed to President Richard Cyert’s plan to elevate the Department of Computer Science to college status. In addition to the Computer Science Department, SCS also incorporated the Robotics Institute, the Center for Machine Translation, and researchers from the Information Technology Center, which had developed Andrew. On Dec. 13, 1988, Cyert (H’89) told faculty and staff that Habermann had been appointed CMU’s first Dean of Computer Science, effective Dec. 1, and that the School of Computer Science would soon begin operations. SCS made its formal debut on Dec. 22, 1988, with a reception in the Wherrett Room of Skibo Hall, CMU’s student union. The official announcement of CMU’s new “graduate School of Computer Science” was made Jan. 3, 1989.Undergraduate degrees beginFor several years, undergraduates interested in computer science pursued an “applied math/CS” bachelor’s degree offered by the Mathematics Department. CSD professor Mary Shaw (CS’72) led CMU’s first effort to design an undergraduate curriculum solely in computer science. She and her colleagues were guided by the Carnegie Plan—guidelines established in 1938 under Carnegie Tech President Robert Doherty (A’40, E’48, H’50), outlining the principles of a sound professional education. Drawing on Shaw’s plan and also on the work of other faculty members, an undergraduate program in computer science was created during the 1989-90 academic year. Seven CS majors were admitted to the program as sophomores. Another 73 undergraduates were admitted in 1990–91.
Answer: "
"What is the number of the HR person at LTI?
","['carolyn_rose.txt', 'miis-handbook_2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the number of the HR person at LTI?

Context: Carolyn Rosé
Professor, Language Technologies Institute
Human-Computer Interaction Institute
Contact
5415 —Gates & Hillman Centers
Email
412-268-7130
Research Area
Computer-Supported Collaborative Learning/MOOCs, Information Retrieval, Text Mining and Analytics, Language Technologies for Education, Natural Language Processing and Computational Linguistics

Education
Ph.D. in Language and Information Technology
1997
Employer Upon Graduation: 
Faculty, LTI CMU
Research
My research focuses on better understanding the social and pragmatic nature of conversation, and using this understanding to build computational systems that improve the efficacy of conversation both between people, and between people and computers. In order to pursue these goals, I invoke approaches from computational discourse analysis and text mining, conversational agents, and computer-supported collaborative learning. I ground my research in the fields of language technologies and human-computer interaction, and am fortunate to work closely with students and post-docs from the LTI and the Human-Computer Interaction Institute, as well as to direct my own lab, TELEDIA. My group’s highly interdisciplinary work, published in 160 peer-reviewed publications, is represented in the top venues in five fields: language technologies, learning sciences, cognitive science, educational technology and human-computer interaction.

An exciting direction of my group's work is spearheading a satellite working group to support social interaction for learning in MOOCs with EdX called DANCE. My research toward this end has birthed and substantially contributed to the growth of two thriving interrelated research areas: automated analysis of collaborative learning processes and dynamic support for collaborative learning. Both areas use intelligent conversational agents to support collaborative learning in a context-sensitive way.

All of my work draws insight from rich theoretical models from sociolinguistics and discourse analysis, and pares them down to precise operationalizations that capture the most important essence for achieving impact. I always start by investigating how conversation works and formalizing this understanding in models that are precise enough to be reproducible and that demonstrate explanatory power in connection with outcomes with real-world value. The next step is to adapt, extend and apply machine learning and text mining technologies that leverage this deep understanding to build computational models capable of automatically applying these constructs to naturally occurring language interactions. Finally, with the technology to automatically monitor naturalistic language communication in place, we can build interventions with real-world benefits.
MIIS Graduate Student Handbook 
Page 8 
 
The Language Technologies Institute is located primarily on the 5th and 6th floors of the Gates 
Hillman Complex (GHC) on Carnegie Mellon’s Pittsburgh campus.  The address and contact 
information of the LTI are: 
Language Technologies Institute 
Carnegie Mellon University 
5000 Forbes Avenue 
Gates Hillman Complex 5404, LTI 
Pittsburgh, PA 15241-3891 
 
412-268-6591 (phone) 
412-268-6298 (fax) 
 
http://www.lti.cs.cmu.edu/ 
1.5 Carnegie Mellon University Statement of Assurance  
Carnegie Mellon University does not discriminate in admission, employment or administration 
of its programs or activities on the basis of race, color, national origin, sex, handicap or disability, 
age, sexual orientation, gender identity, religion, creed, ancestry, belief, veteran status or genetic 
information. Furthermore, Carnegie Mellon University does not discriminate and is required 
not to discriminate in violation of federal, state, or local laws or executive orders. 
  
Inquiries concerning the application of and compliance with this statement should be directed 
to the university ombudsperson, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, 
PA 15213, telephone 412-268-1018.  Obtain general information about Carnegie Mellon 
University by calling 412-268-2000. 
  
Carnegie Mellon University publishes an annual campus security and fire safety report 
describing the university's security, alcohol and drug, sexual assault, and fire safety policies, and 
containing statistics about the number and type of crimes committed on the campus, and the 
number and cause of fires in campus residence facilities during the preceding three years. You 
can obtain a copy by contacting the Carnegie Mellon Police Department at 412-268-2323. The 
annual 
security 
and 
fire 
safety 
report 
also 
is 
available 
online 
at www.cmu.edu/police/annualreports. 
 
Information regarding the application of Title IX, including to admission and employment 
decisions, the sexual misconduct grievance procedures and process, including how to file a 
report or a complaint of sex discrimination, how to file a report of sexual harassment, and how 
the university responds to such reports is available at www.cmu.edu/title-ix.
Answer: "
"In spring 2025, What is the deadline for Mini-3 pass/no pass and withdrawal?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-2425.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2025, What is the deadline for Mini-3 pass/no pass and withdrawal?

Context: On Wednesday, 05 February, 2025, during the Spring 2025 (S25) semester, Mini-3 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 17 February, 2025, during the Spring 2025 (S25) semester, Mini-3 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 17 February, 2025, during the Spring 2025 (S25) semester, Mini-3 Faculty Course Evaluations open is observed.
On Monday, 24 February, 2025, during the Spring 2025 (S25) semester, Semester course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 28 February, 2025, during the Spring 2025 (S25) semester, Mini-3 Last Day of Classes is observed.
On Friday, 28 February, 2025, during the Spring 2025 (S25) semester, Mini-3 voucher deadline (4) is observed.
On Saturday, 01 March, 2025, during the Spring 2025 (S25) semester, Mini-3 Exams is observed.
On Saturday, 01 March, 2025, during the Spring 2025 (S25) semester, Mini-3 Faculty Course Evaluations close is observed.
From 03 March, 2025,Monday to 07 March, 2023,Friday marks the Spring Break for Spring 2025 (S25) semester which leads to No Classes.
On Monday, 10 March, 2025, during the Spring 2025 (S25) semester, First day of Mini-4 Classes is observed.
On Tuesday, 11 March, 2025, during the Spring 2025 (S25) semester, Summer 2025 Registration Opens is observed.
On Wednesday, 12 March, 2025, during the Spring 2025 (S25) semester, Mid-Semester & Mini-3 grades due by 4 pm is observed.
On Friday, 14 March, 2025, during the Spring 2025 (S25) semester, Mini-4 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 31 March, 2025, during the Spring 2025 (S25) semester, Semester pass/no pass & withdrawal deadline is observed.
Day: 'Monday', Event: 'Mini-3 Faculty Course Evaluations open', Semester: 'Spring 2025 (S25)'
Date: '2025-02-24', Day: 'Monday', Event: 'Semester course drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Spring 2025 (S25)'
Date: '2025-02-28', Day: 'Friday', Event: 'Mini-3 Last Day of Classes ', Semester: 'Spring 2025 (S25)'
Date: '2025-02-28', Day: 'Friday', Event: 'Mini-3 voucher deadline (4)', Semester: 'Spring 2025 (S25)'
Date: '2025-03-01', Day: 'Saturday', Event: 'Mini-3 Exams', Semester: 'Spring 2025 (S25)'
Date: '2025-03-01', Day: 'Saturday', Event: 'Mini-3 Faculty Course Evaluations close', Semester: 'Spring 2025 (S25)'
Start Date: '2025-03-03', End Date: '2023-03-07', Days: 'Monday to Friday', Event: 'Spring Break; No Classes', Semester: 'Spring 2025 (S25)'
Date: '2025-03-10', Day: 'Monday', Event: 'First day of Mini-4 Classes', Semester: 'Spring 2025 (S25)'
Date: '2025-03-11', Day: 'Tuesday', Event: 'Summer 2025 Registration Opens', Semester: 'Spring 2025 (S25)'
Date: '2025-03-12', Day: 'Wednesday', Event: 'Mid-Semester & Mini-3 grades due by 4 pm', Semester: 'Spring 2025 (S25)'
Date: '2025-03-14', Day: 'Friday', Event: 'Mini-4 add, audit & tuition adjustment drop deadline (1)', Semester: 'Spring 2025 (S25)'
Date: '2025-03-31', Day: 'Monday', Event: 'Semester pass/no pass & withdrawal deadline', Semester: 'Spring 2025 (S25)'
Date: '2025-04-02', Day: 'Wednesday', Event: 'Mini-4 course drop deadline; withdrawal grade assigned after this date (2)',
Answer: "
"In fall 2023, What is the deadline for adding, auditing, and tuition adjustment drop for Mini-1 (deadline 1)?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-2324.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, What is the deadline for adding, auditing, and tuition adjustment drop for Mini-1 (deadline 1)?

Context: On Monday, 28 August, 2023, during the Fall 2023 (F23) semester, Semester & Mini-1 Classes Begin is observed.
On Friday, 01 September, 2023, during the Fall 2023 (F23) semester, Mini-1 add, audit & tuition adjustment drop deadline  (1) is observed.
On Monday, 04 September, 2023, during the Fall 2023 (F23) semester, Labor Day is observed, leading to No Classes & University Closed.
On Monday, 11 September, 2023, during the Fall 2023 (F23) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Wednesday, 20 September, 2023, during the Fall 2023 (F23) semester, Mini-1 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 02 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Pass/no pass & withdrawal deadline (3) is observed.
On Monday, 02 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Faculty Course Evaluations open is observed.
On Monday, 09 October, 2023, during the Fall 2023 (F23) semester, Semester drop deadline is observed, leading to withdrawal grade assigned after this date.
On Friday, 13 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Last Day of Classes is observed.
On Friday, 13 October, 2023, during the Fall 2023 (F23) semester, Mini-1 voucher election deadline (4) is observed.
From 13 October, 2023,Friday to 14 October, 2023,Saturday marks the Family Weekend for Fall 2023 (F23) semester.
On Saturday, 14 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Exams is observed.
On Saturday, 14 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Faculty Course Evaluations close is observed.
From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
Date: '2023-08-28', Day: 'Monday', Event: 'Semester & Mini-1 Classes Begin', Semester: 'Fall 2023 (F23)'
Date: '2023-09-01', Day: 'Friday', Event: 'Mini-1 add, audit & tuition adjustment drop deadline  (1)', Semester: 'Fall 2023 (F23)'
Date: '2023-09-04', Day: 'Monday', Event: 'Labor Day; No Classes & University Closed', Semester: 'Fall 2023 (F23)'
Date: '2023-09-11', Day: 'Monday', Event: 'Semester add, audit & tuition adjustment drop deadline (1)', Semester: 'Fall 2023 (F23)'
Date: '2023-09-20', Day: 'Wednesday', Event: 'Mini-1 drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Fall 2023 (F23)'
Date: '2023-10-02', Day: 'Monday', Event: 'Mini-1 Pass/no pass & withdrawal deadline (3)', Semester: 'Fall 2023 (F23)'
Date: '2023-10-02', Day: 'Monday', Event: 'Mini-1 Faculty Course Evaluations open', Semester: 'Fall 2023 (F23)'
Date: '2023-10-09', Day: 'Monday', Event: 'Semester drop deadline; withdrawal grade assigned after this date', Semester: 'Fall 2023 (F23)'
Date: '2023-10-13', Day: 'Friday', Event: 'Mini-1 Last Day of Classes', Semester: 'Fall 2023 (F23)'
Date: '2023-10-13', Day: 'Friday', Event: 'Mini-1 voucher election deadline (4)', Semester: 'Fall 2023 (F23)'
Start Date: '2023-10-13', End Date: '2023-10-14', Days: 'Friday to Saturday', Event: 'Family Weekend', Semester: 'Fall 2023 (F23)'
Date: '2023-10-14', Day: 'Saturday', Event: 'Mini-1 Exams', Semester: 'Fall 2023 (F23)'
Date: '2023-10-14', Day: 'Saturday', Event: 'Mini-1 Faculty Course Evaluations close',
Answer: "
"Which LTI faculty member is on the paper titled ""Fusion-in-T5: Unifying Document Ranking Signals for Improved Information Retrieval""?
","['chenyan xiong_275da3802142fc42f6fab2ce2104223b2e0ef40d_metadata.txt', 'chenyan xiong_275da3802142fc42f6fab2ce2104223b2e0ef40d_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which LTI faculty member is on the paper titled ""Fusion-in-T5: Unifying Document Ranking Signals for Improved Information Retrieval""?

Context: Faculty Name: chenyan xiong
Paperid: 275da3802142fc42f6fab2ce2104223b2e0ef40d
Title: Fusion-in-T5: Unifying Document Ranking Signals for Improved Information Retrieval
Year: 2023
Abstract: Common IR pipelines are typically cascade systems that may involve multiple rankers and/or fusion models to integrate different information step-by-step. In this paper, we propose a novel re-ranker named Fusion-in-T5 (FiT5), which integrates document text information, retrieval features, and global document information into a single unified model using templated-based input and global attention. Experiments on passage ranking benchmarks MS MARCO and TREC DL show that FiT5 significantly improves ranking performance over prior pipelines. Analyses find that through global attention, FiT5 is able to jointly utilize the ranking features via gradually attending to related documents, and thus improve the detection of subtle nuances between them. Our code will be open-sourced.
Authors: S. Yu, Cheng-Chung Fan, Chenyan Xiong, David Jin, Zhiyuan Liu, Zhenghao Liu Tsinghua University, Huazhong University of Science, Technology, Microsoft Research, M. I. O. Technology, N. University
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A novel re-ranker named Fusion-in-T5 (FiT5), which integrates document text information, retrieval features, and global document information into a single unified model using templated-based input and global attention, is proposed.'}
Url: http://arxiv.org/pdf/2305.14685
Title: Fusion-in-T5: Unifying Document Ranking Signals for Improved Information Retrieval
Authors: Shi Yu, Chenghao Fan, Chenyan Xiong, David Jin, Zhiyuan Liu, Zhenghao Liu
Section: 1 Introduction
better differentiate between similar documents and ultimately produce better ranking results. ar X iv :2 30 5. 14 68 5v 1 [ cs .I R ] 2 4 M ay 2 02 3
Answer: "
"What pre-trained model does MOSAIC leverage knowledge from?
","['yonatan bisk_69b8cd15966c4c9c3e44e71769e557f1c87fb3f9_metadata.txt', 'yonatan bisk_69b8cd15966c4c9c3e44e71769e557f1c87fb3f9_content_3.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What pre-trained model does MOSAIC leverage knowledge from?

Context: Faculty Name: yonatan bisk
Paperid: 69b8cd15966c4c9c3e44e71769e557f1c87fb3f9
Title: MOSAIC: Learning Unified Multi-Sensory Object Property Representations for Robot Perception
Year: 2023
Abstract: A holistic understanding of object properties across diverse sensory modalities (e.g., visual, audio, and haptic) is essential for tasks ranging from object categorization to complex manipulation. Drawing inspiration from cognitive science studies that emphasize the significance of multi-sensory integration in human perception, we introduce MOSAIC (Multimodal Object property learning with Self-Attention and Interactive Comprehension), a novel framework designed to facilitate the learning of unified multi-sensory object property representations. While it is undeniable that visual information plays a prominent role, we acknowledge that many fundamental object properties extend beyond the visual domain to encompass attributes like texture, mass distribution, or sounds, which significantly influence how we interact with objects. In MOSAIC, we leverage this profound insight by distilling knowledge from multimodal foundation models and aligning these representations not only across vision but also haptic and auditory sensory modalities. Through extensive experiments on a dataset where a humanoid robot interacts with 100 objects across 10 exploratory behaviors, we demonstrate the versatility of MOSAIC in two task families: object categorization and object-fetching tasks. Our results underscore the efficacy of MOSAIC's unified representations, showing competitive performance in category recognition through a simple linear probe setup and excelling in the fetch object task under zero-shot transfer conditions. This work pioneers the application of sensory grounding in foundation models for robotics, promising a significant leap in multi-sensory perception capabilities for autonomous systems. We have released the code, datasets, and additional results: https://github.com/gtatiya/MOSAIC.
Authors: Gyan Tatiya, Jonathan M Francis, Ho-Hsiang Wu, Yonatan Bisk, Jivko Sinapov
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work pioneers the application of sensory grounding in foundation models for robotics, promising a significant leap in multi-sensory perception capabilities for autonomous systems.'}
Url: https://arxiv.org/pdf/2309.08508
Title: MOSAIC: Learning Unified Multi-Sensory Object Property Representations for Robot Learning via Interactive Perception
Authors: Gyan Tatiya, Jonathan Francis, Ho-Hsiang Wu, Yonatan Bisk, Jivko Sinapov
Section: IV. EXPERIMENTAL DESIGN
that is plastic.” Level 5 was introduced to assess the robot’s performance across various property categories. For each level, we created 20 commands for target objects and carefully selected corresponding distractor objects for each of the 5 previously explained folds. For each object (target and distractor(s)), we calculated its selection percentage, defined as S = number of times the object is selectedtotal number of commands (%). Our results are reported as the mean selection percentage across the 5 folds. We employ the approach outlined in Algorithm 2 for this task. Initially, we convert the natural language instruction into a text embedding, denoted as tc, using CLIP’s text encoder (step 1). Subsequently, the robot interacts with the presented objects, including the target object and distractors, using various available behaviors while simultaneously recording sensory signals (step 5). To simulate this step, we randomly select a trial from our dataset among 5 trials of each object. Leveraging our trained framework, we generate unified representations, denoted as ub, by processing the sensory inputs for each behavior (step 6). Next, we calculate the cosine similarity between the command embedding (tc) and the unified representation (ub) for each behavior, maintaining a cumulative similarity score (step 7). Finally, once all behaviors are considered, the object with the highest cumulative similarity score is identified as the target object, concluding the task (step 11). Baselines, Ablations, and Comparisons. We ablate our full framework (MOSAIC), featuring the multi-sensory selfattention module, against a framework that omits this component (MOSAIC-w/o-SA). We conduct these evaluations under two conditions: a non-interactive condition, where the robot solely performs the Look behavior, and an interactive condition, where the robot engages in all 9 aforementioned interactive behaviors. Notably, in the Look behavior, only visual embeddings are employed as the unified representations after passing through the self-attention layer in MOSAIC, while, in MOSAIC-w/o-SA, the Look behavior utilizes only CLIP’s vision encoder.
Answer: "
"What are the names of the people from LTI who co-authored the paper Approach to Learning Generalized Audio Representation Through Batch Embedding Covariance Regularization and Constant-Q Transforms
","['bhiksha raj_740488982dee323d559f2dae70b1f4b3aa5f7171_content_0.txt', 'bhiksha raj_740488982dee323d559f2dae70b1f4b3aa5f7171_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the names of the people from LTI who co-authored the paper Approach to Learning Generalized Audio Representation Through Batch Embedding Covariance Regularization and Constant-Q Transforms

Context: Title: Approach to Learning Generalized Audio Representation Through Batch Embedding Covariance Regularization and Constant-Q Transforms
Authors: Ankit Shah, Shuyi Chen
Section: Acknowledgement
Thanks to Chaoran Zhang, Yuxiang Zhang for their helpful comments on the work.
Faculty Name: bhiksha raj
Paperid: 740488982dee323d559f2dae70b1f4b3aa5f7171
Title: Approach to Learning Generalized Audio Representation Through Batch Embedding Covariance Regularization and Constant-Q Transforms
Year: 2023
Abstract: General-purpose embedding is highly desirable for few-shot even zero-shot learning in many application scenarios, including audio tasks. In order to understand representations better, we conducted a thorough error analysis and visualization of HEAR 2021 submission results. Inspired by the analysis, this work experiments with different front-end audio preprocessing methods, including Constant-Q Transform (CQT) and Short-time Fourier transform (STFT), and proposes a Batch Embedding Covariance Regularization (BECR) term to uncover a more holistic simulation of the frequency information received by the human auditory system. We tested the models on the suite of HEAR 2021 tasks, which encompass a broad category of tasks. Preliminary results show (1) the proposed BECR can incur a more dispersed embedding on the test set, (2) BECR improves the PaSST model without extra computation complexity, and (3) STFT preprocessing outperforms CQT in all tasks we tested. Github:https://github.com/ankitshah009/general_audio_embedding_hear_2021
Authors: Ankit Shah, Shuyi Chen, Kejun Zhou, Yue Chen, B. Raj
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'Experiments with different front-end audio preprocessing methods are experiments, and a Batch Embedding Covariance Regularization (BECR) term is proposed to uncover a more holistic simulation of the frequency information received by the human auditory system.'}
Url: http://arxiv.org/pdf/2303.03591
Answer: "
"In spring 2024, What is the title of course 15122?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, What is the title of course 15122?

Context: In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Imperative Computation' with Course ID 15122 and Section NA offers 12.0 units. The Class meets Friday between 05:00PM and 05:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor NA located in Building GHC, Room 4211.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Imperative Computation' with Course ID 15122 and Section R offers 12.0 units. The Class meets Monday between 05:00PM and 05:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Cervesato, Kohlbrenner located in Building GHC, Room CLSTR.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Imperative Computation' with Course ID 15122 and Section NA offers 12.0 units. The Class meets Friday between 05:00PM and 05:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor NA located in Building GHC, Room 4102.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Imperative Computation' with Course ID 15122 and Section W offers 12.0 units. The Class meets Tuesday Thursday between 08:45AM and 09:35AM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Hammoud located in Building CMB, Room 2163.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section Lec 1 offers 12.0 units. The Class meets Tuesday Thursday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building DH, Room 2210.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section Lec 2 offers 12.0 units.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Imperative Computation' with Course ID 15122 and Section NA offers 12.0 units. The Class meets Friday between 05:00PM and 05:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor NA located in Building GHC, Room 4211.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Imperative Computation' with Course ID 15122 and Section R offers 12.0 units. The Class meets Monday between 05:00PM and 05:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Cervesato, Kohlbrenner located in Building GHC, Room CLSTR.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Imperative Computation' with Course ID 15122 and Section NA offers 12.0 units. The Class meets Friday between 05:00PM and 05:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor NA located in Building GHC, Room 4102.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Imperative Computation' with Course ID 15122 and Section W offers 12.0 units. The Class meets Tuesday Thursday between 08:45AM and 09:35AM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Hammoud located in Building CMB, Room 2163.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section Lec 1 offers 12.0 units. The Class meets Tuesday Thursday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building DH, Room 2210.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section Lec 2 offers 12.0 units.
Answer: "
"What models does ESPnet-ST-v2 offer?
","['shinji watanabe_dab8e7dc79085774eea58bcb9ea2ed0ee20377eb_metadata.txt', 'shinji watanabe_dab8e7dc79085774eea58bcb9ea2ed0ee20377eb_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What models does ESPnet-ST-v2 offer?

Context: Faculty Name: shinji watanabe
Paperid: dab8e7dc79085774eea58bcb9ea2ed0ee20377eb
Title: ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit
Year: 2023
Abstract: ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broadening interests of the spoken language translation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simultaneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) – each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language translation toolkits. This toolkit offers state-of-the-art architectures such as transducers, hybrid CTC/attention, multi-decoders with searchable intermediates, time-synchronous blockwise CTC/attention, Translatotron models, and direct discrete unit models. In this paper, we describe the overall design, example models for each task, and performance benchmarking behind ESPnet-ST-v2, which is publicly available at https://github.com/espnet/espnet.
Authors: Brian Yan, Jiatong Shi, Yun Tang, H. Inaguma, Yifan Peng, Siddharth Dalmia, Peter Pol'ak, Patrick Fernandes, Dan Berrebbi, Tomoki Hayashi, Xiaohui Zhang, Zhaoheng Ni, Moto Hira, Soumi Maiti, J. Pino, Shinji Watanabe
Venue: Annual Meeting of the Association for Computational Linguistics
Tldr: {'model': 'tldr@v2.0.0', 'text': 'The overall design, example models for each task, and performance benchmarking behind ESPnet-ST-v2 are described, which is publicly available at https://github.com/espnet/esp net.'}
Url: https://arxiv.org/pdf/2304.04596
Title: ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit
Authors: Brian Yan, Jiatong Shi, Yun Tang, Hirofumi Inaguma, Yifan Peng, Siddharth Dalmia, Peter Polák, Patrick Fernandes, Dan Berrebbi, Tomoki Hayashi, Xiaohui Zhang, Zhaoheng Ni, Moto Hira, Soumi Maiti, Juan Pino, Shinji Watanabe
Section: 3.2 Key Features
(e.g. BLEU) to compare and rank n-best outputs from one or more models. For S2ST, neural vocoders are supported for both spectral and discrete inputs (Hayashi et al., 2020, 2021).
Answer: "
"In fall 2024, When is the Convocation?
","['Meta-Data-Calendar-2425.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2024, When is the Convocation?

Context: Start Date: '2024-08-18', End Date: '2024-08-23', Days: 'Saturday to Friday', Event: 'First-Year Orientation', Semester: 'Fall 2024 (F24)'
Date: '2024-08-22', Day: 'Thursday', Event: 'Convocation', Semester: 'Fall 2024 (F24)'
Date: '2024-08-26', Day: 'Monday', Event: 'Semester & Mini-1 Classes Begin', Semester: 'Fall 2024 (F24)'
Date: '2024-08-30', Day: 'Friday', Event: 'Mini-1 add, audit & tuition adjustment drop deadline  (1)', Semester: 'Fall 2024 (F24)'
Date: '2024-09-02', Day: 'Monday', Event: 'Labor Day; No Classes & University Closed', Semester: 'Fall 2024 (F24)'
Date: '2024-09-09', Day: 'Monday', Event: 'Semester add, audit & tuition adjustment drop deadline (1)', Semester: 'Fall 2024 (F24)'
Date: '2024-09-18', Day: 'Monday', Event: 'Mini-1 drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Fall 2024 (F24)'
Date: '2024-09-30', Day: 'Monday', Event: 'Mini-1 Pass/no pass & withdrawal deadline (3)', Semester: 'Fall 2024 (F24)'
Date: '2024-09-30', Day: 'Monday', Event: 'Mini-1 Faculty Course Evaluations open', Semester: 'Fall 2024 (F24)'
Date: '2024-10-07', Day: 'Monday', Event: 'Semester drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Fall 2024 (F24)'
Date: '2024-10-11', Day: 'Friday', Event: 'Mini-1 Last Day of Classes', Semester: 'Fall 2024 (F24)'
Date: '2024-10-11', Day: 'Friday', Event: 'Mini-1 voucher election deadline (4)', Semester: 'Fall 2024 (F24)'
Date: '2024-10-12', Day: 'Saturday', Event: 'Mini-1 Exams',
On Monday, 08 July, 2024, during the Summer Two 2024 (N24) semester, Summer Semester  Two drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 22 July, 2024, during the Summer Two 2024 (N24) semester, Summer Semester  Two pass/no pass & withdrawal deadline (3) is observed.
On Monday, 29 July, 2024, during the Summer Two 2024 (N24) semester, Summer Semester  Two Faculty Course Evaulations open is observed.
On Thursday, 01 August, 2024, during the Summer Two 2024 (N24) semester, Summer Semester  Two Last Day of Classes is observed.
On Thursday, 01 August, 2024, during the Summer Two 2024 (N24) semester, Summer Semester  Two voucher deadline (4) is observed.
On Friday, 02 August, 2024, during the Summer Two 2024 (N24) semester, Summer Semester  Two Final Exams*** is observed.
On Friday, 02 August, 2024, during the Summer Two 2024 (N24) semester, Summer Semester  Two Faculty Course Evaluations close is observed.
On Tuesday, 06 August, 2024, during the Summer Two 2024 (N24) semester, Summer Semester  Two Final Grades Due by 4 pm is observed.
From 18 August, 2024,Saturday to 23 August, 2024,Friday marks the First-Year Orientation for Fall 2024 (F24) semester.
On Thursday, 22 August, 2024, during the Fall 2024 (F24) semester, Convocation is observed.
On Monday, 26 August, 2024, during the Fall 2024 (F24) semester, Semester & Mini-1 Classes Begin is observed.
On Friday, 30 August, 2024, during the Fall 2024 (F24) semester, Mini-1 add, audit & tuition adjustment drop deadline  (1) is observed.
On Monday, 02 September, 2024, during the Fall 2024 (F24) semester, Labor Day is observed, leading to No Classes & University Closed.
Answer: "
"What are the acronmys for all LTI programs that have capstone requirements?
","['program_info_MasterofComputationalDataScience.txt', 'mcds-student-handbook-2023_2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the acronmys for all LTI programs that have capstone requirements?

Context: Academic Program Name:
Master of Computational Data Science

Website:
https://lti.cs.cmu.edu/academics/masters-programs/mcds.html

Overview:
The MCDS degree focuses on engineering and deploying large-scale information systems. Our comprehensive curriculum equips you with the skills and knowledge to develop the layers of technology involved in the next generation of massive information system deployments and analyze the data these systems generate. When you graduate, you’ll have a unified vision of these systems from your core courses; internship experience; and semester-long, group-oriented capstone project. MCDS graduates are sought-after software engineers, data scientists and project managers at leading information technology, software services and social media companies.

Requirements:
The MCDS program offers three majors: Systems, Analytics, and Human-Centered Data Science. All three require the same total number of course credits, split among required core courses, electives, data science seminar and capstone courses specifically defined for each major. The degree can also be earned in two different ways, depending on the length of time you spend working on it. Regardless of the timing option, all MCDS students must complete a minimum of 144 units to graduate.
Here are the options:
Standard Timing — a 16-month degree consisting of study for fall and spring semesters, a summer internship, and fall semester of study. Each semester comprises a minimum of 48 units. This timing is typical for most students. Students graduate in December.
Extended Timing — a 20-month degree consisting of study for fall and spring semesters, a summer internship, and a second year of fall and spring study. Each semester comprises a minimum of 36 units. Students graduate in May.
For a complete overview of the MCDS requirements read the MCDS Handbook.

Curriculum:
To earn an MCDS degree, students must pass courses in the core curriculum, the MCDS seminar, a concentration area, and electives. Students must also complete a capstone project in which they work on a research project at CMU or on an industry-sponsored project.
In total, students must complete 144 eligible units of study, including eight 12-unit courses, two 12-unit seminar courses, and one 24-unit capstone course. Students must choose at minimum five core courses. The remainder of the 12-unit courses with course numbers 600 or greater can be electives chosen from the SCS course catalog.
While some MCDS graduates continue on to PhD programs in the 
LTI or other leading universities, most graduates go on to jobs in 
 
 
7
corporate research and development laboratories. 
 
The program consists entirely of coursework and a Capstone Project, and 
no Master’s Thesis is required. All Capstone projects are structured as 
research activities and may lead to a publication. There is no Doctoral 
program in Computational Data Science. Because of the highly selective 
nature of the MCDS program and quality of the MCDS curriculum, 
performing well in the program will give a boost to a PhD application. MS 
graduates are welcome to apply to CMU PhD programs but will not receive 
preferential treatment. 
 
There are significant differences between CMU's different departments 
and degree programs in philosophical approach, procedures, policies and 
regulations. Each department issues a handbook that informs graduate 
students of their program requirements and procedures and ensures that 
students have written access to the standard information outlined below. 
This handbook describes the policies, procedures, and requirements for 
the Master of Computational Data Science (MCDS) degree. 
All policies not explicitly described in this document conform to School of 
Computer Science (SCS) policies and university policies described in The 
Word, Carnegie Mellon University Student Handbook and at the 
University Policies website. 
1.2 Vision 
Carnegie Mellon University will have a transformative impact on society 
through continual innovation in education, research, creativity, and 
entrepreneurship. 
1.3 Mission 
To create a transformative educational experience for students focused on 
deep disciplinary knowledge; problem solving; leadership, 
communication, and interpersonal skills; and personal health and well-
being. 
To cultivate a transformative university community committed to (a) 
attracting and retaining diverse, world-class talent; (b) creating a 
collaborative environment open to the free exchange of ideas, where 
 
 
8
research, creativity, innovation, and entrepreneurship can flourish; and 
(c) ensuring individuals can achieve their full potential. 
To impact society in a transformative way — regionally, nationally, and 
globally — by engaging with partners outside the traditional borders of the 
university campus.
Answer: "
"Who is the point of contact for Naval ROTC Commissioning ceremony?
","['Commencement.txt', 'combined_metadata_final.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who is the point of contact for Naval ROTC Commissioning ceremony?

Context: Invitation, along with registration details, will be sent in late April.

First Gen Graduation Recognition
Reception: 5-5:30 p.m.
Alumni Concert Hall, College of Fine Arts
Ceremony: 5:30-6:30p.m.
Kresge Theater, College of Fine Arts

Contact:
M. Shernell Smith
mssmith@andrew.cmu.edu 

Sam Colavecchio
scolavec@andrew.cmu.edu 
412-268-7733


Friday, May 10
Diploma Ceremonies
Various times

Senior Leadership Recognition Ceremony
4–5:30 p.m.
Wiegand Gym, Cohon University Center
Undergraduate students and their guests by invitation only. This ceremony recognizes nominated seniors who have reflected upon their specific leadership contributions during their time at CMU.


Saturday, May 11
Diploma Ceremonies
Various times

Center for Student Diversity and Inclusion Ceremony
Reception: 12:30 p.m.
Program: 1-2:15 p.m.
Simmons Auditorium, Tepper Building

Contact:
M. Shernell Smith
mssmith@andrew.cmu.edu 

Rowshan Lang
rowshan@andrew.cmu.edu
Naval ROTC Commissioning
Ceremony: 1:30-2:30 p.m.
Auditorium, Soldiers & Sailors Memorial Hall & Museum *
4141 Fifth Avenue, Pittsburgh, PA 15213
Contact:
Mike Danko
mdanko@andrew.cmu.edu 

The President’s Reception in honor of CMU’s Doctoral Candidates
4–6 p.m.
Tepper Building Atrium


Sunday, May 12
Gesling Stadium opens to guests
8 a.m.

Robing and procession for graduates
9–10 a.m.
Various locations across campus

Student procession begins
9:15 a.m.
All guests in stadium must be seated. Access to guest seating will be restricted once the procession begins.

Commencement Ceremony
10–11:30 a.m.
Gesling Stadium, CMU’s campus

Diploma Ceremonies
Various times
The Class meets Tuesday Thursday between 07:30AM and 08:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Stewart located in Building HBH, Room 1006.
In Semester Fall 2023, from the department of Naval Science - ROTC, the subject titled 'Naval Laboratory' with Course ID 32400 and Section A offers 3.0 units. The Class meets Wednesday between 01:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Boerio located in Building MI, Room AUD.
In Semester Fall 2023, from the department of Naval Science - ROTC, the subject titled 'Leadership and Ethics' with Course ID 32402 and Section A offers 9.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Naval Science - ROTC, the subject titled 'Amphibious Warfare/Operations & The Fundamentals of Maneuver Warfare:' with Course ID 32410 and Section NA offers 9.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Naval Science - ROTC, the subject titled 'Fundamentals of Maneuver Warefare and Amphibious Operations' with Course ID 32410 and Section A offers 9.0 units. The Class meets Tuesday Thursday between 08:00AM and 09:20AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Faddish located in Building FRB, Room 215.
In Semester Fall 2023, from the department of Naval Science - ROTC, the subject titled 'Naval Operations and Seamanship' with Course ID 32411 and Section A offers 9.0 units. The Class meets Tuesday Thursday between 07:30AM and 08:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Boerio located in Building FRB, Room 210.
Answer: "
"In fall 2023, What are the units for unit 02712?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, What are the units for unit 02712?

Context: In Semester Fall 2023, from the department of Computational Biology, the subject titled 'M.S. Thesis Research' with Course ID 02700 and Section A offers 1-12 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Garrand located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'CPCB Course - Current Topics in Computational Biology' with Course ID 02701 and Section A offers 3.0 units. The Class meets Monday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ma located in Building WEH, Room 5403.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Biology Seminar' with Course ID 02702 and Section A offers 3.0 units. The Class meets Friday between 10:30AM and 12:00PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Carja located in Building OFF, Room PITT.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Methods for Biological Modeling and Simulation' with Course ID 02712 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Schwartz, Carja located in Building HH, Room B131.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Medicine' with Course ID 02718 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Wu located in Building WEH, Room 5403.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Laboratory Methods for Computational Biologists' with Course ID 02760 and Section A offers 9.0 units. The Class meets Tuesday between 11:00AM and 12:50PM ET.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'M.S. Thesis Research' with Course ID 02700 and Section A offers 1-12 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Garrand located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'CPCB Course - Current Topics in Computational Biology' with Course ID 02701 and Section A offers 3.0 units. The Class meets Monday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ma located in Building WEH, Room 5403.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Biology Seminar' with Course ID 02702 and Section A offers 3.0 units. The Class meets Friday between 10:30AM and 12:00PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Carja located in Building OFF, Room PITT.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Methods for Biological Modeling and Simulation' with Course ID 02712 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Schwartz, Carja located in Building HH, Room B131.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Computational Medicine' with Course ID 02718 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Wu located in Building WEH, Room 5403.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Laboratory Methods for Computational Biologists' with Course ID 02760 and Section A offers 9.0 units. The Class meets Tuesday between 11:00AM and 12:50PM ET.
Answer: "
"What number do all of the Biomedical Engineering classes start with?
","['metadata_course_fall_23.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What number do all of the Biomedical Engineering classes start with?

Context: In Semester Fall 2023, from the department of Biomedical Engineering, the subject titled 'Biomedical Engineering Systems Modeling and Analysis:' with Course ID 42302 and Section NA offers 9.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Biomedical Engineering, the subject titled 'TBA' with Course ID 42302 and Section B offers 9.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Chalacheva located in Building BH, Room A53.
In Semester Fall 2023, from the department of Biomedical Engineering, the subject titled 'Senior BME Research Project' with Course ID 42400 and Section A offers 3-18 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Palchesko located in Building TBA, Room None.
In Semester Fall 2023, from the department of Biomedical Engineering, the subject titled 'Foundation of BME Design' with Course ID 42401 and Section A offers 6.0 units. The Class meets Tuesday Thursday between 04:00PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Zapanta located in Building BH, Room A36.
In Semester Fall 2023, from the department of Biomedical Engineering, the subject titled 'Medical Devices' with Course ID 42444 and Section A offers 9.0 units. The Class meets Monday Wednesday between 08:00AM and 09:20AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Cheng located in Building HH, Room B131.
In Semester Fall 2023, from the department of Biomedical Engineering, the subject titled 'Tissue Engineering:' with Course ID 42612 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Biomedical Engineering, the subject titled 'Introduction to Biomedical Engineering' with Course ID 42101 and Section A offers 12.0 units. The Class meets Monday Friday between 01:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Abbott located in Building POS, Room 151.
In Semester Fall 2023, from the department of Biomedical Engineering, the subject titled 'Introduction to Biomedical Engineering' with Course ID 42101 and Section NA offers 12.0 units. The Class meets Wednesday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor NA located in Building POS, Room 151.
In Semester Fall 2023, from the department of Biomedical Engineering, the subject titled 'Sophomore BME Research Project' with Course ID 42200 and Section A offers 3-12 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Palchesko located in Building TBA, Room None.
In Semester Fall 2023, from the department of Biomedical Engineering, the subject titled 'Professional Issues in Biomedical Engineering' with Course ID 42201 and Section A offers 3.0 units. The Class meets Wednesday between 12:00PM and 12:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor He located in Building GHC, Room 4215.
In Semester Fall 2023, from the department of Biomedical Engineering, the subject titled 'Physiology' with Course ID 42202 and Section A offers 9.0 units. The Class meets Monday Wednesday Friday between 11:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Campbell located in Building POS, Room 151.
In Semester Fall 2023, from the department of Biomedical Engineering, the subject titled 'Biomedical Engineering Laboratory:' with Course ID 42203 and Section NA offers 9.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
Answer: "
"What is the full name of the conference where the paper Rethinking Voice-Face Correlation: A Geometry View, got published?
","['bhiksha raj_5a3307b2e64bbcaff1202e261b8a83f7d03418a8_content_0.txt', 'rita singh_5a3307b2e64bbcaff1202e261b8a83f7d03418a8_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the full name of the conference where the paper Rethinking Voice-Face Correlation: A Geometry View, got published?

Context: Title: Rethinking Voice-Face Correlation: A Geometry View
Authors: Xiang Li, Yandong Wen, Muqiao Yang, Jinglu Wang, Rita Singh, Bhiksha Raj, Mohamed bin Zayed
Section: 5 CONCLUSION
In conclusion, this paper presents a novel approach to exploring the voice-face correlation by focusing on the geometric aspects of the face rather than relying on semantic cues such as gender, age, and emotion. The proposed voice-anthropometric measurement (AM)face paradigm identifies predictable facial AMs from the voice to guide 3D face reconstruction, which results in significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium. This approach not only eliminates the influence of unpredictable AMs but also offers a new perspective on voice-face correlation, which can be valuable for anthropometry science. The results of this study open up possibilities for future research in this area, such as developing more accurate voice-guided face synthesis techniques and a better understanding of the relationship between voice and facial geometry. Rethinking Voice-Face Correlation: A Geometry View Conference’23, July 2023, Ottawa, Canada
Title: Rethinking Voice-Face Correlation: A Geometry View
Authors: Xiang Li, Yandong Wen, Muqiao Yang, Jinglu Wang, Rita Singh, Bhiksha Raj, Mohamed bin Zayed
Section: 5 CONCLUSION
In conclusion, this paper presents a novel approach to exploring the voice-face correlation by focusing on the geometric aspects of the face rather than relying on semantic cues such as gender, age, and emotion. The proposed voice-anthropometric measurement (AM)face paradigm identifies predictable facial AMs from the voice to guide 3D face reconstruction, which results in significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium. This approach not only eliminates the influence of unpredictable AMs but also offers a new perspective on voice-face correlation, which can be valuable for anthropometry science. The results of this study open up possibilities for future research in this area, such as developing more accurate voice-guided face synthesis techniques and a better understanding of the relationship between voice and facial geometry. Rethinking Voice-Face Correlation: A Geometry View Conference’23, July 2023, Ottawa, Canada
Answer: "
"In summer 2024, When do Semester & Mini-6 Faculty Course Evaluations open?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In summer 2024, When do Semester & Mini-6 Faculty Course Evaluations open?

Context: On Friday, 07 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 pass/no pass & withdrawal deadline (3) is observed.
On Friday, 14 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Faculty Course Evaluations open is observed.
On Tuesday, 18 June, 2024, during the Summer One 2024 (M24) semester, Semester course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Wednesday, 19 June, 2022, during the Summer One 2024 (M24) semester, Juneteenth is observed, leading to University Closed & No Classes.
On Thursday, 20 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Last Day of Classes is observed.
On Thursday, 20 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 voucher deadline (4) is observed.
On Friday, 21 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Final Exams is observed.
On Friday, 21 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Faculty Course Evaluations close is observed.
On Monday, 24 June, 2024, during the Summer One 2024 (M24) semester, Mini-6 First Day of Classes is observed.
On Tuesday, 25 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Final Grades Due by 4 pm is observed.
On Friday, 28 June, 2024, during the Summer One 2024 (M24) semester, Mini-6 add, audit, & tuition adjustment drop deadline (1) is observed.
On Thursday, 04 July, 2024, during the Summer One 2024 (M24) semester, Independence Day is observed, leading to University Closed & No Classes.
On Friday, 05 July, 2024, during the Summer One 2024 (M24) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Friday, 13 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Faculty Course Evaluations open is observed.
On Tuesday, 17 June, 2025, during the Summer 2025 (M25) semester, Summer All course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Wednesday, 18 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Last Day of Classes is observed.
On Wednesday, 18 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 voucher deadline is observed.
On Thursday, 19 June, 2025, during the Summer 2025 (M25) semester, Juneteenth is observed, leading to University Closed & No Classes.
On Friday, 20 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Final Exams is observed.
On Friday, 20 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Faculty Course Evaluations close is observed.
On Monday, 23 June, 2025, during the Summer 2025 (M25) semester, Summer Two & Mini-6 First Day of Classes is observed.
On Tuesday, 24 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Final Grades Due by 4 pm is observed.
On Friday, 27 June, 2025, during the Summer 2025 (M25) semester, Summer Two & Mini-6 add, audit, & tuition adjustment drop deadline (1) is observed.
On Thursday, 03 July, 2025, during the Summer 2025 (M25) semester, Summer All pass/no pass & withdrawal grade deadline (3) is observed.
On Friday, 04 July, 2025, during the Summer 2025 (M25) semester, Independence Day is observed, leading to University Closed & No Classes.
On Monday, 07 July, 2025, during the Summer 2025 (M25) semester, Summer Two & Mini-6 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
Answer: "
"Which LTI faculty member does the most work on robots?
","['25things_d400.txt', 'history_d401.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which LTI faculty member does the most work on robots?

Context: It was developed by future Turing Award winner and future SCS dean Raj Reddy along with his students. Their work on subsequent systems established many of the principles that still underlie speech recognition software. 5. Emoticons, 1982Sure, it was just a joke, but (for better or worse) it’s endured for more than three decades. CMU researcher Scott Fahlman created the emoticon to clear up misunderstandings on computer message boards. We’ve been looking at the world sideways ever since. :-) 6. Andrew project, 1982It was long the dream of computer scientists to put a workstation in every home and office, but no one had actually tried to accomplish it until researchers from Carnegie Mellon University and IBM launched the Andrew Project. Soon, every student, faculty member and employee had access to email, word processing, file-transfer services and graphics programs, and CMU was the most-wired campus in the world. 7. Autonomous robots, 1983Thanks to Red Whittaker (E'75,'79), robots moved off of the assembly lines and into places no human ever could go. His Robotic Reconnaissance Vehicle spent four years inspecting and cleaning up the contaminated reactor building at the crippled Three Mile Island nuclear plant. 8. User interfaces, 1983Why should humans adapt to fit computers? Shouldn’t computers adapt to fit humans? That was the attitude of CMU researchers, who applied design principles to computer science to develop better, easier-to-use interfaces. They called the new field “human-computer interaction.” 9. Machine translation, 1984Every “Star Trek” fan knows about the universal translator. Scientists in the Language Technologies Institute are moving those gadgets from science fiction to real life. Their pioneering systems include handheld, portable speech-to-speech translators, just like those depicted on the USS Enterprise. 10. Mach kernel, 1985In computer parlance, a “kernel” is the heart of an operating system, passing input and output requests to and from the processor. At the core of all modern Apple devices --- including iPhones, iPads and MacBooks --- is the Mach kernel, developed at CMU under the leadership of then-professor Rick Rashid. 11. Computer chess, 1990Could a computer play chess at the level of the world’s best players?
As the 1970s progressed, Newell became interested in human-computer interaction, and began a long relationship with Xerox’s Palo Alto Research Center, or PARC, which released the Xerox Alto in 1973. Considered a forerunner to many of the computing environments that followed, Alto featured a graphical-user interface and was among the first commercially available workstations controlled with a mouse. Inspired by Alto, Reddy launched a drive for development of CMU’s own “three-M” machine—a personal workstation with a megabyte of memory, a megapixel display and at least one million instructions per second of processing power.Launching a Robotics InstituteIn 1979, an executive at Pittsburgh’s Westinghouse Electric Corp., Tom Murrin, collaborated with Jordan and Reddy to create the Robotics Institute, with Reddy as its first director. By 1982, the Computer Science Department included more than 30 faculty members and 100 graduate students.The best-wired campus in the worldWorking with IBM in the early 1980s, the university and the Computer Science Department established another new research frontier: Development of a high-speed computer network that would reach virtually every room on campus, along with a GUI-based computing environment, and providing networked PCs or workstations for 7,000 students, faculty members and employees. Called the Andrew Project, it turned Carnegie Mellon into the best-connected, most-wired university in the world—a process Newell called “greening up the campus with computer science.” CMU also became home to a new Software Engineering Institute, funded by the Defense Department, to study computer security and develop best practices in the design of operating systems. Between 1982 and 1985, the amount of sponsored research in the Computer Science Department doubled, from $7.2 million to $15.3 million—more than the other four departments in the Mellon College of Science combined.A “school of computer science” is proposedFeeling that CSD’s needs were inadequately represented in MCS, CSD head A. Nico Habermann and then-CMU provost Angel Jordan in 1986 wrote a white paper proposing the creation of “a School of Computer Science.” Responding to concerns from the faculty that the change might be taking place too quickly, the university first established a free-floating Department of Computer Science. The experiment, which lasted two years, was an unqualified success.
Answer: "
"In fall 2023, When do the Mid-semester & Mini-1 grades need to be submitted?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, When do the Mid-semester & Mini-1 grades need to be submitted?

Context: From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mini-2 Classes Begin is observed.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mid-Semester & Mini-1 grades due by 4 pm is observed.
On Friday, 27 October, 2023, during the Fall 2023 (F23) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
On Tuesday, 07 November, 2023, during the Fall 2023 (F23) semester, Democracy Day is observed, leading to No Classes, except Evening classes after 5 pm will still meet.
On Saturday, 11 November, 2023, during the Fall 2023 (F23) semester, Homecoming is observed.
From 13 November, 2023,Monday to 17 November, 2023,Friday marks the Spring 2024 Registration Week for Fall 2023 (F23) semester.
On Monday, 13 November, 2023, during the Fall 2023 (F23) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Wednesday, 15 November, 2023, during the Fall 2023 (F23) semester, Mini-2 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 22 November, 2023,Wednesday to 24 November, 2023,Friday marks the Thanksgiving Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Mini-2 pass/no pass & withdrawal deadline (2) is observed.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Faculty Course Evaluations open is observed.
On Friday, 08 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Last Day of Classes is observed.
On Monday, 28 August, 2023, during the Fall 2023 (F23) semester, Semester & Mini-1 Classes Begin is observed.
On Friday, 01 September, 2023, during the Fall 2023 (F23) semester, Mini-1 add, audit & tuition adjustment drop deadline  (1) is observed.
On Monday, 04 September, 2023, during the Fall 2023 (F23) semester, Labor Day is observed, leading to No Classes & University Closed.
On Monday, 11 September, 2023, during the Fall 2023 (F23) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Wednesday, 20 September, 2023, during the Fall 2023 (F23) semester, Mini-1 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 02 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Pass/no pass & withdrawal deadline (3) is observed.
On Monday, 02 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Faculty Course Evaluations open is observed.
On Monday, 09 October, 2023, during the Fall 2023 (F23) semester, Semester drop deadline is observed, leading to withdrawal grade assigned after this date.
On Friday, 13 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Last Day of Classes is observed.
On Friday, 13 October, 2023, during the Fall 2023 (F23) semester, Mini-1 voucher election deadline (4) is observed.
From 13 October, 2023,Friday to 14 October, 2023,Saturday marks the Family Weekend for Fall 2023 (F23) semester.
On Saturday, 14 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Exams is observed.
On Saturday, 14 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Faculty Course Evaluations close is observed.
From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
Answer: "
"Which Carnegie Tech School before 1973 was a college for women?
","['cmuhistory_d402.txt', 'cmuhistory_d402.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which Carnegie Tech School before 1973 was a college for women?

Context: The first U.S. drama degree was awarded in 1914 at Carnegie Tech. It began offering graduate degrees. In 1919, the first doctorate (in civil engineering) was awarded to Mao Yisheng, a student from China. It laid the groundwork for a research institution, recruiting leading scientists, offering sponsored fellowships with government and industry leaders and pioneering nontraditional interdisciplinary research, which brought together physicists, chemists and metallurgists, for example. Interdisciplinary research would become the hallmark of Carnegie Mellon research. It initiated the 'Carnegie Plan' in 1938, a new curriculum that required science and engineer students to take courses in humanities and social sciences in order to better understand the needs of society. Carnegie died in 1919, but his vision for an educated public lived on after him. Carnegie Tech - Post-war Years With the end of World War II, the latter half of the 20th century brought unprecedented growth to Carnegie Tech. In 1956, the arrival of the first IBM computer to campus was revolutionary, initiating a university culture where information technology pervaded virtually all areas of study. University culture also changed in 1973 when Margaret Morrison closed and women joined their male peers in classrooms and dorms. The times were changing, and Tech positioned itself at the forefront, opening three new schools: 1948: The Graduate School of Industrial Administration, later renamed the David A. Tepper School of Business, focusing on quantitative analysis and pioneering the field of management science. 1968: School of Urban and Public Affairs, later renamed the H. John Heinz III College, providing graduate training for work in the public sector. 1986: School of Computer Science, pioneering computing and artificial intelligence, led by interdisciplinary efforts of Allen Newell and Herbert Simon. Carnegie Mellon University In 1967, Carnegie Tech merged with the Mellon Institute, a science research center founded by the Mellon family of Pittsburgh, to become known as Carnegie Mellon University. The merger built upon a long history of support from the Mellons. It allowed Carnegie Mellon to establish the last of its current pillars: the Mellon College of Science and the College of Humanities and Social Sciences, now known as Marianna Brown Dietrich College of Humanities and Social Sciences.
History - CMU - Carnegie Mellon University Carnegie Mellon University ——— Search Search CMU › About › History Andrew Carnegie A self-educated ""working boy"" who loved books, Andrew Carnegie emigrated from Scotland in 1848 and settled in Pittsburgh, Pa. Attending night school and borrowing books, Carnegie went from factory worker in a textile mill to successful entrepreneur and industrialist. He rose to prominence by founding what became the world's largest steel producing company by the end of the 19th century. Carnegie Technical Schools At one point the richest man in the world, Carnegie believed that ""to die rich is to die disgraced."" He turned his attention to writing, social activism and philanthropy, determined to establish educational opportunities for the general public where few existed. In 1900, he donated $1 million for the creation of a technical institute for the city of Pittsburgh, envisioning a school where working-class men and women of Pittsburgh could learn practical skills, trades and crafts that would enhance their careers, lives and communities. ""My heart is in the work,"" he stated, which would become part of the school's official motto. The Carnegie Technical Schools offered two- and three-year certificates in the arts as well as in engineering disciplines and included a college for women, Margaret Morrison Carnegie College. Carnegie Tech – Early Years Soon faced with the demand for baccalaureate programs, Carnegie Technical Schools began offering bachelor's degrees through its College of Engineering and College of Fine Arts, becoming the Carnegie Institute of Technology, or ""Carnegie Tech."" During the first half of the 20th century, with support from Andrew Carnegie and other funders, Carnegie Tech laid the foundation for a school on the cutting edge. Some key developments were: It expanded from two buildings into an elegant 20th century campus designed in the beaux arts architectural style, housing a wealth of machine shops, studios and laboratories — the hands-on center of learning that persists today. It pioneered conservatory degree programs in music and drama, in addition to visual art and design programs. The first U.S. drama degree was awarded in 1914 at Carnegie Tech. It began offering graduate degrees. In 1919, the first doctorate (in civil engineering) was awarded to Mao Yisheng, a student from China.
Answer: "
"What was the conclusion of the study regarding the effectiveness of query rewriting techniques using large language models for multilingual, document-grounded question-answering systems?
","['eric nyberg_444737639aeea4e1e616509e368afb0bae8f89d6_metadata.txt', 'teruko mitamura_444737639aeea4e1e616509e368afb0bae8f89d6_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What was the conclusion of the study regarding the effectiveness of query rewriting techniques using large language models for multilingual, document-grounded question-answering systems?

Context: Faculty Name: eric nyberg
Paperid: 444737639aeea4e1e616509e368afb0bae8f89d6
Title: Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting for Multilingual Document-Grounded QA
Year: 2023
Abstract: The DialDoc 2023 shared task has expanded the document-grounded dialogue task to encompass multiple languages, despite having limited annotated data. This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-agnostic approach is superior. Additionally, the study investigates the impact of query rewriting techniques using large language models, such as ChatGPT, on multilingual, document-grounded question-answering systems. The experiments conducted demonstrate that, for the examples examined, query rewriting does not enhance performance compared to the original queries. This failure is due to topic switching in final dialogue turns and irrelevant topics being considered for query rewriting.
Authors: Srinivas Gowriraj, Soham Dinesh Tiwari, Mitali Potnis, Srijan Bansal, T. Mitamura, Eric Nyberg
Venue: Workshop on Document-grounded Dialogue and Conversational Question Answering
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-gnostic approach is superior.'}
Url: https://aclanthology.org/2023.dialdoc-1.11.pdf
Faculty Name: teruko mitamura
Paperid: 444737639aeea4e1e616509e368afb0bae8f89d6
Title: Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting for Multilingual Document-Grounded QA
Year: 2023
Abstract: The DialDoc 2023 shared task has expanded the document-grounded dialogue task to encompass multiple languages, despite having limited annotated data. This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-agnostic approach is superior. Additionally, the study investigates the impact of query rewriting techniques using large language models, such as ChatGPT, on multilingual, document-grounded question-answering systems. The experiments conducted demonstrate that, for the examples examined, query rewriting does not enhance performance compared to the original queries. This failure is due to topic switching in final dialogue turns and irrelevant topics being considered for query rewriting.
Authors: Srinivas Gowriraj, Soham Dinesh Tiwari, Mitali Potnis, Srijan Bansal, T. Mitamura, Eric Nyberg
Venue: Workshop on Document-grounded Dialogue and Conversational Question Answering
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-gnostic approach is superior.'}
Url: https://aclanthology.org/2023.dialdoc-1.11.pdf
Answer: "
"Where did Graham Neubig get his PhD?
","['graham_neubig.txt', 'graham neubig_9bce3661f01825ad56dc9d2b3d254fd9e3792360_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Where did Graham Neubig get his PhD?

Context: Graham Neubig
Associate Professor, Language Technologies Institute
Research Area
Machine Learning, Machine Translation, Natural Language Processing and Computational Linguistics, Spoken Interfaces and Dialogue Processing

Education
Master of Science in Intelligent Information Systems
Research
My research is concerned with language and its role in human communication. In particular, my long-term research goal is to break down barriers in human-human or human-machine communication through the development of natural language processing (NLP) technologies. This includes the development of technology for machine translation, which helps break down barriers in communication for people who speak different languages, and natural language understanding, which helps computers understand and respond to human language. Within this overall goal of breaking down barriers to human communication, I have focused on several aspects of language that both make it interesting as a scientific subject, and hold potential for the construction of practical systems.

Advances in core NLP technology: Human language is complex and nuanced, and handling this complexity in a computational way requires sophisticated algorithms or learning techniques, the development of which is far from a solved problem. The first major area of my work focuses on advances in core NLP technology, which improve the accuracy with which we can analyze, translate, generate, or reply to textual inputs. 

Models of language associated with other modalities: Human language does not exist in a vacuum, and is often associated with context from the world around it. The second thread of my research focuses on models of natural language associated with other modalities, tackling the problems, and exploiting the fascinating possibilities presented by having text associated with other types of information. Specifically, I have worked extensively with speech, and am also interested in associations between natural and formal languages such as source code.

Learning from naturally occurring data: Language data exists in abundance, particularly due to the advent of the internet, and it is possible to acquire extremely large amounts of this data for scientific or engineering purposes. A third thread of my research focuses on learning NLP models from naturally occurring data through the use of unsupervised, semi-supervised, or active learning, which allows us to exploit this data to improve models while reducing the necessity for costly manual annotation.

http://www.phontron.com/
5409 —Gates & Hillman Centers
gneubig@cs.cmu.edu
Faculty Name: graham neubig
Paperid: 9bce3661f01825ad56dc9d2b3d254fd9e3792360
Title: Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach
Year: 2023
Abstract: Humans work together to solve common problems by having discussions, explaining, and agreeing or disagreeing with each other. Similarly, if a system can have discussions with humans when solving tasks, it can improve the system's performance and reliability. In previous research on explainability, it has only been possible for the system to make predictions and for humans to ask questions about them rather than having a mutual exchange of opinions. This research aims to create a dataset and computational framework for systems that discuss and refine their predictions through dialogue. Through experiments, we show that the proposed system can have beneficial discussions with humans improving the accuracy by up to 25 points in the natural language inference task.
Authors: Masahiro Kaneko, Graham Neubig, Naoaki Okazaki
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This research aims to create a dataset and computational framework for systems that discuss and refine their predictions through dialogue and shows that the proposed system can have beneficial discussions with humans improving the accuracy by up to 25 points in the natural language inference task.'}
Url: http://arxiv.org/pdf/2305.11789
Answer: "
"In fall 2023, When are the Semester & Mini-2 Faculty Course Evaluations open?
","['Meta-Data-Calendar-2425.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, When are the Semester & Mini-2 Faculty Course Evaluations open?

Context: Day: 'Monday', Event: 'Mini-2 pass/no pass & withdrawal deadline (3)', Semester: 'Fall 2024 (F24)'
Date: '2024-11-25', Day: 'Monday', Event: 'Semester & Mini-2 Faculty Course Evaluations open', Semester: 'Fall 2024 (F24)'
Start Date: '2024-11-27', End Date: '2023-11-29', Days: 'Wednesday to Friday', Event: 'Thanksgiving Break; No Classes', Semester: 'Fall 2024 (F24)'
Date: '2024-12-06', Day: 'Friday', Event: 'Semester & Mini-2 Last Day of Classes ', Semester: 'Fall 2024 (F24)'
Date: '2024-12-06', Day: 'Friday', Event: 'Semester & Mini-2 voucher deadline (4)', Semester: 'Fall 2024 (F24)'
Start Date: '2024-12-09', End Date: '2023-12-10', Days: 'Monday to Tuesday', Event: 'Final Exams ', Semester: 'Fall 2024 (F24)'
Date: '2024-12-11', Day: 'Monday', Event: 'Reading Day ', Semester: 'Fall 2024 (F24)'
Start Date: '2024-12-12', End Date: '2023-12-13', Days: 'Thursday to Friday', Event: 'Final Exams ', Semester: 'Fall 2024 (F24)'
Date: '2024-12-14', Day: 'Saturday', Event: 'Reading Day ', Semester: 'Fall 2024 (F24)'
Date: '2024-12-15', Day: 'Sunday', Event: 'Final Exams ', Semester: 'Fall 2024 (F24)'
Date: '2024-12-15', Day: 'Sunday', Event: 'Semester & Mini-2 Faculty Course Evaluations close', Semester: 'Fall 2024 (F24)'
Date: '2024-12-16', Day: 'Monday', Event: 'Make-Up Final Exams', Semester: 'Fall 2024 (F24)'
Date: '2024-12-18', Day: 'Wednesday', Event: 'Final Grades Due by 4 pm ',
On Monday, 28 August, 2023, during the Fall 2023 (F23) semester, Semester & Mini-1 Classes Begin is observed.
On Friday, 01 September, 2023, during the Fall 2023 (F23) semester, Mini-1 add, audit & tuition adjustment drop deadline  (1) is observed.
On Monday, 04 September, 2023, during the Fall 2023 (F23) semester, Labor Day is observed, leading to No Classes & University Closed.
On Monday, 11 September, 2023, during the Fall 2023 (F23) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Wednesday, 20 September, 2023, during the Fall 2023 (F23) semester, Mini-1 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 02 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Pass/no pass & withdrawal deadline (3) is observed.
On Monday, 02 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Faculty Course Evaluations open is observed.
On Monday, 09 October, 2023, during the Fall 2023 (F23) semester, Semester drop deadline is observed, leading to withdrawal grade assigned after this date.
On Friday, 13 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Last Day of Classes is observed.
On Friday, 13 October, 2023, during the Fall 2023 (F23) semester, Mini-1 voucher election deadline (4) is observed.
From 13 October, 2023,Friday to 14 October, 2023,Saturday marks the Family Weekend for Fall 2023 (F23) semester.
On Saturday, 14 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Exams is observed.
On Saturday, 14 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Faculty Course Evaluations close is observed.
From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
Answer: "
"How many StuCo or Student Led Courses are going to be held in Spring 2024?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many StuCo or Student Led Courses are going to be held in Spring 2024?

Context: In Semester Spring 2024, from the department of StuCo (Student Led Courses), the subject titled 'Student Taught Courses (StuCo): Taylor Swift Through the Eras' with Course ID 98301 and Section A offers 3.0 units. The Class meets Tuesday between 08:00PM and 08:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Slomsky located in Building BH, Room 235A.
In Semester Spring 2024, from the department of StuCo (Student Led Courses), the subject titled 'Student Taught Courses (StuCo): Introduction to Freestyle Rap' with Course ID 98303 and Section A offers 3.0 units. The Class meets Tuesday between 07:00PM and 07:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Abel, Yallapragada, Naidu located in Building WEH, Room 5316.
In Semester Spring 2024, from the department of StuCo (Student Led Courses), the subject titled 'Student Taught Courses (StuCo): Hype for Types' with Course ID 98317 and Section A offers 3.0 units. The Class meets Tuesday between 07:00PM and 07:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Alsuhaibani, Battleman, Simkin, O'Flynn located in Building WEH, Room 5312.
In Semester Spring 2024, from the department of StuCo (Student Led Courses), the subject titled 'Student Taught Courses (StuCo): Introduction to Greek Mythology' with Course ID 98336 and Section A offers 3.0 units. The Class meets Tuesday between 07:00PM and 07:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Laroia located in Building GHC, Room 5222.
In Semester Spring 2024, from the department of StuCo (Student Led Courses), the subject titled 'Student Taught Courses (StuCo): Build Your Own Breadboard Computer' with Course ID 98341 and Section A offers 3.0 units. The Class meets Tuesday between 07:00PM and 08:20PM ET.
In Semester Spring 2024, from the department of StuCo (Student Led Courses), the subject titled 'Student Taught Courses (StuCo): Taylor Swift Through the Eras' with Course ID 98301 and Section A offers 3.0 units. The Class meets Tuesday between 08:00PM and 08:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Slomsky located in Building BH, Room 235A.
In Semester Spring 2024, from the department of StuCo (Student Led Courses), the subject titled 'Student Taught Courses (StuCo): Introduction to Freestyle Rap' with Course ID 98303 and Section A offers 3.0 units. The Class meets Tuesday between 07:00PM and 07:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Abel, Yallapragada, Naidu located in Building WEH, Room 5316.
In Semester Spring 2024, from the department of StuCo (Student Led Courses), the subject titled 'Student Taught Courses (StuCo): Hype for Types' with Course ID 98317 and Section A offers 3.0 units. The Class meets Tuesday between 07:00PM and 07:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Alsuhaibani, Battleman, Simkin, O'Flynn located in Building WEH, Room 5312.
In Semester Spring 2024, from the department of StuCo (Student Led Courses), the subject titled 'Student Taught Courses (StuCo): Introduction to Greek Mythology' with Course ID 98336 and Section A offers 3.0 units. The Class meets Tuesday between 07:00PM and 07:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Laroia located in Building GHC, Room 5222.
In Semester Spring 2024, from the department of StuCo (Student Led Courses), the subject titled 'Student Taught Courses (StuCo): Build Your Own Breadboard Computer' with Course ID 98341 and Section A offers 3.0 units. The Class meets Tuesday between 07:00PM and 08:20PM ET.
Answer: "
"What is the benefit of FLARE over existing retrieval augmented LMs?
","['jamie callan_88884b8806262a4095036041e3567d450dba39f7_metadata.txt', 'graham neubig_88884b8806262a4095036041e3567d450dba39f7_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the benefit of FLARE over existing retrieval augmented LMs?

Context: Faculty Name: jamie callan
Paperid: 88884b8806262a4095036041e3567d450dba39f7
Title: Active Retrieval Augmented Generation
Year: 2023
Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.
Authors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work proposes Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens.'}
Url: http://arxiv.org/pdf/2305.06983
Faculty Name: graham neubig
Paperid: 88884b8806262a4095036041e3567d450dba39f7
Title: Active Retrieval Augmented Generation
Year: 2023
Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.
Authors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work proposes Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens.'}
Url: http://arxiv.org/pdf/2305.06983
Answer: "
"What is Robert Frederking's phone number according to the MCDS handbook?
","['mcds-student-handbook-2023_2024.txt', 'robert_frederking.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is Robert Frederking's phone number according to the MCDS handbook?

Context: To impact society in a transformative way — regionally, nationally, and 
globally — by engaging with partners outside the traditional borders of the 
university campus. 
1.4 MCDS Contact Information 
The people responsible for administering the MCDS degree are: 
 
Jennifer M Lucas 
Academic Program Manager 
Master of Computational Data Science  
Language Technologies Institute 
School of Computer Science  
Carnegie Mellon University 
Gates-Hillman Center 6415 
5000 Forbes Avenue, Pgh, PA 15213 
Phone: (412) 268-9870 
Fax: (412) 268-7287 
 
 
Carolyn Penstein Rosé, Director 
Master of Computational Data Science  
Language Technologies Institute 
School of Computer Science 
Carnegie Mellon University 
Gates-Hillman Center 5419 
5000 Forbes Avenue, Pgh, PA 15213 
Phone: (412) 268-4525 
Fax: (412) 268-7287 
Robert Frederking 
Graduate Program Chair 
Language Technologies Institute 
School of Computer Science  
Carnegie Mellon University 
Gates-Hillman Center 6515 
5000 Forbes Avenue, Pgh, PA 15213 
Phone: (412) 268-6656 
Mona Diab, LTI Director 
Language Technologies Institute 
School of Computer Science  
Carnegie Mellon University 
Gates-Hillman Center 5723 
5000 Forbes Avenue, Pgh, PA 15213 
Phone: (412) 268-3669 
 
The Language Technologies Institute is located primarily on the 5 th and 6th floors of the 
Gates Hillman Complex (GHC) on Carnegie Mellon’s Pittsburgh campus: 
 
Language Technologies Institute 
Carnegie Mellon University 
5000 Forbes Avenue 
Gates Hillman Complex 5402, LTI 
Pittsburgh, PA 15241-3891 
 
412-268-6591 (phone) 
412-268-6298 (fax) 
http://www.lti.cs.cmu.edu/ 
 
 
9
1.5 University Policies and Expectations 
Each member of the Carnegie Mellon community must be familiar with university 
policies and guidelines.
Robert Frederking
Associate Dean for PhD Programs and Chair for MLT Program, Language Technologies Institute
Contact 6515 —Gates & Hillman Centers
Email ref@cs.cmu.edu
Phone 412-268-6656
Answer: "
"How many Electrical & Computer Engineering courses are going to be held in Summer 2024?
","['metadata_course_summer_one_all_24.txt', 'metadata_course_summer_one_all_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many Electrical & Computer Engineering courses are going to be held in Summer 2024?

Context: In Semester Summer One(All) 2024, from the department of Electrical & Computer Engineering, the subject titled 'Reading and Research' with Course ID 18990 and Section R offers 0-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Bain located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Electrical & Computer Engineering, the subject titled 'Reading and Research' with Course ID 18990 and Section SV offers 0-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the San Jose, California location,led by experienced instructor Bain located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Electrical & Computer Engineering, the subject titled 'Internship for Electrical and Computer Engineering Applied MS Students' with Course ID 18993 and Section A offers 0.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Bain located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Electrical & Computer Engineering, the subject titled 'Internship for Electrical and Computer Engineering Applied MS Students' with Course ID 18993 and Section B offers 0.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Bain located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Electrical & Computer Engineering, the subject titled 'Internship for Electrical and Computer Engineering Applied MS Students' with Course ID 18993 and Section SA offers 0.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the San Jose, California location,led by experienced instructor Bain located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Electrical & Computer Engineering, the subject titled 'Internship for Electrical and Computer Engineering Applied MS Students' with Course ID 18993 and Section SB offers 0.0 units. The Class meets Schedule will be added between NA and NA ET.
The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Electrical & Computer Engineering, the subject titled 'Summer Internship' with Course ID 18499 and Section A offers 0-3 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Bain located in Building TBA, Room None.
In Semester Summer One(All) 2024, from the department of Electrical & Computer Engineering, the subject titled 'Undergraduate Projects' with Course ID 18580 and Section A offers VAR units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Bain located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Electrical & Computer Engineering, the subject titled 'Foundations of Computer Systems' with Course ID 18613 and Section A offers 12.0 units. The Class meets Tuesday Wednesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kesden located in Building CMU, Room REMOTE.
In Semester Summer One(All) 2024, from the department of Electrical & Computer Engineering, the subject titled 'Foundations of Computer Systems' with Course ID 18613 and Section SV offers 12.0 units. The Class meets Tuesday Wednesday Thursday between 06:30AM and 07:50AM ET. Students attend lectures at the San Jose, California location,led by experienced instructor Kesden located in Building CMU, Room REMOTE.
In Semester Summer One(All) 2024, from the department of Electrical & Computer Engineering, the subject titled 'M.S. Graduate Project I' with Course ID 18980 and Section A offers 0-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Bain located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Electrical & Computer Engineering, the subject titled 'M.S.
Answer: "
"In spring 2024, Who is the instructor for course 15195?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, Who is the instructor for course 15195?

Context: In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section T offers 12.0 units. The Class meets Wednesday between 07:00PM and 08:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5210.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section U offers 12.0 units. The Class meets Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5208.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Mathematical Foundations for Computer Science' with Course ID 15151 and Section NA offers 12.0 units. The Class meets Monday Wednesday Friday between 10:00AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Peng located in Building GHC, Room 4307.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Mathematical Foundations for Computer Science' with Course ID 15151 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 01:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Peng located in Building GHC, Room 4301.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Mathematical Foundations for Computer Science' with Course ID 15151 and Section B offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Peng located in Building GHC, Room 4301.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Competition Programming I' with Course ID 15195 and Section A offers 5.0 units. The Class meets Wednesday between 07:00PM and 09:50PM ET.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section T offers 12.0 units. The Class meets Wednesday between 07:00PM and 08:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5210.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section U offers 12.0 units. The Class meets Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5208.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Mathematical Foundations for Computer Science' with Course ID 15151 and Section NA offers 12.0 units. The Class meets Monday Wednesday Friday between 10:00AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Peng located in Building GHC, Room 4307.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Mathematical Foundations for Computer Science' with Course ID 15151 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 01:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Peng located in Building GHC, Room 4301.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Mathematical Foundations for Computer Science' with Course ID 15151 and Section B offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Peng located in Building GHC, Room 4301.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Competition Programming I' with Course ID 15195 and Section A offers 5.0 units. The Class meets Wednesday between 07:00PM and 09:50PM ET.
Answer: "
"The first two years of the PhD program are similar to what master's program?
","['mlt-student-handbook-2023-2024.txt', 'mlt-student-handbook-2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: The first two years of the PhD program are similar to what master's program?

Context: The proposal MUST be accepted by the committee by the end of 
October, or the student will not be allowed to enroll in 11-929  Masters Thesis II! 
 Sometime during the semester when the student enrolls in 11-929  Masters Thesis II 
(typically, their final semester), the student will distribute a draft of the thesis to the 
committee for initial review. This should be done as early as feasible, to avoid last-
minute surprises that could delay final approval of the thesis. 
 The thesis work culminates in submission of the final version of the thesis document, 
followed by a public presentation of the work in an LTI seminar (or other suitable public 
forum). Since the defense is public, the LTI graduate program administrator must 
receive all the information required for a public announcement at least one week before 
the defense. The Masters Thesis presentation is somewhat less rigorous than a PhD 
thesis defense. The presentation must communicate the research work done, similar to 
a conference paper presentation. The committee will observe the presentation, and 
then decide whether the thesis and presentation were acceptable, or whether further 
work is required. Unlike a PhD defense, only a simple majority vote of the committee is 
required for approval. 
 Although students are required to enroll in the appropriate course sequence of two 
Masters Thesis courses, it is not required that students finish the thesis by the end of 
that second semester. If a student requires more time to revise the thesis to the 
committee's satisfaction, and adequately present the work, an incomplete grade will be 
assessed in the Masters Thesis course, until such time as the work and presentation are 
accepted. The student will still be allowed to walk in Spring Commencement, if all other 
requirements for the MLT degree have been completed. Students should note that any 
financial support beyond the end of the semester will be on a case-by-case basis, and 
must be arranged in advance with the project supporting them. Students are strongly 
encouraged to finish the thesis work within one (1) year following the semester they 
enroll for the first Masters Thesis course.
It is to the student's advantage to avoid switching advisors, especially late in 
their graduate studies, because forging a strong student-advisor relationship takes time. 
4.3 Optional Masters Thesis 
MLT students may optionally elect to complete a Masters Thesis; it is not required. Students 
who intend to apply for the PhD program should consider the MLT Thesis as one of several 
ways to demonstrate their research potential. Other possibilities include innovative research in 
a research project or class project work. 
The MLT Thesis must be completed according to the following guidelines. Note in particular the 
October 31 proposal deadline! 
 There are two course components required for students who elect to complete the M.S. 
Thesis: 11-928  Masters Thesis I (with the chosen thesis advisor, typically for 12 units, 
typically in the Fall of their second year) and 11-929  Masters Thesis II in the following 
Spring. 
 Each of these two courses counts towards 6 units of MLT-level LTI course credit; if the 
student goes on to an LTI PhD, the second course (11-929) counts towards one PhD lab 
requirement.   
 The MLT thesis may focus on either a significant research result or a significant 
implementation of a new technology. The student should work with their advisor to 
define an interesting but bounded thesis research project. The MLT thesis culminates in 
MLT Graduate Student Handbook 
Page 16 
 
a publication-quality paper that clearly describes the work in terms of its research 
significance. 
 The MLT thesis committee will be composed of a thesis advisor and two additional 
readers, to be chosen by the student in consultation with the advisor, during the Fall 
Masters Thesis I course. At least 2 of the 3 committee members must be SCS faculty 
working in Language Technologies. 
 To receive a final grade for the first semester's course, the student must have prepared 
a written proposal of at least five pages, describing the scientific thesis to be 
investigated, any relevant research already completed by the student, a comparison 
with related work by others, and a detailed description of the work to be done in the 
next semester. The proposal MUST be accepted by the committee by the end of 
October, or the student will not be allowed to enroll in 11-929  Masters Thesis II!
Answer: "
"What is the MOS-Q achieved by the MQTTS quantizer with a code size of 1024, on the VoxCeleb test set?
","['alexander rudnicky_4b8d3ede673ddeab9dfb5184da6b748d7a526754_content_1.txt', 'shinji watanabe_4b8d3ede673ddeab9dfb5184da6b748d7a526754_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the MOS-Q achieved by the MQTTS quantizer with a code size of 1024, on the VoxCeleb test set?

Context: Title: A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech
Authors: Li-Wei Chen, Shinji Watanabe, Alexander Rudnicky
Section: 5.2 Performance
MOS-N scores. However, the P-FID of MQTTS is noticeably lower than that of VITS, suggesting that MQTTS generates more diverse samples. We conjecture that generating speech signals in parallel is a much harder task compared to autoregressive modeling. Therefore, non-autoregressive models tend to focus on a smaller set of common speaking styles across the corpus due to insufficient capacity. If we scale VITS from 40M to 100M, we see a decrease in P-FID with the same MOS-N, suggesting that bigger model capacity enables modeling of higher diversity, but without improvements in naturalness. We did not include a 200M version of VITS as we failed to find a configuration that makes the training converge. 5 sam- ples with the same text from both systems in Figure 4 further support our claim of the diversity comparison. This might also explain the lower MCD of VITS, where the syntheses have conservative prosody patterns that are more tolerant in terms of MCD. One possible example is the duration of internal pauses. VITS syntheses mostly contain short intermittent pauses, while MQTTS often generates longer pauses while not uncommon in natural speech can potentially cause large misalignment with the ground truth, lowering MCD. MQTTS performs better in terms of both intelligibility and speaker transferability. We find MQTTS captures utterance-level properties (i.e., emotion, speaking rate) better compared to VITS. For naturalness, we observe a consistently improving MOS-N of MQTTS as the model capacity grows. It demonstrates different scaling properties: higher model capacity brings naturalness to MQTTS, but diversity to VITS. Comparing the same parameter size (100M) for both VITS and MQTTS, MQTTS wins out in all metrics except MCD, which we explained earlier. This suggests MQTTS is generally better than VITS, given enough resources. Additionally, we observed overfitting for both 100M and 200M of MQTTS, but with a higher severity for the 200M version.
Title: A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech
Authors: Li-Wei Chen, Shinji Watanabe, Alexander Rudnicky
Section: 5.2 Performance
MOS-N scores. However, the P-FID of MQTTS is noticeably lower than that of VITS, suggesting that MQTTS generates more diverse samples. We conjecture that generating speech signals in parallel is a much harder task compared to autoregressive modeling. Therefore, non-autoregressive models tend to focus on a smaller set of common speaking styles across the corpus due to insufficient capacity. If we scale VITS from 40M to 100M, we see a decrease in P-FID with the same MOS-N, suggesting that bigger model capacity enables modeling of higher diversity, but without improvements in naturalness. We did not include a 200M version of VITS as we failed to find a configuration that makes the training converge. 5 sam- ples with the same text from both systems in Figure 4 further support our claim of the diversity comparison. This might also explain the lower MCD of VITS, where the syntheses have conservative prosody patterns that are more tolerant in terms of MCD. One possible example is the duration of internal pauses. VITS syntheses mostly contain short intermittent pauses, while MQTTS often generates longer pauses while not uncommon in natural speech can potentially cause large misalignment with the ground truth, lowering MCD. MQTTS performs better in terms of both intelligibility and speaker transferability. We find MQTTS captures utterance-level properties (i.e., emotion, speaking rate) better compared to VITS. For naturalness, we observe a consistently improving MOS-N of MQTTS as the model capacity grows. It demonstrates different scaling properties: higher model capacity brings naturalness to MQTTS, but diversity to VITS. Comparing the same parameter size (100M) for both VITS and MQTTS, MQTTS wins out in all metrics except MCD, which we explained earlier. This suggests MQTTS is generally better than VITS, given enough resources. Additionally, we observed overfitting for both 100M and 200M of MQTTS, but with a higher severity for the 200M version.
Answer: "
"Where will the Buggy Showcase happen this year?
","['cmubuggy_d403.txt', 'Apr-11_Eventno_5_BuggyShowcase.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Where will the Buggy Showcase happen this year?

Context: This year's buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It's one of the few times spectators can see buggies up close. Dave Singh explains how Fringe builds its buggies. Each team keeps its actual processes a secret, but each buggy has certain features such as a body, pushbar, wheels and driving and braking mechanisms. Behind Singh are other buggies built by Fringe. Behind the Wheel Drivers, have the closest connection to the vehicles, aside from the mechanics who spent countless hours building and maintaining individual buggies. While races are brief, each driver must log a number of practice runs to qualify to race. Teams practice nights and weekends throughout the fall and spring semesters as weather permits. Buggies are often built around drivers, so the fit can be snug, and drivers are often less than 5 feet, 3 inches in height. Most — but not all — are women. Tishya Girdhar, a junior in neuroscience, is social chair for Fringe. She came to CMU wanting to drive, but first she had to get over being claustrophobic. She started as a mechanic and got behind the wheel for last year's Sweepstakes. ""Our team's philosophy is to teach everyone how to do everything,"" Girdhar said. ""But I came to CMU wanting to drive. I wanted to drive so badly."" Amy Chen demonstrates how a driver is positioned inside of Boson, a Fringe buggy built in 2016. Safety is a top priority for teams. All drivers are required to have five pieces of safety gear: mouth guard, goggles, a harness that includes three points of contact to the body of the buggy, gloves and a helmet. And during practice runs and races, flaggers man the course to let drivers know if it's safe to proceed. The streets of the race course haven't changed, but the condition of the course varies due to potholes, which can make or break a fast run. The last course records were set in 2017, but this year potholes are making for some challenging conditions, said Amy Chen, a senior in psychology who has been driving for four years.
Event: Buggy Showcase
Date: 4/11/24
Time: 12:00 PM-2:00 PM ET
Participants/Audience: Open to entire CMU community 
Event Details: 
Kick off your Carnival Weekend with Buggy! View the latest buggy designs, talk with the teams and vote for the Buggy People’s Choice Award. 

Note: No registration required. No event fee. This event is open to the entire CMU community and their guests.
Answer: "
"What percentage of the families investigated in ""SHAP-based Prediction of Mother's History of Depression to Understand the Influence on Child Behavior"" were white?
","['louis philippe morency_6838c43e702a3f995967ba2e3edd5f65ff5f5511_metadata.txt', 'louis philippe morency_6838c43e702a3f995967ba2e3edd5f65ff5f5511_content_3.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What percentage of the families investigated in ""SHAP-based Prediction of Mother's History of Depression to Understand the Influence on Child Behavior"" were white?

Context: Faculty Name: louis philippe morency
Paperid: 6838c43e702a3f995967ba2e3edd5f65ff5f5511
Title: SHAP-based Prediction of Mother's History of Depression to Understand the Influence on Child Behavior
Year: 2023
Abstract: Depression strongly impacts parents’ behavior. Does parents’ depression strongly affect the behavior of their children as well? To investigate this question, we compared dyadic interactions between 73 depressed and 75 non-depressed mothers and their adolescent child. Families were of low income and 84% were white. Child behavior was measured from audio-video recordings using manual annotation of verbal and nonverbal behavior by expert coders and by multimodal computational measures of facial expression, face and head dynamics, prosody, speech behavior, and linguistics. For both sets of measures, we used Support Vector Machines. For computational measures, we investigated the relative contribution of single versus multiple modalities using a novel approach to SHapley Additive exPlanations (SHAP). Computational measures outperformed manual ratings by human experts. Among individual computational measures, prosody was the most informative. SHAP reduction resulted in a four-fold decrease in the number of features and highest performance (77% accuracy; positive and negative agreements at 75% and 76%, respectively). These findings suggest that maternal depression strongly impacts the behavior of adolescent children; differences are most revealed in prosody; multimodal features together with SHAP reduction are most powerful.
Authors: Maneesh Bilalpur, Saurabh Hinduja, Laura Cariola, Lisa B. Sheeber, Nicholas B Allen, Louis-Philippe Morency, Jeffrey F. Cohn
Venue: International Conference on Multimodal Interaction
Tldr: None
Url: https://dl.acm.org/doi/pdf/10.1145/3577190.3614136
Title: SHAP-based Prediction of Mother’s History of Depression to Understand the Influence on Child Behavior
Authors: Maneesh Bilalpur, Laura Cariola, Lisa Sheeber, Saurabh Hinduja, Nicholas Allen, Louis-Philippe Morency, Jefrey F. Cohn
Section: 5 DISCUSSION
classifer leaves room for exploring ensemble approaches such as random forest and XGBoost. Four, generalization of the proposed feature selection approach across diferent prediction frameworks remains as future work.
Answer: "
"What loss functions are proposed in the Fairness Continual Learning approach?
","['bhiksha raj_ac856b6b7b3f32fb34320b7170526d3ab15ba5f3_metadata.txt', 'bhiksha raj_ac856b6b7b3f32fb34320b7170526d3ab15ba5f3_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What loss functions are proposed in the Fairness Continual Learning approach?

Context: Faculty Name: bhiksha raj
Paperid: ac856b6b7b3f32fb34320b7170526d3ab15ba5f3
Title: Fairness Continual Learning Approach to Semantic Scene Understanding in Open-World Environments
Year: 2023
Abstract: Continual semantic segmentation aims to learn new classes while maintaining the information from the previous classes. Although prior studies have shown impressive progress in recent years, the fairness concern in the continual semantic segmentation needs to be better addressed. Meanwhile, fairness is one of the most vital factors in deploying the deep learning model, especially in human-related or safety applications. In this paper, we present a novel Fairness Continual Learning approach to the semantic segmentation problem. In particular, under the fairness objective, a new fairness continual learning framework is proposed based on class distributions. Then, a novel Prototypical Contrastive Clustering loss is proposed to address the significant challenges in continual learning, i.e., catastrophic forgetting and background shift. Our proposed loss has also been proven as a novel, generalized learning paradigm of knowledge distillation commonly used in continual learning. Moreover, the proposed Conditional Structural Consistency loss further regularized the structural constraint of the predicted segmentation. Our proposed approach has achieved State-of-the-Art performance on three standard scene understanding benchmarks, i.e., ADE20K, Cityscapes, and Pascal VOC, and promoted the fairness of the segmentation model.
Authors: Thanh-Dat Truong, Hoang-Quan Nguyen, B. Raj, Khoa Luu
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A novel Fairness Continual Learning approach to the semantic segmentation problem is presented, in particular, a new fairness continual learning framework is proposed based on class distributions and a novel Prototypical Contrastive Clustering loss is proposed to address the significant challenges in continual learning.'}
Url: https://arxiv.org/pdf/2305.15700
Title: Fairness Continual Learning Approach to Semantic Scene Understanding in Open-World Environments
Authors: Thanh-Dat Truong, Hoang-Quan Nguyen, Bhiksha Raj, Khoa Luu
Section: 10.5 Memory Efficiency
We would like to highlight that storing prototypical vectors requires significantly less memory than using the additional teacher model as used in distillation approaches [12]. For example, in the ADE20K benchmark, storing DeepLab-V3 (151 classes) requires 58.664M parameters, while storing 152 prototypical 2048-D vectors (including the unknown cluster) only uses 0.311M parameters. In addition, the computation of loss with is cheaper than a forward pass of the entire network used in distillation. Therefore, in terms of computational cost and memory, our approach remains more efficient compared to knowledge distillation approaches.
Answer: "
"What was the title of paper that proposed a new task, OUTDOOR?
","['yonatan bisk_8035a247980cb18abf2bb7b9d96e7d4c63622ef2_content_0.txt', 'yonatan bisk_8035a247980cb18abf2bb7b9d96e7d4c63622ef2_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What was the title of paper that proposed a new task, OUTDOOR?

Context: Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation
Authors: Quanting Xie, Tianyi Zhang, Kedi Xu, Matthew Johnson-Roberson, Yonatan Bisk
Section: VII. CONCLUSIONS
The emergence of LLMs has opened new avenues in the embodied agent domain. This paper introduced the OUTDOOR task, a pioneering approach aimed at propelling embodied agents into challenging outdoor settings. Further, we introduced a novel, general, mechanism for using LLMs to reason about robot plans in unseen environments, and proposed the CASR, the first metric to assess balance between reasoning and action for embodied agents. Our formulation more closely mirrors how humans navigate and explore, trading off between thinking and acting to both leverage what we know in general and can see in the specific.
Faculty Name: yonatan bisk
Paperid: 8035a247980cb18abf2bb7b9d96e7d4c63622ef2
Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation
Year: 2023
Abstract: Robots should exist anywhere humans do: indoors, outdoors, and even unmapped environments. In contrast, the focus of recent advancements in Object Goal Navigation(OGN) has targeted navigating in indoor environments by leveraging spatial and semantic cues that do not generalize outdoors. While these contributions provide valuable insights into indoor scenarios, the broader spectrum of real-world robotic applications often extends to outdoor settings. As we transition to the vast and complex terrains of outdoor environments, new challenges emerge. Unlike the structured layouts found indoors, outdoor environments lack clear spatial delineations and are riddled with inherent semantic ambiguities. Despite this, humans navigate with ease because we can reason about the unseen. We introduce a new task OUTDOOR, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain. Additionally, we show impressive results on both a simulated drone and physical quadruped in outdoor environments. Our agent has no premapping and our formalism outperforms naive LLM-based approaches
Authors: Quanting Xie, Tianyi Zhang, Kedi Xu, M. Johnson-Roberson, Yonatan Bisk
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A new task OUTDOOR is introduced, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain are introduced.'}
Url: https://arxiv.org/pdf/2309.10103
Answer: "
"Which stage of a model's lifecycle does the Pentathlon benchmark focus on?
","['emma strubell_84d20ad9f42d80dfd5130a6362d5422be8a6bdc3_metadata.txt', 'yonatan bisk_e41482f4ee984f17382f6cdd900df094d928be06_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which stage of a model's lifecycle does the Pentathlon benchmark focus on?

Context: Faculty Name: emma strubell
Paperid: 84d20ad9f42d80dfd5130a6362d5422be8a6bdc3
Title: Efficiency Pentathlon: A Standardized Arena for Efficiency Evaluation
Year: 2023
Abstract: Rising computational demands of modern natural language processing (NLP) systems have increased the barrier to entry for cutting-edge research while posing serious environmental concerns. Yet, progress on model efficiency has been impeded by practical challenges in model evaluation and comparison. For example, hardware is challenging to control due to disparate levels of accessibility across different institutions. Moreover, improvements in metrics such as FLOPs often fail to translate to progress in real-world applications. In response, we introduce Pentathlon, a benchmark for holistic and realistic evaluation of model efficiency. Pentathlon focuses on inference, which accounts for a majority of the compute in a model's lifecycle. It offers a strictly-controlled hardware platform, and is designed to mirror real-world applications scenarios. It incorporates a suite of metrics that target different aspects of efficiency, including latency, throughput, memory overhead, and energy consumption. Pentathlon also comes with a software library that can be seamlessly integrated into any codebase and enable evaluation. As a standardized and centralized evaluation platform, Pentathlon can drastically reduce the workload to make fair and reproducible efficiency comparisons. While initially focused on natural language processing (NLP) models, Pentathlon is designed to allow flexible extension to other fields. We envision Pentathlon will stimulate algorithmic innovations in building efficient models, and foster an increased awareness of the social and environmental implications in the development of future-generation NLP models.
Authors: Hao Peng, Qingqing Cao, Jesse Dodge, Matthew E. Peters, Jared Fernandez, Tom Sherborne, Kyle Lo, Sam Skjonsberg, Emma Strubell, Darrell Plessas, Iz Beltagy, Pete Walsh, Noah A. Smith, Hannaneh Hajishirzi
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': ""Pentathlon is a benchmark for holistic and realistic evaluation of model efficiency, which focuses on inference, which accounts for a majority of the compute in a model's lifecycle, and is designed to mirror real-world applications scenarios.""}
Url: https://arxiv.org/pdf/2307.09701
Faculty Name: yonatan bisk
Paperid: e41482f4ee984f17382f6cdd900df094d928be06
Title: WebArena: A Realistic Web Environment for Building Autonomous Agents
Year: 2023
Abstract: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.
Authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper builds an environment for language-guided agents that is highly realistic and reproducible, and creates an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.'}
Url: https://arxiv.org/pdf/2307.13854
Answer: "
"When is the Tartans Got Talent show at the carnival?
","['Apr-12_Eventno_54_TartansGotTalent.txt', 'Apr-12_Eventno_18_CarnivalRides.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When is the Tartans Got Talent show at the carnival?

Context: Event: Tartans Got Talent
Date: 4/12/24
Time: 8:30 PM-10:00 PM ET
Participants/Audience: Open to entire CMU community 
Event Details: 
Join the Spring Carnival Committee (SCC) in celebrating talented students with the annual ""Tartans Got Talent!"" show. 

Note: No registration required. No event fee. This event is open to the entire CMU community and their guests.
Event: Carnival Rides
Date: 4/12/24
Time: 11:00 AM-11:00 PM ET
Participants/Audience: Open to entire CMU community 
Event Details: 
The Carnival Headquarters Tent is #1 on the map - look for the giant inflatable Scotty dog!

Make this your first stop to check in, download the app and register to win the daily Tartan Swag Bag giveaway.

Weekend hours:

Thursday: 3-11 p.m.
Friday & Saturday: 11 a.m.-11 p.m.

Note: No registration required. Individual tickets or all-day passes may be purchased onsite at the vendor’s ticketing booth.
Answer: "
"Who is the first author of the paper BASS: Block-wise Adaptation for Speech Summarization?
","['bhiksha raj_3bd320ddb25886417ae90011b00f13f5d558097b_content_2.txt', 'rita singh_3bd320ddb25886417ae90011b00f13f5d558097b_content_2.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who is the first author of the paper BASS: Block-wise Adaptation for Speech Summarization?

Context: Title: BASS: Block-wise Adaptation for Speech Summarization
Authors: Roshan Sharma, Kenneth Zheng, Rita Singh, Bhiksha Raj
Section: 7. References
Ann Arbor, Michigan: Association for Computational Linguistics, Jun. 2005, pp. 65–72. [Online]. Available: https://aclanthology.org/W05-0909 [27] T. Zhang*, V. Kishore*, F. Wu*, K. Q. Weinberger, and Y. Artzi, “Bertscore: Evaluating text generation with bert,” in International Conference on Learning Representations, 2020. [Online]. Available: https://openreview.net/forum?id=SkeHuCVFDr
Title: BASS: Block-wise Adaptation for Speech Summarization
Authors: Roshan Sharma, Kenneth Zheng, Rita Singh, Bhiksha Raj
Section: 7. References
Ann Arbor, Michigan: Association for Computational Linguistics, Jun. 2005, pp. 65–72. [Online]. Available: https://aclanthology.org/W05-0909 [27] T. Zhang*, V. Kishore*, F. Wu*, K. Q. Weinberger, and Y. Artzi, “Bertscore: Evaluating text generation with bert,” in International Conference on Learning Representations, 2020. [Online]. Available: https://openreview.net/forum?id=SkeHuCVFDr
Answer: "
"What is the name of the method introduced in ""Semantic Pyramid AutoEncoder for Multimodal Generation""?
","['alexander hauptmann_376f494126d1ea4f571ea0263c43ac2b6331800a_metadata.txt', 'yonatan bisk_376f494126d1ea4f571ea0263c43ac2b6331800a_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the name of the method introduced in ""Semantic Pyramid AutoEncoder for Multimodal Generation""?

Context: Faculty Name: alexander hauptmann
Paperid: 376f494126d1ea4f571ea0263c43ac2b6331800a
Title: SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs
Year: 2023
Abstract: In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.
Authors: Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A. Ross, Irfan Essa, Yonatan Bisk, Ming Yang, K. Murphy, A. Hauptmann, Lu Jiang
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.'}
Url: http://arxiv.org/pdf/2306.17842
Faculty Name: yonatan bisk
Paperid: 376f494126d1ea4f571ea0263c43ac2b6331800a
Title: SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs
Year: 2023
Abstract: In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.
Authors: Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A. Ross, Irfan Essa, Yonatan Bisk, Ming Yang, K. Murphy, A. Hauptmann, Lu Jiang
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.'}
Url: http://arxiv.org/pdf/2306.17842
Answer: "
"For additional information about the MSAII program, who should you contact?
","['handbook-msaii-2022-2023.txt', 'handbook-msaii-2022-2023.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: For additional information about the MSAII program, who should you contact?

Context: 1 
 
 
 
 
 
 
Master of Science in 
Artificial Intelligence and Innovation 
(MSAII) 
 
Student Handbook 
2022-2023 
 
 
 
 
 
 
Revised:  October 2, 2022 
2 
 
Table of Contents 
1 
Welcome . 6 
2 
Mission, Vision and Philosophy. 6 
3 
Carnegie Mellon Statement of Assurance . 6 
4 
Carnegie Mellon Code . 7 
5 
University Policies and Expectations . 8 
6 
Academic Calendar . 8 
7 
School and Departmental Information . 8 
8 
MSAII Policies . 9 
8.1 
The MSAII Degree . 9 
8.2 
Program Contacts . 10 
8.3 
The Reasonable Person Principle (RPP) . 10 
8.4 
Academic Calendar . 10 
9 
The Language Technologies Institute . 11 
9.1 
Working Space for MS Students . 11 
9.2 
Photocopies and Printers . 11 
9.3 
Computers for MS Students . 11 
10 
Degree Requirements . 12 
10.1 
Course Grade and GPA Requirements . 12 
10.2 
Course Requirements . 13 
10.2.1 
Incomplete Grades . 16 
10.2.2 
Change of grades and missing grades . 16 
10.2.3 
Transferring to another program . 17 
10.2.4 
Intellectual Property Policy . 17 
10.2.5 
Teaching Requirements . 17 
10.3 
Advising . 17 
10.3.1 
Student Advising . 17 
10.3.2 
Monitoring Progress . 17 
10.3.3 
Degree Certification . 17 
10.4 
Internship . 18 
10.5 
Employment Eligibility Verification .
MSAII students who require longer than the standard time to complete their degree 
requirements are expected to remain in close contact with the program, and will be certified at 
the end of the semester in which they have completed their degree requirements. Students must 
refer to the CMU Policy on Master’s Student Statute of Limitations 
(https://www.cmu.edu/policies/student-and-student-life/masters-students-statute-of-
limitations.html) regarding guidelines and restrictions which place an upper limit on the 
maximum length of time allowable for master’s degree completion and certification. 
11.1.11 Additional Guidance for Students   
Program of study. Students seeking guidance about their program of study and degree 
requirements should consult with their academic advisor and/or appropriate associate dean.  
Financial aid and student account. Students are expected to make normal progress toward 
their degree in order to graduate within the standard timeframe for their program of study. 
Under U.S. Federal Title IV regulations, student eligibility for federal financial aid is contingent 
upon enrollment in and successful completion of courses that are counted as credit toward their 
current degree program. To receive the maximum amount of federal financial aid for which they 
may be eligible, students must enroll each semester in at least 36 units that count toward their 
current degree level. (See separate guidance regarding integrated degree completion.)  
Students should consult with their designated college liaison in The HUB regarding billing and 
financial aid, particularly for early completion, longer-than-standard completion, or integrated 
undergraduate and master’s degree programs.  
International students. Immigration status for students in F-1 and J-1 non-immigrant status is 
tied to making normal progress toward completing degree requirements. Therefore, F-1 and J-1 
27 
 
students who are considering completing their degree requirements early, anticipating longer-
than-standard completion, or moving from an undergraduate to a graduate student classification 
(integrated undergraduate-graduate study) should consult with their designated advisor in the 
Office of International Education (OIE) to ensure compliance with immigration regulations. 
11.1.12 Withdrawal   
Students who need to withdraw from the program (leave the university with no intention of 
returning) for personal, medical or academic reasons must contact the MSAII Director to 
discuss their plans and fill out the appropriate form. For more information refer to 
www.cmu.edu/policies/student-and-student-life/student-leave.html.
Answer: "
"What is the 5 letter abbreviation for the MS in artificial intelligence and innovation degree?
","['program_info_MasterofScienceinArtificialIntelligenceandInnovation.txt', 'handbook-msaii-2022-2023.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the 5 letter abbreviation for the MS in artificial intelligence and innovation degree?

Context: Academic Program Name:
Master of Science in Artificial Intelligence and Innovation

Website:
https://lti.cs.cmu.edu/academics/masters-programs/msaii.html

Overview:
The Master of Science in Artificial Intelligence and Innovation (MSAII) program is a successor to the M.S. in Biotechnology, Innovation and Computing (MSBIC). It combines a rigorous AI and machine learning curriculum with real-world team experience in identifying an AI market niche and developing a responsive product in cooperation with external stakeholders. The core program, which lasts four semesters and leads to a capstone project, focuses on both intrapreneurship and entrepreneurship, equipping graduates to either begin a startup or develop a new organization within an existing company. Students will also gain critical practical skills, such as making persuasive technical presentations, assembling development teams, and evaluating the potential of new market ideas.

Requirements:
Incoming students generally hold undergraduate degrees in computer science, software engineering, bioinformatics or bioengineering. To earn the MSAII degree, you must pass courses in the Core Curriculum, the Knowledge Requirements and Electives. You must also complete a capstone project in which you work on a development project as part of the Core Curriculum. In total, you will complete 192 eligible units of study, including 84 units of Core Curriculum (including the 36-unit Capstone), 72 units of Knowledge Requirements and at least 36 units of approved Electives.
For full requirements and program details, read the MSAII Handbook.

Curriculum:
To earn the MSAII degree, you must pass courses in the Core Curriculum, the Knowledge Requirements and Electives. You must also complete a capstone project in which you work on a development project as part of the Core Curriculum. In total, you will complete 195 eligible units of study, including 84 units of Core Curriculum (including the 36-unit Capstone), 72 units of Knowledge Requirements, at least 36 units of approved Electives and the LTI Practicum (3 units, associated with your summer internship).  The purpose of the Core Curriculum is to prepare you to discover new AI applicants and develop them into a product suitable for further development, often leading to a startup enterprise.
Here's a detailed breakdown of the curriculum.
Preparation Prerequisite
Historically, students typically need a refresher on basic computer science systems before beginning graduate work at CMU.
1 
 
 
 
 
 
 
Master of Science in 
Artificial Intelligence and Innovation 
(MSAII) 
 
Student Handbook 
2022-2023 
 
 
 
 
 
 
Revised:  October 2, 2022 
2 
 
Table of Contents 
1 
Welcome . 6 
2 
Mission, Vision and Philosophy. 6 
3 
Carnegie Mellon Statement of Assurance . 6 
4 
Carnegie Mellon Code . 7 
5 
University Policies and Expectations . 8 
6 
Academic Calendar . 8 
7 
School and Departmental Information . 8 
8 
MSAII Policies . 9 
8.1 
The MSAII Degree . 9 
8.2 
Program Contacts . 10 
8.3 
The Reasonable Person Principle (RPP) . 10 
8.4 
Academic Calendar . 10 
9 
The Language Technologies Institute . 11 
9.1 
Working Space for MS Students . 11 
9.2 
Photocopies and Printers . 11 
9.3 
Computers for MS Students . 11 
10 
Degree Requirements . 12 
10.1 
Course Grade and GPA Requirements . 12 
10.2 
Course Requirements . 13 
10.2.1 
Incomplete Grades . 16 
10.2.2 
Change of grades and missing grades . 16 
10.2.3 
Transferring to another program . 17 
10.2.4 
Intellectual Property Policy . 17 
10.2.5 
Teaching Requirements . 17 
10.3 
Advising . 17 
10.3.1 
Student Advising . 17 
10.3.2 
Monitoring Progress . 17 
10.3.3 
Degree Certification . 17 
10.4 
Internship . 18 
10.5 
Employment Eligibility Verification .
Answer: "
"What is the process of exchanging pushers during the race called?
","['cmubuggy_d403.txt', 'cmubuggy_d403.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the process of exchanging pushers during the race called?

Context: An mural celebrating Buggy can be found in CMU's Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions. Sweepstakes Slang Buggy: Vehicle being raced and also a nickname for the competition. Chute: A section of the freeroll portion of the buggy course (near the southwestern end of Frew Street at its intersection with Schenley Drive) where buggies make the sharp righthand turn from Schenley Drive onto Frew Street. Chute Flagger: Team member who provides a signal for buggy drivers to know when to start the right-hand turn from Schenley Drive onto Frew Street. Driver: Person who travels with a buggy and controls the vehicles via steering and braking systems. Pushbar: Structure attached to a buggy that a person pushes to propel that buggy forward. Pusher: Person who propels a buggy via a pushbar along one of the five hills of the buggy course. Shell: Entire outer structure or covering of a buggy that determines that buggy's aerodynamic characteristics. Transition: Procedure whereby one pusher finishes pushing a buggy and the next pusher in sequence starts to push that same buggy. Members of Fringe, celebrating their 50th year of Buggy racing, allowed a rare visit into its shop for a behind-the-scenes tour. This year Fringe is planning to roll four different vehicles, built and maintained in the Fringe workshop in the basement of the East Campus Garage, known as the ""Froom."" ""We're used to saying everything with 'fr' in front of it but when we say something in front of other people, it gets them confused,"" said Fringe head mechanic Dave Singh, who will graduate in May with a bachelor's degree in mechanical engineering and biomedical engineering. Fringe vehicles often are named with the letter ""B,"" like Boson, Blueshift, Bissa and Bumper. Other teams, such as Apex, often use names that connotate fire, while the SDC (Student Dormitory Committee) team, uses names such as Vice, Bane, Avarice and Malice. This year's buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It's one of the few times spectators can see buggies up close.
Buggy Races Keep Rolling at Carnegie Mellon - News - Carnegie Mellon University Carnegie Mellon University ——— Search Search Search this site only News News › Stories › Archives › 2019 › April › Buggy Races Keep Rolling at Carnegie Mellon April 10, 2019 Buggy Races Keep Rolling at Carnegie Mellon In its 99th year, the tradition is a Spring Carnival treat By Heidi Opdyke opdyke(through)andrew.cmu.edu Media Inquiries Julie Mattera Marketing and Communications jmattera(through)cmu.edu 412-268-2902 Sweepstakes, also known as the Buggy Races, has come a long way at Carnegie Mellon University. The slick, torpedo-like vessels carrying drivers with nerves of steel are a far cry from the two-man teams that once changed places halfway through a race and rode in everything from rain barrels with bicycle wheels to three-wheeled ash cans 99 years ago. Today, it takes six people to maneuver the .84 -mile course around Schenley Park's Flagstaff Hill. But while five pushers and a driver navigate the course's hills, dozens of people are needed to make a successful race happen. A year of planning goes into just over two minutes of racing. The Machine The basics of a buggy are straightforward, but teams are often secretive in how they build the machines, in particular the way they brake, steer and what types of wheels are used. Each has a body, pushbar for runners to move the machine up the hills, wheels, a safety harness and driving and braking mechanisms. Some also include fairings, a type of housing around the wheels that help reduce drag, make the vehicle quieter and just looks cool. Fairings have been a key feature for the Fringe team in recent years, which is celebrating its 50th anniversary of Buggy. ""We have a reputation of being the quietest on the course,"" said Diya Nuxoll, who wrapped up her bachelor's degree in mechanical engineering in December and is working on an advanced studies master's degree in design and manufacturing. Nuxoll leads the Fringe team as one of two head mechanics. An mural celebrating Buggy can be found in CMU's Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions. Sweepstakes Slang Buggy: Vehicle being raced and also a nickname for the competition.
Answer: "
"What does POMDP stand for?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What does POMDP stand for?

Context: The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Poczos located in Building TEP, Room 1403.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Representation and Generation in Neuroscience and AI' with Course ID 10733 and Section A offers 12.0 units. The Class meets Monday Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Wehbe located in Building GHC, Room 4102.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Responsible AI' with Course ID 10735 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Heidari, London located in Building POS, Room 151.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Game Theoretic Probability, Statistics and Learning' with Course ID 10880 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ramdas located in Building DH, Room 1211.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Graduate Reading and Research' with Course ID 10920 and Section A offers 12-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Mitchell located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Dissertation Research' with Course ID 10930 and Section A offers 5-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Mitchell located in Building DNM, Room DNM.
The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Poczos located in Building TEP, Room 1403.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Representation and Generation in Neuroscience and AI' with Course ID 10733 and Section A offers 12.0 units. The Class meets Monday Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Wehbe located in Building GHC, Room 4102.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Responsible AI' with Course ID 10735 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Heidari, London located in Building POS, Room 151.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Game Theoretic Probability, Statistics and Learning' with Course ID 10880 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ramdas located in Building DH, Room 1211.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Graduate Reading and Research' with Course ID 10920 and Section A offers 12-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Mitchell located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Dissertation Research' with Course ID 10930 and Section A offers 5-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Mitchell located in Building DNM, Room DNM.
Answer: "
"Which LTI prof co-authored the paper titled ""Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning""?
","['sean welleck_papers.txt', 'eric xing_papers.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which LTI prof co-authored the paper titled ""Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning""?

Context: List of 2023 Open Access papers by sean welleck are:
Self-Refine: Iterative Refinement with Self-Feedback
Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning
List of 2023 Open Access papers by eric xing are:
SlimPajama-DC: Understanding Data Combinations for LLM Training
Fusing Models with Complementary Expertise
Making Scalable Meta Learning Practical
3D Open-vocabulary Segmentation with Foundation Models
Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective
One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning
Defending Against Malicious Behaviors in Federated Learning with Blockchain
Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming
Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models
KD-DLGAN: Data Limited Image Generation via Knowledge Distillation
3D Semantic Segmentation in the Wild: Learning Generalized Models for Adverse-Condition Point Clouds
Cuttlefish: Low-Rank Model Training without All the Tuning
Does compressing activations help model parallel training?
Federated Learning as Variational Inference: A Scalable Expectation Propagation Approach
Memory-adaptive Depth-wise Heterogenous Federated Learning
Identification of Nonlinear Latent Hierarchical Models
StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields
Judging LLM-as-a-judge with MT-Bench and Chatbot Arena
Understanding Masked Autoencoders via Hierarchical Latent Variable Models
LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers
LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset
FedNAR: Federated Optimization with Normalized Annealing Regularization
US residents' preferences for sharing of electronic health record and genetic information: a discrete choice experiment.
Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning
GET: a foundation model of transcription across human cell types
Contextualized Networks Reveal Heterogeneous Transcriptomic Regulation in Tumors at Sample-Specific Resolution
Research on the Training Path of Live E-commerce Talents Oriented by Industry Development
Recent progresses on the gamma-ray observations of DAMPE
Effective Multi-Agent Deep Reinforcement Learning Control with Relative Entropy Regularization
Convolutional Neural Network Measurement of Non-Fiducial Electrons Cosmic-Rays Using the DAMPE Experiment.
Answer: "
"In Spring 2024, what time does ""Issues of Practice"" course start at in the morning?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In Spring 2024, what time does ""Issues of Practice"" course start at in the morning?

Context: The Class meets Monday Wednesday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Torello located in Building MM, Room 409.
In Semester Spring 2024, from the department of Architecture, the subject titled 'Constructing Value(s): Economies of Design' with Course ID 48380 and Section A offers 6.0 units. The Class meets Tuesday between 09:00AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Volcy located in Building CFA, Room 214.
In Semester Spring 2024, from the department of Architecture, the subject titled 'Issues of Practice' with Course ID 48381 and Section A offers 6.0 units. The Class meets Thursday between 10:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Coppedge located in Building MM, Room 103.
In Semester Spring 2024, from the department of Architecture, the subject titled 'Ethics and Decision Making in Architecture' with Course ID 48383 and Section A offers 6.0 units. The Class meets Tuesday between 10:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Vavasis located in Building MM, Room 103.
In Semester Spring 2024, from the department of Architecture, the subject titled 'Advanced Synthesis Options Studio II' with Course ID 48410 and Section NA offers 18.0 units. The Class meets Tuesday Thursday between 01:00PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Arscott located in Building POS, Room 146.
In Semester Spring 2024, from the department of Architecture, the subject titled 'Advanced Synthesis Options Studio II' with Course ID 48410 and Section A offers 18.0 units. The Class meets Tuesday Thursday between 01:00PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Bista located in Building MM, Room 312.
The Class meets Monday Wednesday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Torello located in Building MM, Room 409.
In Semester Spring 2024, from the department of Architecture, the subject titled 'Constructing Value(s): Economies of Design' with Course ID 48380 and Section A offers 6.0 units. The Class meets Tuesday between 09:00AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Volcy located in Building CFA, Room 214.
In Semester Spring 2024, from the department of Architecture, the subject titled 'Issues of Practice' with Course ID 48381 and Section A offers 6.0 units. The Class meets Thursday between 10:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Coppedge located in Building MM, Room 103.
In Semester Spring 2024, from the department of Architecture, the subject titled 'Ethics and Decision Making in Architecture' with Course ID 48383 and Section A offers 6.0 units. The Class meets Tuesday between 10:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Vavasis located in Building MM, Room 103.
In Semester Spring 2024, from the department of Architecture, the subject titled 'Advanced Synthesis Options Studio II' with Course ID 48410 and Section NA offers 18.0 units. The Class meets Tuesday Thursday between 01:00PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Arscott located in Building POS, Room 146.
In Semester Spring 2024, from the department of Architecture, the subject titled 'Advanced Synthesis Options Studio II' with Course ID 48410 and Section A offers 18.0 units. The Class meets Tuesday Thursday between 01:00PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Bista located in Building MM, Room 312.
Answer: "
"What are the protected attributes that CMU does not use in deciding the admission of PhD students?
","['mlt-student-handbook-2023-2024.txt', 'handbook_phd_2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the protected attributes that CMU does not use in deciding the admission of PhD students?

Context: Students must begin their study at CMU in the program that admitted them; this is a university 
policy. 
Students may request to transfer into the MLT program after completing their first semester. 
The student must make the request in writing (or email) to the MLT/PhD Admissions Chair. The 
MLT/PhD Admissions Chair will inform the student about what application materials are 
required; for example, an explanation of why a transfer is desired, a proposed plan of study, a 
proposed advisor, and CMU transcripts. Students that are already enrolled in an LTI degree 
program are not required to retake GRE and TOEFL exams or to produce new transcripts from 
other universities. The MLT program will conduct an expedited admissions process after 
receiving such a request.   
5.1.8 Transferring Out of the MLT Program 
The MLT program does not prevent students from transferring to another degree program. 
Each degree program has its own rules about whether and when transfers into the program are 
permitted. A student that is interested in transferring out of the MLT degree program should 
consult the handbook and Program Director of the desired degree program to learn whether 
transfers are permitted, and if so, how and when to request such a transfer. 
5.1.9 Drop/Add/Withdraw Procedures 
Students taking undergraduate and Masters level courses must follow the procedures and 
deadlines for adding, dropping, and withdrawing from courses as identified on the academic 
calendar. Information can be found at https://www.cmu.edu/hub/registrar/course-
changes.index. Please note that there is a separate calendar for doctoral courses that does not 
apply to Masters students. 
5.1.10  Statute of Limitations 
As outlined in the Masters Students Statute of Limitations, 
https://www.cmu.edu/policies/student-and-student-life/masters-students-statute-of- 
limitations.html. Students will complete all requirements for the masters degree within a 
maximum of seven years from original matriculation as a masters student, or less if required by 
a more restrictive department, school, or college policy. Once this time-to-degree limit has 
lapsed, the person may resume work towards a masters degree only if newly admitted to a 
currently offered masters degree program under criteria determined by that program. 
See also the Duration of Study policy.
Some doctoral course-sections follow a separate Academic Calendar. 
1.5 
Carnegie Mellon University Statement of Assurance  
Carnegie Mellon University does not discriminate in admission, employment, or administration 
of its programs or activities on the basis of race, color, national origin, sex, handicap or disability, 
age, sexual orientation, gender identity, religion, creed, ancestry, belief, veteran status, or genetic 
information.  Furthermore, Carnegie Mellon University does not discriminate and is required not 
to discriminate in violation of federal, state, or local laws or executive orders. 
Inquiries concerning the application of a compliance with this statement should be directed to 
the university ombudsperson, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA 
15213 (412-268-1018). Obtain general information about Carnegie Mellon University by calling 
412-268-2000. 
Carnegie Mellon University publishes an annual campus security and fire safety report describing 
the universitys security, alcohol and drug, sexual assault, and fire safety policies, and containing 
statistics about the number and type of crimes committed on campus, and the number and cause 
of fires in campus residence facilities during the preceding three years. You can obtain a copy by 
contacting the Carnegie Mellon Police Department at 412-268-2323. The annual security and fire 
safety report also is available online at www.cmu.edu/police/annualreports . 
Information regarding the applicable grievance procedures for alleged violations 
of the Statement of Assurance is available at 
https://www.cmu.edu/policies/forms-and-documents/soa-violations.pdf.   
The Office for Institutional Equity and Title IX may be reached at 412-268-7125  
LTI Ph.D. Graduate Student Handbook 
Page 12 
 
or  institutionalequity@cmu.edu. 
1.6 
The Carnegie Mellon Code 
Students at Carnegie Mellon, because they are members of an academic community dedicated to 
the achievement of excellence, are expected to meet the highest standards of personal, ethical, and 
moral conduct possible. 
 These standards require personal integrity, a commitment to honesty without compromise, as 
well as truth without equivocation and a willingness to place the good of the community above 
the good of the self. Obligations once undertaken must be met, commitments kept.
Answer: "
"Did anyone from LTI worked on the paper Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting?
","['maarten sap_57d65e85c62aef04dfb2a48380e415fbb790e5ee_metadata.txt', 'maarten sap_a89c30ceca55783a1b2ff843eb6a4793e4a54b66_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Did anyone from LTI worked on the paper Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting?

Context: Faculty Name: maarten sap
Paperid: 57d65e85c62aef04dfb2a48380e415fbb790e5ee
Title: Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting
Year: 2023
Abstract: None
Authors: Akhila Yerukola, Xuhui Zhou, Elizabeth Clark, Maarten Sap
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: None
Url: https://aclanthology.org/2023.emnlp-main.701.pdf
Title: “Don’t Take This Out of Context!” On the Need for Contextual Models and Evaluations for Stylistic Rewriting
Authors: Akhila Yerukola, Xuhui Zhou, Elizabeth Clark, Maarten Sap
Section: 9 Limitations & Ethical Considerations
our work, we exposed human annotators to toxic content during the evaluation of the de-toxification task. Exposure to such offensive content can be harmful to the annotators (Liu et al., 2016). We aim to work towards developing evaluation strategies that can minimize the exposure of annotators to toxic content. Potentially Inconsistent Human Evaluations In our work, we also assume human judgments as the gold standard. Concurrent work has shown that human evaluation might not always be consistent (Clark et al., 2021; Karpinska et al., 2021); however human judgments continue to be the gold standard for evaluating open-ended text generation.
Answer: "
"In fall 2023, When is Democracy Day and are there any classes?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, When is Democracy Day and are there any classes?

Context: On Friday, 25 October, 2024, during the Fall 2024 (F24) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
On Tuesday, 05 November, 2024, during the Fall 2024 (F24) semester, Democracy Day is observed, leading to No Classes, except Evening classes after 5 pm will still meet.
On Monday, 11 November, 2024, during the Fall 2024 (F24) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Wednesday, 13 November, 2024, during the Fall 2024 (F24) semester, Mini-2 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 18 November, 2024,Monday to 22 November, 2024,Friday marks the Spring 2025 Registration Week for Fall 2024 (F24) semester.
On Monday, 25 November, 2024, during the Fall 2024 (F24) semester, Mini-2 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 25 November, 2024, during the Fall 2024 (F24) semester, Semester & Mini-2 Faculty Course Evaluations open is observed.
From 27 November, 2024,Wednesday to 29 November, 2023,Friday marks the Thanksgiving Break for Fall 2024 (F24) semester which leads to No Classes.
On Friday, 06 December, 2024, during the Fall 2024 (F24) semester, Semester & Mini-2 Last Day of Classes is observed.
On Friday, 06 December, 2024, during the Fall 2024 (F24) semester, Semester & Mini-2 voucher deadline (4) is observed.
From 09 December, 2024,Monday to 10 December, 2023,Tuesday marks the Final Exams for Fall 2024 (F24) semester.
On Monday, 11 December, 2024, during the Fall 2024 (F24) semester, Reading Day is observed.
From 12 December, 2024,Thursday to 13 December, 2023,Friday marks the Final Exams for Fall 2024 (F24) semester.
From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mini-2 Classes Begin is observed.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mid-Semester & Mini-1 grades due by 4 pm is observed.
On Friday, 27 October, 2023, during the Fall 2023 (F23) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
On Tuesday, 07 November, 2023, during the Fall 2023 (F23) semester, Democracy Day is observed, leading to No Classes, except Evening classes after 5 pm will still meet.
On Saturday, 11 November, 2023, during the Fall 2023 (F23) semester, Homecoming is observed.
From 13 November, 2023,Monday to 17 November, 2023,Friday marks the Spring 2024 Registration Week for Fall 2023 (F23) semester.
On Monday, 13 November, 2023, during the Fall 2023 (F23) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Wednesday, 15 November, 2023, during the Fall 2023 (F23) semester, Mini-2 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 22 November, 2023,Wednesday to 24 November, 2023,Friday marks the Thanksgiving Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Mini-2 pass/no pass & withdrawal deadline (2) is observed.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Faculty Course Evaluations open is observed.
On Friday, 08 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Last Day of Classes is observed.
Answer: "
"What are the three main reasons why kNN-LM performs better than standard LMs?
","['graham neubig_c432aff446d55e72a28394a1508e760cc9a25c08_metadata.txt', 'graham neubig_c432aff446d55e72a28394a1508e760cc9a25c08_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the three main reasons why kNN-LM performs better than standard LMs?

Context: Faculty Name: graham neubig
Paperid: c432aff446d55e72a28394a1508e760cc9a25c08
Title: Why do Nearest Neighbor Language Models Work?
Year: 2023
Abstract: Language models (LMs) compute the probability of a text by sequentially computing a representation of an already-seen context and using this representation to predict the next word. Currently, most LMs calculate these representations through a neural network consuming the immediate previous context. However recently, retrieval-augmented LMs have shown to improve over standard neural LMs, by accessing information retrieved from a large datastore, in addition to their standard, parametric, next-word prediction. In this paper, we set out to understand why retrieval-augmented language models, and specifically why k-nearest neighbor language models (kNN-LMs) perform better than standard parametric LMs, even when the k-nearest neighbor component retrieves examples from the same training set that the LM was originally trained on. To this end, we perform a careful analysis of the various dimensions over which kNN-LM diverges from standard LMs, and investigate these dimensions one by one. Empirically, we identify three main reasons why kNN-LM performs better than standard LMs: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature for the kNN distribution. Further, we incorporate these insights into the model architecture or the training procedure of the standard parametric LM, improving its results without the need for an explicit retrieval component. The code is available at https://github.com/frankxu2004/knnlm-why.
Authors: Frank F. Xu, Uri Alon, Graham Neubig
Venue: International Conference on Machine Learning
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper identifies three main reasons why k-nearest neighbor language models (kNN-LM) perform better than standard LMs: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature for the kNN distribution.'}
Url: http://arxiv.org/pdf/2301.02828
Title: Why do Nearest Neighbor Language Models Work?
Authors: Frank F. Xu, Uri Alon, Graham Neubig
Section: E.6.3 Training to Optimize Interpolated Loss
In Section 4.2, we discover that using over-parameterization with standard LM training loss does not further close the gap towards kNN-LM. This suggests that some regularization term may be needed during training to make the multiple embeddings not converge to the same vector, rendering over-parameterization useless. From Table 2, we see that a better interpolated perplexity may not require a very low perplexity when measured only with the extra input representation. However, we still use a standard LM loss to only train the additional embedding matrix, that directly minimizes the perplexity using only the extra input representation. This discrepancy between training and the evaluation with interpolation suggests that training with an alternative loss function that interpolates the base LM’s output with the output using the extra input representation may be beneficial. To test the hypothesis that standard LM training loss do not emphasize the examples where base LM performs badly, we train the extra model’s parameter Wds, with interpolated loss L: L = CrossEntropy(λsoftmax(Wds · hds) + (1− λ)softmax(Wsm · hsm), y) (12) y represents the ground truth label for each context. We only learn the parameter Wds while freezing all other parameters, similar to all other experiments. We choose λ = 0.25 as it is the best hyper-parameter for kNN-LM experiments and our goal for this training is to mimic the loss of kNN-LM after interpolation. This training loss effectively assigns a higher value to the training examples where the base LM’s loss is high, suggesting the need for the extra Wds to help with these hard cases. However, for either “att” for “ffn” for hds, either V or 3V for the number of embeddings in Wds, we are unable to achieve a better perplexity than just the base LM. This suggests that, while nice on paper, the interpolated loss optimization process is not trivial.
Answer: "
"In fall 2023, When do the semester and Mini-1 classes begin?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, When do the semester and Mini-1 classes begin?

Context: On Monday, 28 August, 2023, during the Fall 2023 (F23) semester, Semester & Mini-1 Classes Begin is observed.
On Friday, 01 September, 2023, during the Fall 2023 (F23) semester, Mini-1 add, audit & tuition adjustment drop deadline  (1) is observed.
On Monday, 04 September, 2023, during the Fall 2023 (F23) semester, Labor Day is observed, leading to No Classes & University Closed.
On Monday, 11 September, 2023, during the Fall 2023 (F23) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Wednesday, 20 September, 2023, during the Fall 2023 (F23) semester, Mini-1 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 02 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Pass/no pass & withdrawal deadline (3) is observed.
On Monday, 02 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Faculty Course Evaluations open is observed.
On Monday, 09 October, 2023, during the Fall 2023 (F23) semester, Semester drop deadline is observed, leading to withdrawal grade assigned after this date.
On Friday, 13 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Last Day of Classes is observed.
On Friday, 13 October, 2023, during the Fall 2023 (F23) semester, Mini-1 voucher election deadline (4) is observed.
From 13 October, 2023,Friday to 14 October, 2023,Saturday marks the Family Weekend for Fall 2023 (F23) semester.
On Saturday, 14 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Exams is observed.
On Saturday, 14 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Faculty Course Evaluations close is observed.
From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mini-2 Classes Begin is observed.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mid-Semester & Mini-1 grades due by 4 pm is observed.
On Friday, 27 October, 2023, during the Fall 2023 (F23) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
On Tuesday, 07 November, 2023, during the Fall 2023 (F23) semester, Democracy Day is observed, leading to No Classes, except Evening classes after 5 pm will still meet.
On Saturday, 11 November, 2023, during the Fall 2023 (F23) semester, Homecoming is observed.
From 13 November, 2023,Monday to 17 November, 2023,Friday marks the Spring 2024 Registration Week for Fall 2023 (F23) semester.
On Monday, 13 November, 2023, during the Fall 2023 (F23) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Wednesday, 15 November, 2023, during the Fall 2023 (F23) semester, Mini-2 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 22 November, 2023,Wednesday to 24 November, 2023,Friday marks the Thanksgiving Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Mini-2 pass/no pass & withdrawal deadline (2) is observed.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Faculty Course Evaluations open is observed.
On Friday, 08 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Last Day of Classes is observed.
Answer: "
"When did the MLT application period for Fall 2024 admissions start?
","['Meta-Data-Calendar-2324.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When did the MLT application period for Fall 2024 admissions start?

Context: Day: 'Monday', Event: 'Semester & Mini-2 Faculty Course Evaluations close', Semester: 'Fall 2023 (F23)'
Date: '2023-12-20', Day: 'Wednesday', Event: 'Final Grades Due by 4 pm ', Semester: 'Fall 2023 (F23)'
Start Date: '2023-12-23', End Date: '2024-01-02', Days: 'Saturday to Tuesday', Event: 'Winter Break; University Closed', Semester: 'Fall 2023 (F23)'
Date: '2023-01-08', Day: 'Monday', Event: 'Fall Deans' Lists Posted', Semester: 'Fall 2023 (F23)'
Date: '2024-01-15', Day: 'Monday', Event: 'Martin Luther King Day; No Classes', Semester: 'Spring 2024 (S24)'
Date: '2024-01-16', Day: 'Tuesday', Event: 'First Day of Class', Semester: 'Spring 2024 (S24)'
Date: '2024-01-22', Day: 'Monday', Event: 'Mini-3 add, audit & tuition adjustment drop deadline (1)', Semester: 'Spring 2024 (S24)'
Date: '2024-01-29', Day: 'Monday', Event: 'Semester add, audit & tuition adjustment drop deadline (1)', Semester: 'Spring 2024 (S24)'
Date: '2024-02-07', Day: 'Wednesday', Event: 'Mini-3 course drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Spring 2024 (S24)'
Date: '2024-02-19', Day: 'Monday', Event: 'Mini-3 pass/no pass & withdrawal deadline (3)', Semester: 'Spring 2024 (S24)'
Date: '2024-02-19', Day: 'Monday', Event: 'Mini-3 Faculty Course Evaluations open', Semester: 'Spring 2024 (S24)'
Date: '2024-02-26', Day: 'Monday', Event: 'Semester course drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Spring 2024 (S24)'
Date: '2024-03-01', Day: 'Friday', Event: 'Mini-3 Last Day of Classes ',
From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mini-2 Classes Begin is observed.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mid-Semester & Mini-1 grades due by 4 pm is observed.
On Friday, 27 October, 2023, during the Fall 2023 (F23) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
On Tuesday, 07 November, 2023, during the Fall 2023 (F23) semester, Democracy Day is observed, leading to No Classes, except Evening classes after 5 pm will still meet.
On Saturday, 11 November, 2023, during the Fall 2023 (F23) semester, Homecoming is observed.
From 13 November, 2023,Monday to 17 November, 2023,Friday marks the Spring 2024 Registration Week for Fall 2023 (F23) semester.
On Monday, 13 November, 2023, during the Fall 2023 (F23) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Wednesday, 15 November, 2023, during the Fall 2023 (F23) semester, Mini-2 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 22 November, 2023,Wednesday to 24 November, 2023,Friday marks the Thanksgiving Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Mini-2 pass/no pass & withdrawal deadline (2) is observed.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Faculty Course Evaluations open is observed.
On Friday, 08 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Last Day of Classes is observed.
Answer: "
"How much increase in throughput (single GPU setup) does SAMA showcase in large-scale meta learning benchmarks?
","['emma strubell_a815c3209e7baff4466dbf6e129129511f842b7e_metadata.txt', 'eric xing_a815c3209e7baff4466dbf6e129129511f842b7e_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How much increase in throughput (single GPU setup) does SAMA showcase in large-scale meta learning benchmarks?

Context: Faculty Name: emma strubell
Paperid: a815c3209e7baff4466dbf6e129129511f842b7e
Title: Making Scalable Meta Learning Practical
Year: 2023
Abstract: Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and 2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains.
Authors: Sang Keun Choe, Sanket Vaibhav Mehta, Hwijeen Ahn, W. Neiswanger, Pengtao Xie, Emma Strubell, Eric P. Xing
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients.'}
Url: https://arxiv.org/pdf/2310.05674
Faculty Name: eric xing
Paperid: a815c3209e7baff4466dbf6e129129511f842b7e
Title: Making Scalable Meta Learning Practical
Year: 2023
Abstract: Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and 2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains.
Authors: Sang Keun Choe, Sanket Vaibhav Mehta, Hwijeen Ahn, W. Neiswanger, Pengtao Xie, Emma Strubell, Eric P. Xing
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients.'}
Url: https://arxiv.org/pdf/2310.05674
Answer: "
"Who are the instructors for the data science seminar?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who are the instructors for the data science seminar?

Context: In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Data Science Seminar' with Course ID 11631 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Sap, Strubell located in Building BH, Room A36.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Data Science Seminar' with Course ID 11631 and Section B offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Strubell, Sap located in Building GHC, Room 4307.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Data Science Capstone' with Course ID 11632 and Section A offers 12.0 units. The Class meets Monday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Nyberg located in Building TEP, Room 1403.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'MCDS Independent Study:' with Course ID 11633 and Section NA offers 3-36 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'MCDS Independent Study' with Course ID 11633 and Section A offers 3-36 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rose located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'MCDS Independent Study' with Course ID 11633 and Section B offers 3-36 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Data Science Seminar' with Course ID 11631 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Sap, Strubell located in Building BH, Room A36.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Data Science Seminar' with Course ID 11631 and Section B offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Strubell, Sap located in Building GHC, Room 4307.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Data Science Capstone' with Course ID 11632 and Section A offers 12.0 units. The Class meets Monday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Nyberg located in Building TEP, Room 1403.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'MCDS Independent Study:' with Course ID 11633 and Section NA offers 3-36 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'MCDS Independent Study' with Course ID 11633 and Section A offers 3-36 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rose located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'MCDS Independent Study' with Course ID 11633 and Section B offers 3-36 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building DNM, Room DNM.
Answer: "
"What is the BigCode project about?
","['daniel fried_1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a_metadata.txt', 'daniel fried_1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the BigCode project about?

Context: Faculty Name: daniel fried
Paperid: 1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a
Title: SantaCoder: don't reach for the stars!
Year: 2023
Abstract: The BigCode project is an open-scientific collaboration working on the responsible development of large language models for code. This tech report describes the progress of the collaboration until December 2022, outlining the current state of the Personally Identifiable Information (PII) redaction pipeline, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data. We train 1.1B parameter models on the Java, JavaScript, and Python subsets of The Stack and evaluate them on the MultiPL-E text-to-code benchmark. We find that more aggressive filtering of near-duplicates can further boost performance and, surprisingly, that selecting files from repositories with 5+ GitHub stars deteriorates performance significantly. Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and infilling on the Java, JavaScript, and Python portions of MultiPL-E, despite being a substantially smaller model. All models are released under an OpenRAIL license at https://hf.co/bigcode.
email_pattern = r’([ˆ\s@,?!;:\’\""=)(]+@[ˆ,\s!?;,\’\""=]{3,}[\.][ˆ\s \b\’\""@,?!;:)(.]+)’ For IP addresses, we used the same regular expression as the one used for PII detection. C.2 LIST OF PRIVATE IP ADDRESSES AND POPULAR DNS SERVERS • 8.8.8.8 • 8.8.4.4 • 1.1.1.1 • 1.0.0.1 • 76.76.19.19 • 76.223.122.150 • 9.9.9.9 • 149.112.112.112 • 208.67.222.222 • 208.67.220.220 • 8.26.56.26 • 8.20.247.20 • 94.140.14.14 • 94.140.15.15 C.3 DETECT-SECRETS FILTERS • detect secrets.filters.heuristic.is potential uuid • detect secrets.filters.heuristic.is likely id string • detect secrets.filters.heuristic.is templated secret • detect secrets.filters.heuristic.is sequential string Implementation available at https://github.com/bigcode-project/ bigcode-dataset/blob/6b3f54751b6e38e1ed70f2307331d6943ba39eae/ pii/utils/keys_detection.py#L11. C.4 DETECT-SECRETS PLUGINS • ArtifactoryDetector • AWSKeyDetector • Base64HighEntropyString • HexHighEntropyString • AzureStorageKeyDetector • CloudantDetector • DiscordBotTokenDetector • GitHubTokenDetector • IbmCloudIamDetector • IbmCosHmacDetector • JwtTokenDetector • MailchimpDetector • NpmDetector • SendGridDetector • SlackDetector • SoftlayerDetector • StripeDetector • TwilioKeyDetector Implementation available at https://github.com/bigcode-project/ bigcode-dataset/blob/6b3f54751b6e38e1ed70f2307331d6943ba39eae/ pii/utils/keys_detection.py#L19.
Answer: "
"At what conference was Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning published?
","['sean welleck_papers.txt', 'eric xing_papers.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: At what conference was Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning published?

Context: List of 2023 Open Access papers by sean welleck are:
Self-Refine: Iterative Refinement with Self-Feedback
Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning
List of 2023 Open Access papers by eric xing are:
SlimPajama-DC: Understanding Data Combinations for LLM Training
Fusing Models with Complementary Expertise
Making Scalable Meta Learning Practical
3D Open-vocabulary Segmentation with Foundation Models
Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective
One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning
Defending Against Malicious Behaviors in Federated Learning with Blockchain
Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming
Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models
KD-DLGAN: Data Limited Image Generation via Knowledge Distillation
3D Semantic Segmentation in the Wild: Learning Generalized Models for Adverse-Condition Point Clouds
Cuttlefish: Low-Rank Model Training without All the Tuning
Does compressing activations help model parallel training?
Federated Learning as Variational Inference: A Scalable Expectation Propagation Approach
Memory-adaptive Depth-wise Heterogenous Federated Learning
Identification of Nonlinear Latent Hierarchical Models
StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields
Judging LLM-as-a-judge with MT-Bench and Chatbot Arena
Understanding Masked Autoencoders via Hierarchical Latent Variable Models
LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers
LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset
FedNAR: Federated Optimization with Normalized Annealing Regularization
US residents' preferences for sharing of electronic health record and genetic information: a discrete choice experiment.
Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning
GET: a foundation model of transcription across human cell types
Contextualized Networks Reveal Heterogeneous Transcriptomic Regulation in Tumors at Sample-Specific Resolution
Research on the Training Path of Live E-commerce Talents Oriented by Industry Development
Recent progresses on the gamma-ray observations of DAMPE
Effective Multi-Agent Deep Reinforcement Learning Control with Relative Entropy Regularization
Convolutional Neural Network Measurement of Non-Fiducial Electrons Cosmic-Rays Using the DAMPE Experiment.
Answer: "
"At what journal was ""Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient"" published?
","['lori levin_2cdc646a6b70418e7cbd7fbdb8bb113176c4659f_metadata.txt', 'lori levin_papers.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: At what journal was ""Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient"" published?

Context: Faculty Name: lori levin
Paperid: 2cdc646a6b70418e7cbd7fbdb8bb113176c4659f
Title: Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient
Year: 2023
Abstract: None
Authors: W. Gaetz, C. Dockstader, P. Furlong, S. Amaral, A. Vossough, E. Schwartz, T. Roberts, Lori S. Levin
Venue: Brain Research
Tldr: {'model': 'tldr@v2.0.0', 'text': None}
Url: N/A
List of 2023 Open Access papers by lori levin are:
SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing
Construction Grammar Provides Unique Insight into Neural Language Models
Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation
Syntax and Semantics Meet in the “Middle”: Probing the Syntax-Semantics Interface of LMs Through Agentivity
Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient
Identifying Health-Related Quality of Life Domains after Upper Extremity Transplantation.
Assessment of quality of life after upper extremity transplantation: Framework for patient-reported outcome scale domains
Answer: "
"Does LTI offer a course on ethics?
","['handbook_phd_2023-2024.txt', 'miis-handbook_2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Does LTI offer a course on ethics?

Context: It is the 
responsibility of the entire community to establish and maintain the integrity of our university.  
Carnegie Mellon University educates its students to become professionals who will serve society 
with integrity. The university also creates and disseminates new knowledge and expressions of 
knowledge in ways that benefit society.  
Carnegie Mellon strives to serve the changing needs of society through the three primary goals 
outlined in its mission statement: To create and disseminate knowledge and art through research 
and artistic expression, teaching and learning and transfer to society; to serve students by 
teaching them leadership, problem- solving skills, and the values of quality, ethical behavior, 
responsibility to society and commitments to work; and to pursue the advantages provided by a 
diverse community, open to the exchange of ideas, where discovery and artistic creativity can 
flourish.  
In any presentation, creative, artistic or research, it is the ethical responsibility of each student to 
identify the conceptual sources of the work submitted. Failure to do so is dishonest and is the 
basis for a charge of cheating or plagiarism, which is subject to disciplinary action. 
Please review the University Policy on Academic Integrity. 
The policy includes the University expectations around academic integrity and provides 
definitions of cheating, plagiarism, and unauthorized assistance.  
A review of the Universitys Academic Disciplinary Actions procedures is also recommended.  
Important note: The LTI implements the above policys option of conven[ing] a disciplinary 
hearing according to the procedures of the department/program. Our procedure is as follows: A 
first violation is grounds for dismissal from the graduate program. If we decide not to immediately 
dismiss, the first violation will result in the student being on disciplinary probation.  
If the student commits a second violation while on probation, the penalty is dismissal from the 
graduate program. 
These procedures outline the process for investigating, reporting, and adjudicating violations of 
the University Policy on Academic Integrity. The procedures also outline the appeal process. 
Please see the Appeals of Course Level Action section of The Word and the Office of Community 
Standards & Integrity web page for more information. 
LTI Ph.D.
Carnegie Mellon University educates its students to become professionals who will serve 
society with integrity. The university also creates and disseminates new knowledge and 
expressions of knowledge in ways that benefit society. Carnegie Mellon strives to serve the 
changing needs of society through the three primary goals outlined in its mission statement: to 
create and disseminate knowledge and art through research and artistic expression, teaching 
and learning and transfer to society, to serve students by teaching them leadership and 
problem-solving skills, and the values of quality, ethical behavior, responsibility to society and 
commitments to work, to pursue the advantages provided by a diverse community, open to the 
exchange of ideas, where discovery and artistic creativity can flourish. 
In any presentation, creative, artistic or research, it is the ethical responsibility of each student 
to identify the conceptual sources of the work submitted. Failure to do so is dishonest and is 
the basis for a charge of cheating or plagiarism, which is subject to disciplinary action. 
 
The university has a very clear and specific protocol for responding to alleged violations of 
academic integrity.  Carnegie Mellon's Academic Disciplinary Actions Overview for Graduate 
Students describes procedures for disciplinary actions against graduate students in cases of 
alleged violations of academic regulations and the appeal process.   
Important note:  The LTI implements the above policy’s option of “conven[ing] a disciplinary 
hearing according to the procedures of the department/program”.  Our procedure is as follows: 
a first violation is grounds for dismissal from the graduate program.  If we decide to not 
immediately dismiss, the first violation will result in the student being on disciplinary 
probation.  If a student commits a second violation while on probation, the penalty is 
dismissal from the graduate program. 
6 Academic Policies 
6.1 MIIS Academic Policies 
MIIS Graduate Student Handbook 
Page 24 
 
6.1.1 Double Counting Courses 
A Masters student who uses courses taken as part of another degree program (at Carnegie 
Mellon or elsewhere) toward their program requirements cannot use those same courses 
toward any other M.S. degree offered by the School of Computer Science without prior 
approval. (SCS policy) 
6.1.2 Duration of Study 
MIIS-16 students enrolled for full-time study are expected to complete the degree in three 
semesters of academic study and one summer internship (16 months total).
Answer: "
"What score does the global model achieve in the 5K data NER setting in Zhisong Zhang, Emma Strubell, and Eduard Hovy's paper on data constraints and structured prediction?
","['emma strubell_71debf888acd57bb1baa4c146f31e58c66ea51af_content_1.txt', 'emma strubell_71debf888acd57bb1baa4c146f31e58c66ea51af_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What score does the global model achieve in the 5K data NER setting in Zhisong Zhang, Emma Strubell, and Eduard Hovy's paper on data constraints and structured prediction?

Context: Title: On the Interactions of Structural Constraints and Data Resources for Structured Prediction
Authors: Zhisong Zhang, Emma Strubell, Eduard Hovy
Section: 1 Introduction
influences the outputs and how such influences change with dif- 147 ferent amounts of training data. RQ2: What is the influence of constraints when using more efficient models? Although neural models can obtain impressive results, one shortcoming is that they are usually computationally expensive. Recently, there have been many works on improving model efficiency. Knowledge distillation is one of the most widelyutilized methods, learning a smaller student model from a larger teacher model (Kim and Rush, 2016; Sanh et al., 2019; Jiao et al., 2020). An interesting question to explore is how these more efficient models interact with the explicit incorporation of structural constraints. RQ3: What is the influence of constraints for out-of-domain generalization? We usually expect the model to be able to generalize to scenarios that can be different from those represented by the training data, for example, to different domains or text genres. It will be interesting to explore how the constraints influence predictions for these cases and especially whether there are specific patterns with regard to the discrepancies between the source and the target. To answer these questions, we conduct extensive experiments on three typical structured prediction tasks, including named entity recognition (NER), dependency parsing (DPAR) and an information extraction task of event argument extraction (EAE). We find that models trained with less training data tend to produce outputs that contain more structural violations when using constraint-agnostic greedy decoding. Further applying constrained decoding brings consistent performance improvements and the benefits are more prominent in lower data scenarios (§3.2). A similar trend can be found with regard to model size: Smaller models tend to output more violations with greedy decoding and benefit more from constrained decoding (§3.3). Finally, in cross-genre settings, we find a weak pattern with regard to genre discrepancies: More structural violations tend to be made with greedy decoding when transferring to more distant genres (§3.4).
Title: On the Interactions of Structural Constraints and Data Resources for Structured Prediction
Authors: Zhisong Zhang, Emma Strubell, Eduard Hovy
Section: 5 Conclusion
In this work, we explore the interactions of constraint-based decoding algorithms and the amounts of training data for typical structured prediction tasks in NLP. Specifically, we train local models with different amounts of training data and analyze the influence of whether to adopt constrained decoding or not. The results show that when the model is trained with less data, the predictions contain more structural violations with greedy decoding and there are more benefits on model performance by further applying constrained decoding. Such patterns also generally hold with more efficient models and when transferring across text genres, where there are further interesting patterns with regard to model sizes and genre distances. Limitations This work has several limitations. First, we only experiment on English datasets. It would be interesting to explore whether the general patterns hold for non-English languages with different structural properties. Moreover, we only explore incorporating hard constraints for decoding with local models at testing time. Exploring more applications of structural constraints, such as learning with constraints, or incorporating other types of constraints, such as soft ones, would be promising future directions. Finally, we only explore three simple sentence-level structured prediction tasks, while extentions can be made to more complex tasks with larger output space, such as text generation or document-level information extraction, where constraints may play more interesting roles.
Answer: "
"In the paper ""COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive Statements"", what is the name of the dataset created for studying the contextual dynamics of offensiveness?
","['maarten sap_185ace5661963e2e1eb998e739e4110272a6bb43_metadata.txt', 'maarten sap_185ace5661963e2e1eb998e739e4110272a6bb43_content_2.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the paper ""COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive Statements"", what is the name of the dataset created for studying the contextual dynamics of offensiveness?

Context: Faculty Name: maarten sap
Paperid: 185ace5661963e2e1eb998e739e4110272a6bb43
Title: COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive Statements
Year: 2023
Abstract: Warning: This paper contains content that may be offensive or upsetting. Understanding the harms and offensiveness of statements requires reasoning about the social and situational context in which statements are made. For example, the utterance""your English is very good""may implicitly signal an insult when uttered by a white man to a non-white colleague, but uttered by an ESL teacher to their student would be interpreted as a genuine compliment. Such contextual factors have been largely ignored by previous approaches to toxic language detection. We introduce COBRA frames, the first context-aware formalism for explaining the intents, reactions, and harms of offensive or biased statements grounded in their social and situational context. We create COBRACORPUS, a dataset of 33k potentially offensive statements paired with machine-generated contexts and free-text explanations of offensiveness, implied biases, speaker intents, and listener reactions. To study the contextual dynamics of offensiveness, we train models to generate COBRA explanations, with and without access to the context. We find that explanations by context-agnostic models are significantly worse than by context-aware ones, especially in situations where the context inverts the statement's offensiveness (29% accuracy drop). Our work highlights the importance and feasibility of contextualized NLP by modeling social factors.
Authors: Xuhui Zhou, Haojie Zhu, Akhila Yerukola, Thomas Davidson, Jena D. Hwang, Swabha Swayamdipta, Maarten Sap
Venue: Annual Meeting of the Association for Computational Linguistics
Tldr: {'model': 'tldr@v2.0.0', 'text': 'COBRA frames are introduced, the first context-aware formalism for explaining the intents, reactions, and harms of offensive or biased statements grounded in their social and situational context, and the importance and feasibility of contextualized NLP by modeling social factors are highlighted.'}
Url: http://arxiv.org/pdf/2306.01985
Title: COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive Statements
Authors: Xuhui Zhou, Hao Zhu, Akhila Yerukola, Thomas Davidson, Jena D. Hwang, Swabha Swayamdipta, Maarten Sap
Section: 6 Conclusion & Discussion
approach where we would examine different interpretations. We leave that up for future work. Dual Use We aim to combat the negative effects and harms of discriminatory language on already marginalized people (Sap et al., 2019b; Davidson et al., 2019). It is possible however that our frames, dataset, and models could be used to perpetuate harm against those very people. We do not endorse the use of our data for those purposes. Risk of Suppressing Speech Our frames, dataset, and models are built with content moderation in mind, as online spaces are increasingly riddled with hate and abuse and content moderators are struggling to sift through all of the content. We hope future work will examine frameworks for using our frames to help content moderators. We do not endorse the use of our system to suppress speech without human oversight and encourage practitioners to take non-censorship-oriented approaches to content moderation (e.g., counterspeech (Tekiroğlu et al., 2022)). Harms of Exposing Workers to Toxic Content The verification process of COBRACORPUS and COBRACORPUS-CF is performed by human annotators. Exposure to such offensive content can be harmful to the annotators (Liu et al., 2016). We mitigated these by designing minimum annotation workload, paying workers above minimum wage ($7-12), and providing them with crisis management resources. Our annotation work is also supervised by an Institutional Review Board (IRB).
Answer: "
"What is Martial Herbert's email address?
","['25things_d400.txt', 'handbook-msaii-2022-2023.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is Martial Herbert's email address?

Context: At the core of all modern Apple devices --- including iPhones, iPads and MacBooks --- is the Mach kernel, developed at CMU under the leadership of then-professor Rick Rashid. 11. Computer chess, 1990Could a computer play chess at the level of the world’s best players? For many years, it was considered the “holy grail” of artificial intelligence. Hitech, developed by CMU researcher Hans Berliner (CS’74), was the first computer to achieve grandmaster status. CMU alumni played key roles in developing “Deep Blue,” the IBM machine that beat human chess champion Garry Kasparov in 1997. 12. Java, 1991 As a CMU grad student, James Gosling (CS’83) worked on the Andrew project, which stressed interoperability between computers, whether they were Macs, IBMs or Unix machines. Those lessons served Gosling well when he developed Java, the first programming language able to run on almost any platform. 13. Email attachments, 1992Steve Jobs liked the email system built into CMU’s Andrew so much that he tried to hire Nathaniel Borenstein (CS’81,’85) and the rest of his team to create a similar program for Apple. Borenstein didn’t take the offer, but he did like Jobs’ idea about attaching documents to email. Borenstein went on to develop the MIME standard that’s used by all email programs to send photos and other files over the Internet. 14. Web search engines, 1994The World Wide Web was still in its toddler stage when CMU researcher Michael “Fuzzy” Mauldin (CS’83,’89) developed one of the first successful search engines, Lycos. It was the most-visited site on the Web by 1999. 15. Model checking, 1994CMU professor Edmund Clarke had long stressed the importance of verifying computer hardware and software through a formal problem-solving technique called “model checking.” In 1994, his arguments gained new weight with the discovery that Intel’s amazing new Pentium chip made errors on certain math problems. Clarke would go on to receive the Turing Award for his role in the development of model checking. 16.
6 Academic Calendar 
The Academic Calendar can be found at https://www.cmu.edu/hub/calendar/index.html and 
provides information on all deadlines including registration dates, class start dates, add/drop 
deadlines, exam dates and more. 
7 School and Departmental Information 
The following are key personnel with whom you may interact during your time at Carnegie 
Mellon: 
 
Martial Hebert 
 
 
 
 
 
Dean, School of Computer Science 
9 
 
 
University Professor 
 
GHC 6105 
 
412-268-5704 
 
hebert@cs.cmu.edu  
https://www.cs.cmu.edu/~hebert/  
 
David Garlan 
Associate Dean for Master’s Programs, SCS 
Professor 
TCS 420 
garlan@cs.cmu.edu 
http://www.cs.cmu.edu/~garlan/academics.htm  
 
Carolyn Rosé  
 
 
 
 
Interim Director, Language Technologies Institute 
 
Professor 
 
GHC 5415 
 
412-268-7130 
 
cprose@cs.cmu.edu 
8 MSAII Policies 
The Language Technologies Institute (LTI) has prepared this statement of policies, program 
requirements, guidance, process and procedures for students in the M.S. in Artificial Intelligence 
and Innovation (MSAII) program. A copy of this handbook is also available online at the 
program website.   
The University Student guide, The Word, which details university-wide policies, is also 
available online at www.cmu.edu/student-affairs/theword.     
Additional resources specific to graduate students can be found at 
www.cmu.edu/graduate/policies/    
8.1 The MSAII Degree 
The M.S. in Artificial Intelligence and Innovation (MSAII) is a professional master's program 
offered by the Language Technologies Institute (LTI) in the School of Computer Science at 
Carnegie Mellon University. This program seeks to educate students in the innovative use of 
artificial intelligence to create practical solutions in a wide variety of application areas. As 
artificial intelligence matures, there is a great need for students who can envision, design, plan 
and deliver solutions that integrate AI technologies such as deep learning, natural language 
processing, robotics and big data analytics into new applications.  The program stresses both 
10 
 
intrapreneurship and entrepreneurship and encourages students to develop ideas for startups 
and for introduction into established companies.
Answer: "
"Who taught Natural Language Processing last fall?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who taught Natural Language Processing last fall?

Context: The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Natural Language Processing' with Course ID 11611 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Oflazer, Mortensen located in Building POS, Room A35.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Natural Language Processing' with Course ID 11611 and Section B offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Mortensen, Oflazer located in Building CMU, Room REMOTE.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Natural Language Processing:' with Course ID 11611 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Natural Language Processing' with Course ID 11611 and Section R offers 12.0 units. The Class meets Tuesday Thursday between 08:00PM and 10:20PM ET. Students attend lectures at the Kigali, Rwanda location,led by experienced instructor Mortensen, Oflazer located in Building CMU, Room REMOTE.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Human Language for Artificial Intelligence' with Course ID 11624 and Section A offers 12.0 units. The Class meets Monday Wednesday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin located in Building WEH, Room 5409.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Data Science Seminar' with Course ID 11631 and Section A offers 12.0 units.
The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Natural Language Processing' with Course ID 11611 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Oflazer, Mortensen located in Building POS, Room A35.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Natural Language Processing' with Course ID 11611 and Section B offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Mortensen, Oflazer located in Building CMU, Room REMOTE.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Natural Language Processing:' with Course ID 11611 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Natural Language Processing' with Course ID 11611 and Section R offers 12.0 units. The Class meets Tuesday Thursday between 08:00PM and 10:20PM ET. Students attend lectures at the Kigali, Rwanda location,led by experienced instructor Mortensen, Oflazer located in Building CMU, Room REMOTE.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Human Language for Artificial Intelligence' with Course ID 11624 and Section A offers 12.0 units. The Class meets Monday Wednesday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin located in Building WEH, Room 5409.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Data Science Seminar' with Course ID 11631 and Section A offers 12.0 units.
Answer: "
"What is the Spearman Correlation of CodeBERTScore with human preference?
","['graham neubig_31366ff634fc905affd78dbd8ddc9a872c006a87_metadata.txt', 'kemal oflazer_3b623333145c69cb29c63975213f7b3bac025954_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the Spearman Correlation of CodeBERTScore with human preference?

Context: Faculty Name: graham neubig
Paperid: 31366ff634fc905affd78dbd8ddc9a872c006a87
Title: CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code
Year: 2023
Abstract: Since the rise of neural natural-language-to-code models (NL->Code) that can generate long expressions and statements rather than a single next-token, one of the major problems has been reliably evaluating their generated output. In this paper, we propose CodeBERTScore: an evaluation metric for code generation, which builds on BERTScore (Zhang et al., 2020). Instead of encoding only the generated tokens as in BERTScore, CodeBERTScore also encodes the natural language input preceding the generated code, thus modeling the consistency between the generated code and its given natural language context as well. We perform an extensive evaluation of CodeBERTScore across four programming languages. We find that CodeBERTScore achieves a higher correlation with human preference and with functional correctness than all existing metrics. That is, generated code that receives a higher score by CodeBERTScore is more likely to be preferred by humans, as well as to function correctly when executed. We release five language-specific pretrained models to use with our publicly available code. Our language-specific models have been downloaded more than 1,000,000 times from the Huggingface Hub. Our code and data are available at https://github.com/neulab/code-bert-score
Authors: Shuyan Zhou, Uri Alon, Sumit Agarwal, Graham Neubig
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'It is found that generated code that receives a higher score by CodeBERTScore is more likely to be preferred by humans, as well as to function correctly when executed, than all existing metrics.'}
Url: http://arxiv.org/pdf/2302.05527
Title: Abstractive summarization with deep reinforcement learning using semantic similarity rewards
Authors: Figen Beken Fikri, Kemal Oflazer, Berrin Yanıkoğlu, Figen Beken
Section: 6. Summary and conclusion
In this work, we focused on two main issues in abstractive summarization: how to evaluate the results and what is a good training objective. We presented semantic similarity-based summarization evaluation measures and a reinforcement learning framework with the semantic similarity rewards. We proposed evaluationmeasures using similarity scores obtained by fine-tuning the BERTurk model using cross-encoder and bi-encodermodel architectures onNLI-TR (Budur et al. 2020) and STSb-TR (Beken Fikri et al. 2021) datasets. We showed that the proposed evaluation measures have better correlations with human evaluations compared to ROUGE scores, according to both Pearson and Spearman correlations. We further showed that using bi-encoder and cross-encoder similarities as rewards improved the model results in terms of the proposed evaluation measures, as well as BERTScore and ROUGE scores. Our qualitative analyses demonstrated that the proposed models can generate summaries that are more similar to the ground truth, as compared to MLE-only models and RL models with ROUGE rewards. It is worth mentioning that our rewards are not model-dependent in our reinforcement learning framework and can be explored in other downstream sequence-to-sequence tasks like paraphrase generation, text simplification, and semantic search. Also, the suggested framework can be applied to other languages following the described methodology. Acknowledgments. None. Competing interests. The authors declare none.
Answer: "
"When is CMU's main commencement ceremony in 2024?
","['Commencement.txt', 'Commencement.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When is CMU's main commencement ceremony in 2024?

Context: Main Commencement Ceremony
Bachelor’s, master’s and doctoral degree candidates and their guests are invited to join the main commencement ceremony on Sunday, May 12, for the conferral of all degrees.
The main ceremony will include remarks from the president, keynote speaker, student speaker and academic deans, in addition to recognition of the honorary degree recipients.
There is no limit on number of guests who can attend the main commencement ceremony and tickets are not needed.
The ceremony will take place on CMU’s campus beginning at 10 a.m. and will be approximately 1.5 hours long. All guests must be seated by 9:15 a.m. for the start of the student procession. Access to guest seating will be restricted once the student procession begins.

Diploma Ceremonies
In addition to the main commencement ceremony, each college/school/department will host a diploma ceremony to recognize all graduating students. Diploma ceremonies will be held over the course of the weekend (Friday, May 10–Sunday, May 12).
Diploma ceremonies will include the presentation of diplomas to graduates, hooding of doctoral candidates and remarks from their college/school/department leadership. Each ceremony is organized and customized by their college/school/department.
Diploma ceremonies will take place both on and off campus. Locations and times for diploma ceremonies will be provided soon.
Diploma ceremonies typically include a reception and the length of time for each ceremony varies based on the number of graduates.
More details on the weekend schedule, including a diploma ceremony schedule, will be provided in the coming weeks. 

Thursday, May 9
Phi Beta Kappa Initiation Ceremony
Ceremony: 2–3 p.m.
McConomy Auditorium, Cohon University Center
Reception: 3–4 p.m.
Connan Room, Cohon University Center

Contact:
Joseph Devine
jd0x@andrew.cmu.edu 

Joanne Ursenbach
joanneu@andrew.cmu.edu 

President's Graduates Toast (bachelor's students)
3:30–4:30 p.m.
Location TBD
Registration required. Invitation, along with registration details, will be sent in late April.

First Gen Graduation Recognition
Reception: 5-5:30 p.m.
Alumni Concert Hall, College of Fine Arts
Ceremony: 5:30-6:30p.m.
Invitation, along with registration details, will be sent in late April.

First Gen Graduation Recognition
Reception: 5-5:30 p.m.
Alumni Concert Hall, College of Fine Arts
Ceremony: 5:30-6:30p.m.
Kresge Theater, College of Fine Arts

Contact:
M. Shernell Smith
mssmith@andrew.cmu.edu 

Sam Colavecchio
scolavec@andrew.cmu.edu 
412-268-7733


Friday, May 10
Diploma Ceremonies
Various times

Senior Leadership Recognition Ceremony
4–5:30 p.m.
Wiegand Gym, Cohon University Center
Undergraduate students and their guests by invitation only. This ceremony recognizes nominated seniors who have reflected upon their specific leadership contributions during their time at CMU.


Saturday, May 11
Diploma Ceremonies
Various times

Center for Student Diversity and Inclusion Ceremony
Reception: 12:30 p.m.
Program: 1-2:15 p.m.
Simmons Auditorium, Tepper Building

Contact:
M. Shernell Smith
mssmith@andrew.cmu.edu 

Rowshan Lang
rowshan@andrew.cmu.edu
Naval ROTC Commissioning
Ceremony: 1:30-2:30 p.m.
Auditorium, Soldiers & Sailors Memorial Hall & Museum *
4141 Fifth Avenue, Pittsburgh, PA 15213
Contact:
Mike Danko
mdanko@andrew.cmu.edu 

The President’s Reception in honor of CMU’s Doctoral Candidates
4–6 p.m.
Tepper Building Atrium


Sunday, May 12
Gesling Stadium opens to guests
8 a.m.

Robing and procession for graduates
9–10 a.m.
Various locations across campus

Student procession begins
9:15 a.m.
All guests in stadium must be seated. Access to guest seating will be restricted once the procession begins.

Commencement Ceremony
10–11:30 a.m.
Gesling Stadium, CMU’s campus

Diploma Ceremonies
Various times
Answer: "
"Who provides a signal for buggy drivers to start the right-hand turn from Schenley Drive onto Frew Street?
","['cmubuggy_d403.txt', 'cmubuggy_d403.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who provides a signal for buggy drivers to start the right-hand turn from Schenley Drive onto Frew Street?

Context: An mural celebrating Buggy can be found in CMU's Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions. Sweepstakes Slang Buggy: Vehicle being raced and also a nickname for the competition. Chute: A section of the freeroll portion of the buggy course (near the southwestern end of Frew Street at its intersection with Schenley Drive) where buggies make the sharp righthand turn from Schenley Drive onto Frew Street. Chute Flagger: Team member who provides a signal for buggy drivers to know when to start the right-hand turn from Schenley Drive onto Frew Street. Driver: Person who travels with a buggy and controls the vehicles via steering and braking systems. Pushbar: Structure attached to a buggy that a person pushes to propel that buggy forward. Pusher: Person who propels a buggy via a pushbar along one of the five hills of the buggy course. Shell: Entire outer structure or covering of a buggy that determines that buggy's aerodynamic characteristics. Transition: Procedure whereby one pusher finishes pushing a buggy and the next pusher in sequence starts to push that same buggy. Members of Fringe, celebrating their 50th year of Buggy racing, allowed a rare visit into its shop for a behind-the-scenes tour. This year Fringe is planning to roll four different vehicles, built and maintained in the Fringe workshop in the basement of the East Campus Garage, known as the ""Froom."" ""We're used to saying everything with 'fr' in front of it but when we say something in front of other people, it gets them confused,"" said Fringe head mechanic Dave Singh, who will graduate in May with a bachelor's degree in mechanical engineering and biomedical engineering. Fringe vehicles often are named with the letter ""B,"" like Boson, Blueshift, Bissa and Bumper. Other teams, such as Apex, often use names that connotate fire, while the SDC (Student Dormitory Committee) team, uses names such as Vice, Bane, Avarice and Malice. This year's buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It's one of the few times spectators can see buggies up close.
Buggy Races Keep Rolling at Carnegie Mellon - News - Carnegie Mellon University Carnegie Mellon University ——— Search Search Search this site only News News › Stories › Archives › 2019 › April › Buggy Races Keep Rolling at Carnegie Mellon April 10, 2019 Buggy Races Keep Rolling at Carnegie Mellon In its 99th year, the tradition is a Spring Carnival treat By Heidi Opdyke opdyke(through)andrew.cmu.edu Media Inquiries Julie Mattera Marketing and Communications jmattera(through)cmu.edu 412-268-2902 Sweepstakes, also known as the Buggy Races, has come a long way at Carnegie Mellon University. The slick, torpedo-like vessels carrying drivers with nerves of steel are a far cry from the two-man teams that once changed places halfway through a race and rode in everything from rain barrels with bicycle wheels to three-wheeled ash cans 99 years ago. Today, it takes six people to maneuver the .84 -mile course around Schenley Park's Flagstaff Hill. But while five pushers and a driver navigate the course's hills, dozens of people are needed to make a successful race happen. A year of planning goes into just over two minutes of racing. The Machine The basics of a buggy are straightforward, but teams are often secretive in how they build the machines, in particular the way they brake, steer and what types of wheels are used. Each has a body, pushbar for runners to move the machine up the hills, wheels, a safety harness and driving and braking mechanisms. Some also include fairings, a type of housing around the wheels that help reduce drag, make the vehicle quieter and just looks cool. Fairings have been a key feature for the Fringe team in recent years, which is celebrating its 50th anniversary of Buggy. ""We have a reputation of being the quietest on the course,"" said Diya Nuxoll, who wrapped up her bachelor's degree in mechanical engineering in December and is working on an advanced studies master's degree in design and manufacturing. Nuxoll leads the Fringe team as one of two head mechanics. An mural celebrating Buggy can be found in CMU's Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions. Sweepstakes Slang Buggy: Vehicle being raced and also a nickname for the competition.
Answer: "
"Are there any authors of the paper Understanding Political Polarisation using Language Models: A dataset and method, not from CMU?
","['bhiksha raj_4e6a004e4f9a3b489004a2efb5b25b0bcd0f48b5_content_0.txt', 'bhiksha raj_22c9eb4868c5cabb26d132e0a160b9a093579f08_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Are there any authors of the paper Understanding Political Polarisation using Language Models: A dataset and method, not from CMU?

Context: Title: Understanding Political Polarisation using Language Models: A dataset and method
Authors: Samiran Gode, Supreeth Bare, Bhiksha Raj, Hyungon Yoo
Section: Acknowledgments
We would like to thank Yash Jain and Viraj Ranade for their contributions.
Faculty Name: bhiksha raj
Paperid: 22c9eb4868c5cabb26d132e0a160b9a093579f08
Title: Understanding political polarization using language models: A dataset and method
Year: 2023
Abstract: Our paper aims to analyze political polarization in US political system using language models, and thereby help candidates make an informed decision. The availability of this information will help voters understand their candidates' views on the economy, healthcare, education, and other social issues. Our main contributions are a dataset extracted from Wikipedia that spans the past 120 years and a language model‐based method that helps analyze how polarized a candidate is. Our data are divided into two parts, background information and political information about a candidate, since our hypothesis is that the political views of a candidate should be based on reason and be independent of factors such as birthplace, alma mater, and so forth. We further split this data into four phases chronologically, to help understand if and how the polarization amongst candidates changes. This data has been cleaned to remove biases. To understand the polarization, we begin by showing results from some classical language models in Word2Vec and Doc2Vec. And then use more powerful techniques like the Longformer, a transformer‐based encoder, to assimilate more information and find the nearest neighbors of each candidate based on their political view and their background. The code and data for the project will be available here: “https://github.com/samirangode/Understanding_Polarization”
Authors: Samiran Gode, Supreeth Bare, B. Raj, H. Yoo
Venue: The AI Magazine
Tldr: {'model': 'tldr@v2.0.0', 'text': 'The main contributions are a dataset extracted from Wikipedia that spans the past 120 years and a language model‐based method that helps analyze how polarized a candidate is.'}
Url: https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/aaai.12104
Answer: "
"At what conference was ""To Build Our Future, We Must Know Our Past: Contextualizing Paradigm Shifts in Natural Language Processing"" published?
","['emma strubell_1433b8d43d446fcc7f3e1370b22f744a4dd7c8e4_content_3.txt', 'emma strubell_1433b8d43d446fcc7f3e1370b22f744a4dd7c8e4_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: At what conference was ""To Build Our Future, We Must Know Our Past: Contextualizing Paradigm Shifts in Natural Language Processing"" published?

Context: Title: To Build Our Future, We Must Know Our Past: Contextualizing Paradigm Shifts in Natural Language Processing
Authors: Sireesh Gururaja, Amanda Bertsch, Clara Na, David Gray Widder, Emma Strubell
Section: 3 Exploit-explore cycles of work
that is farther from current norms/methodologies requires higher upfront time investment. This competitiveness can manifest in harsher reviews, and one participant described a “deadly combination” (19) of higher standards for papers and lower quality of reviews. Some participants described this as a reason they were choosing to engage less with NLP conferences; one industry researcher stated that “I just find it difficult to publish papers in *CL that have ideas in them.” (22).
Faculty Name: emma strubell
Paperid: 1433b8d43d446fcc7f3e1370b22f744a4dd7c8e4
Title: To Build Our Future, We Must Know Our Past: Contextualizing Paradigm Shifts in Natural Language Processing
Year: 2023
Abstract: NLP is in a period of disruptive change that is impacting our methodologies, funding sources, and public perception. In this work, we seek to understand how to shape our future by better understanding our past. We study factors that shape NLP as a field, including culture, incentives, and infrastructure by conducting long-form interviews with 26 NLP researchers of varying seniority, research area, institution, and social identity. Our interviewees identify cyclical patterns in the field, as well as new shifts without historical parallel, including changes in benchmark culture and software infrastructure. We complement this discussion with quantitative analysis of citation, authorship, and language use in the ACL Anthology over time. We conclude by discussing shared visions, concerns, and hopes for the future of NLP. We hope that this study of our field's past and present can prompt informed discussion of our community's implicit norms and more deliberate action to consciously shape the future.
Authors: Sireesh Gururaja, Amanda Bertsch, Clara Na, D. Widder, Emma Strubell
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work conducts long-form interviews with 26 NLP researchers of varying seniority, research area, institution, and social identity to study factors that shape NLP as a field, including culture, incentives, and infrastructure.'}
Url: https://arxiv.org/pdf/2310.07715
Answer: "
"What is HomeRobot?
","['yonatan bisk_3b0c02955e88f5862e61b560c7f70ba8cf235b1d_metadata.txt', 'yonatan bisk_3b0c02955e88f5862e61b560c7f70ba8cf235b1d_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is HomeRobot?

Context: Faculty Name: yonatan bisk
Paperid: 3b0c02955e88f5862e61b560c7f70ba8cf235b1d
Title: HomeRobot: Open-Vocabulary Mobile Manipulation
Year: 2023
Abstract: HomeRobot (noun): An affordable compliant robot that navigates homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future research work improve performance. See videos on our website: https://ovmm.github.io/.
Title: HomeRobot: Open-Vocabulary Mobile Manipulation
Authors: Sriram Yenamandra, Arun Ramachandran, Karmesh Yadav, Austin Wang, Mukul Khanna, Theophile Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander William Clegg, John Turner, Zsolt Kira, Manolis Savva, Angel Chang, Devendra Singh Chaplot, Dhruv Batra, Roozbeh Mottaghi, Yonatan Bisk, Chris Paxton
Section: H.2 Robot Setup
Visualizing The Robot We use RVIZ, a part of ROS, to visualize results and progress. Fig. 22 shows three different outputs from our system: on the far left, an image from the test environment being processed by Detic; in the center, a top-down map generated by the navigation planner described in Sec. E.2; and on the right, an image from RVIZ with the point cloud from the robot’s head camera registered against the 2D lidar map created by Hector SLAM. One advantage of the HomeRobot stack is that it is designed to work with existing debugging tools - especially ROS [113]. ROS is a widely-used framework for robotics software development that comes with a lot of online resources, official support from Hello Robot, and a rich and thriving open-source community with wide industry backing.
Answer: "
"In spring 2024, What is the day and time of course 17422?
","['Meta-Data-Calendar-2425.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, What is the day and time of course 17422?

Context: Day: 'Monday', Event: 'Semester pass/no pass & withdrawal deadline', Semester: 'Spring 2025 (S25)'
Date: '2025-04-02', Day: 'Wednesday', Event: 'Mini-4 course drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Spring 2025 (S25)'
Start Date: '2025-04-03', End Date: '2023-04-05', Days: 'Thursday to Saturday', Event: 'Spring Carnival; No Classes', Semester: 'Spring 2025 (S25)'
Start Date: '2025-04-07', End Date: '2023-04-11', Days: 'Monday to Friday', Event: 'Fall 2025 Registration Week', Semester: 'Spring 2025 (S25)'
Date: '2025-04-14', Day: 'Monday', Event: 'Mini-4 pass/no pass & withdrawal deadline (3)', Semester: 'Spring 2025 (S25)'
Date: '2025-04-14', Day: 'Monday', Event: 'Semester & Mini-4 Faculty Course Evaluations open ', Semester: 'Spring 2025 (S25)'
Date: '2025-04-25', Day: 'Friday', Event: 'Last Day of Classes', Semester: 'Spring 2025 (S25)'
Date: '2025-04-25', Day: 'Friday', Event: 'Semester & Mini-4 voucher deadline (4)', Semester: 'Spring 2025 (S25)'
Start Date: '2025-04-28', End Date: '2023-04-29', Days: 'Monday to Tuesday', Event: 'Final Examinations ', Semester: 'Spring 2025 (S25)'
Date: '2025-04-30', Day: 'Wednesday', Event: 'Reading Day', Semester: 'Spring 2025 (S25)'
Start Date: '2025-05-01', End Date: '2023-05-02', Days: 'Thursday to Friday', Event: 'Final Examinations ', Semester: 'Spring 2025 (S25)'
Start Date: '2025-05-03', End Date: '2023-05-04', Days: 'Saturday to Sunday', Event: 'Reading Days', Semester: 'Spring 2025 (S25)'
Date: '2025-05-05',
From 04 May, 2024,Saturday to 05 May, 2024,Sunday marks the Reading Days for Spring 2024 (S24) semester.
On Monday, 06 May, 2024, during the Spring 2024 (S24) semester, Final Examinations is observed.
On Monday, 06 May, 2024, during the Spring 2024 (S24) semester, Semester & Mini-4 Faculty Course Evaluations close is observed.
On Tuesday, 07 May, 2024, during the Spring 2024 (S24) semester, Make-Up Final Examinations is observed.
On Wednesday, 08 May, 2024, during the Spring 2024 (S24) semester, Graduating Final Grades Due by 4 pm is observed.
On Sunday, 12 May, 2024, during the Spring 2024 (S24) semester, Commencement is observed.
On Tuesday, 14 May, 2024, during the Spring 2024 (S24) semester, Final Grades Due by 4 pm is observed.
On Tuesday, 21 May, 2023, during the Spring 2024 (S24) semester, Spring Deans' Lists Posted is observed.
On Monday, 13 May, 2024, during the Summer One 2024 (M24) semester, Semester & Mini-5 Classes Begin is observed.
On Friday, 17 May, 2024, during the Summer One 2024 (M24) semester, Mini-5 add, audit, & tuition adjustment drop deadline (1) is observed.
On Friday, 24 May, 2024, during the Summer One 2024 (M24) semester, Semester add, audit, & tuition adjustment drop deadline (1) is observed.
On Monday, 27 May, 2024, during the Summer One 2024 (M24) semester, Memorial Day is observed, leading to University Closed & No Classes.
On Tuesday, 28 May, 2024, during the Summer One 2024 (M24) semester, Mini-5 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 07 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 pass/no pass & withdrawal deadline (3) is observed.
Answer: "
"The analysis in the paper Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models, includes how many diverse languages?
","['david mortensen_17fbffb05fa14e21d1c506fd5f0f568b955fe983_metadata.txt', 'david mortensen_17fbffb05fa14e21d1c506fd5f0f568b955fe983_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: The analysis in the paper Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models, includes how many diverse languages?

Context: Faculty Name: david mortensen
Paperid: 17fbffb05fa14e21d1c506fd5f0f568b955fe983
Title: Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models
Year: 2023
Abstract: Language models have graduated from being research prototypes to commercialized products offered as web APIs, and recent works have highlighted the multilingual capabilities of these products. The API vendors charge their users based on usage, more specifically on the number of ``tokens'' processed or generated by the underlying language models. What constitutes a token, however, is training data and model dependent with a large variance in the number of tokens required to convey the same information in different languages. In this work, we analyze the effect of this non-uniformity on the fairness of an API's pricing policy across languages. We conduct a systematic analysis of the cost and utility of OpenAI's language model API on multilingual benchmarks in 22 typologically diverse languages. We show evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results. These speakers tend to also come from regions where the APIs are less affordable to begin with. Through these analyses, we aim to increase transparency around language model APIs' pricing policies and encourage the vendors to make them more equitable.
Authors: Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David R. Mortensen, Noah A. Smith, Yulia Tsvetkov
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': ""This work conducts a systematic analysis of the cost and utility of OpenAI's language model API on multilingual benchmarks in 22 typologically diverse languages and shows evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results.""}
Url: http://arxiv.org/pdf/2305.13707
Title: Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models
Authors: Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David R. Mortensen, Noah A. Smith, Yulia Tsvetkov
Section: 7 Conclusion
we cannot ascertain the actual statistics of all the languages in their training data. We use CC100 (Wenzek et al., 2020), a large multilingual corpus, to estimate these statistics.
Answer: "
"What is the main goal of event grounding?
","['teruko mitamura_ff77105b2c345f54e1a87f4fbb3a701201f0c1a8_metadata.txt', 'teruko mitamura_papers.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the main goal of event grounding?

Context: Faculty Name: teruko mitamura
Paperid: ff77105b2c345f54e1a87f4fbb3a701201f0c1a8
Title: Hierarchical Event Grounding
Year: 2023
Abstract: Event grounding aims at linking mention references in text corpora to events from a knowledge base (KB). Previous work on this task focused primarily on linking to a single KB event, thereby overlooking the hierarchical aspects of events. Events in documents are typically described at various levels of spatio-temporal granularity. These hierarchical relations are utilized in downstream tasks of narrative understanding and schema construction. In this work, we present an extension to the event grounding task that requires tackling hierarchical event structures from the KB. Our proposed task involves linking a mention reference to a set of event labels from a subevent hierarchy in the KB. We propose a retrieval methodology that leverages event hierarchy through an auxiliary hierarchical loss. On an automatically created multilingual dataset from Wikipedia and Wikidata, our experiments demonstrate the effectiveness of the hierarchical loss against retrieve and re-rank baselines. Furthermore, we demonstrate the systems' ability to aid hierarchical discovery among unseen events. Code is available at https://github.com/JefferyO/Hierarchical-Event-Grounding
Authors: Jiefu Ou, Adithya Pratapa, Rishubh Gupta, T. Mitamura
Venue: AAAI Conference on Artificial Intelligence
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work presents an extension to the event grounding task that requires tackling hierarchical event structures from the KB, and proposes a retrieval methodology that leverages event hierarchy through an auxiliary hierarchical loss.'}
Url: http://arxiv.org/pdf/2302.04197
List of 2023 Open Access papers by teruko mitamura are:
Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting for Multilingual Document-Grounded QA
Hierarchical Event Grounding
Answer: "
"In summer 2024, When is Independence Day and what is the University's policy on classes?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In summer 2024, When is Independence Day and what is the University's policy on classes?

Context: On Friday, 07 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 pass/no pass & withdrawal deadline (3) is observed.
On Friday, 14 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Faculty Course Evaluations open is observed.
On Tuesday, 18 June, 2024, during the Summer One 2024 (M24) semester, Semester course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Wednesday, 19 June, 2022, during the Summer One 2024 (M24) semester, Juneteenth is observed, leading to University Closed & No Classes.
On Thursday, 20 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Last Day of Classes is observed.
On Thursday, 20 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 voucher deadline (4) is observed.
On Friday, 21 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Final Exams is observed.
On Friday, 21 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Faculty Course Evaluations close is observed.
On Monday, 24 June, 2024, during the Summer One 2024 (M24) semester, Mini-6 First Day of Classes is observed.
On Tuesday, 25 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Final Grades Due by 4 pm is observed.
On Friday, 28 June, 2024, during the Summer One 2024 (M24) semester, Mini-6 add, audit, & tuition adjustment drop deadline (1) is observed.
On Thursday, 04 July, 2024, during the Summer One 2024 (M24) semester, Independence Day is observed, leading to University Closed & No Classes.
On Friday, 05 July, 2024, during the Summer One 2024 (M24) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Friday, 05 July, 2024, during the Summer One 2024 (M24) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Monday, 08 July, 2024, during the Summer One 2024 (M24) semester, Mini-6 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 22 July, 2024, during the Summer One 2024 (M24) semester, Mini-6 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 29 July, 2024, during the Summer One 2024 (M24) semester, Semester & Mini-6 Faculty Course Evalutations open is observed.
On Thursday, 01 August, 2024, during the Summer One 2024 (M24) semester, Semester & Mini-6 Last Day of Classes is observed.
On Thursday, 01 August, 2024, during the Summer One 2024 (M24) semester, Semester & Mini-6 voucher deadline (4) is observed.
On Friday, 02 August, 2024, during the Summer One 2024 (M24) semester, Semester & Mini-6 Final Exams*** is observed.
On Friday, 02 August, 2024, during the Summer One 2024 (M24) semester, Semester & Mini-6 Faculty Course Evaluations close is observed.
On Tuesday, 06 August, 2024, during the Summer One 2024 (M24) semester, Semester & Mini-6 Final Grades Due by 4 pm is observed.
On Monday, 24 June, 2024, during the Summer Two 2024 (N24) semester, Summer Semester Two Classes Begin is observed.
On Friday, 28 June, 2024, during the Summer Two 2024 (N24) semester, Summer Semester  Two add, audit & tuition adjustment drop deadline (1) is observed.
On Thursday, 04 July, 2024, during the Summer Two 2024 (N24) semester, Independence Day is observed, leading to University Closed & No Classes.
On Monday, 08 July, 2024, during the Summer Two 2024 (N24) semester, Summer Semester  Two drop deadline is observed, leading to withdrawal grade assigned after this date (2).
Answer: "
"Who is the current director of The Kiltie Band?
","['kiltieband_d406.txt', 'kiltieband_d406.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who is the current director of The Kiltie Band?

Context: in the CUC Studio Theater. If you have questions, please email the Direcator at
jolisar@andrew.cmu.edu
.
Q: Can I borrow a school instrument?
A: There are several Kiltie-owned instruments available for your use. Loans begin at 4:30 p.m. before the first rehearsal.
Q: What do they wear under those kilts?
A: Join and you’ll find out!
For more information, visit the
Kiltie Band website
.
Interested in Joining?
Please email the following information to Kiltie Band Director
Jeremy Olisar.
Name
High School
Address at Carnegie Mellon (if known)
Home address
Cell number
Home number
Whether you plan on being in the band or colorguard
If in the band what instrument(s) you play
Whether you need to borrow an instrument or equipment
To see and hear the band in action, visit our
YouTube channel
.
The Kiltie Band
The Kiltie Band began in 1908 with a group of just seven students dedicated to supporting Carnegie Tech football, and today's Kiltie Band continues a tradition of excellence originated over a century ago. The Band took the field for its first official performance on November 25th, 1922, on what would have been Andrew Carnegie's 87th birthday.
The Kiltie Marching Band consists of musicians and colorguard and plays at all Carnegie Mellon home football games. There is a new show every week with music ranging from traditional marches and oldies to current pop tunes and jazz standards.
After football season, the Kiltie Concert Band plays a holiday concert and two spring concerts, including a performance at Carnegie Mellon’s Spring Carnival. Additionally, a part of the band gets together and acts as a pep band for the basketball teams.
The Kiltie Band is a campus organization recognized by Student Senate and administered by the Department of Athletics, Physical Education and Recreation. Membership is open to all members of the campus/community without audition. Participants must do their best to promote the success of the ensemble through attendance, attitude, and dependability.
FAQs
Q: When are rehearsals?
A: Football season: Mondays and Thursdays from 5:30 p.m.-7:30 p.m. and Game Day 9:30-11:00a.m. Concert Season: Mondays and Thursdays from 5:30-6:30 p.m.
Location is the CUC Studio Theater.
Q: When are performances?
A: The Kiltie Band performs at all home football games, a holiday concert, and two spring concerts. Times and dates for all performances are announced at the first practice.
Q: Are there auditions?
A: No, any member of the campus community with music experience is able to join the Kiltie Band!
Q: Do I have to memorize music?
A: No, the music is changed for every show. You should invest in a good and trusty lyre.
Q: When is the first rehearsal?
A: On the first day of class (Monday) at 5:30 p.m. in the CUC Studio Theater. If you have questions, please email the Direcator at
jolisar@andrew.cmu.edu
.
Q: Can I borrow a school instrument?
A: There are several Kiltie-owned instruments available for your use.
Answer: "
"What is the full name of the conference where the paper CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code, got published? 
","['graham neubig_31366ff634fc905affd78dbd8ddc9a872c006a87_metadata.txt', 'graham neubig_papers.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the full name of the conference where the paper CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code, got published? 

Context: Faculty Name: graham neubig
Paperid: 31366ff634fc905affd78dbd8ddc9a872c006a87
Title: CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code
Year: 2023
Abstract: Since the rise of neural natural-language-to-code models (NL->Code) that can generate long expressions and statements rather than a single next-token, one of the major problems has been reliably evaluating their generated output. In this paper, we propose CodeBERTScore: an evaluation metric for code generation, which builds on BERTScore (Zhang et al., 2020). Instead of encoding only the generated tokens as in BERTScore, CodeBERTScore also encodes the natural language input preceding the generated code, thus modeling the consistency between the generated code and its given natural language context as well. We perform an extensive evaluation of CodeBERTScore across four programming languages. We find that CodeBERTScore achieves a higher correlation with human preference and with functional correctness than all existing metrics. That is, generated code that receives a higher score by CodeBERTScore is more likely to be preferred by humans, as well as to function correctly when executed. We release five language-specific pretrained models to use with our publicly available code. Our language-specific models have been downloaded more than 1,000,000 times from the Huggingface Hub. Our code and data are available at https://github.com/neulab/code-bert-score
Authors: Shuyan Zhou, Uri Alon, Sumit Agarwal, Graham Neubig
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'It is found that generated code that receives a higher score by CodeBERTScore is more likely to be preferred by humans, as well as to function correctly when executed, than all existing metrics.'}
Url: http://arxiv.org/pdf/2302.05527
List of 2023 Open Access papers by graham neubig are:
Cross-Modal Fine-Tuning: Align then Refine
ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages
GlobalBench: A Benchmark for Global Progress in Natural Language Processing
Learning Performance-Improving Code Edits
CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code
Neural Machine Translation for the Indigenous Languages of the Americas: An Introduction
User-Centric Evaluation of OCR Systems for Kwak’wala
Multi-Dimensional Evaluation of Text Summarization with In-Context Learning
A Gold Standard Dataset for the Reviewer Assignment Problem
SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing
Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation
FacTool: Factuality Detection in Generative AI - A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios
Active Retrieval Augmented Generation
Large Language Models Enable Few-Shot Clustering
Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach
Crossing the Threshold: Idiomatic Machine Translation through Retrieval Augmentation and Loss Weighting
Why do Nearest Neighbor Language Models Work?
Syntax and Semantics Meet in the “Middle”: Probing the Syntax-Semantics Interface of LMs Through Agentivity
DataFinder: Scientific Dataset Recommendation from Natural Language Descriptions
Multi-lingual and Multi-cultural Figurative Language Understanding
It’s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk
Unlimiformer: Long-Range Transformers with Unlimited Length Input
WebArena: A Realistic Web Environment for Building Autonomous Agents
Prompt2Model: Generating Deployable Models from Natural Language Instructions
Computational Language Acquisition with Theory of Mind
Improving Factuality of Abstractive Summarization via Contrastive Reward Learning
The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation
Answer: "
"What is the LTI director's phone number?
","['mona_diab.txt', 'handbook_phd_2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the LTI director's phone number?

Context: Mona Diab
LTI Director and Tenured Professor, Language Technologies Institute
Research Area : Fairness and Ethics in Language Technology
Contact: 5723 Gates & Hillman Centers
Email : mdiab@andrew.cmu.edu
Phone : 412-268-3669
Address
5000 Forbes Avenue
Pittsburgh, PA 15213
The Assistant Directors in The HUB serve as contacts for 
LTI Ph.D. Graduate Student Handbook 
Page 34 
 
specific colleges and assist enrolled students with key aspects of the enrollment process. Student 
can find their assigned HUB Assistant Director on their Student Information Online (SIO) 
Resource page. Questions that need specialized, in-depth attention can be directed to the 
student's assigned Assistant Director. For general questions and information, students may email 
The HUB or call 412-268-8186. 
6.2 
Student Information Online (SIO)  
Student Information Online (SIO) is a secure site where students can find important, 
personalized information, including E-Bills and student account information, financial aid status 
and eligibility, grades and QPA, and course schedules. Students can update their and their spouses 
or domestic partner's contact information, sign up for E-Check & E-Refund, authorize their 
spouses, domestic partners or other individual to receive a copy of their E-Bill, request 
verifications, view their housing and meal plan assignments, and much more. Students can log on 
to SIO by going to www.cmu.edu/hub/sio and entering their Andrew User ID and password. 
On SIO, students are encouraged to keep their current local address up to date. This supports a 
university initiative to have accurate living information for students for official 
program/department/college/university notices, the ability to facilitate wellness checks, ensure 
international students are in compliance with visa requirements.  
It will designate an emergency contact address of a relative or family friend to be contacted in the 
case of an emergency. If students do not want their name and address published in the campus 
directory, they must notify the HUB in writing. 
6.3 
ID Cards 
Graduate students can obtain their ID card from The HUB once they have been entered into SIO 
for the semester. These cards identify their holders as members of the campus community. Student 
cards 
are 
deactivated 
upon 
the 
cardholders 
separation 
from 
the 
university. 
 
Affiliate ID Cards are available for spouses and partners of graduate students that allow them to 
access Carnegie Mellons campus. These cards are available through The HUB to spouses and 
partners of graduate students who are enrolled for the current academic year in a full-time 
graduate degree program.
Answer: "
"What are the course number(s) for the NLP course?
","['miis-handbook_2023-2024.txt', 'handbook-msaii-2022-2023.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the course number(s) for the NLP course?

Context: Introduction to Question Answering 
• 11-751, Speech Recognition and Understanding 
• 11-767, On-Service Machine Learning 
• 11-797, Question Answering 
• 11-830, Computational Ethics For NLP 
4.4.3 Breadth Courses:  Machine Learning 
• 11-641, Machine Learning for Text and Graph-Based Mining 
• 11-661, Language and Statistics 
• 11-663, Applied Machine Learning 
• 11-747, Neural Networks for NLP 
• 11-755, Machine Learning for Signal Processing 
• 11-761, Language and Statistics 
• 11-777, Multimodal Machine Learning 
MIIS Graduate Student Handbook 
Page 16 
 
• 11-785, Introduction to Deep Learning 
• 10-601, Introduction to Machine Learning (Master’s) 
• 10-605, Machine Learning with Large Datasets 
• 10-701, Introduction to Machine Learning (PhD) 
• 10-707, Advanced Deep Learning 
• 10-708, Probabilistic Graphical Models 
• 10-714, Deep Learning Systems 
• 10-715, Advanced Introduction to Machine Learning 
4.5 Practice Requirements 
A student must complete at least 66 practice-oriented course units and satisfy the following 
practice-oriented requirements for both MIIS-16 and MIIS-21 programs. 
1. Directed study requirement: Students must pass 24 units (typically 12 units x 2 
semesters) in directed study under the supervision of their advisor. Directed study is a 
structured, task-oriented form of independent study that provides deep, hands-on 
experience in a particular technology area and an opportunity to work closely with a 
member of the faculty. 
2. Internship requirement: Students must complete a one-semester (typically summer) 
internship at an organization (typically a company or government agency) approved by 
the MIIS Program Director. Internships are an opportunity to apply new skills in a 
professional setting and to learn about software development in a ‘real world’ 
organization. Students with prior professional experience may petition the MIIS 
Program Director to waive this requirement.  
MIIS students that do an internship during the summer semester are required to present 
their internship at a poster session at the beginning of the following Fall semester.
A student who wants to take a course not listed 
must obtain approval of the Director prior to registering.  In general, the Director will 
approve any graduate SCS course of not more than 12 units.  It is also possible to seek 
approval for courses in other Colleges at CMU.  However, those courses must be 
rationally related to an AI career in order to be approved as electives.  
11-636 MSAII Independent Study (may be taken ONCE as an elective unless permission 
of the Director is obtained) 
11-641 Machine Learning for Text Mining 
11-642 Search Engines 
11-676 Big Data Analytics 
11-747 Neural Networks for NLP 
11-755 Machine Learning for Signal Processing 
11-777 Advanced Multimodal Machine Learning 
11-791 Design of Intelligent Information Systems 
10-605 Machine Learning with Large Datasets 
10-608 Conversational Machine Learning 
10-702 Statistical Machine Learning 
15-624 Foundations of Cyber-Physical Systems 
15-645 Database Systems 
15-681 AI: Representation and Problem Solving 
15-688 Practical Data Science 
16-720 Computer Vision 
16-725 Medical Image Analysis 
16-772 Sensing and Sensors 
16-824 Visual Learning and Recognition 
17-637 Web Application Development 
17-639 Management of Software Development 
17-652 Methods: Deciding What to Design 
17-653 Managing Software Development 
17-766 Software Engineering for Startups 
16 
 
02-604 Fundamentals of Bioinformatics 
02-718 Computational Medicine 
 
Independent study (11-636) is encouraged.  The process is that you find a faculty 
member willing to supervise your independent study, assign a number of credits not to 
exceed 12, and assign a grade at the end of the study.  The faculty member must send 
an email to the Director indicating such willingness.  You can then register for 11-636. 
 
Courses carrying more than 12 units in a semester are NOT APPROVED as electives.  
 
Students may take athletics or music or theatrical performance courses that do not 
require work outside of class times.  However, such courses will NOT count toward 
the 195 units required for graduation. 
10.2.1 Incomplete Grades  
Carnegie Mellon University students are expected to complete a course during the academic 
semester in which the course was taken.
Answer: "
"Which LTI prof co-authored the paper titled ""StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields""?
","['eric xing_8cc1cd002bfc36a8cba8bcbe63d32eacc656097f_content_0.txt', 'eric xing_8cc1cd002bfc36a8cba8bcbe63d32eacc656097f_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which LTI prof co-authored the paper titled ""StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields""?

Context: Title: StyleRF: Zero-shot 3D Style Transfer of Neural Radiance Fields
Authors: Kunhao Liu, Fangneng Zhan, Yiwen Chen, Jiahui Zhang, Yingchen Yu, Abdulmotaleb El Saddik, Shijian Lu, Eric Xing
Section: Acknowledgement
This project is funded by the Ministry of Education Singapore, under the Tier-1 project scheme with project number RT18/22.
Faculty Name: eric xing
Paperid: 8cc1cd002bfc36a8cba8bcbe63d32eacc656097f
Title: StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields
Year: 2023
Abstract: 3D style transfer aims to render stylized novel views of a 3D scene with multiview consistency. However, most existing work suffers from a three-way dilemma over accurate geometry reconstruction, high-quality stylization, and being generalizable to arbitrary new styles. We propose StyleRF (Style Radiance Fields), an innovative 3D style transfer technique that resolves the three-way dilemma by performing style transformation within the feature space of a radiance field. StyleRF employs an explicit grid of high-level features to represent 3D scenes, with which highfidelity geometry can be reliably restored via volume rendering. In addition, it transforms the grid features according to the reference style which directly leads to high-quality zero-shot style transfer. StyleRF consists of two innovative designs. The first is sampling-invariant content transformation that makes the transformation invariant to the holistic statistics of the sampled 3D points and accordingly ensures multi-view consistency. The second is deferred style transformation of 2D feature maps which is equivalent to the transformation of 3D points but greatly reduces memory footprint without degrading multi-view consistency. Extensive experiments show that StyleRF achieves superior 3D stylization quality with precise geometry reconstruction and it can generalize to various new styles in a zero-shot manner. Project website: https://kunhao-liu.github.io/StyleRF/
Authors: Kunhao Liu, Fangneng Zhan, Yiwen Chen, Jiahui Zhang, Yingchen Yu, Abdulmotaleb El Saddik, Shijian Lu, E. Xing
Venue: Computer Vision and Pattern Recognition
Tldr: {'model': 'tldr@v2.0.0', 'text': 'StyleRF (Style Radiance Fields), an innovative 3D style transfer technique that resolves the three-way dilemma by performing style transformation within the feature space of a radiance field, achieves superior 3D stylization quality with precise geometry reconstruction and it can generalize to various new styles in a zero-shot manner.'}
Url: https://arxiv.org/pdf/2303.10598
Answer: "
"What saying by Andrew Carnegie is now CMU's school moto?
","['cmuhistory_d402_metadata.txt', 'combined_metadata_final.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What saying by Andrew Carnegie is now CMU's school moto?

Context: {
  ""viewport"": ""width=device-width, initial-scale=1.0"",
  ""description"": ""CMU \u2014 now a global research university \u2014 began when Andrew Carnegie famously stated, \""My Heart is in the Work,\"" and founded Carnegie Technical Schools in 1900."",
  ""author"": ""Carnegie Mellon University"",
  ""og:title"": ""History - CMU - Carnegie Mellon University"",
  ""og:description"": ""CMU \u2014 now a global research university \u2014 began when Andrew Carnegie famously stated, \""My Heart is in the Work,\"" and founded Carnegie Technical Schools in 1900."",
  ""og:type"": ""website"",
  ""og:url"": ""http://www.cmu.edu/about/history.html"",
  ""msapplication-TileColor"": ""#9f0000"",
  ""msapplication-TileImage"": ""//www.cmu.edu/favicon-144.png""
}
Topic category is cmuhistory
The Class meets Friday between 05:00PM and 08:00PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Mozisek located in Building CMU, Room REMOTE.
In Semester Spring 2024, from the department of Carnegie Mellon University-Wide Studies, the subject titled 'Technology, Humanity, and Social Justice' with Course ID 99385 and Section NA offers 3.0 units. The Class meets Saturday between 08:30AM and 06:00PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor NA located in Building CMU, Room REMOTE.
In Semester Spring 2024, from the department of Carnegie Mellon University-Wide Studies, the subject titled 'Technology, Humanity, and Social Justice' with Course ID 99385 and Section NA offers 3.0 units. The Class meets Sunday between 09:00AM and 01:00PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor NA located in Building CMU, Room REMOTE.
In Semester Spring 2024, from the department of Carnegie Mellon University-Wide Studies, the subject titled 'Technology, Humanity, and Social Justice: Criminal Justice:' with Course ID 99385 and Section NA offers 3.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Carnegie Mellon University-Wide Studies, the subject titled 'Technology, Humanity, and Social Justice:' with Course ID 99385 and Section B4 offers 3.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Mozisek located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Carnegie Mellon University-Wide Studies, the subject titled 'Interdisciplinary Undergraduate Research' with Course ID 99400 and Section A offers 3-12 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Burkert located in Building DNM, Room DNM.
Answer: "
"Which LTI professors wrote ""BASS: Block-wise Adaptation for Speech Summarization""?
","['bhiksha raj_3bd320ddb25886417ae90011b00f13f5d558097b_content_2.txt', 'rita singh_3bd320ddb25886417ae90011b00f13f5d558097b_content_2.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which LTI professors wrote ""BASS: Block-wise Adaptation for Speech Summarization""?

Context: Title: BASS: Block-wise Adaptation for Speech Summarization
Authors: Roshan Sharma, Kenneth Zheng, Rita Singh, Bhiksha Raj
Section: 7. References
Ann Arbor, Michigan: Association for Computational Linguistics, Jun. 2005, pp. 65–72. [Online]. Available: https://aclanthology.org/W05-0909 [27] T. Zhang*, V. Kishore*, F. Wu*, K. Q. Weinberger, and Y. Artzi, “Bertscore: Evaluating text generation with bert,” in International Conference on Learning Representations, 2020. [Online]. Available: https://openreview.net/forum?id=SkeHuCVFDr
Title: BASS: Block-wise Adaptation for Speech Summarization
Authors: Roshan Sharma, Kenneth Zheng, Rita Singh, Bhiksha Raj
Section: 7. References
Ann Arbor, Michigan: Association for Computational Linguistics, Jun. 2005, pp. 65–72. [Online]. Available: https://aclanthology.org/W05-0909 [27] T. Zhang*, V. Kishore*, F. Wu*, K. Q. Weinberger, and Y. Artzi, “Bertscore: Evaluating text generation with bert,” in International Conference on Learning Representations, 2020. [Online]. Available: https://openreview.net/forum?id=SkeHuCVFDr
Answer: "
"What are some benefits of using a hybrid model approach for identifying hedges?
","['justine cassell_b3efaa75beada858414a5ba2346dec317203633c_metadata.txt', 'justine cassell_b3efaa75beada858414a5ba2346dec317203633c_content_2.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are some benefits of using a hybrid model approach for identifying hedges?

Context: Faculty Name: justine cassell
Paperid: b3efaa75beada858414a5ba2346dec317203633c
Title: ""You might think about slightly revising the title”: Identifying Hedges in Peer-tutoring Interactions
Year: 2023
Abstract: Hedges have an important role in the management of rapport. In peer-tutoring, they are notably used by tutors in dyads experiencing low rapport to tone down the impact of instructions and negative feedback.Pursuing the objective of building a tutoring agent that manages rapport with teenagers in order to improve learning, we used a multimodal peer-tutoring dataset to construct a computational framework for identifying hedges. We compared approaches relying on pre-trained resources with others that integrate insights from the social science literature. Our best performance involved a hybrid approach that outperforms the existing baseline while being easier to interpret. We employ a model explainability tool to explore the features that characterize hedges in peer-tutoring conversations, and we identify some novel features, and the benefits of a such a hybrid model approach.
Authors: Yann Raphalen, C. Clavel, Justine Cassell
Venue: Annual Meeting of the Association for Computational Linguistics
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A model explainability tool is employed to explore the features that characterize hedges in peer-tutoring conversations, and some novel features, and the benefits of a such a hybrid model approach are identified.'}
Url: https://aclanthology.org/2022.acl-long.153.pdf
Title: ""You might think about slightly revising the title"": Identifying Hedges in Peer-tutoring Interactions
Authors: Yann Raphalen, Chloé Clavel, Justine Cassell
Section: 2 Related work
one that we also use), and they achieved their best classification result by employing an Attention-CNN model, inspired by Adel and Schütze (2017).
Answer: "
"In the paper titled ""WebArena: A Realistic Web Environment for Building Autonomous Agents"", what was the human performance on the proposed benchmark?
","['graham neubig_e41482f4ee984f17382f6cdd900df094d928be06_metadata.txt', 'yonatan bisk_e41482f4ee984f17382f6cdd900df094d928be06_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the paper titled ""WebArena: A Realistic Web Environment for Building Autonomous Agents"", what was the human performance on the proposed benchmark?

Context: Faculty Name: graham neubig
Paperid: e41482f4ee984f17382f6cdd900df094d928be06
Title: WebArena: A Realistic Web Environment for Building Autonomous Agents
Year: 2023
Abstract: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.
Authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper builds an environment for language-guided agents that is highly realistic and reproducible, and creates an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.'}
Url: https://arxiv.org/pdf/2307.13854
Faculty Name: yonatan bisk
Paperid: e41482f4ee984f17382f6cdd900df094d928be06
Title: WebArena: A Realistic Web Environment for Building Autonomous Agents
Year: 2023
Abstract: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.
Authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper builds an environment for language-guided agents that is highly realistic and reproducible, and creates an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.'}
Url: https://arxiv.org/pdf/2307.13854
Answer: "
"What single letter grade do you get for an incomplete grade?
","['handbook-msaii-2022-2023.txt', 'handbook-msaii-2022-2023.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What single letter grade do you get for an incomplete grade?

Context: However, such courses will NOT count toward 
the 195 units required for graduation. 
10.2.1 Incomplete Grades  
Carnegie Mellon University students are expected to complete a course during the academic 
semester in which the course was taken. However, if the instructor agrees, a grade of “I” 
(incomplete) may be given when a student has been unable to complete the work of a course. 
However, the work completed up to that date must be of passing quality and the grade of 
incomplete provides no undue advantage to that student over other students.  
By awarding an “I” grade, an instructor must specify the requirements for the completion of the 
work and designate a default letter grade in the event that the student fails to complete the 
remaining work.  
Students must complete the required course work by no later than the end of the following 
academic semester or sooner if required by the instructor.  
The instructor must record the permanent course grade by the last day of the examination 
period of the following semester, or the Registrar will automatically assign the default grade.  
If further work has not been completed after one semester and a default grade is rendered, the 
default grade will become the grade of record. 
10.2.2 Change of grades and missing grades  
If a grade has been assigned in error, it can be changed to a different permanent grade. The 
procedure for changing a grade is as follows: 
 Discuss the matter with the course instructor; provide evidence that the grade issued was 
not the grade earned. 
17 
 
 If the instructor agrees, the student should contact the program administrator to process 
a Change of Grade Form in order to correct the grade that was issued in error. Generally, 
the instructor is the final authority for a course grade. 
 If a grade has not been assigned, please notify the course instructor for the completion of 
a Missing Grade Form. 
10.2.3 Transferring to another program  
A student may withdraw from the MSAII program at any time.  If any requirement for the 
MSAII degree has not been met, the degree will not be awarded.  Occasionally, students are 
accepted into a different degree program at CMU and transfer to that program.  International 
students who do this should check with OIE to ensure that their visa is not jeopardized.
All grades count towards the program QPA, except for repeated courses, in which case the final 
grade replaces the previous grade.  Courses taken pass/no pass do not affect the student’s QPA.  
However, ONLY ELECTIVE COURSES MAY BE TAKEN PASS/NO PASS, unless the Provost 
directs to the contrary.  Students who do not achieve the required minimum grade in a required 
course, who elect to take an “incomplete” in a required course, or whose cumulative grade point 
average is below a B (3.0), will be placed on academic probation. Students on probation are not 
eligible to graduate. Students who are placed on academic probation shall receive written  
notice of this finding, including a list of measures that need to be taken to be removed from 
13 
 
academic probation. A student on academic probation for two consecutive semesters may be 
dropped from the program.    
The MSAII program conducts an academic progress review at the conclusion of each semester in 
order to monitor individual student progress toward graduation. Should a student’s effort fall 
below the acceptable academic performance and/or fail to meet the standards established by the 
MSAII program and Carnegie Mellon University, the student may be dismissed from the 
program. 
Should a student’s overall QPA drop below 3.0 during any given semester, he/she will be placed 
on academic probation for the following semester and will be required to improve his/her grades 
to no less than overall 3.0 QPA during that period. Failure to improve to an overall 3.0 QPA or 
better the following semester may result in termination from the program. In addition, a student 
who violates policies established by Carnegie Mellon University may be dismissed from the 
program. 
After each academic progress review, a student may receive one or more letters, indicating the 
result of the review. Warning letters are issued for the following cases: 
1) Overall QPA below 3.0 (Academic probation) 
2) Semester QPA is below 3.0 
3) Grade below B in a Core course 
4) Academic Integrity Violation (AIV) according to the university policies.  This includes any 
AIV letter received in during the summer preceding matriculation.
Answer: "
"What is the language technologies institute's fax number according to the MCDS handbook?
","['mcds-student-handbook-2023_2024.txt', 'combined_metadata_final.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the language technologies institute's fax number according to the MCDS handbook?

Context: To impact society in a transformative way — regionally, nationally, and 
globally — by engaging with partners outside the traditional borders of the 
university campus. 
1.4 MCDS Contact Information 
The people responsible for administering the MCDS degree are: 
 
Jennifer M Lucas 
Academic Program Manager 
Master of Computational Data Science  
Language Technologies Institute 
School of Computer Science  
Carnegie Mellon University 
Gates-Hillman Center 6415 
5000 Forbes Avenue, Pgh, PA 15213 
Phone: (412) 268-9870 
Fax: (412) 268-7287 
 
 
Carolyn Penstein Rosé, Director 
Master of Computational Data Science  
Language Technologies Institute 
School of Computer Science 
Carnegie Mellon University 
Gates-Hillman Center 5419 
5000 Forbes Avenue, Pgh, PA 15213 
Phone: (412) 268-4525 
Fax: (412) 268-7287 
Robert Frederking 
Graduate Program Chair 
Language Technologies Institute 
School of Computer Science  
Carnegie Mellon University 
Gates-Hillman Center 6515 
5000 Forbes Avenue, Pgh, PA 15213 
Phone: (412) 268-6656 
Mona Diab, LTI Director 
Language Technologies Institute 
School of Computer Science  
Carnegie Mellon University 
Gates-Hillman Center 5723 
5000 Forbes Avenue, Pgh, PA 15213 
Phone: (412) 268-3669 
 
The Language Technologies Institute is located primarily on the 5 th and 6th floors of the 
Gates Hillman Complex (GHC) on Carnegie Mellon’s Pittsburgh campus: 
 
Language Technologies Institute 
Carnegie Mellon University 
5000 Forbes Avenue 
Gates Hillman Complex 5402, LTI 
Pittsburgh, PA 15241-3891 
 
412-268-6591 (phone) 
412-268-6298 (fax) 
http://www.lti.cs.cmu.edu/ 
 
 
9
1.5 University Policies and Expectations 
Each member of the Carnegie Mellon community must be familiar with university 
policies and guidelines.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Data Science Seminar' with Course ID 11631 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Sap, Strubell located in Building BH, Room A36.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Data Science Seminar' with Course ID 11631 and Section B offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Strubell, Sap located in Building GHC, Room 4307.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Data Science Capstone' with Course ID 11632 and Section A offers 12.0 units. The Class meets Monday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Nyberg located in Building TEP, Room 1403.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'MCDS Independent Study:' with Course ID 11633 and Section NA offers 3-36 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'MCDS Independent Study' with Course ID 11633 and Section A offers 3-36 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rose located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'MCDS Independent Study' with Course ID 11633 and Section B offers 3-36 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building DNM, Room DNM.
Answer: "
"What is the title of Scotch'n'Soda's performance at the Spring Carnival?
","['Apr-12_Eventno_55_ScotchnSodaTheatreCarnivalShowTheLittleMermaid.txt', 'Apr-13_Eventno_46_ScotchnSodaTheatreCarnivalShowTheLittleMermaid.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the title of Scotch'n'Soda's performance at the Spring Carnival?

Context: Event: Scotch'n'Soda Theatre Carnival Show: The Little Mermaid
Date: 4/12/24
Time: 4/12/24 – 4/13/24
Participants/Audience: 10:00 PM-12:30 AM ET 
Event Details: 
Rangos Ballroom, Cohon University Center
Join Scotch'n'Soda Theatre for a performance under the sea! This year, Scotch'n'Soda is thrilled to presentDisney'sThe Little Mermaid! Adapted from the wildly popular animated film and Hans Christian Anderson's original fairytale, the musical follows Ariel, King Triton's youngest daughter, who wishes to explore the world up above and pursues the human Prince Eric. But the bargains and sea witches aren't all that they seem, and Ariel needs the help of her colorful friends, Flounder the fish, Scuttle the seagull and Sebastian the crab to restore order. Sing along with all of your favorite childhood classics, and come be a part of our world! Funded in part by the Student Activities Fee. Tickets will be available online in March and at the door. Cost: $5 for students/faculty/staff; $10 for alumni and guests.Note: Disney's The Little Mermaid is presented through special arrangement with Music Theatre International (MTI). All authorized performance materials are also supplied by MTI.

Cost
Tickets will be available online in March and at the door. Cost: $5 for students/faculty/staff; $10 for alumni and guests.

Weekend show times

Thursday: 7-9:30 p.m.
Friday: 6-8:30 p.m. and 10 p.m.-12:30 a.m.
Saturday: 3-5:30 p.m. and 7-9:30 p.m.
Event: Scotch'n'Soda Theatre Carnival Show: The Little Mermaid
Date: 4/13/24
Time: 3:00 PM-5:00 PM ET
Participants/Audience: Open to entire CMU community 
Event Details: 
Come celebrate Carnival with Scotch'n'Soda Theatre, and join us for a performance under the sea! This year, Scotch'n'Soda is thrilled to present our production of Disney's The Little Mermaid! Adapted from the wildly popular animated film and Hans Christian Anderson's original fairytale, the musical follows Ariel, King Triton's youngest daughter, who wishes to explore the world up above and pursue the human Prince Eric. But the bargains and sea witches aren't all that they seem, and Ariel needs the help of her colorful friends, Flounder the fish, Scuttle the seagull and Sebastian the crab to restore order under the sea. Sing along with all of your favorite childhood classics, and come be a part of our world! Funded in part by the Student Activities Fee. Note: Disney's The Little Mermaid is presented through special arrangement with Music Theatre International (MTI). All authorized performance materials are also supplied by MTI.  

Cost
Tickets will be available online in March and at the door. Cost: $5 for students/faculty/staff; $10 for alumni and guests. 

Weekend show times

Thursday: 7-9:30 p.m.
Friday: 6-8:30 p.m. and 10 p.m.-12:30 a.m.
Saturday: 3-5:30 p.m. and 7-9:30 p.m.
Answer: "
"In summer 2024, What is the deadline for Mini-5 pass/no pass and withdrawal?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-2324.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In summer 2024, What is the deadline for Mini-5 pass/no pass and withdrawal?

Context: On Friday, 07 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 pass/no pass & withdrawal deadline (3) is observed.
On Friday, 14 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Faculty Course Evaluations open is observed.
On Tuesday, 18 June, 2024, during the Summer One 2024 (M24) semester, Semester course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Wednesday, 19 June, 2022, during the Summer One 2024 (M24) semester, Juneteenth is observed, leading to University Closed & No Classes.
On Thursday, 20 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Last Day of Classes is observed.
On Thursday, 20 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 voucher deadline (4) is observed.
On Friday, 21 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Final Exams is observed.
On Friday, 21 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Faculty Course Evaluations close is observed.
On Monday, 24 June, 2024, during the Summer One 2024 (M24) semester, Mini-6 First Day of Classes is observed.
On Tuesday, 25 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Final Grades Due by 4 pm is observed.
On Friday, 28 June, 2024, during the Summer One 2024 (M24) semester, Mini-6 add, audit, & tuition adjustment drop deadline (1) is observed.
On Thursday, 04 July, 2024, during the Summer One 2024 (M24) semester, Independence Day is observed, leading to University Closed & No Classes.
On Friday, 05 July, 2024, during the Summer One 2024 (M24) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
Event: 'Mini-5 course drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-07', Day: 'Friday', Event: 'Mini-5 pass/no pass & withdrawal deadline (3)', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-14', Day: 'Friday', Event: 'Mini-5 Faculty Course Evaluations open', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-18', Day: 'Tuesday', Event: 'Semester course drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Summer One 2024 (M24)'
Date: '2022-06-19', Day: 'Wednesday', Event: 'Juneteenth; University Closed & No Classes', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-20', Day: 'Thursday', Event: 'Mini-5 Last Day of Classes', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-20', Day: 'Thursday', Event: 'Mini-5 voucher deadline (4)', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-21', Day: 'Friday', Event: 'Mini-5 Final Exams', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-21', Day: 'Friday', Event: 'Mini-5 Faculty Course Evaluations close ', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-24', Day: 'Monday', Event: 'Mini-6 First Day of Classes', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-25', Day: 'Tuesday', Event: 'Mini-5 Final Grades Due by 4 pm', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-28', Day: 'Friday', Event: 'Mini-6 add, audit, & tuition adjustment drop deadline (1)', Semester: 'Summer One 2024 (M24)'
Date: '2024-07-04', Day: 'Thursday', Event: 'Independence Day; University Closed & No Classes',
Answer: "
"How many courses does Mechanical Engineering offer in Summer 2024?
","['combined_metadata_final.txt', 'combined_metadata_final.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many courses does Mechanical Engineering offer in Summer 2024?

Context: In Semester Summer One(All) 2024, from the department of Mathematical Sciences, the subject titled 'Master's Degree Research' with Course ID 21901 and Section A offers 0-99 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Mathematical Sciences, the subject titled 'Doctoral Thesis Research' with Course ID 21902 and Section R offers 0-99 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Conley located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Mechanical Engineering, the subject titled 'Mechanical Engineering Project' with Course ID 24391 and Section A offers 3-12,18 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hertz located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Mechanical Engineering, the subject titled 'Mechanical Engineering Project' with Course ID 24391 and Section S offers 3-12,18 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hertz located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Mechanical Engineering, the subject titled 'Mechanical Engineering Project' with Course ID 24392 and Section A offers 3-18 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hertz located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Mechanical Engineering, the subject titled 'Mechanical Engineering Project' with Course ID 24392 and Section S offers 3-18 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hertz located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Mechanical Engineering, the subject titled 'Master of Science Project' with Course ID 24794 and Section A offers 0-99 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hertz located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Mechanical Engineering, the subject titled 'Graduate Reading and Research' with Course ID 24796 and Section R offers 0-99 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hertz located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Medical Management:Sch of Pub Pol & Mgt, the subject titled 'Health Care Quality' with Course ID 92897 and Section A offers 4.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Block located in Building TBA, Room None.
In Semester Summer One(All) 2024, from the department of Modern Languages, the subject titled 'Elementary French I Online' with Course ID 82103 and Section S offers 12.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Niang located in Building TBA, Room None.
In Semester Summer One(All) 2024, from the department of Modern Languages, the subject titled 'Elementary Chinese Online I' with Course ID 82133 and Section S offers 12.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Wu located in Building TBA, Room None.
In Semester Summer One(All) 2024, from the department of Modern Languages, the subject titled 'Elementary Chinese Online II' with Course ID 82134 and Section S offers 12.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Wu located in Building TBA, Room None.
Answer: "
"When was the Institute for Software Research formed?
","['michael_shamos.txt', 'history_d401.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When was the Institute for Software Research formed?

Context: Michael Shamos
Distinguished Career Professor, Language Technologies Institute
Institute for Software Research
Contact
6707 —Gates & Hillman Centers
Email shamos@cs.cmu.edu
412-268-8193
Institute for Software Research https://www.cs.cmu.edu/~brassmars/
Personal Website http://www.cs.cmu.edu/~jbigham/
As the 1970s progressed, Newell became interested in human-computer interaction, and began a long relationship with Xerox’s Palo Alto Research Center, or PARC, which released the Xerox Alto in 1973. Considered a forerunner to many of the computing environments that followed, Alto featured a graphical-user interface and was among the first commercially available workstations controlled with a mouse. Inspired by Alto, Reddy launched a drive for development of CMU’s own “three-M” machine—a personal workstation with a megabyte of memory, a megapixel display and at least one million instructions per second of processing power.Launching a Robotics InstituteIn 1979, an executive at Pittsburgh’s Westinghouse Electric Corp., Tom Murrin, collaborated with Jordan and Reddy to create the Robotics Institute, with Reddy as its first director. By 1982, the Computer Science Department included more than 30 faculty members and 100 graduate students.The best-wired campus in the worldWorking with IBM in the early 1980s, the university and the Computer Science Department established another new research frontier: Development of a high-speed computer network that would reach virtually every room on campus, along with a GUI-based computing environment, and providing networked PCs or workstations for 7,000 students, faculty members and employees. Called the Andrew Project, it turned Carnegie Mellon into the best-connected, most-wired university in the world—a process Newell called “greening up the campus with computer science.” CMU also became home to a new Software Engineering Institute, funded by the Defense Department, to study computer security and develop best practices in the design of operating systems. Between 1982 and 1985, the amount of sponsored research in the Computer Science Department doubled, from $7.2 million to $15.3 million—more than the other four departments in the Mellon College of Science combined.A “school of computer science” is proposedFeeling that CSD’s needs were inadequately represented in MCS, CSD head A. Nico Habermann and then-CMU provost Angel Jordan in 1986 wrote a white paper proposing the creation of “a School of Computer Science.” Responding to concerns from the faculty that the change might be taking place too quickly, the university first established a free-floating Department of Computer Science. The experiment, which lasted two years, was an unqualified success.
Answer: "
"Where is Teruko Mitamura's Hierarchical Event Grounding published at?
","['teruko mitamura_ff77105b2c345f54e1a87f4fbb3a701201f0c1a8_metadata.txt', 'teruko mitamura_papers.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Where is Teruko Mitamura's Hierarchical Event Grounding published at?

Context: Faculty Name: teruko mitamura
Paperid: ff77105b2c345f54e1a87f4fbb3a701201f0c1a8
Title: Hierarchical Event Grounding
Year: 2023
Abstract: Event grounding aims at linking mention references in text corpora to events from a knowledge base (KB). Previous work on this task focused primarily on linking to a single KB event, thereby overlooking the hierarchical aspects of events. Events in documents are typically described at various levels of spatio-temporal granularity. These hierarchical relations are utilized in downstream tasks of narrative understanding and schema construction. In this work, we present an extension to the event grounding task that requires tackling hierarchical event structures from the KB. Our proposed task involves linking a mention reference to a set of event labels from a subevent hierarchy in the KB. We propose a retrieval methodology that leverages event hierarchy through an auxiliary hierarchical loss. On an automatically created multilingual dataset from Wikipedia and Wikidata, our experiments demonstrate the effectiveness of the hierarchical loss against retrieve and re-rank baselines. Furthermore, we demonstrate the systems' ability to aid hierarchical discovery among unseen events. Code is available at https://github.com/JefferyO/Hierarchical-Event-Grounding
Authors: Jiefu Ou, Adithya Pratapa, Rishubh Gupta, T. Mitamura
Venue: AAAI Conference on Artificial Intelligence
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work presents an extension to the event grounding task that requires tackling hierarchical event structures from the KB, and proposes a retrieval methodology that leverages event hierarchy through an auxiliary hierarchical loss.'}
Url: http://arxiv.org/pdf/2302.04197
List of 2023 Open Access papers by teruko mitamura are:
Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting for Multilingual Document-Grounded QA
Hierarchical Event Grounding
Answer: "
"Pittsburgh Supercomputing Center was created as a joint effort between which entities?
","['history_d401.txt', 'history_d401.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Pittsburgh Supercomputing Center was created as a joint effort between which entities?

Context: The experiment, which lasted two years, was an unqualified success. Separately, and also in 1986, the Pittsburgh Supercomputing Center was created as a joint effort between CMU, the University of Pittsburgh and Westinghouse Electric Corp.SCS is officially formedCMU’s Faculty Senate in the fall of 1988 agreed to President Richard Cyert’s plan to elevate the Department of Computer Science to college status. In addition to the Computer Science Department, SCS also incorporated the Robotics Institute, the Center for Machine Translation, and researchers from the Information Technology Center, which had developed Andrew. On Dec. 13, 1988, Cyert (H’89) told faculty and staff that Habermann had been appointed CMU’s first Dean of Computer Science, effective Dec. 1, and that the School of Computer Science would soon begin operations. SCS made its formal debut on Dec. 22, 1988, with a reception in the Wherrett Room of Skibo Hall, CMU’s student union. The official announcement of CMU’s new “graduate School of Computer Science” was made Jan. 3, 1989.Undergraduate degrees beginFor several years, undergraduates interested in computer science pursued an “applied math/CS” bachelor’s degree offered by the Mathematics Department. CSD professor Mary Shaw (CS’72) led CMU’s first effort to design an undergraduate curriculum solely in computer science. She and her colleagues were guided by the Carnegie Plan—guidelines established in 1938 under Carnegie Tech President Robert Doherty (A’40, E’48, H’50), outlining the principles of a sound professional education. Drawing on Shaw’s plan and also on the work of other faculty members, an undergraduate program in computer science was created during the 1989-90 academic year. Seven CS majors were admitted to the program as sophomores. Another 73 undergraduates were admitted in 1990–91.
As the 1970s progressed, Newell became interested in human-computer interaction, and began a long relationship with Xerox’s Palo Alto Research Center, or PARC, which released the Xerox Alto in 1973. Considered a forerunner to many of the computing environments that followed, Alto featured a graphical-user interface and was among the first commercially available workstations controlled with a mouse. Inspired by Alto, Reddy launched a drive for development of CMU’s own “three-M” machine—a personal workstation with a megabyte of memory, a megapixel display and at least one million instructions per second of processing power.Launching a Robotics InstituteIn 1979, an executive at Pittsburgh’s Westinghouse Electric Corp., Tom Murrin, collaborated with Jordan and Reddy to create the Robotics Institute, with Reddy as its first director. By 1982, the Computer Science Department included more than 30 faculty members and 100 graduate students.The best-wired campus in the worldWorking with IBM in the early 1980s, the university and the Computer Science Department established another new research frontier: Development of a high-speed computer network that would reach virtually every room on campus, along with a GUI-based computing environment, and providing networked PCs or workstations for 7,000 students, faculty members and employees. Called the Andrew Project, it turned Carnegie Mellon into the best-connected, most-wired university in the world—a process Newell called “greening up the campus with computer science.” CMU also became home to a new Software Engineering Institute, funded by the Defense Department, to study computer security and develop best practices in the design of operating systems. Between 1982 and 1985, the amount of sponsored research in the Computer Science Department doubled, from $7.2 million to $15.3 million—more than the other four departments in the Mellon College of Science combined.A “school of computer science” is proposedFeeling that CSD’s needs were inadequately represented in MCS, CSD head A. Nico Habermann and then-CMU provost Angel Jordan in 1986 wrote a white paper proposing the creation of “a School of Computer Science.” Responding to concerns from the faculty that the change might be taking place too quickly, the university first established a free-floating Department of Computer Science. The experiment, which lasted two years, was an unqualified success.
Answer: "
"When is Douse-a-Dean event at this year's Spring Carnival?
","['Apr-13_Eventno_36_SpringCarnivalParentsandFamiliesHospitalitySuite.txt', 'Apr-13_Eventno_49_SpringCarnivalBoothSweepstakesAwardCeremony.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When is Douse-a-Dean event at this year's Spring Carnival?

Context: Event: Spring Carnival Parents and Families Hospitality Suite
Date: 4/13/24
Time: 1:30 PM-3:00 PM ET
Participants/Audience: Open to parents and families 
Event Details: 
All families are welcome to drop in to our hospitality lounge to take a break, grab some snacks, take a photo, mingle with fellow families and chat with staff. 

Note: No registration required. No event fee. This event is open to parents and families of current students.
Event: Spring Carnival Booth & Sweepstakes Award Ceremony
Date: 4/13/24
Time: 4:30 PM-5:30 PM ET
Participants/Audience: Open to entire CMU community 
Event Details: 
Note: No registration required. No event fee.
Answer: "
"What time does Leading in a Lean and Six Sigma World start in Summer 2024?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What time does Leading in a Lean and Six Sigma World start in Summer 2024?

Context: On Friday, 07 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 pass/no pass & withdrawal deadline (3) is observed.
On Friday, 14 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Faculty Course Evaluations open is observed.
On Tuesday, 18 June, 2024, during the Summer One 2024 (M24) semester, Semester course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Wednesday, 19 June, 2022, during the Summer One 2024 (M24) semester, Juneteenth is observed, leading to University Closed & No Classes.
On Thursday, 20 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Last Day of Classes is observed.
On Thursday, 20 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 voucher deadline (4) is observed.
On Friday, 21 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Final Exams is observed.
On Friday, 21 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Faculty Course Evaluations close is observed.
On Monday, 24 June, 2024, during the Summer One 2024 (M24) semester, Mini-6 First Day of Classes is observed.
On Tuesday, 25 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Final Grades Due by 4 pm is observed.
On Friday, 28 June, 2024, during the Summer One 2024 (M24) semester, Mini-6 add, audit, & tuition adjustment drop deadline (1) is observed.
On Thursday, 04 July, 2024, during the Summer One 2024 (M24) semester, Independence Day is observed, leading to University Closed & No Classes.
On Friday, 05 July, 2024, during the Summer One 2024 (M24) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Friday, 13 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Faculty Course Evaluations open is observed.
On Tuesday, 17 June, 2025, during the Summer 2025 (M25) semester, Summer All course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Wednesday, 18 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Last Day of Classes is observed.
On Wednesday, 18 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 voucher deadline is observed.
On Thursday, 19 June, 2025, during the Summer 2025 (M25) semester, Juneteenth is observed, leading to University Closed & No Classes.
On Friday, 20 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Final Exams is observed.
On Friday, 20 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Faculty Course Evaluations close is observed.
On Monday, 23 June, 2025, during the Summer 2025 (M25) semester, Summer Two & Mini-6 First Day of Classes is observed.
On Tuesday, 24 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Final Grades Due by 4 pm is observed.
On Friday, 27 June, 2025, during the Summer 2025 (M25) semester, Summer Two & Mini-6 add, audit, & tuition adjustment drop deadline (1) is observed.
On Thursday, 03 July, 2025, during the Summer 2025 (M25) semester, Summer All pass/no pass & withdrawal grade deadline (3) is observed.
On Friday, 04 July, 2025, during the Summer 2025 (M25) semester, Independence Day is observed, leading to University Closed & No Classes.
On Monday, 07 July, 2025, during the Summer 2025 (M25) semester, Summer Two & Mini-6 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
Answer: "
"What street is CMU LTI located one?
","['miis-handbook_2023-2024.txt', 'mona_diab.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What street is CMU LTI located one?

Context: MIIS Graduate Student Handbook 
Page 8 
 
The Language Technologies Institute is located primarily on the 5th and 6th floors of the Gates 
Hillman Complex (GHC) on Carnegie Mellon’s Pittsburgh campus.  The address and contact 
information of the LTI are: 
Language Technologies Institute 
Carnegie Mellon University 
5000 Forbes Avenue 
Gates Hillman Complex 5404, LTI 
Pittsburgh, PA 15241-3891 
 
412-268-6591 (phone) 
412-268-6298 (fax) 
 
http://www.lti.cs.cmu.edu/ 
1.5 Carnegie Mellon University Statement of Assurance  
Carnegie Mellon University does not discriminate in admission, employment or administration 
of its programs or activities on the basis of race, color, national origin, sex, handicap or disability, 
age, sexual orientation, gender identity, religion, creed, ancestry, belief, veteran status or genetic 
information. Furthermore, Carnegie Mellon University does not discriminate and is required 
not to discriminate in violation of federal, state, or local laws or executive orders. 
  
Inquiries concerning the application of and compliance with this statement should be directed 
to the university ombudsperson, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, 
PA 15213, telephone 412-268-1018.  Obtain general information about Carnegie Mellon 
University by calling 412-268-2000. 
  
Carnegie Mellon University publishes an annual campus security and fire safety report 
describing the university's security, alcohol and drug, sexual assault, and fire safety policies, and 
containing statistics about the number and type of crimes committed on the campus, and the 
number and cause of fires in campus residence facilities during the preceding three years. You 
can obtain a copy by contacting the Carnegie Mellon Police Department at 412-268-2323. The 
annual 
security 
and 
fire 
safety 
report 
also 
is 
available 
online 
at www.cmu.edu/police/annualreports. 
 
Information regarding the application of Title IX, including to admission and employment 
decisions, the sexual misconduct grievance procedures and process, including how to file a 
report or a complaint of sex discrimination, how to file a report of sexual harassment, and how 
the university responds to such reports is available at www.cmu.edu/title-ix.
Mona Diab
LTI Director and Tenured Professor, Language Technologies Institute
Research Area : Fairness and Ethics in Language Technology
Contact: 5723 Gates & Hillman Centers
Email : mdiab@andrew.cmu.edu
Phone : 412-268-3669
Address
5000 Forbes Avenue
Pittsburgh, PA 15213
Answer: "
"How many units is the MIIS Capstone Project with course number 11927 for?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many units is the MIIS Capstone Project with course number 11927 for?

Context: In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'MIIS Capstone Project' with Course ID 11927 and Section A offers 36.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Brown located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'MIIS Capstone Project' with Course ID 11927 and Section B offers 36.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Brown located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Masters Thesis I' with Course ID 11928 and Section A offers 5-36 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Masters Thesis I' with Course ID 11928 and Section AB offers 5-36 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Masters Thesis I' with Course ID 11928 and Section AE offers 5-36 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Masters Thesis I' with Course ID 11928 and Section B offers 5-36 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'MIIS Capstone Project' with Course ID 11927 and Section A offers 36.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Brown located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'MIIS Capstone Project' with Course ID 11927 and Section B offers 36.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Brown located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Masters Thesis I' with Course ID 11928 and Section A offers 5-36 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Masters Thesis I' with Course ID 11928 and Section AB offers 5-36 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Masters Thesis I' with Course ID 11928 and Section AE offers 5-36 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Masters Thesis I' with Course ID 11928 and Section B offers 5-36 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building TBA, Room None.
Answer: "
"Which fraternity won the first race in 1920?
","['Apr-12_Eventno_6_BuggyAlumniAssociationInformationTent.txt', 'Apr-13_Eventno_50_SRSPostSweepstakesAwardsGathering.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which fraternity won the first race in 1920?

Context: Event: Buggy Alumni Association Information Tent
Date: 4/12/24
Time: 8:00 AM-12:00 PM ET
Participants/Audience: Open to entire CMU community 
Event Details: 
Come by the Buggy Alumni Association tent during Sweepstakes races to meet your BAA officers and learn about our work. 

Note: No registration required. No event fee. This event is open to the entire CMU community and their guests.
Event: SRS Post-Sweepstakes Awards Gathering
Date: 4/13/24
Time: 5:45 PM-6:45 PM ET
Participants/Audience: Open to SRS Community 
Event Details: 
Join SPIRIT Racing Systems (SRS) alumni, students, friends and family for an update on and celebration of SRS and their accomplishments. Can't make it back to campus? We'll livestream the gathering, so be sure to sign up for the livestream event during registration. 

Note: Registration required. Walk-ins are welcome if space permits. No event fee. Open to the Kiltie community and guests.
Answer: "
"What is the minimum GPA for the MSAII program?
","['handbook-msaii-2022-2023.txt', 'handbook-msaii-2022-2023.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the minimum GPA for the MSAII program?

Context: See Section 11.1.10, below.   Upon 
completion of the graduate program degree requirements, the degree will be certified by the student’s 
academic program in the semester in which the student completes the requirements. 
10.1 Course Grade and GPA Requirements   
To receive the M.S. degree, all students must take and successfully complete at least 195 units 
of coursework. All required Core Courses must be completed with a grade of B or better. For 
the Knowledge Area Courses, students must maintain a B average.  No grade lower than C (or 
Pass in the case of Pass/No Pass courses) shall be used for credit applied toward graduation.  
Under SCS policy, only a grade of C or higher will be regarded as a Pass in a Pass/No Pass 
course.  No undergraduate courses count toward the 195-unit requirement. 
All Core courses are organized in sequence, and must be taken in the following order: 
 Artificial Intelligence and Future Markets (11-651) – No prerequisite. 
 AI Engineering (11-695) – Prerequisite is the completion of 11-651 with a grade of “B” or 
better. 
 LTI Practicum (11-935) – Three credit units for completing the required Internship. 
 AI Innovation (11-654) – Prerequisite is the completion of 11-695 with a grade of “B” or 
better. 
 Capstone (11-699) – Prerequisite is the completion of 11-654 with a grade of “B” or better.  
Students are required to repeat any required course that they have completed with a grade less 
than the required minimum, preferably at the next offering. (Please note that Core courses and 
Knowledge courses are only offered once per year.) Students will be allowed to retake a required 
course only once. If a student fails in the second attempt, he or she will be dropped from the 
MSAII program.    
If a student fails an elective course, she or he will need to repeat the same or take a substitute 
course (equivalent course approved by the Director).    
All grades count towards the program QPA, except for repeated courses, in which case the final 
grade replaces the previous grade.  Courses taken pass/no pass do not affect the student’s QPA.
Form I-9 must be completed within 3 business days of beginning work for any type of 
compensation (stipend or employment). Additional details are highlighted below. 
20 
 
To ensure compliance with federal law, Carnegie Mellon University maintains the Employment 
Eligibility Verification (I-9) Policy [pdf] covering the university’s I-9 and E-Verify requirements:  
Every individual receiving a stipend from CMU or employed by CMU must comply with the I-9 
Policy by completing the Form I-9 within three business days following the first day of stipend 
start date/employment.  
Individuals who expect to work on a federally funded project are further responsible for 
submitting an E-Verify Processing Request Form to the Office of Human Resources.  
For more information, please see CMU’s Guidance for Completing the Form I-9 and E-Verify 
Requirements at CMU [pdf], or visit the Human Resources Service website to learn more about 
Form I-9 and E-Verify and to schedule an appointment to complete the Form I-9. 
10.6 MSAII Orientation  
Each Fall semester, the MSAII program provides a program orientation to help new students 
learn about the program. All new students are required to attend them and treat them seriously 
because they explain the program sequence, its Core and Knowledge courses requirements and 
how students will meet the program learning outcomes.  
10.7 End of Semester Evaluation 
The MSAII program conducts an academic progress review at the conclusion of each semester in 
order to monitor individual student progress towards graduation. Should a student’s effort fall 
below the acceptable level of academic performance and/or fail to meet the standards 
established by the MSAII program, the student may be dismissed from the program. 
After each academic progress review, a student may receive one or more letters, indicating the 
result of the review. See Section 3.1, Course Grade and GPA Requirements, above. 
At the end of the semester, the faculty evaluates each student's academic progress. If a student 
seems to be having trouble, the faculty determines whether it believes that the student can 
finish the degree, and if so, what needs to be accomplished to get back on track. This type of 
letter should be considered a serious warning.
Answer: "
"What are the names of the 15.5B parameter models introduced by The BigCode community?
","['daniel fried_3e4085e5869f1b7959707a1e1d7d273b6057eb4e_metadata.txt', 'daniel fried_1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the names of the 15.5B parameter models introduced by The BigCode community?

Context: Faculty Name: daniel fried
Paperid: 3e4085e5869f1b7959707a1e1d7d273b6057eb4e
Title: StarCoder: may the source be with you!
Year: 2023
Abstract: The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.
Faculty Name: daniel fried
Paperid: 1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a
Title: SantaCoder: don't reach for the stars!
Year: 2023
Abstract: The BigCode project is an open-scientific collaboration working on the responsible development of large language models for code. This tech report describes the progress of the collaboration until December 2022, outlining the current state of the Personally Identifiable Information (PII) redaction pipeline, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data. We train 1.1B parameter models on the Java, JavaScript, and Python subsets of The Stack and evaluate them on the MultiPL-E text-to-code benchmark. We find that more aggressive filtering of near-duplicates can further boost performance and, surprisingly, that selecting files from repositories with 5+ GitHub stars deteriorates performance significantly. Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and infilling on the Java, JavaScript, and Python portions of MultiPL-E, despite being a substantially smaller model. All models are released under an OpenRAIL license at https://hf.co/bigcode.
Answer: "
"What does FLARE stand for?
","['graham neubig_88884b8806262a4095036041e3567d450dba39f7_metadata.txt', 'jamie callan_88884b8806262a4095036041e3567d450dba39f7_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What does FLARE stand for?

Context: Faculty Name: graham neubig
Paperid: 88884b8806262a4095036041e3567d450dba39f7
Title: Active Retrieval Augmented Generation
Year: 2023
Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.
Authors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work proposes Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens.'}
Url: http://arxiv.org/pdf/2305.06983
Faculty Name: jamie callan
Paperid: 88884b8806262a4095036041e3567d450dba39f7
Title: Active Retrieval Augmented Generation
Year: 2023
Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.
Authors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work proposes Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens.'}
Url: http://arxiv.org/pdf/2305.06983
Answer: "
"What is StyleRF's solution to the three-way dilemma in 3D style transfer?
","['eric xing_8cc1cd002bfc36a8cba8bcbe63d32eacc656097f_metadata.txt', 'eric xing_8cc1cd002bfc36a8cba8bcbe63d32eacc656097f_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is StyleRF's solution to the three-way dilemma in 3D style transfer?

Context: Faculty Name: eric xing
Paperid: 8cc1cd002bfc36a8cba8bcbe63d32eacc656097f
Title: StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields
Year: 2023
Abstract: 3D style transfer aims to render stylized novel views of a 3D scene with multiview consistency. However, most existing work suffers from a three-way dilemma over accurate geometry reconstruction, high-quality stylization, and being generalizable to arbitrary new styles. We propose StyleRF (Style Radiance Fields), an innovative 3D style transfer technique that resolves the three-way dilemma by performing style transformation within the feature space of a radiance field. StyleRF employs an explicit grid of high-level features to represent 3D scenes, with which highfidelity geometry can be reliably restored via volume rendering. In addition, it transforms the grid features according to the reference style which directly leads to high-quality zero-shot style transfer. StyleRF consists of two innovative designs. The first is sampling-invariant content transformation that makes the transformation invariant to the holistic statistics of the sampled 3D points and accordingly ensures multi-view consistency. The second is deferred style transformation of 2D feature maps which is equivalent to the transformation of 3D points but greatly reduces memory footprint without degrading multi-view consistency. Extensive experiments show that StyleRF achieves superior 3D stylization quality with precise geometry reconstruction and it can generalize to various new styles in a zero-shot manner. Project website: https://kunhao-liu.github.io/StyleRF/
Authors: Kunhao Liu, Fangneng Zhan, Yiwen Chen, Jiahui Zhang, Yingchen Yu, Abdulmotaleb El Saddik, Shijian Lu, E. Xing
Venue: Computer Vision and Pattern Recognition
Tldr: {'model': 'tldr@v2.0.0', 'text': 'StyleRF (Style Radiance Fields), an innovative 3D style transfer technique that resolves the three-way dilemma by performing style transformation within the feature space of a radiance field, achieves superior 3D stylization quality with precise geometry reconstruction and it can generalize to various new styles in a zero-shot manner.'}
Url: https://arxiv.org/pdf/2303.10598
Title: StyleRF: Zero-shot 3D Style Transfer of Neural Radiance Fields
Authors: Kunhao Liu, Fangneng Zhan, Yiwen Chen, Jiahui Zhang, Yingchen Yu, Abdulmotaleb El Saddik, Shijian Lu, Eric Xing
Section: 2. Related Work
styles consistently across adjacent video frames. Several studies leverage optical flow [5,18,47,56,57] as temporal constraints to estimate the movement of video contents. They can produce smooth videos, but have little knowledge of the underlying 3D geometry and cannot render consistent frames in arbitrary views [19, 37]. Huang et al. first tackle stylizing complex 3D scenes [19]. They construct a 3D scene by back-projecting image features into the 3D space to form a point cloud and then perform style transformation on the features of 3D points. Their method can achieve zero-shot style transfer, but requires an error-prone pre-trained depth estimator to model scene geometry. [37] also constructs a point cloud for stylization but it mainly focuses on monocular images. Instead, [6, 8, 11,22, 39, 63] use NeRF [36] as the 3D representation which can reconstruct scene geometry more faithfully. [6] is a photorealistic style transfer method that can only transfer the color tone of style images. [39,63] achieve 3D style transfer via optimization and can produce visually high-quality stylization, but they require a time-consuming optimization procedure for every reference style. [11, 22] employ latent codes to represent a set of pre-defined styles, but cannot generalize to unseen styles. [8] can achieve arbitrary style transfer by implicitly instilling the style information into MLP parameters. However, it can only transfer the color tone of style images but cannot capture detailed style patterns. StyleRF can transfer arbitrary style in a zero-shot manner, and it can capture style details such as strokes and textures as well.
Answer: "
"Where is the Center for Student Diversity and Inclusion Ceremony held on May 11, 2024?
","['Commencement.txt', 'mcds-student-handbook-2023_2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Where is the Center for Student Diversity and Inclusion Ceremony held on May 11, 2024?

Context: Invitation, along with registration details, will be sent in late April.

First Gen Graduation Recognition
Reception: 5-5:30 p.m.
Alumni Concert Hall, College of Fine Arts
Ceremony: 5:30-6:30p.m.
Kresge Theater, College of Fine Arts

Contact:
M. Shernell Smith
mssmith@andrew.cmu.edu 

Sam Colavecchio
scolavec@andrew.cmu.edu 
412-268-7733


Friday, May 10
Diploma Ceremonies
Various times

Senior Leadership Recognition Ceremony
4–5:30 p.m.
Wiegand Gym, Cohon University Center
Undergraduate students and their guests by invitation only. This ceremony recognizes nominated seniors who have reflected upon their specific leadership contributions during their time at CMU.


Saturday, May 11
Diploma Ceremonies
Various times

Center for Student Diversity and Inclusion Ceremony
Reception: 12:30 p.m.
Program: 1-2:15 p.m.
Simmons Auditorium, Tepper Building

Contact:
M. Shernell Smith
mssmith@andrew.cmu.edu 

Rowshan Lang
rowshan@andrew.cmu.edu
Naval ROTC Commissioning
Ceremony: 1:30-2:30 p.m.
Auditorium, Soldiers & Sailors Memorial Hall & Museum *
4141 Fifth Avenue, Pittsburgh, PA 15213
Contact:
Mike Danko
mdanko@andrew.cmu.edu 

The President’s Reception in honor of CMU’s Doctoral Candidates
4–6 p.m.
Tepper Building Atrium


Sunday, May 12
Gesling Stadium opens to guests
8 a.m.

Robing and procession for graduates
9–10 a.m.
Various locations across campus

Student procession begins
9:15 a.m.
All guests in stadium must be seated. Access to guest seating will be restricted once the procession begins.

Commencement Ceremony
10–11:30 a.m.
Gesling Stadium, CMU’s campus

Diploma Ceremonies
Various times
The Center for Student Diversity & Inclusion actively 
cultivates a strong, diverse and inclusive community capable of living out these 
values and advancing research, creativity, learning and development that 
changes the world.  
The Center offers resources to enhance an inclusive and transformative 
 
 
3 
student experience in dimensions such as access, success, campus climate and 
intergroup dialogue. Additionally, the Center supports and connects 
historically underrepresented students and those who are first in their family 
to attend college in a setting where students’ differences and talents are 
appreciated and reinforced, both at the graduate and undergraduate level. 
Initiatives coordinated by the Center include, but are not limited to:  
 First generation/first in family to attend college programs 
 LGBTQ+ Initiatives  
 Race and ethnically focused programs, including Inter-University 
Graduate Students of Color Series (SOC) and PhD SOC Network  
 Women’s empowerment programs, including Graduate Women’s 
Gatherings (GWGs)  
12 Assistance for Individuals with Disabilities  
https://www.cmu.edu/disability-resources/  
The Office of Disability Resources at Carnegie Mellon University has a 
continued mission to provide physical, digital, and programmatic access to 
ensure that students with disabilities have equal access to their educational 
experience.  The Office works to ensure that qualified individuals receive 
reasonable accommodations as guaranteed by the Americans with Disabilities 
Act (ADA) and Section 504 of the Rehabilitation Act of 1973. Students who 
would like to receive accommodations can begin the process through Disability 
Resources' secure online portal or email access@andrew.cmu.edu to begin the 
interactive accommodation Process. 
Students with physical, sensory, cognitive, or emotional disabilities are 
encouraged to self-identify with the Office of Disability Resources and request 
needed accommodations. Any questions about the process can be directed to 
access@andrew.cmu.edu, or call (412) 268- 6121.  
13 Eberly Center for Teaching Excellence & Educational Innovation  
https://www.cmu.edu/teaching/  
The Eberly Center offers a wide variety of confidential, consultation services 
and professional development programs to support graduate students as 
teaching assistants or instructors of record during their time at Carnegie 
Mellon University and as future faculty members at other institutions.
Answer: "
"What is Fernando Diaz's job title?
","['fernando diaz_5e1ba2d4416333dd6f6a31a4ff4221b40dfb74b1_metadata.txt', 'fernando diaz_e59a99c198ee4012e34fd79d7cb33be75d0a120d_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is Fernando Diaz's job title?

Context: Faculty Name: fernando diaz
Paperid: 5e1ba2d4416333dd6f6a31a4ff4221b40dfb74b1
Title: Overview of the TREC 2021 Fair Ranking Track
Year: 2023
Abstract: The TREC Fair Ranking Track aims to provide a platform for participants to develop and evaluate novel retrieval algorithms that can provide a fair exposure to a mixture of demographics or attributes, such as ethnicity, that are represented by relevant documents in response to a search query. For example, particular demographics or attributes can be represented by the documents' topical content or authors. The 2021 Fair Ranking Track adopted a resource allocation task. The task focused on supporting Wikipedia editors who are looking to improve the encyclopedia's coverage of topics under the purview of a WikiProject. WikiProject coordinators and/or Wikipedia editors search for Wikipedia documents that are in need of editing to improve the quality of the article. The 2021 Fair Ranking track aimed to ensure that documents that are about, or somehow represent, certain protected characteristics receive a fair exposure to the Wikipedia editors, so that the documents have an fair opportunity of being improved and, therefore, be well-represented in Wikipedia. The under-representation of particular protected characteristics in Wikipedia can result in systematic biases that can have a negative human, social, and economic impact, particularly for disadvantaged or protected societal groups.
Authors: Asia J. Biega, Fernando Diaz, Michael D. Ekstrand, Sebastian Kohlmeier
Venue: Text Retrieval Conference
Tldr: {'model': 'tldr@v2.0.0', 'text': 'The 2021 Fair Ranking track aimed to ensure that documents that are about, or somehow represent, certain protected characteristics receive a fair exposure to the Wikipedia editors, so that the documents have an fair opportunity of being improved and, therefore, be well-represented in Wikipedia.'}
Url: http://arxiv.org/pdf/2302.10856
Faculty Name: fernando diaz
Paperid: e59a99c198ee4012e34fd79d7cb33be75d0a120d
Title: Amplification-free, highly sensitive electrochemical DNA-based sensor for simultaneous detection of stx1 and stx2 genes of Shiga toxin-producing E. coli (STEC)
Year: 2023
Abstract: None
Authors: L. Wasiewska, F. Diaz, S. Teixeira, C. Burgess, G. Duffy, A. O’Riordan
Venue: Electrochimica Acta
Tldr: None
Url: N/A
Answer: "
"In spring 2024, What is the title of course 15150?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, What is the title of course 15150?

Context: In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section O offers 12.0 units. The Class meets Wednesday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section P offers 12.0 units. The Class meets Wednesday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section Q offers 12.0 units. The Class meets Wednesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section R offers 12.0 units. The Class meets Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section S offers 12.0 units. The Class meets Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5210.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section T offers 12.0 units. The Class meets Wednesday between 07:00PM and 08:20PM ET.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section O offers 12.0 units. The Class meets Wednesday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section P offers 12.0 units. The Class meets Wednesday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section Q offers 12.0 units. The Class meets Wednesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section R offers 12.0 units. The Class meets Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section S offers 12.0 units. The Class meets Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5210.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section T offers 12.0 units. The Class meets Wednesday between 07:00PM and 08:20PM ET.
Answer: "
"What are the events on May 10 as part of the Commencement program for 2024?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the events on May 10 as part of the Commencement program for 2024?

Context: On Monday, 05 May, 2025, during the Spring 2025 (S25) semester, Semester & Mini-4 Faculty Course Evaluations close is observed.
On Tuesday, 06 May, 2025, during the Spring 2025 (S25) semester, Make-Up Final Examinations is observed.
On Wednesday, 08 May, 2025, during the Spring 2025 (S25) semester, Graduating Students' Final Grades Due by 4 pm is observed.
On Sunday, 11 May, 2025, during the Spring 2025 (S25) semester, Commencement is observed.
On Tuesday, 13 May, 2025, during the Spring 2025 (S25) semester, Final Grades Due by 4 pm is observed.
On Tuesday, 20 May, 2025, during the Spring 2025 (S25) semester, Spring Deans' Lists Posted is observed.
On Monday, 12 May, 2025, during the Summer 2025 (M25) semester, Summer Session All, One & Mini-5 Classes Begin is observed.
On Friday, 16 May, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 add, audit, & tuition adjustment drop deadline (1) is observed.
On Friday, 23 May, 2025, during the Summer 2025 (M25) semester, Summer All add, audit, & tuition adjustment drop deadline (1) is observed.
On Friday, 26 May, 2025, during the Summer 2025 (M25) semester, Memorial Day is observed, leading to University Closed & No Classes.
On Tuesday, 27 May, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 09 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 pass/no pass & withdrawal deadline (3) is observed.
On Friday, 13 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Faculty Course Evaluations open is observed.
From 04 May, 2024,Saturday to 05 May, 2024,Sunday marks the Reading Days for Spring 2024 (S24) semester.
On Monday, 06 May, 2024, during the Spring 2024 (S24) semester, Final Examinations is observed.
On Monday, 06 May, 2024, during the Spring 2024 (S24) semester, Semester & Mini-4 Faculty Course Evaluations close is observed.
On Tuesday, 07 May, 2024, during the Spring 2024 (S24) semester, Make-Up Final Examinations is observed.
On Wednesday, 08 May, 2024, during the Spring 2024 (S24) semester, Graduating Final Grades Due by 4 pm is observed.
On Sunday, 12 May, 2024, during the Spring 2024 (S24) semester, Commencement is observed.
On Tuesday, 14 May, 2024, during the Spring 2024 (S24) semester, Final Grades Due by 4 pm is observed.
On Tuesday, 21 May, 2023, during the Spring 2024 (S24) semester, Spring Deans' Lists Posted is observed.
On Monday, 13 May, 2024, during the Summer One 2024 (M24) semester, Semester & Mini-5 Classes Begin is observed.
On Friday, 17 May, 2024, during the Summer One 2024 (M24) semester, Mini-5 add, audit, & tuition adjustment drop deadline (1) is observed.
On Friday, 24 May, 2024, during the Summer One 2024 (M24) semester, Semester add, audit, & tuition adjustment drop deadline (1) is observed.
On Monday, 27 May, 2024, during the Summer One 2024 (M24) semester, Memorial Day is observed, leading to University Closed & No Classes.
On Tuesday, 28 May, 2024, during the Summer One 2024 (M24) semester, Mini-5 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 07 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 pass/no pass & withdrawal deadline (3) is observed.
Answer: "
"What is the structure attached to a buggy that a person pushes to propel it forward?
","['cmubuggy_d403.txt', 'Apr-12_Eventno_40_HistoryofBuggy.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the structure attached to a buggy that a person pushes to propel it forward?

Context: An mural celebrating Buggy can be found in CMU's Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions. Sweepstakes Slang Buggy: Vehicle being raced and also a nickname for the competition. Chute: A section of the freeroll portion of the buggy course (near the southwestern end of Frew Street at its intersection with Schenley Drive) where buggies make the sharp righthand turn from Schenley Drive onto Frew Street. Chute Flagger: Team member who provides a signal for buggy drivers to know when to start the right-hand turn from Schenley Drive onto Frew Street. Driver: Person who travels with a buggy and controls the vehicles via steering and braking systems. Pushbar: Structure attached to a buggy that a person pushes to propel that buggy forward. Pusher: Person who propels a buggy via a pushbar along one of the five hills of the buggy course. Shell: Entire outer structure or covering of a buggy that determines that buggy's aerodynamic characteristics. Transition: Procedure whereby one pusher finishes pushing a buggy and the next pusher in sequence starts to push that same buggy. Members of Fringe, celebrating their 50th year of Buggy racing, allowed a rare visit into its shop for a behind-the-scenes tour. This year Fringe is planning to roll four different vehicles, built and maintained in the Fringe workshop in the basement of the East Campus Garage, known as the ""Froom."" ""We're used to saying everything with 'fr' in front of it but when we say something in front of other people, it gets them confused,"" said Fringe head mechanic Dave Singh, who will graduate in May with a bachelor's degree in mechanical engineering and biomedical engineering. Fringe vehicles often are named with the letter ""B,"" like Boson, Blueshift, Bissa and Bumper. Other teams, such as Apex, often use names that connotate fire, while the SDC (Student Dormitory Committee) team, uses names such as Vice, Bane, Avarice and Malice. This year's buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It's one of the few times spectators can see buggies up close.
Event: History of Buggy
Date: 4/12/24
Time: 4:00 PM-5:00 PM ET
Participants/Audience: Open to entire CMU community 
Event Details: 
Learn about the origin and development of the Sweepstakes races and what students are doing to push the boundaries of the sport in new directions. 

Note: Registration required. Walk-ins are welcome as space permits. No event fee. This event is open to the entire CMU community and their guests.
Answer: "
"If you applied to both the MIIS and MSAII programs on the day before the deadline, how much would it cost?
","['miis-handbook_2023-2024.txt', 'miis-handbook_2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: If you applied to both the MIIS and MSAII programs on the day before the deadline, how much would it cost?

Context: Typically, the student will provide the Program Director with the syllabus of the 
external course, and the Program Director will use that and the student’s transcript to make 
the decision.  
Satisfying a breadth requirement with a course from another institution does not reduce the 
number of CMU course units that must be taken to attain the MIIS degree.  
See the section on “Definition of transfer credit versus course exemption” on page 25. 
6.1.6 Transferring into the MIIS Program 
Transfers into the MIIS program are not permitted during a student’s first semester at CMU.  
Students must begin their study at CMU in the program that admitted them. 
Students may request to transfer into the MIIS program after completing their first semester 
and before the add/drop deadline of the spring semester.  The student must make the request 
in writing (or email) to the MIIS Program Director.  The MIIS Program Director will inform 
the student about what application materials are required, for example, an explanation of why 
a transfer is desired, a proposed plan of study, a proposed advisor, and CMU transcripts.  
Students that are already enrolled in an LTI degree program are not required to retake GRE 
and TOEFL/IELTS/Duolingo exams or to produce new transcripts from other universities.   
The MIIS program will conduct an expedited admissions process after receiving such a 
request.  The program will explicitly consider whether the coursework done prior to the 
transfer will allow the student to be “on schedule” by the end of the spring semester, so that 
the student can participate in a capstone project with other MIIS students during the next fall 
semester. 
MIIS students are allowed to switch tracks (MIIS-16 to MIIS-21 and vice versa) after 
beginning their first semester at CMU. Ideally, all switch requests must be filed within the 
first semester of the program. It is highly recommended to not switch tracks more than 
once! Though not forbidden, multiple track switching may negatively impact immigration 
records of international students. Students also must understand that extension of their stay 
in the program will lead to additional financial obligations.  
U.S. citizens, virtually, have no restrictions on how many times they can switch if it is done for 
a legitimate reason.
The course includes several administrative tasks, 
training, and educational sessions. The students will be enrolled into the course by the 
program administrator in early August. All MIIS students are expected to complete all 
assignments by the end of their first semester in the program. Each assignment has its own due 
date.  
4.12 Summary of Graduate Student Appeal and Grievance Procedures 
Graduate students will find the Summary of Graduate Student Appeal and Grievance 
Procedures on the Graduate Education Resource webpage. This document summarizes 
MIIS Graduate Student Handbook 
Page 20 
 
processes available to graduate students who seek review of academic and non-academic issues. 
Generally, graduate students are expected to seek informal resolution of all concerns within the 
applicable department, unit, or program before invoking formal processes. When an informal 
resolution cannot be reached, however, a graduate student who seeks further review of the 
matter is to follow the formal procedures outlined here. These appeal and grievance procedures 
shall apply to students in all graduate programs of the University. Students should refer to the 
department specific information in this handbook for department and college information about 
the administration and academic policies of the program. 
Summary of Graduate Student Appeal and Grievance Procedures - Office of Graduate and 
Postdoctoral Affairs - Carnegie Mellon University (cmu.edu) 
4.13 Intellectual Property Policy 
The MIIS degree program adheres to Carnegie Mellon University policy on intellectual 
property: http://www.cmu.edu/policies/documents/IntellProp.html 
5 Grading and Evaluation 
5.1 Grading Scale/System 
Students must demonstrate their mastery of material taught in courses and their success in 
applying their skills in directed study and capstone projects by satisfying the following grade 
requirements: 
1. Minimum grade:   A student must obtain a B- or better grade in all courses, which count 
towards core requirements. If a student receives a C or better, that course may count as an 
elective towards the degree requirements. 
2. Minimum QPA:  A student must maintain an average QPA of at least 3.0 in courses and 
projects used to satisfy degree requirements. 
3. Pass/fail:  Pass/fail grades are not permitted for courses and projects used to satisfy a degree 
requirement.
Answer: "
"How many papers does Alexander Hauptmann have on Semantic Scholar?
","['alexander hauptmann_papers.txt', 'alexander hauptmann_376f494126d1ea4f571ea0263c43ac2b6331800a_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many papers does Alexander Hauptmann have on Semantic Scholar?

Context: List of 2023 Open Access papers by alexander hauptmann are:
Towards Open-Domain Twitter User Profile Inference
Zero-Shot and Few-Shot Stance Detection on Varied Topics via Conditional Generation
SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs
STMT: A Spatial-Temporal Mesh Transformer for MoCap-Based Action Recognition
DocumentNet: Bridging the Data Gap in Document Pre-training
Language Model Beats Diffusion - Tokenizer is Key to Visual Generation
Hyperbolic vs Euclidean Embeddings in Few-Shot Learning: Two Sides of the Same Coin
Faculty Name: alexander hauptmann
Paperid: 376f494126d1ea4f571ea0263c43ac2b6331800a
Title: SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs
Year: 2023
Abstract: In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.
Authors: Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A. Ross, Irfan Essa, Yonatan Bisk, Ming Yang, K. Murphy, A. Hauptmann, Lu Jiang
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.'}
Url: http://arxiv.org/pdf/2306.17842
Answer: "
"In spring 2025, When do Mini-3 course drop and withdrawal grade assignment occur?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2025, When do Mini-3 course drop and withdrawal grade assignment occur?

Context: On Wednesday, 05 February, 2025, during the Spring 2025 (S25) semester, Mini-3 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 17 February, 2025, during the Spring 2025 (S25) semester, Mini-3 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 17 February, 2025, during the Spring 2025 (S25) semester, Mini-3 Faculty Course Evaluations open is observed.
On Monday, 24 February, 2025, during the Spring 2025 (S25) semester, Semester course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 28 February, 2025, during the Spring 2025 (S25) semester, Mini-3 Last Day of Classes is observed.
On Friday, 28 February, 2025, during the Spring 2025 (S25) semester, Mini-3 voucher deadline (4) is observed.
On Saturday, 01 March, 2025, during the Spring 2025 (S25) semester, Mini-3 Exams is observed.
On Saturday, 01 March, 2025, during the Spring 2025 (S25) semester, Mini-3 Faculty Course Evaluations close is observed.
From 03 March, 2025,Monday to 07 March, 2023,Friday marks the Spring Break for Spring 2025 (S25) semester which leads to No Classes.
On Monday, 10 March, 2025, during the Spring 2025 (S25) semester, First day of Mini-4 Classes is observed.
On Tuesday, 11 March, 2025, during the Spring 2025 (S25) semester, Summer 2025 Registration Opens is observed.
On Wednesday, 12 March, 2025, during the Spring 2025 (S25) semester, Mid-Semester & Mini-3 grades due by 4 pm is observed.
On Friday, 14 March, 2025, during the Spring 2025 (S25) semester, Mini-4 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 31 March, 2025, during the Spring 2025 (S25) semester, Semester pass/no pass & withdrawal deadline is observed.
On Monday, 22 January, 2024, during the Spring 2024 (S24) semester, Mini-3 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 29 January, 2024, during the Spring 2024 (S24) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Wednesday, 07 February, 2024, during the Spring 2024 (S24) semester, Mini-3 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 19 February, 2024, during the Spring 2024 (S24) semester, Mini-3 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 19 February, 2024, during the Spring 2024 (S24) semester, Mini-3 Faculty Course Evaluations open is observed.
On Monday, 26 February, 2024, during the Spring 2024 (S24) semester, Semester course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 01 March, 2024, during the Spring 2024 (S24) semester, Mini-3 Last Day of Classes is observed.
On Friday, 01 March, 2024, during the Spring 2024 (S24) semester, Mini-3 voucher deadline (4) is observed.
On Saturday, 02 March, 2024, during the Spring 2024 (S24) semester, Mini-3 Exams is observed.
On Saturday, 02 March, 2024, during the Spring 2024 (S24) semester, Mini-3 Faculty Course Evaluations close is observed.
From 04 March, 2024,Monday to 08 March, 2024,Friday marks the Spring Break for Spring 2024 (S24) semester which leads to No Classes.
On Monday, 11 March, 2024, during the Spring 2024 (S24) semester, First day of Mini-4 Classes is observed.
On Monday, 11 March, 2024, during the Spring 2024 (S24) semester, Mid-Semester & Mini-3 grades due by 4 pm is observed.
On Tuesday, 12 March, 2024, during the Spring 2024 (S24) semester, Summer 2024 Registration Opens is observed.
Answer: "
"How many Academy Awards have alumni and current/former faculty won so far?
","['fact_sheet_d407.txt', 'fact_sheet_d407.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many Academy Awards have alumni and current/former faculty won so far?

Context: :-)
LEADERSHIP
Farnam Jahanian  
President
David Coulter 
Chair, Board of Trustees
James H. Garrett Jr. 
Provost
2 National Academy of Engineering       3 National Academy of Sciences        4 National Academy of Medicine
Won by alumni and current/former faculty
The leading university center 
for cybersecurity, providing 
support to more than 110 
centers around the world 
The world’s first university 
robotics department,  
founded in 1979
Alumnus Andy Warhol  
(CFA1949), pop artist pioneer  
and cultural icon
One of only 29 universities 
invited to be a member of the 
World Economic Forum’s 
Global University  
Leaders Forum
CMU.EDU
ECONOMIC IMPACT
• Attracting major companies —  
including Google, Intel, Uber and GE —  
to locate operations and create new  
jobs in Pittsburgh
• To date, the CMU community has 
launched more than 400 startups 
and created more than 152 spinoff 
companies.
• Contributing to the cultural and civic life of  
the city with performances, exhibitions 
and research collaborations
13
ACADEMY  
AWARDS
65
MEMBERS  
OF NAE2
146
EMMY 
AWARDS
20
MEMBERS 
OF NAS3
6
MEMBERS 
OF NAM4
13
TURING 
AWARDS
20
NOBEL 
LAUREATES
58
TONY 
AWARDS
May 2023
Carnegie Mellon University has been a birthplace 
of innovation since its founding in 1900.
Today, CMU is a global leader bringing groundbreaking 
ideas to market and creating successful startup 
businesses. Our award-winning faculty are renowned for 
working closely with students to solve major scientific, 
technological and societal challenges. We put a strong 
emphasis on creating things — from art to robots. We 
have become a model for economic development in 
forming partnerships with companies such as Uber, 
Google and Disney. Our students are recruited by some 
of the world’s most innovative companies.
MANAGEMENT 
INFORMATION 
SYSTEMS
U.S. News & World Report, 2022
#1
ARTIFICIAL 
INTELLIGENCE
U.S. News & World Report, 2022
#1
AMONG U.S. 
UNIVERSITIES
Times Higher Education  
World University 
Ranking, 2023
#19
SOFTWARE 
ENGINEERING
U.S. News & World Report, 2022
COLLEGE OF 
ENGINEERING
U.S. News & World Report, 2022
#1
#4
#1
SCHOOL OF 
COMPUTER 
SCIENCE
U.S. News & World Report, 2022
UNIVERSITY  
IN THE WORLD
Times Higher Education  
World University  
Ranking, 2023
#28
TIME-BASED/ 
NEW MEDIA
U.S. News & World Report, 2019
#1
1,529  
FACULTY
16,779 
STUDENTS
Graduate
Undergraduate
117,257 
ALUMNI (LIVING)
85%  U.S.
15%  International
35%  U.S. 
65%  International
76%  U.S. 
24%  International
GLOBAL COMMUNITY
Students representing
126 countries
Faculty representing
56 countries
Alumni representing 
148 countries
OF COMPUTER 
SCIENCE’S FIRST-
YEAR STUDENTS  
WERE WOMEN  
IN 2019
Nearly triple the  
national average
“ My Heart is in the Work.” 
 
Andrew Carnegie, Founder 
 
November 15, 1900
49.8%
87%  U.S.
Answer: "
"How many languages does GlobalBench currently cover?
","['graham neubig_17605c43ca3eb982c99642052ddc21a93d116594_metadata.txt', 'graham neubig_17605c43ca3eb982c99642052ddc21a93d116594_content_2.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many languages does GlobalBench currently cover?

Context: Faculty Name: graham neubig
Paperid: 17605c43ca3eb982c99642052ddc21a93d116594
Title: GlobalBench: A Benchmark for Global Progress in Natural Language Processing
Year: 2023
Abstract: Despite the major advances in NLP, significant disparities in NLP system performance across languages still exist. Arguably, these are due to uneven resource allocation and sub-optimal incentives to work on less resourced languages. To track and further incentivize the global development of equitable language technology, we introduce GlobalBench. Prior multilingual benchmarks are static and have focused on a limited number of tasks and languages. In contrast, GlobalBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring accuracy, GlobalBench also tracks the estimated per-speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served languages are the ones with a relatively high population, but nonetheless overlooked by composite multilingual benchmarks (like Punjabi, Portuguese, and Wu Chinese). Currently, GlobalBench covers 966 datasets in 190 languages, and has 1,128 system submissions spanning 62 languages.
Authors: Yueqi Song, Catherine Cui, Simran Khanuja, Pengfei Liu, FAHIM FAISAL, Alissa Ostapenko, Genta Indra Winata, Alham Fikri Aji, Samuel Cahyawijaya, Yulia Tsvetkov, Antonios Anastasopoulos, Graham Neubig
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work introduces GlobalBench, an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages and tracks the estimated per-speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world.'}
Url: http://arxiv.org/pdf/2305.14716
Title: GlobalBench: A Benchmark for Global Progress in Natural Language Processing
Authors: Yueqi Song, Catherine Cui, Simran Khanuja, Pengfei Liu, Fahim Faisal, Alissa Ostapenko, Genta Indra Winata, Alham Fikri Aji, Samuel Cahyawijaya, Yulia Tsvetkov, Antonios Anastasopoulos, Graham Neubig
Section: 2 GlobalBench Design Principles
when there is support for only one language, G = 1. Formally, if performance of a language for a task is yi, (i = 1 ... n where n is the total number of languages), and is indexed in non-decreasing order (yi ≤ yi+1), then the Gini coefficient (G) can be calculated as: G = 1 n ( n + 1− 2 ∑n i=1(n + 1− i)yi∑n i=1 yi ) (4) For each task, we obtain equity values, calculated using the maximum performance of submitted systems for each language in a task. For languages that are not supported by any dataset, we assume the system performance to be zero. The global equity values for each task are in Table 2. Apart from the above, we keep track of system performances (F1/Accuracy/BLEU etc.). For each task, we take the system output with the highest performance among all system outputs with the same language, and provide a ranking of languages with the highest system performances. We also maintain a ranking of most under-served languages (sorted based on utilities), for reasons detailed below. 2.3 Incentivization: Reward Improvement We can estimate current global progress in language technologies using the demographic and lin- guistic weighted global averages, and the global equity values. In GlobalBench, we also encourage development of systems that improve upon these metrics. We accomplish this in two ways: First, we identify areas with the greatest potential for improvement, i.e., we identify the most under-served languages. We choose a parameter of τ = 0.4 to better strike a balance between demographic- and linguistic-weighted utility. Languages farthest from the ideal τ -weighted utility are expected to be most under-served. Hence, (1− τ -weighted utility) of a language gives us this measure.
Answer: "
"What does TASTE use to better characterize user behaviors?
","['chenyan xiong_159100c8323fc558e4073a3a006f3f243aca3a60_metadata.txt', 'chenyan xiong_159100c8323fc558e4073a3a006f3f243aca3a60_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What does TASTE use to better characterize user behaviors?

Context: Faculty Name: chenyan xiong
Paperid: 159100c8323fc558e4073a3a006f3f243aca3a60
Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases
Year: 2023
Abstract: This paper proposes Text mAtching based SequenTial rEcommenda-tion model (TASTE), which maps items and users in an embedding space and recommends items by matching their text representations. TASTE verbalizes items and user-item interactions using identifiers and attributes of items. To better characterize user behaviors, TASTE additionally proposes an attention sparsity method, which enables TASTE to model longer user-item interactions by reducing the self-attention computations during encoding. Our experiments show that TASTE outperforms the state-of-the-art methods on widely used sequential recommendation datasets. TASTE alleviates the cold start problem by representing long-tail items using full-text modeling and bringing the benefits of pretrained language models to recommendation systems. Our further analyses illustrate that TASTE significantly improves the recommendation accuracy by reducing the popularity bias of previous item id based recommendation models and returning more appropriate and text-relevant items to satisfy users. All codes are available at https://github.com/OpenMatch/TASTE.
Authors: Zhenghao Liu, Senkun Mei, Chenyan Xiong, Xiaohua Li, Shi Yu, Zhiyuan Liu, Yu Gu, Ge Yu
Venue: International Conference on Information and Knowledge Management
Tldr: {'model': 'tldr@v2.0.0', 'text': None}
Url: https://arxiv.org/pdf/2308.14029
Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases
Authors: Zhenghao Liu, Sen Mei, Chenyan Xiong, Xiaohua Li, Shi Yu, Zhiyuan Liu, Yu Gu, Ge Yu
Section: 5.3 Effectiveness of Item Verbalization Methods
more to memorize some characteristics of users, such as the taste, preferred cuisines, and active area of users. Besides, the prompt-based modeling method (Prompt) outperforms the embedding-based method (Embed), which illustrates that pretrained language models have the ability to understand the user/item identifiers and establish relevance between users and items via identifiers. It further supports the motivation of TASTE, which fully uses the learned knowledge from pretrained language models to build sequential recommendation systems. Evaluation on Recommendation Behaviors. Finally, we explore the recommendation behaviors of TASTE using different item modeling methods. As shown in Table 7, three models, T5-ID, TASTE w/o ID, and TASTE, are compared. T5-ID randomly initializes item embeddings and directly predicts the item ids. TASTE w/o ID and TASTE employ a two-tower architecture [29] and encode items using attributes and identifiers & attributes, respectively. As shown in our evaluation results, T5-ID returns an average of 49.5% of popular products in the recommendation results of all datasets, showing that it faces the popularity bias problem during recommending items. TASTE alleviates the popularity bias by reducing on average 18.75% popular items in its recommendation results. It represents items using full texts and utilizes text matching to calibrate the popularity-oriented recommendation behavior of T5ID. TASTE demonstrates its effectiveness in recommending more appropriate and text-relevant items by achieving higher Bleu scores and Recall scores. Besides, compared with TASTE w/o ID, TASTE achieves higher Bleu and Dist scores with the help of item ids. It shows that the item ids can serve as a kind of prompt to provide additional matching signals beyond item attributes to better model the relevance between users and items.
Answer: "
"In the Value Kaleidescope paper by Maarten Sap's group, what is the name of the dataset that is introduced?
","['maarten sap_d655f652d02251b45db43181c5e3c73dfc59cd51_metadata.txt', 'maarten sap_d655f652d02251b45db43181c5e3c73dfc59cd51_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the Value Kaleidescope paper by Maarten Sap's group, what is the name of the dataset that is introduced?

Context: Faculty Name: maarten sap
Paperid: d655f652d02251b45db43181c5e3c73dfc59cd51
Title: Value Kaleidoscope: Engaging AI with Pluralistic Human Values, Rights, and Duties
Year: 2023
Abstract: Human values are crucial to human decision-making. Value pluralism is the view that multiple correct values may be held in tension with one another (e.g., when considering lying to a friend to protect their feelings, how does one balance honesty with friendship?). As statistical learners, AI systems fit to averages by default, washing out these potentially irreducible value conflicts. To improve AI systems to better reflect value pluralism, the first-order challenge is to explore the extent to which AI systems can model pluralistic human values, rights, and duties as well as their interaction. We introduce ValuePrism, a large-scale dataset of 218k values, rights, and duties connected to 31k human-written situations. ValuePrism's contextualized values are generated by GPT-4 and deemed high-quality by human annotators 91% of the time. We conduct a large-scale study with annotators across diverse social and demographic backgrounds to try to understand whose values are represented. With ValuePrism, we build Kaleido, an open, light-weight, and structured language-based multi-task model that generates, explains, and assesses the relevance and valence (i.e., support or oppose) of human values, rights, and duties within a specific context. Humans prefer the sets of values output by our system over the teacher GPT-4, finding them more accurate and with broader coverage. In addition, we demonstrate that Kaleido can help explain variability in human decision-making by outputting contrasting values. Finally, we show that Kaleido's representations transfer to other philosophical frameworks and datasets, confirming the benefit of an explicit, modular, and interpretable approach to value pluralism. We hope that our work will serve as a step to making more explicit the implicit values behind human decision-making and to steering AI systems to make decisions that are more in accordance with them.
Title: VALUE KALEIDOSCOPE : Engaging AI with Pluralistic Human Values, Rights, and Duties
Authors: Taylor Sorensen, Liwei Jiang, Jena D. Hwang, Valentina Pyatkin, Peter West, Nouha Dziri, Ximing Lu, Kavel Rao, Chandra Bhagavatula, Maarten Sap, John Tasioulas, Yejin Choi
Section: N.3 Data Collection Process
Appendix M.1. using Flan-T5 (Chung et al. 2022). We take 95% of our situations determinnistically from those that have less toxic/NSFW/explicit content, and sample the other 5% uniformly from the rest of the data so as to include the entire spectrum of inputs. We find that this succeeds in increasing the diversity of the dataset, as measured by unique n-grams divided by the length of the dataset (dist-2: .23→.36, dist-3: .54→.67).
Answer: "
"HomeRobot OVMM benchmarks include two components or environments. What are they?
","['yonatan bisk_3b0c02955e88f5862e61b560c7f70ba8cf235b1d_metadata.txt', 'yonatan bisk_3b0c02955e88f5862e61b560c7f70ba8cf235b1d_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: HomeRobot OVMM benchmarks include two components or environments. What are they?

Context: Faculty Name: yonatan bisk
Paperid: 3b0c02955e88f5862e61b560c7f70ba8cf235b1d
Title: HomeRobot: Open-Vocabulary Mobile Manipulation
Year: 2023
Abstract: HomeRobot (noun): An affordable compliant robot that navigates homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future research work improve performance. See videos on our website: https://ovmm.github.io/.
We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future research work improve performance. See videos on our website: https://ovmm.github.io/.
Authors: Sriram Yenamandra, A. Ramachandran, Karmesh Yadav, Austin S. Wang, Mukul Khanna, Théophile Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander Clegg, John Turner, Z. Kira, M. Savva, Angel X. Chang, Devendra Singh Chaplot, Dhruv Batra, Roozbeh Mottaghi, Yonatan Bisk, Chris Paxton
Venue: Conference on Robot Learning
Tldr: {'model': 'tldr@v2.0.0', 'text': 'The HomeRobot OVMM benchmark is introduced, where an agent navigates household environments to grasp novel objects and place them on target receptacles, and baselines achieve a 20% success rate in the real world; the experiments identify ways future research work improve performance.'}
Url: http://arxiv.org/pdf/2306.11565
Answer: "
"In spring 2024, Who are the instructors for course 15122?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, Who are the instructors for course 15122?

Context: In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Imperative Computation' with Course ID 15122 and Section NA offers 12.0 units. The Class meets Friday between 05:00PM and 05:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor NA located in Building GHC, Room 4211.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Imperative Computation' with Course ID 15122 and Section R offers 12.0 units. The Class meets Monday between 05:00PM and 05:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Cervesato, Kohlbrenner located in Building GHC, Room CLSTR.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Imperative Computation' with Course ID 15122 and Section NA offers 12.0 units. The Class meets Friday between 05:00PM and 05:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor NA located in Building GHC, Room 4102.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Imperative Computation' with Course ID 15122 and Section W offers 12.0 units. The Class meets Tuesday Thursday between 08:45AM and 09:35AM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Hammoud located in Building CMB, Room 2163.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section Lec 1 offers 12.0 units. The Class meets Tuesday Thursday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building DH, Room 2210.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section Lec 2 offers 12.0 units.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Imperative Computation' with Course ID 15122 and Section NA offers 12.0 units. The Class meets Friday between 05:00PM and 05:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor NA located in Building GHC, Room 4211.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Imperative Computation' with Course ID 15122 and Section R offers 12.0 units. The Class meets Monday between 05:00PM and 05:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Cervesato, Kohlbrenner located in Building GHC, Room CLSTR.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Imperative Computation' with Course ID 15122 and Section NA offers 12.0 units. The Class meets Friday between 05:00PM and 05:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor NA located in Building GHC, Room 4102.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Imperative Computation' with Course ID 15122 and Section W offers 12.0 units. The Class meets Tuesday Thursday between 08:45AM and 09:35AM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Hammoud located in Building CMB, Room 2163.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section Lec 1 offers 12.0 units. The Class meets Tuesday Thursday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building DH, Room 2210.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section Lec 2 offers 12.0 units.
Answer: "
"In fall 2023, What is the deadline for adding, auditing, and tuition adjustment drop for the semester (deadline 1)?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, What is the deadline for adding, auditing, and tuition adjustment drop for the semester (deadline 1)?

Context: On Friday, 25 October, 2024, during the Fall 2024 (F24) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
On Tuesday, 05 November, 2024, during the Fall 2024 (F24) semester, Democracy Day is observed, leading to No Classes, except Evening classes after 5 pm will still meet.
On Monday, 11 November, 2024, during the Fall 2024 (F24) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Wednesday, 13 November, 2024, during the Fall 2024 (F24) semester, Mini-2 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 18 November, 2024,Monday to 22 November, 2024,Friday marks the Spring 2025 Registration Week for Fall 2024 (F24) semester.
On Monday, 25 November, 2024, during the Fall 2024 (F24) semester, Mini-2 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 25 November, 2024, during the Fall 2024 (F24) semester, Semester & Mini-2 Faculty Course Evaluations open is observed.
From 27 November, 2024,Wednesday to 29 November, 2023,Friday marks the Thanksgiving Break for Fall 2024 (F24) semester which leads to No Classes.
On Friday, 06 December, 2024, during the Fall 2024 (F24) semester, Semester & Mini-2 Last Day of Classes is observed.
On Friday, 06 December, 2024, during the Fall 2024 (F24) semester, Semester & Mini-2 voucher deadline (4) is observed.
From 09 December, 2024,Monday to 10 December, 2023,Tuesday marks the Final Exams for Fall 2024 (F24) semester.
On Monday, 11 December, 2024, during the Fall 2024 (F24) semester, Reading Day is observed.
From 12 December, 2024,Thursday to 13 December, 2023,Friday marks the Final Exams for Fall 2024 (F24) semester.
On Monday, 28 August, 2023, during the Fall 2023 (F23) semester, Semester & Mini-1 Classes Begin is observed.
On Friday, 01 September, 2023, during the Fall 2023 (F23) semester, Mini-1 add, audit & tuition adjustment drop deadline  (1) is observed.
On Monday, 04 September, 2023, during the Fall 2023 (F23) semester, Labor Day is observed, leading to No Classes & University Closed.
On Monday, 11 September, 2023, during the Fall 2023 (F23) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Wednesday, 20 September, 2023, during the Fall 2023 (F23) semester, Mini-1 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 02 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Pass/no pass & withdrawal deadline (3) is observed.
On Monday, 02 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Faculty Course Evaluations open is observed.
On Monday, 09 October, 2023, during the Fall 2023 (F23) semester, Semester drop deadline is observed, leading to withdrawal grade assigned after this date.
On Friday, 13 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Last Day of Classes is observed.
On Friday, 13 October, 2023, during the Fall 2023 (F23) semester, Mini-1 voucher election deadline (4) is observed.
From 13 October, 2023,Friday to 14 October, 2023,Saturday marks the Family Weekend for Fall 2023 (F23) semester.
On Saturday, 14 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Exams is observed.
On Saturday, 14 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Faculty Course Evaluations close is observed.
From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
Answer: "
"What is the name of Graham Neubig's lab?
","['graham neubig_cc1705fe421c70d85254b557634bd4669fdd49b0_metadata.txt', 'graham neubig_659be1ff350634f50cc066d258ee6a45e697e552_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the name of Graham Neubig's lab?

Context: Faculty Name: graham neubig
Paperid: cc1705fe421c70d85254b557634bd4669fdd49b0
Title: DataFinder: Scientific Dataset Recommendation from Natural Language Descriptions
Year: 2023
Abstract: Modern machine learning relies on datasets to develop and validate research ideas. Given the growth of publicly available data, finding the right dataset to use is increasingly difficult. Any research question imposes explicit and implicit constraints on how well a given dataset will enable researchers to answer this question, such as dataset size, modality, and domain. We operationalize the task of recommending datasets given a short natural language description of a research idea, to help people find relevant datasets for their needs. Dataset recommendation poses unique challenges as an information retrieval problem; datasets are hard to directly index for search and there are no corpora readily available for this task. To facilitate this task, we build the DataFinder Dataset which consists of a larger automatically-constructed training set (17.5K queries) and a smaller expert-annotated evaluation set (392 queries). Using this data, we compare various information retrieval algorithms on our test set and present a superior bi-encoder retriever for text-based dataset recommendation. This system, trained on the DataFinder Dataset, finds more relevant search results than existing third-party dataset search engines. To encourage progress on dataset recommendation, we release our dataset and models to the public.
Authors: Vijay Viswanathan, Luyu Gao, Tongshuang Sherry Wu, Pengfei Liu, Graham Neubig
Venue: Annual Meeting of the Association for Computational Linguistics
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work operationalizes the task of recommending datasets given a short natural language description of a research idea, to help people find relevant datasets for their needs, and builds the DataFinder Dataset, a larger automatically-constructed training set and a smaller expert-annotated evaluation set.'}
Url: http://arxiv.org/pdf/2305.16636
Faculty Name: graham neubig
Paperid: 659be1ff350634f50cc066d258ee6a45e697e552
Title: SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing
Year: 2023
Abstract: In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.
Authors: Taiqi He, Lindia Tjuatja, Nathaniel R. Robinson, Shinji Watanabe, David R. Mortensen, Graham Neubig, L. Levin
Venue: Special Interest Group on Computational Morphology and Phonology Workshop
Tldr: {'model': 'tldr@v2.0.0', 'text': 'In this submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), approaches to data augmentation and modeling across seven low-resource languages are explored and token classification models are found to be the best performing.'}
Url: https://aclanthology.org/2023.sigmorphon-1.22.pdf
Answer: "
"In fall 2023, What is the deadline for Mini-1 Pass/No Pass and withdrawal?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, What is the deadline for Mini-1 Pass/No Pass and withdrawal?

Context: On Monday, 28 August, 2023, during the Fall 2023 (F23) semester, Semester & Mini-1 Classes Begin is observed.
On Friday, 01 September, 2023, during the Fall 2023 (F23) semester, Mini-1 add, audit & tuition adjustment drop deadline  (1) is observed.
On Monday, 04 September, 2023, during the Fall 2023 (F23) semester, Labor Day is observed, leading to No Classes & University Closed.
On Monday, 11 September, 2023, during the Fall 2023 (F23) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Wednesday, 20 September, 2023, during the Fall 2023 (F23) semester, Mini-1 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 02 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Pass/no pass & withdrawal deadline (3) is observed.
On Monday, 02 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Faculty Course Evaluations open is observed.
On Monday, 09 October, 2023, during the Fall 2023 (F23) semester, Semester drop deadline is observed, leading to withdrawal grade assigned after this date.
On Friday, 13 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Last Day of Classes is observed.
On Friday, 13 October, 2023, during the Fall 2023 (F23) semester, Mini-1 voucher election deadline (4) is observed.
From 13 October, 2023,Friday to 14 October, 2023,Saturday marks the Family Weekend for Fall 2023 (F23) semester.
On Saturday, 14 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Exams is observed.
On Saturday, 14 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Faculty Course Evaluations close is observed.
From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mini-2 Classes Begin is observed.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mid-Semester & Mini-1 grades due by 4 pm is observed.
On Friday, 27 October, 2023, during the Fall 2023 (F23) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
On Tuesday, 07 November, 2023, during the Fall 2023 (F23) semester, Democracy Day is observed, leading to No Classes, except Evening classes after 5 pm will still meet.
On Saturday, 11 November, 2023, during the Fall 2023 (F23) semester, Homecoming is observed.
From 13 November, 2023,Monday to 17 November, 2023,Friday marks the Spring 2024 Registration Week for Fall 2023 (F23) semester.
On Monday, 13 November, 2023, during the Fall 2023 (F23) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Wednesday, 15 November, 2023, during the Fall 2023 (F23) semester, Mini-2 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 22 November, 2023,Wednesday to 24 November, 2023,Friday marks the Thanksgiving Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Mini-2 pass/no pass & withdrawal deadline (2) is observed.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Faculty Course Evaluations open is observed.
On Friday, 08 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Last Day of Classes is observed.
Answer: "
"What country does the Dual-Degree Ph.D. in Language and Information Technologies have a partnership with?
","['program_info_Dual-DegreePhDinLanguageandInformationTechnologies(PortugalPartnership).txt', 'handbook_phd_2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What country does the Dual-Degree Ph.D. in Language and Information Technologies have a partnership with?

Context: Academic Program Name:
Dual-Degree Ph.D. in Language and Information Technologies (Portugal Partnership)

Website:
https://lti.cs.cmu.edu/academics/phd-programs/dual-degree-phd-lti.html

Overview:
The LTI offers a dual-degree Ph.D. in Language and Information Technologies in cooperation with:
Universidade de Aveiro (Ph.D. in Computer Engineering), Universidade do Minho (Ph.D. in Informatics)  and the Universidade do Porto (FCUP, Ph.D. in Computer Science and FEUP, Ph.D. in Computer Science) as part of MAPi;
Universidade de Lisboa, Faculdade de Ciências – FCUL (Ph.D. in Informatics) 
Universidade de Lisboa, Instituto Superior Técnico – IST  (Ph.D. in Computer Science and Engineering, Ph.D. in Electrical and Computer Engineering, Ph.D. in Information Security)
Universidade Nova de Lisboa, Faculdade de Ciências e Tecnologia – FCTUNL (Ph.D. in Computer Science)
Universidade de Coimbra, Faculdade de Ciências e Tecnologia – FCTUC (Ph.D. in Information Science and Technology)
Students jointly enrolled in the LTI Ph.D program spend a year in Portugal, then two years at Carnegie Mellon taking classes in linguistics, computer science, statistical learning and task orientation.
After completing the majority of their academic requirements, students return to Portugal for the next two years to conduct extensive research, ultimately leading to a dissertation topic that will be publicly defended. One adviser from each institution co-supervises their student’s progress and helps to define their final thesis topic.

Requirements:
Students participating in the dual-degree program will spend their first year in Portugal, followed by two years in Pittsburgh to complete their coursework. They will complete a maximum of eight courses with a proper balance of focus areas (linguistics, computer science, statistical/learning and task orientation). After that, they will return to Portugal for their last two years, pursuing research and completing their dissertation. For more, see the Carnegie Mellon | Portugal page.

Curriculum:
While in the dual Ph.D. program, your schedule may look like this.
For information about Carnegie Mellon requirements and 
policies, please see the universitys handbook The Word, the Office of Graduate and Postdoctoral 
Affairs web page, the Office of the Dean of Students web page, and other resources contained in 
Appendix A of this handbook. 
Welcome! We hope that your time here is a life-changing experience. 
1.1 
Degrees Offered 
The Language Technologies Institute offers two Ph.D. programs and four Master degrees. 
 Ph.D. in Language and Information Technologies (LTI Ph.D.) 
 Dual-Degree Ph.D. in Language and Information Technologies (CMU-PT Ph.D.) 
 Masters in Language Technologies (MLT) 
 Master of Science in Intelligent Information Systems (MIIS) 
 Master of Computational Data Science (MCDS) 
 Master of Science in Artificial Intelligence and Innovation (MSAII) 
LTI Ph.D. Graduate Student Handbook 
Page 10 
 
This handbook applies to the LTI Ph.D. 
The Ph.D. in Language and Information Technologies (LTI Ph.D.) is focused on understanding 
and extending the state of the art in computational linguistics, natural language processing, 
dialogue systems, information retrieval, machine translation, speech processing, video 
understanding, multimodal systems, automated reasoning, and other topics related to analysis 
and understanding of unstructured information (e.g., machine learning, and software engineering 
of intelligent systems). 
1.2 
Department Personnel 
The people responsible for administering the LTI Ph.D. degree are: 
Jamie Callan 
Ph.D. Program Director 
Professor 
GHC 5419 
callan@cs.cmu.edu 
412-268-4525 
Stacey Young 
Ph.D.
Answer: "
"Carnegie Mellon University is ranked #1 according to which report in 2022?
","['fact_sheet_d407.txt', 'history_d401.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Carnegie Mellon University is ranked #1 according to which report in 2022?

Context: Carnegie Mellon University has been a birthplace 
of innovation since its founding in 1900.
Today, CMU is a global leader bringing groundbreaking 
ideas to market and creating successful startup 
businesses. Our award-winning faculty are renowned for 
working closely with students to solve major scientific, 
technological and societal challenges. We put a strong 
emphasis on creating things — from art to robots. We 
have become a model for economic development in 
forming partnerships with companies such as Uber, 
Google and Disney. Our students are recruited by some 
of the world’s most innovative companies.
MANAGEMENT 
INFORMATION 
SYSTEMS
U.S. News & World Report, 2022
#1
ARTIFICIAL 
INTELLIGENCE
U.S. News & World Report, 2022
#1
AMONG U.S. 
UNIVERSITIES
Times Higher Education  
World University 
Ranking, 2023
#19
SOFTWARE 
ENGINEERING
U.S. News & World Report, 2022
COLLEGE OF 
ENGINEERING
U.S. News & World Report, 2022
#1
#4
#1
SCHOOL OF 
COMPUTER 
SCIENCE
U.S. News & World Report, 2022
UNIVERSITY  
IN THE WORLD
Times Higher Education  
World University  
Ranking, 2023
#28
TIME-BASED/ 
NEW MEDIA
U.S. News & World Report, 2019
#1
1,529  
FACULTY
16,779 
STUDENTS
Graduate
Undergraduate
117,257 
ALUMNI (LIVING)
85%  U.S.
15%  International
35%  U.S. 
65%  International
76%  U.S. 
24%  International
GLOBAL COMMUNITY
Students representing
126 countries
Faculty representing
56 countries
Alumni representing 
148 countries
OF COMPUTER 
SCIENCE’S FIRST-
YEAR STUDENTS  
WERE WOMEN  
IN 2019
Nearly triple the  
national average
“ My Heart is in the Work.” 
 
Andrew Carnegie, Founder 
 
November 15, 1900
49.8%
87%  U.S.
Drawing on Shaw’s plan and also on the work of other faculty members, an undergraduate program in computer science was created during the 1989-90 academic year. Seven CS majors were admitted to the program as sophomores. Another 73 undergraduates were admitted in 1990–91. By 1995, there were 401 undergraduates in the School of Computer Science; in fall 2013, more than 600 undergraduates made up about 37 percent of student enrollment at SCS, along with more than 600 master’s degree students.New departments, new areas of studyAlong the way, the Center for Machine Translation became the Language Technologies Institute, and other new departments were formed, including the Human-Computer Interaction Institute (1993), the Institute for Software Research (1999), the Machine Learning Department (2006) and the Ray and Stephanie Lane Center for Computational Biology (2009). SCS’s seven degree-granting departments draw faculty and students from a wide variety of disciplines, including engineering, mathematics, social sciences, linguistics and design.Committed to extending our founders’ visionThe School of Computer Science at Carnegie Mellon University enters its second quarter century as a world-leading educational and research institution, embracing all facets of computing. Its graduate programs are consisted ranked with the best in the world by a leading U.S. magazine, while its undergraduate programs are also rated the best in the U.S. by corporate recruiters. In 2013, SCS had 284 faculty members and a total student enrollment of nearly 1,700, including undergraduate, master’s and Ph.D. students, and conducted $124 million in research. Indeed, by itself, the Robotics Institute is the largest university robotics research group in the world, with more than 500 people and more than 100 ongoing research projects. A half-century ago, Perlis, Simon and Newell outlined a vision for computer science. The School of Computer Science at CMU remains committed to continuing and extending their vision in the context of big data and connected computing in the 21st century.
Answer: "
"In summer 2024, When do Mini-5 Faculty Course Evaluations open?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In summer 2024, When do Mini-5 Faculty Course Evaluations open?

Context: On Friday, 07 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 pass/no pass & withdrawal deadline (3) is observed.
On Friday, 14 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Faculty Course Evaluations open is observed.
On Tuesday, 18 June, 2024, during the Summer One 2024 (M24) semester, Semester course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Wednesday, 19 June, 2022, during the Summer One 2024 (M24) semester, Juneteenth is observed, leading to University Closed & No Classes.
On Thursday, 20 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Last Day of Classes is observed.
On Thursday, 20 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 voucher deadline (4) is observed.
On Friday, 21 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Final Exams is observed.
On Friday, 21 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Faculty Course Evaluations close is observed.
On Monday, 24 June, 2024, during the Summer One 2024 (M24) semester, Mini-6 First Day of Classes is observed.
On Tuesday, 25 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Final Grades Due by 4 pm is observed.
On Friday, 28 June, 2024, during the Summer One 2024 (M24) semester, Mini-6 add, audit, & tuition adjustment drop deadline (1) is observed.
On Thursday, 04 July, 2024, during the Summer One 2024 (M24) semester, Independence Day is observed, leading to University Closed & No Classes.
On Friday, 05 July, 2024, during the Summer One 2024 (M24) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Friday, 13 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Faculty Course Evaluations open is observed.
On Tuesday, 17 June, 2025, during the Summer 2025 (M25) semester, Summer All course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Wednesday, 18 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Last Day of Classes is observed.
On Wednesday, 18 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 voucher deadline is observed.
On Thursday, 19 June, 2025, during the Summer 2025 (M25) semester, Juneteenth is observed, leading to University Closed & No Classes.
On Friday, 20 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Final Exams is observed.
On Friday, 20 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Faculty Course Evaluations close is observed.
On Monday, 23 June, 2025, during the Summer 2025 (M25) semester, Summer Two & Mini-6 First Day of Classes is observed.
On Tuesday, 24 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Final Grades Due by 4 pm is observed.
On Friday, 27 June, 2025, during the Summer 2025 (M25) semester, Summer Two & Mini-6 add, audit, & tuition adjustment drop deadline (1) is observed.
On Thursday, 03 July, 2025, during the Summer 2025 (M25) semester, Summer All pass/no pass & withdrawal grade deadline (3) is observed.
On Friday, 04 July, 2025, during the Summer 2025 (M25) semester, Independence Day is observed, leading to University Closed & No Classes.
On Monday, 07 July, 2025, during the Summer 2025 (M25) semester, Summer Two & Mini-6 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
Answer: "
"What does Self-Refine use to provide feedback and refine the initial output?
","['bhiksha raj_0a8d38686b18f28aae1222529e6b9e8a60cab1c2_metadata.txt', 'sean welleck_papers.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What does Self-Refine use to provide feedback and refine the initial output?

Context: Faculty Name: bhiksha raj
Paperid: 0a8d38686b18f28aae1222529e6b9e8a60cab1c2
Title: UTOPIA: Unconstrained Tracking Objects without Preliminary Examination via Cross-Domain Adaptation
Year: 2023
Abstract: Multiple Object Tracking (MOT) aims to find bounding boxes and identities of targeted objects in consecutive video frames. While fully-supervised MOT methods have achieved high accuracy on existing datasets, they cannot generalize well on a newly obtained dataset or a new unseen domain. In this work, we first address the MOT problem from the cross-domain point of view, imitating the process of new data acquisition in practice. Then, a new cross-domain MOT adaptation from existing datasets is proposed without any pre-defined human knowledge in understanding and modeling objects. It can also learn and update itself from the target data feedback. The intensive experiments are designed on four challenging settings, including MOTSynth to MOT17, MOT17 to MOT20, MOT17 to VisDrone, and MOT17 to DanceTrack. We then prove the adaptability of the proposed self-supervised learning strategy. The experiments also show superior performance on tracking metrics MOTA and IDF1, compared to fully supervised, unsupervised, and self-supervised state-of-the-art methods.
Authors: Pha Nguyen, Kha Gia Quach, J. Gauch, S. Khan, B. Raj, Khoa Luu
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A new cross-domain MOT adaptation from existing datasets is proposed without any pre-defined human knowledge in understanding and modeling objects, and it can also learn and update itself from the target data feedback.'}
Url: http://arxiv.org/pdf/2306.09613
List of 2023 Open Access papers by sean welleck are:
Self-Refine: Iterative Refinement with Self-Feedback
Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning
Answer: "
"In November 2006, who chaired the Mascot Identity Task Force? 
","['mascot_d405.txt', 'tartanfacts_d404.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In November 2006, who chaired the Mascot Identity Task Force? 

Context: About Scotty
The Scottish terrier has long been a familiar figure around Carnegie Mellon's campus. For years students have suited up in an unofficial Scottish terrier costume to excite the fans at athletic events. But the relationship between the Scottish terrier breed and Carnegie Mellon far precedes anybody doing somersaults in a dog costume. Andrew Carnegie, founder of the university, kept a Scottish terrier as his pet.
Scotty's road from popular icon to official mascot of the university began in 2006. Carnegie Mellon formed a Mascot Identity Task Force in November 2006, which consisted of students, faculty, staff and alumni. The Task Force was co-chaired by Director of Athletics Susan Bassett and Dean of Student Affairs Jennifer Church.
The mascot selection process included a series of surveys and a university Town Hall meeting. Nearly 78 percent of 2,370 students surveyed in February 2007 voted for the Scottish terrier, and approximately 25 percent of 400 alumni surveyed thought the Scottish terrier was already the mascot.
In the spring, the Task Force partnered with SME Branding — a firm with more than 17 years of experience creating mascots for professional sports teams and universities — to develop the graphics for the mascot. During October, students and alumni reviewed potential mascot images in focus groups.
Carnegie Mellon's official mascot debuted at the Nov. 10, 2007 home football game. The graphic features a profile of a distinguished, bold Scottish terrier sporting a plaid scarf around his neck. The dog is contained in a shield, representing Carnegie Mellon's Scottish heritage.
The Task Force then partnered with a mascot costume company to design our Scottish terrier in the winter of 2007. The official Scotty costume was unveiled at the 2008 Spring Carnival.
The
Scottish terrier breed
is known for its keen, alert and intelligent expression. Its temperament is described as determined and thoughtful while its physical aspects exemplify strength, power and agility in a small package. Many of these traits are also apparent throughout the university, making the Scottish terrier a natural choice for Carnegie Mellon's mascot.
Fun Fact
In keeping with tradition, students, faculty, staff and alumni voted to name the new official mascot Scotty. But it's not just the costumed mascot that voters named. The live dog is also known as Scotty to the Carnegie Mellon community.
About Scotty
|
Celebrating Scotty
|
Performer Requests
Department Quick Facts
Name of School: Carnegie Mellon University
City/Zip: Pittsburgh, PA 15213
Founded: 1900
Enrollment (undergrad): 6,982- Fall 2021
Enrollment (graduate and doctoral): 7,062 - Fall 2021
Alumni: 102,577
Nickname: Tartans
Mascot: Scottie Dog
School Colors: Cardinal and Gray
Football Stadium and Track and Field: Gesling Stadium
Capacity: 3,500
Surface: FieldTurf
Basketball/Volleyball Gym: Wiegand Gymnasium
Capacity: 500
Soccer Stadium: CMU Soccer Field
Capacity: 250
Surface: FieldTurf
Affiliation: NCAA Division III
Conference: University Athletic Association | Presidents' Athletic Conference (Football Only)
President: Dr. Farnam Jahanian
Alma Mater, Year: University of Texas at Austin
Athletic Director: Dr. Josh Centor
Alma Mater, Year: Brandeis, 2004
Athletic Department Phone: 412-268-8054
Mailing Address:  5000 Forbes Avenue / Pittsburgh, PA 15213
Answer: "
"How much decrease in memory consumption (single GPU setup) does SAMA showcase in large-scale meta learning benchmarks?
","['emma strubell_a815c3209e7baff4466dbf6e129129511f842b7e_metadata.txt', 'eric xing_a815c3209e7baff4466dbf6e129129511f842b7e_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How much decrease in memory consumption (single GPU setup) does SAMA showcase in large-scale meta learning benchmarks?

Context: Faculty Name: emma strubell
Paperid: a815c3209e7baff4466dbf6e129129511f842b7e
Title: Making Scalable Meta Learning Practical
Year: 2023
Abstract: Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and 2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains.
Authors: Sang Keun Choe, Sanket Vaibhav Mehta, Hwijeen Ahn, W. Neiswanger, Pengtao Xie, Emma Strubell, Eric P. Xing
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients.'}
Url: https://arxiv.org/pdf/2310.05674
Faculty Name: eric xing
Paperid: a815c3209e7baff4466dbf6e129129511f842b7e
Title: Making Scalable Meta Learning Practical
Year: 2023
Abstract: Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and 2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains.
Authors: Sang Keun Choe, Sanket Vaibhav Mehta, Hwijeen Ahn, W. Neiswanger, Pengtao Xie, Emma Strubell, Eric P. Xing
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients.'}
Url: https://arxiv.org/pdf/2310.05674
Answer: "
"What percentage of XLS-R’s performance can a vanilla HuBERT Base model maintain with only $3 \%$ of the data, 4 GPUs, and limited trials?
","['shinji watanabe_01a819f7155bb87c32f1e4c13d9439c080e6aa97_metadata.txt', 'shinji watanabe_1028bf42a4c792acefd3be9da45e58f2b1620fe3_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What percentage of XLS-R’s performance can a vanilla HuBERT Base model maintain with only $3 \%$ of the data, 4 GPUs, and limited trials?

Context: Faculty Name: shinji watanabe
Paperid: 01a819f7155bb87c32f1e4c13d9439c080e6aa97
Title: Joint Prediction and Denoising for Large-Scale Multilingual Self-Supervised Learning
Year: 2023
Abstract: Multilingual self-supervised learning (SSL) has often lagged behind state-of-the-art (SOTA) methods due to the expenses and complexity required to handle many languages. This further harms the reproducibility of SSL, which is already limited to few research groups due to its resource usage. We show that more powerful techniques can actually lead to more efficient pre-training, opening SSL to more research groups. We propose WavLabLM, which extends WavLM’s joint prediction and denoising to 40k hours of data across 136 languages. To build WavLabLM, we devise a novel multi-stage pre-training method, designed to address the language imbalance of multilingual data. WavLabLM achieves comparable performance to XLS-R on ML-SUPERB with less than $10 \%$ of the training data, making SSL realizable with academic compute. We show that further efficiency can be achieved with a vanilla HuBERT Base model, which can maintain $94 \%$ of XLS-R’s performance with only $3 \%$ of the data, 4 GPUs, and limited trials. We open-source all code and models in ESPnet.
Authors: William Chen, Jiatong Shi, Brian Yan, Dan Berrebbi, Wangyou Zhang, Yifan Peng, Xuankai Chang, Soumi Maiti, Shinji Watanabe
Venue: Automatic Speech Recognition & Understanding
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work proposes WavLabLM, which extends WavLM’s joint prediction and denoising to 40k hours of data across 136 languages, and devise a novel multi-stage pre-training method, designed to address the language imbalance of multilingual data.'}
Url: https://arxiv.org/pdf/2309.15317
Title: STRUCTURED PRUNING OF SELF-SUPERVISED PRE-TRAINED MODELS FOR SPEECH RECOGNITION AND UNDERSTANDING
Authors: Yifan Peng, Kwangyoun Kim, Felix Wu, Prashant Sridhar, Shinji Watanabe
Section: 4.5. Comparison with other compression methods
As introduced in Sec. 2.1, HJ-Pruning can be directly applied to other SSL models. In Fig. 6, we prune the HuBERT-base model based on the overall MACs for ASR. The performance is similar to that of the wav2vec2. We also include other compressed models for comparison, including DistilHuBERT [15] and LightHuBERT [16]. Note that these results are not really comparable due to: (1) Their WERs are from SUPERB [1], which combines a frozen SSL model with another learnable RNN. We also tried to replace the RNN with a single linear layer and fine-tune the entire model (same as our setting), but the performance was clearly worse. (2) Their compressed models are initially distilled using the 960h unlabeled LibriSpeech data and then fine-tuned on the 100h labeled data, but our taskspecific pruning only utilizes the 100h data. This comparison shows that our task-specific pruning method is highly effective. 5. CONCLUSION In this paper, we propose HJ-Pruning to jointly prune heterogeneous components of SSL speech models, which achieves strong performance-efficiency tradeoffs compared to several baselines. At a small sparsity (0.1 to 0.3), HJ-Pruning improves the wav2vec2 baseline while being faster. Depending on the task, HJ-Pruning saves 40% or 50% MACs while maintaining the performance of wav2vec2. HJ-Pruning is a general method that can be applied to most of speech SSL models such as HuBERT. In the future, we plan to explore the application of HJ-Pruning on encoder-decoder SSL models [43] and other SLU tasks [44, 5]. 6. REFERENCES [1] S. w. Yang, P.-H. Chi, Y.-S. Chuang, et al., “SUPERB: Speech Processing Universal PERformance Benchmark,” in Proc.
Answer: "
"When is the buggy showcase at the spring carnival?
","['cmubuggy_d403.txt', 'Apr-11_Eventno_5_BuggyShowcase.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When is the buggy showcase at the spring carnival?

Context: Buggy Races Keep Rolling at Carnegie Mellon - News - Carnegie Mellon University Carnegie Mellon University ——— Search Search Search this site only News News › Stories › Archives › 2019 › April › Buggy Races Keep Rolling at Carnegie Mellon April 10, 2019 Buggy Races Keep Rolling at Carnegie Mellon In its 99th year, the tradition is a Spring Carnival treat By Heidi Opdyke opdyke(through)andrew.cmu.edu Media Inquiries Julie Mattera Marketing and Communications jmattera(through)cmu.edu 412-268-2902 Sweepstakes, also known as the Buggy Races, has come a long way at Carnegie Mellon University. The slick, torpedo-like vessels carrying drivers with nerves of steel are a far cry from the two-man teams that once changed places halfway through a race and rode in everything from rain barrels with bicycle wheels to three-wheeled ash cans 99 years ago. Today, it takes six people to maneuver the .84 -mile course around Schenley Park's Flagstaff Hill. But while five pushers and a driver navigate the course's hills, dozens of people are needed to make a successful race happen. A year of planning goes into just over two minutes of racing. The Machine The basics of a buggy are straightforward, but teams are often secretive in how they build the machines, in particular the way they brake, steer and what types of wheels are used. Each has a body, pushbar for runners to move the machine up the hills, wheels, a safety harness and driving and braking mechanisms. Some also include fairings, a type of housing around the wheels that help reduce drag, make the vehicle quieter and just looks cool. Fairings have been a key feature for the Fringe team in recent years, which is celebrating its 50th anniversary of Buggy. ""We have a reputation of being the quietest on the course,"" said Diya Nuxoll, who wrapped up her bachelor's degree in mechanical engineering in December and is working on an advanced studies master's degree in design and manufacturing. Nuxoll leads the Fringe team as one of two head mechanics. An mural celebrating Buggy can be found in CMU's Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions. Sweepstakes Slang Buggy: Vehicle being raced and also a nickname for the competition.
Event: Buggy Showcase
Date: 4/11/24
Time: 12:00 PM-2:00 PM ET
Participants/Audience: Open to entire CMU community 
Event Details: 
Kick off your Carnival Weekend with Buggy! View the latest buggy designs, talk with the teams and vote for the Buggy People’s Choice Award. 

Note: No registration required. No event fee. This event is open to the entire CMU community and their guests.
Answer: "
"In summer 2024, What is the deadline for adding, auditing, and tuition adjustment drop for Mini-6?
","['Meta-Data-Calendar-2324.txt', 'Meta-Data-Calendar-2324.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In summer 2024, What is the deadline for adding, auditing, and tuition adjustment drop for Mini-6?

Context: Event: 'Mini-5 course drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-07', Day: 'Friday', Event: 'Mini-5 pass/no pass & withdrawal deadline (3)', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-14', Day: 'Friday', Event: 'Mini-5 Faculty Course Evaluations open', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-18', Day: 'Tuesday', Event: 'Semester course drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Summer One 2024 (M24)'
Date: '2022-06-19', Day: 'Wednesday', Event: 'Juneteenth; University Closed & No Classes', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-20', Day: 'Thursday', Event: 'Mini-5 Last Day of Classes', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-20', Day: 'Thursday', Event: 'Mini-5 voucher deadline (4)', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-21', Day: 'Friday', Event: 'Mini-5 Final Exams', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-21', Day: 'Friday', Event: 'Mini-5 Faculty Course Evaluations close ', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-24', Day: 'Monday', Event: 'Mini-6 First Day of Classes', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-25', Day: 'Tuesday', Event: 'Mini-5 Final Grades Due by 4 pm', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-28', Day: 'Friday', Event: 'Mini-6 add, audit, & tuition adjustment drop deadline (1)', Semester: 'Summer One 2024 (M24)'
Date: '2024-07-04', Day: 'Thursday', Event: 'Independence Day; University Closed & No Classes',
Day: 'Friday', Event: 'Mini-6 add, audit, & tuition adjustment drop deadline (1)', Semester: 'Summer One 2024 (M24)'
Date: '2024-07-04', Day: 'Thursday', Event: 'Independence Day; University Closed & No Classes', Semester: 'Summer One 2024 (M24)'
Date: '2024-07-05', Day: 'Friday', Event: 'Semester pass/no pass & withdrawal deadline (3)', Semester: 'Summer One 2024 (M24)'
Date: '2024-07-08', Day: 'Monday', Event: 'Mini-6 drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Summer One 2024 (M24)'
Date: '2024-07-22', Day: 'Friday', Event: 'Mini-6 pass/no pass & withdrawal deadline (3)', Semester: 'Summer One 2024 (M24)'
Date: '2024-07-29', Day: 'Monday', Event: 'Semester & Mini-6 Faculty Course Evalutations open ', Semester: 'Summer One 2024 (M24)'
Date: '2024-08-01', Day: 'Thursday', Event: 'Semester & Mini-6 Last Day of Classes', Semester: 'Summer One 2024 (M24)'
Date: '2024-08-01', Day: 'Thursday', Event: 'Semester & Mini-6 voucher deadline (4)', Semester: 'Summer One 2024 (M24)'
Date: '2024-08-02', Day: 'Friday', Event: 'Semester & Mini-6 Final Exams***', Semester: 'Summer One 2024 (M24)'
Date: '2024-08-02', Day: 'Friday', Event: 'Semester & Mini-6 Faculty Course Evaluations close', Semester: 'Summer One 2024 (M24)'
Date: '2024-08-06', Day: 'Tuesday', Event: 'Semester & Mini-6 Final Grades Due by 4 pm', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-24', Day: 'Monday', Event: 'Summer Semester Two Classes Begin', Semester: 'Summer Two 2024 (N24)'
Date: '2024-06-28', Day: 'Friday',
Answer: "
"In fall 2023, When are the Semester & Mini-2 Faculty Course Evaluations closed?
","['Meta-Data-Calendar-2324.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, When are the Semester & Mini-2 Faculty Course Evaluations closed?

Context: Day: 'Monday', Event: 'Semester & Mini-2 Faculty Course Evaluations close', Semester: 'Fall 2023 (F23)'
Date: '2023-12-20', Day: 'Wednesday', Event: 'Final Grades Due by 4 pm ', Semester: 'Fall 2023 (F23)'
Start Date: '2023-12-23', End Date: '2024-01-02', Days: 'Saturday to Tuesday', Event: 'Winter Break; University Closed', Semester: 'Fall 2023 (F23)'
Date: '2023-01-08', Day: 'Monday', Event: 'Fall Deans' Lists Posted', Semester: 'Fall 2023 (F23)'
Date: '2024-01-15', Day: 'Monday', Event: 'Martin Luther King Day; No Classes', Semester: 'Spring 2024 (S24)'
Date: '2024-01-16', Day: 'Tuesday', Event: 'First Day of Class', Semester: 'Spring 2024 (S24)'
Date: '2024-01-22', Day: 'Monday', Event: 'Mini-3 add, audit & tuition adjustment drop deadline (1)', Semester: 'Spring 2024 (S24)'
Date: '2024-01-29', Day: 'Monday', Event: 'Semester add, audit & tuition adjustment drop deadline (1)', Semester: 'Spring 2024 (S24)'
Date: '2024-02-07', Day: 'Wednesday', Event: 'Mini-3 course drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Spring 2024 (S24)'
Date: '2024-02-19', Day: 'Monday', Event: 'Mini-3 pass/no pass & withdrawal deadline (3)', Semester: 'Spring 2024 (S24)'
Date: '2024-02-19', Day: 'Monday', Event: 'Mini-3 Faculty Course Evaluations open', Semester: 'Spring 2024 (S24)'
Date: '2024-02-26', Day: 'Monday', Event: 'Semester course drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Spring 2024 (S24)'
Date: '2024-03-01', Day: 'Friday', Event: 'Mini-3 Last Day of Classes ',
On Monday, 28 August, 2023, during the Fall 2023 (F23) semester, Semester & Mini-1 Classes Begin is observed.
On Friday, 01 September, 2023, during the Fall 2023 (F23) semester, Mini-1 add, audit & tuition adjustment drop deadline  (1) is observed.
On Monday, 04 September, 2023, during the Fall 2023 (F23) semester, Labor Day is observed, leading to No Classes & University Closed.
On Monday, 11 September, 2023, during the Fall 2023 (F23) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Wednesday, 20 September, 2023, during the Fall 2023 (F23) semester, Mini-1 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 02 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Pass/no pass & withdrawal deadline (3) is observed.
On Monday, 02 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Faculty Course Evaluations open is observed.
On Monday, 09 October, 2023, during the Fall 2023 (F23) semester, Semester drop deadline is observed, leading to withdrawal grade assigned after this date.
On Friday, 13 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Last Day of Classes is observed.
On Friday, 13 October, 2023, during the Fall 2023 (F23) semester, Mini-1 voucher election deadline (4) is observed.
From 13 October, 2023,Friday to 14 October, 2023,Saturday marks the Family Weekend for Fall 2023 (F23) semester.
On Saturday, 14 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Exams is observed.
On Saturday, 14 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Faculty Course Evaluations close is observed.
From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
Answer: "
"In fall 2023, Is there class and university operation on Labor Day?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, Is there class and university operation on Labor Day?

Context: On Monday, 02 September, 2024, during the Fall 2024 (F24) semester, Labor Day is observed, leading to No Classes & University Closed.
On Monday, 09 September, 2024, during the Fall 2024 (F24) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 18 September, 2024, during the Fall 2024 (F24) semester, Mini-1 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 30 September, 2024, during the Fall 2024 (F24) semester, Mini-1 Pass/no pass & withdrawal deadline (3) is observed.
On Monday, 30 September, 2024, during the Fall 2024 (F24) semester, Mini-1 Faculty Course Evaluations open is observed.
On Monday, 07 October, 2024, during the Fall 2024 (F24) semester, Semester drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 11 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Last Day of Classes is observed.
On Friday, 11 October, 2024, during the Fall 2024 (F24) semester, Mini-1 voucher election deadline (4) is observed.
On Saturday, 12 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Exams is observed.
On Saturday, 12 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Faculty Course Evaluations close is observed.
From 14 October, 2024,Monday to 18 October, 2023,Friday marks the Fall Break for Fall 2024 (F24) semester which leads to No Classes.
On Monday, 21 October, 2024, during the Fall 2024 (F24) semester, Mini-2 Classes Begin is observed.
On Wednesday, 23 October, 2024, during the Fall 2024 (F24) semester, Mid-Semester & Mini-1 grades due by 4 pm is observed.
On Friday, 25 October, 2024, during the Fall 2024 (F24) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 28 August, 2023, during the Fall 2023 (F23) semester, Semester & Mini-1 Classes Begin is observed.
On Friday, 01 September, 2023, during the Fall 2023 (F23) semester, Mini-1 add, audit & tuition adjustment drop deadline  (1) is observed.
On Monday, 04 September, 2023, during the Fall 2023 (F23) semester, Labor Day is observed, leading to No Classes & University Closed.
On Monday, 11 September, 2023, during the Fall 2023 (F23) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Wednesday, 20 September, 2023, during the Fall 2023 (F23) semester, Mini-1 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 02 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Pass/no pass & withdrawal deadline (3) is observed.
On Monday, 02 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Faculty Course Evaluations open is observed.
On Monday, 09 October, 2023, during the Fall 2023 (F23) semester, Semester drop deadline is observed, leading to withdrawal grade assigned after this date.
On Friday, 13 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Last Day of Classes is observed.
On Friday, 13 October, 2023, during the Fall 2023 (F23) semester, Mini-1 voucher election deadline (4) is observed.
From 13 October, 2023,Friday to 14 October, 2023,Saturday marks the Family Weekend for Fall 2023 (F23) semester.
On Saturday, 14 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Exams is observed.
On Saturday, 14 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Faculty Course Evaluations close is observed.
From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
Answer: "
"What are the codes/numbers of the distinct courses, all titled ""Introduction to Computer Systems"", that will be offered in the Summer of 2024?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the codes/numbers of the distinct courses, all titled ""Introduction to Computer Systems"", that will be offered in the Summer of 2024?

Context: In Semester Spring 2024, from the department of Computer Science, the subject titled 'Introduction to Computer Systems' with Course ID 15213 and Section E offers 12.0 units. The Class meets Monday between 01:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Andersen, Railing, Beckmann located in Building WEH, Room 4625.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Introduction to Computer Systems' with Course ID 15213 and Section F offers 12.0 units. The Class meets Monday between 02:00PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Andersen, Railing, Beckmann located in Building WEH, Room 4625.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Introduction to Computer Systems' with Course ID 15213 and Section G offers 12.0 units. The Class meets Monday between 10:00AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Andersen, Railing, Beckmann located in Building PH, Room A18C.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Introduction to Computer Systems' with Course ID 15213 and Section H offers 12.0 units. The Class meets Monday between 04:00PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Andersen, Railing, Beckmann located in Building PH, Room A22.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Great Ideas in Theoretical Computer Science' with Course ID 15251 and Section Lec 1 offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ada, Saad located in Building GHC, Room 4401.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Great Ideas in Theoretical Computer Science' with Course ID 15251 and Section NA offers 12.0 units.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Introduction to Computer Systems' with Course ID 15213 and Section E offers 12.0 units. The Class meets Monday between 01:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Andersen, Railing, Beckmann located in Building WEH, Room 4625.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Introduction to Computer Systems' with Course ID 15213 and Section F offers 12.0 units. The Class meets Monday between 02:00PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Andersen, Railing, Beckmann located in Building WEH, Room 4625.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Introduction to Computer Systems' with Course ID 15213 and Section G offers 12.0 units. The Class meets Monday between 10:00AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Andersen, Railing, Beckmann located in Building PH, Room A18C.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Introduction to Computer Systems' with Course ID 15213 and Section H offers 12.0 units. The Class meets Monday between 04:00PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Andersen, Railing, Beckmann located in Building PH, Room A22.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Great Ideas in Theoretical Computer Science' with Course ID 15251 and Section Lec 1 offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ada, Saad located in Building GHC, Room 4401.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Great Ideas in Theoretical Computer Science' with Course ID 15251 and Section NA offers 12.0 units.
Answer: "
"What time in the day does SafeWalk end?
","['mcds-student-handbook-2023_2024.txt', 'handbook-msaii-2022-2023.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What time in the day does SafeWalk end?

Context: The Parking and Transportation Services office is located in the lower 
level of the University Center, LL#8. There is limited parking on campus 
and the varying permit rates can be found on the website. All parking 
areas of campus are either by permit, metered or by the hour in the 
garage. Parking and Transportation Services will ticket any car parked in a 
permit area without a permit or at an expired meter. The city monitors the 
metered parking along Margaret Morrison, Frew and Tech Streets and will 
ticket at expired meters as well. 
 
More information can be found at: 
http://www.cmu.edu/parking/ 
 
The University offers shuttle and escort services operated through 
University Police. The Shuttle Service operates several routes within 
Oakland, Squirrel Hill and Shadyside areas, as well as to university sites 
located outside of the main campus. The Escort Service offers vehicle 
routes within a radius of campus between 6:30 pm-6 am daily. Information 
regarding up-to-date shuttle and escort schedules, pick-up/drop-off 
locations, routes and usage policies can be found at: 
www.cmu.edu/police/shuttleandescort/. 
 
SafeWalk provides another option to campus community members 
walking across and around campus during late-night hours. SafeWalk is a 
student volunteer organization that provides campus escorts for all 
members of the Carnegie Mellon community. SafeWalk operates nightly 
during the regular academic year (except certain holidays and break 
periods) from 10pm until 2am. Students, faculty and staff may request an 
escort by calling 412-268-SAFE (8-7233 from a campus phone), by 
approaching an escort team, or by stopping by the SafeWalk dispatch area 
in the University Center, Lower Level near the Post Office Package Pick-
 
 
42
Up window between 10pm-2am. SafeWalk will escort to locations 
approximately one mile from campus. Additional SafeWalk information 
can be found at: 
www.studentaffairs.cmu.edu/safewalk. 
6.16 Copying, Printing and Mailing Services 
Carnegie Mellon offers community members easy access to FedEx, copy 
centers, printing and mailing services, and postal services. More 
information regarding these services, locations and contact information 
can be found at the provided link.
The Parking and Transportation Services office is 
located in the East Campus Garage by the Forbes Ave entrance.  There is limited parking on 
campus, and the varying permit rates can be found on the website.  All parking areas of campus 
are either by permit, metered or by the hour in the garage.  Parking and Transportation Services 
will ticket any car parked in a permit area without a permit or at an expired meter.  The city 
monitors the metered parking along Margaret Morrison, Frew and Tech Streets and will ticket 
at expired meters as well. 
The university offers shuttle and escort services operated by University Police.  The Shuttle 
Service operates several routes within Oakland, Squirrel Hill and Shadyside areas, as well as to 
University sites located outside of the main campus.  The Escort Service offers vehicle routes 
within a radius of campus between 6:30 pm-4:15 am daily.  Information regarding up-to-date 
shuttle and escort schedules, pick-up/drop-off locations, routes and usage policies can be found 
at www.cmu.edu/parking/shuttle/index.html.  
SafeWalk provides another option to campus community members walking across and around 
campus during late-night hours. SafeWalk is a student volunteer organization that provides 
campus escorts for all members of the Carnegie Mellon community. SafeWalk operates nightly 
during the regular academic year (except certain holidays and break periods) from 10pm until 
2am. Students, faculty, and staff may request an escort by calling 412-268-SAFE (8-7233 from a 
campus phone), by approaching an escort team, or by stopping by the SafeWalk dispatch area in 
the University Center, Lower Level near the Post Office Package Pick-Up window between 
10pm-2am. SafeWalk will escort to locations approximately one mile from campus. Additional 
40 
 
SafeWalk information can be found at https://www.cmu.edu/admission/campus-
experience/student-services. 
13.16  Copying, Printing and Mailing Services 
Carnegie Mellon offers community members easy access to FedEx, copy centers, printing and 
mailing services, and postal services.
Answer: "
"In Fall 2023, how many sections did Shop Skills 48104 have?
","['metadata_course_fall_23.txt', 'combined_metadata_final.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In Fall 2023, how many sections did Shop Skills 48104 have?

Context: Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building CFA, Room 200.
In Semester Fall 2023, from the department of Architecture, the subject titled 'Shop Skills' with Course ID 48104 and Section A1 offers VAR units. The Class meets Monday Wednesday between 10:00AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Holmes located in Building CFA, Room A9.
In Semester Fall 2023, from the department of Architecture, the subject titled 'Shop Skills' with Course ID 48104 and Section A2 offers VAR units. The Class meets Monday Wednesday between 10:00AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Holmes located in Building CFA, Room A9.
In Semester Fall 2023, from the department of Architecture, the subject titled 'Shop Skills' with Course ID 48104 and Section B1 offers VAR units. The Class meets Monday Wednesday between 11:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Holmes located in Building CFA, Room A9.
In Semester Fall 2023, from the department of Architecture, the subject titled 'Shop Skills' with Course ID 48104 and Section B2 offers VAR units. The Class meets Monday Wednesday between 11:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Holmes located in Building CFA, Room A9.
In Semester Fall 2023, from the department of Architecture, the subject titled 'Shop Skills' with Course ID 48104 and Section C1 offers VAR units. The Class meets Monday Wednesday between 06:00PM and 06:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Sontag located in Building CFA, Room A9.
In Semester Fall 2023, from the department of Architecture, the subject titled 'Shop Skills' with Course ID 48104 and Section C2 offers VAR units. The Class meets Monday Wednesday between 06:00PM and 06:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Sontag located in Building CFA, Room A9.
Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Holmes located in Building CFA, Room A9.
In Semester Fall 2023, from the department of Architecture, the subject titled 'Shop Skills' with Course ID 48104 and Section C1 offers VAR units. The Class meets Monday Wednesday between 06:00PM and 06:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Sontag located in Building CFA, Room A9.
In Semester Fall 2023, from the department of Architecture, the subject titled 'Shop Skills' with Course ID 48104 and Section C2 offers VAR units. The Class meets Monday Wednesday between 06:00PM and 06:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Sontag located in Building CFA, Room A9.
In Semester Fall 2023, from the department of Architecture, the subject titled 'Shop Skills' with Course ID 48104 and Section D1 offers VAR units. The Class meets Monday Wednesday between 07:30PM and 08:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Sontag located in Building CFA, Room A9.
In Semester Fall 2023, from the department of Architecture, the subject titled 'Shop Skills' with Course ID 48104 and Section D2 offers VAR units. The Class meets Monday Wednesday between 07:30PM and 08:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Sontag located in Building CFA, Room A9.
In Semester Fall 2023, from the department of Architecture, the subject titled 'Shop Skills' with Course ID 48104 and Section E2 offers VAR units. The Class meets Tuesday Thursday between 03:00PM and 03:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Holmes located in Building CFA, Room A9.
In Semester Fall 2023, from the department of Architecture, the subject titled 'Introduction to Building Performance' with Course ID 48116 and Section A offers 3.0 units. The Class meets Tuesday between 11:00AM and 12:20PM ET.
Answer: "
"When was the two-wheeled buggy eliminated?
","['cmubuggy_d403.txt', 'cmubuggy_d403.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When was the two-wheeled buggy eliminated?

Context: Buggy Races Keep Rolling at Carnegie Mellon - News - Carnegie Mellon University Carnegie Mellon University ——— Search Search Search this site only News News › Stories › Archives › 2019 › April › Buggy Races Keep Rolling at Carnegie Mellon April 10, 2019 Buggy Races Keep Rolling at Carnegie Mellon In its 99th year, the tradition is a Spring Carnival treat By Heidi Opdyke opdyke(through)andrew.cmu.edu Media Inquiries Julie Mattera Marketing and Communications jmattera(through)cmu.edu 412-268-2902 Sweepstakes, also known as the Buggy Races, has come a long way at Carnegie Mellon University. The slick, torpedo-like vessels carrying drivers with nerves of steel are a far cry from the two-man teams that once changed places halfway through a race and rode in everything from rain barrels with bicycle wheels to three-wheeled ash cans 99 years ago. Today, it takes six people to maneuver the .84 -mile course around Schenley Park's Flagstaff Hill. But while five pushers and a driver navigate the course's hills, dozens of people are needed to make a successful race happen. A year of planning goes into just over two minutes of racing. The Machine The basics of a buggy are straightforward, but teams are often secretive in how they build the machines, in particular the way they brake, steer and what types of wheels are used. Each has a body, pushbar for runners to move the machine up the hills, wheels, a safety harness and driving and braking mechanisms. Some also include fairings, a type of housing around the wheels that help reduce drag, make the vehicle quieter and just looks cool. Fairings have been a key feature for the Fringe team in recent years, which is celebrating its 50th anniversary of Buggy. ""We have a reputation of being the quietest on the course,"" said Diya Nuxoll, who wrapped up her bachelor's degree in mechanical engineering in December and is working on an advanced studies master's degree in design and manufacturing. Nuxoll leads the Fringe team as one of two head mechanics. An mural celebrating Buggy can be found in CMU's Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions. Sweepstakes Slang Buggy: Vehicle being raced and also a nickname for the competition.
The last course records were set in 2017, but this year potholes are making for some challenging conditions, said Amy Chen, a senior in psychology who has been driving for four years. With the vehicles just inches from the ground, ""even poorly filled potholes makes it dangerous to drive,"" she added. Depending on Friday and Saturday's conditions, most heats may run just two lanes instead of three. But still, despite some of the challenges, Chen said she wouldn't miss it. ""Being a driver is really fun,"" Chen said. ""I love going fast and going around the course."" — #CMUcarnival — Powered by Curator.io — Related Content — Media Advisory: Carnegie Mellon Celebrates Spring Carnival Buggy Races Keep Rolling at Carnegie Mellon Enjoy a World of Fun at Spring Carnival Student Architects Design Carnival Archway Take a Ride on The Old Mill at CMU MoBot Turns 25 Witchner, Wood Roll Into Buggy Royalty The Piper: Campus & Community News Official Events Calendar Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA 15213 412-268-2900 Legal Info www.cmu.edu © 2021 Carnegie Mellon University CMU on Facebook CMU on Twitter CMU on LinkedIn CMU YouTube Channel CMU RSS Feed CMU on Instagram CMU Social Media Directory Stories College of Engineering College of Fine Arts Dietrich College of Humanities & Social Sciences Heinz College of Information Systems and Public Policy Mellon College of Science School of Computer Science Tepper School of Business Archives 2021 March February January 2020 December November October September August July June May April March February January 2019 December November October September August July June May April March February January 2018 December November October September August July June May April March February January 2017 January February March April May June July August September October November December 2016 January February March April May June July August September October November December 2015 January February March April May June July August September October November December 2014 January February March April May June July August September October November December 2013 January February March April May June July August September October November December 2012 January February March April May June July August September October November December 2011 January February March April May June July August September October November December Media Highlights Media Resources Experts (Alphabetical) Experts (by Topic) Contact Us The Piper: Campus & Community News
Answer: "
"Is Yonatan the last author on the plan, eliminate and track paper?
","['yonatan bisk_5ce2f1dff23a5620f77f9b11f1e534422ab8ff3f_content_1.txt', 'yonatan bisk_5ce2f1dff23a5620f77f9b11f1e534422ab8ff3f_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Is Yonatan the last author on the plan, eliminate and track paper?

Context: Title: Plan, Eliminate, and Track — Language Models are Good Teachers for Embodied Agents
Authors: Yue Wu, So Yeon Min, Yonatan Bisk, Ruslan Salakhutdinov, Amos Azaria, Yuanzhi Li, Tom M. Mitchell, Shrimai Prabhumoye
Section: 4.1. Experimental Details
DAgger v.s. 6 hours for Behavior Cloning). In addition, we demonstrate that our models surpass the DAgger training performance of the BUTLER (Shridhar et al., 2020b) agents trained with DAgger, even when our agent does not have the option to interact with the environment. Baselines. Our first baseline is the BUTLER::BRAIN (BUTLER) agent (Shridhar et al., 2020b), which consists of an encoder, an aggregator, and a decoder. At each time step t, the encoder takes initial observation s0, current observation st, and task string stask and generates representation rt. The recurrent aggregator combines rt with the last recurrent state ht−1 to produce ht, which is then decoded into a string at representing action. In addition, the BUTLER agent uses beam search to get out of stuck conditions in the event of a failed action. Our second baseline GPT (Micheli & Fleuret, 2021) is a fine-tuned GPT2-medium on 3553 demonstrations from the AlfWorld training set. Specifically, the GPT is fined-tuned to generate each action step word-by-word to mimic the rule-based expert using the standard maximum likelihood loss.
Faculty Name: yonatan bisk
Paperid: 5ce2f1dff23a5620f77f9b11f1e534422ab8ff3f
Title: Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents
Year: 2023
Abstract: Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accomplished each sub-task. On the AlfWorld instruction following benchmark, the PET framework leads to a significant 15% improvement over SOTA for generalization to human goal specifications.
Authors: Yue Wu, So Yeon Min, Yonatan Bisk, R. Salakhutdinov, A. Azaria, Yuan-Fang Li, Tom M. Mitchell, Shrimai Prabhumoye
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A framework to use the knowledge in LLMs to simplify the control problem, rather than solving it is proposed, which leads to a significant 15% improvement over SOTA for generalization to human goal specifications.'}
Url: http://arxiv.org/pdf/2305.02412
Answer: "
"What year is ""Neural Mixed Effects for Nonlinear Personalized Predictions"" published in?
","['louis philippe morency_64703e760f662b1c0f647931bb63fe57e5ba91e4_metadata.txt', 'louis philippe morency_64703e760f662b1c0f647931bb63fe57e5ba91e4_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What year is ""Neural Mixed Effects for Nonlinear Personalized Predictions"" published in?

Context: Faculty Name: louis philippe morency
Paperid: 64703e760f662b1c0f647931bb63fe57e5ba91e4
Title: Neural Mixed Effects for Nonlinear Personalized Predictions
Year: 2023
Abstract: Personalized prediction is a machine learning approach that predicts a person’s future observations based on their past labeled observations and is typically used for sequential tasks, e.g., to predict daily mood ratings. When making personalized predictions, a model can combine two types of trends: (a) trends shared across people, i.e., person-generic trends, such as being happier on weekends, and (b) unique trends for each person, i.e., person-specific trends, such as a stressful weekly meeting. Mixed effect models are popular statistical models to study both trends by combining person-generic and person-specific parameters. Though linear mixed effect models are gaining popularity in machine learning by integrating them with neural networks, these integrations are currently limited to linear person-specific parameters: ruling out nonlinear person-specific trends. In this paper, we propose Neural Mixed Effect (NME) models to optimize nonlinear person-specific parameters anywhere in a neural network in a scalable manner1. NME combines the efficiency of neural network optimization with nonlinear mixed effects modeling. Empirically, we observe that NME improves performance across six unimodal and multimodal datasets, including a smartphone dataset to predict daily mood and a mother-adolescent dataset to predict affective state sequences where half the mothers experience symptoms of depression. Furthermore, we evaluate NME for two model architectures, including for neural conditional random fields (CRF) to predict affective state sequences where the CRF learns nonlinear person-specific temporal transitions between affective states. Analysis of these person-specific transitions on the mother-adolescent dataset shows interpretable trends related to the mother’s depression symptoms.
Title: Neural Mixed Efects for Nonlinear Personalized Predictions
Authors: Torsten Wörtwein, Nicholas B. Allen, Lisa B. Sheeber, Randy P. Auerbach, Jefrey F. Cohn, Louis-Philippe Morency
Section: ACKNOWLEDGMENTS
This material is based upon work partially supported by Meta, National Science Foundation awards 1722822 and 1750439, and National Institutes of Health awards U01MH116923, R01HD081362, R01MH125740, R01MH096951, R21MH130767 and R01MH132225. Any opinions, fndings, conclusions, or recommendations expressed in this material are those of the author(s) and do not necessarily refect the views of the sponsors, and no ofcial endorsement should be inferred.
Answer: "
"In spring 2024, Who are the instructors for course 10615?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, Who are the instructors for course 10615?

Context: In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Independent Study' with Course ID 10520 and Section A offers VAR units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Gormley located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Introduction to Machine Learning (Master's)' with Course ID 10601 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Chai, Gormley, Heidari located in Building TEP, Room 1403.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Introduction to Machine Learning (Master's)' with Course ID 10601 and Section B offers 12.0 units. The Class meets Monday Wednesday Friday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Chai, Gormley, Heidari located in Building GHC, Room 4401.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Machine Learning with Large Datasets' with Course ID 10605 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Gordon, Talwalkar located in Building DH, Room 2210.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Art and Machine Learning' with Course ID 10615 and Section A offers 12.0 units. The Class meets Monday Wednesday between 07:00PM and 08:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kang located in Building TEP, Room 1403.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Independent Study: Research' with Course ID 10620 and Section A offers 612 units. The Class meets Schedule will be added between NA and NA ET.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Independent Study' with Course ID 10520 and Section A offers VAR units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Gormley located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Introduction to Machine Learning (Master's)' with Course ID 10601 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Chai, Gormley, Heidari located in Building TEP, Room 1403.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Introduction to Machine Learning (Master's)' with Course ID 10601 and Section B offers 12.0 units. The Class meets Monday Wednesday Friday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Chai, Gormley, Heidari located in Building GHC, Room 4401.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Machine Learning with Large Datasets' with Course ID 10605 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Gordon, Talwalkar located in Building DH, Room 2210.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Art and Machine Learning' with Course ID 10615 and Section A offers 12.0 units. The Class meets Monday Wednesday between 07:00PM and 08:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kang located in Building TEP, Room 1403.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Independent Study: Research' with Course ID 10620 and Section A offers 612 units. The Class meets Schedule will be added between NA and NA ET.
Answer: "
"What is the full name of the conference where the paper Learning to Ask Questions for Zero-shot Dialogue State Tracking, got published?
","['alexander rudnicky_f743324682d5d50db9b114fa60b908f09c10c9a0_metadata.txt', 'alexander rudnicky_papers.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the full name of the conference where the paper Learning to Ask Questions for Zero-shot Dialogue State Tracking, got published?

Context: Faculty Name: alexander rudnicky
Paperid: f743324682d5d50db9b114fa60b908f09c10c9a0
Title: Learning to Ask Questions for Zero-shot Dialogue State Tracking
Year: 2023
Abstract: We present a method for performing zero-shot Dialogue State Tracking (DST) by casting the task as a learning-to-ask-questions framework. The framework learns to pair the best question generation (QG) strategy with in-domain question answering (QA) methods to extract slot values from a dialogue without any human intervention. A novel self-supervised QA pretraining step using in-domain data is essential to learn the structure without requiring any slot-filling annotations. Moreover, we show that QG methods need to be aligned with the same grammatical person used in the dialogue. Empirical evaluation on the MultiWOZ 2.1 dataset demonstrates that our approach, when used alongside robust QA models, outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation-given a comparable amount of domain knowledge during data creation. Finally, we analyze the impact of the types of questions used, and demonstrate that the algorithmic approach outperforms template-based question generation.
Authors: Diogo Tavares, David Semedo, Alexander I. Rudnicky, João Magalhães
Venue: Annual International ACM SIGIR Conference on Research and Development in Information Retrieval
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work presents a method for performing zero-shot Dialogue State Tracking by casting the task as a learning-to-ask-questions framework that outperforms template-based question generation and shows that QG methods need to be aligned with the same grammatical person used in the dialogue.'}
Url: https://dl.acm.org/doi/pdf/10.1145/3539618.3592010
List of 2023 Open Access papers by alexander rudnicky are:
Advancing Regular Language Reasoning in Linear Recurrent Neural Networks
Structured Dialogue Discourse Parsing
A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech
Overview of Robust and Multilingual Automatic Evaluation Metrics

for Open-Domain Dialogue Systems at DSTC 11 Track 4
Learning to Ask Questions for Zero-shot Dialogue State Tracking
Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings
Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation
Answer: "
"In the paper ""An Approach to Ontological Learning from Weak Labels"", what dataset and ontology were used in the investigation?
","['bhiksha raj_593a603354c09d151440ae044de1d80324a2ab01_metadata.txt', 'bhiksha raj_b7e2074934985b6112b6bce8c3680b14e621fdfe_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the paper ""An Approach to Ontological Learning from Weak Labels"", what dataset and ontology were used in the investigation?

Context: Faculty Name: bhiksha raj
Paperid: 593a603354c09d151440ae044de1d80324a2ab01
Title: An Approach to Ontological Learning from Weak Labels
Year: 2023
Abstract: Ontologies encompass a formal representation of knowledge through the definition of concepts or properties of a domain, and the relationships between those concepts. In this work, we seek to investigate whether using this ontological information will improve learning from weakly labeled data, which are easier to collect since it requires only the presence or absence of an event to be known. We use the AudioSet ontology and dataset, which contains audio clips weakly labeled with the ontology concepts and the ontology providing the ""Is A"" relations between the concepts. We first re-implemented the model proposed by [1] with modifications to fit the multi-label scenario and then expand on that idea by using a Graph Convolutional Network (GCN) to model the ontology information to learn the concepts. We find that the baseline Twin Neural Network (TNN) does not perform better by incorporating ontology information in the weak and multi-label scenario, but that the GCN does capture the ontology knowledge better for weak, multi-labeled data. We also investigate how different modules can tolerate noises introduced from weak labels and better incorporate ontology information. Our best TNN-GCN model achieves mAP=0.45 and AUC=0.87 for lower-level concepts and mAP=0.72 and AUC=0.86 for higher-level concepts, which is an improvement over the baseline TNN but about the same as our models that do not use ontology information.
Authors: Ankit Shah, Larry Tang, Po Hao Chou, Yilun Zheng, Ziqian Ge, B. Raj
Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work re-implements the model proposed by [1] with modifications to fit the multi-label scenario and expands on that idea by using a Graph Convolutional Network (GCN) to model the ontology information to learn the concepts.'}
Url: N/A
Title: IMPORTANCE OF NEGATIVE SAMPLING IN WEAK LABEL LEARNING
Authors: Ankit Shah, Fuyu Tang, Zelin Ye, Rita Singh, Bhiksha Raj
Section: 7. REFERENCES
Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, Philadelphia, PA, USA, 2007, pp. 1027–1035, Society for Industrial and Applied Mathematics. [16] Alex Krizhevsky, “Learning multiple layers of features from tiny images,” pp. 32–33, 2009. [17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, “Deep residual learning for image recognition,” CoRR, vol. abs/1512.03385, 2015. [18] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal, and Marvin Ritter, “Audio set: An ontology and human-labeled dataset for audio events,” in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2017, pp. 776–780. [19] Yuan Gong, Yu-An Chung, and James Glass, “Psla: Improving audio event classification with pretraining, sampling, labeling, and aggregation,” arXiv preprint arXiv:2102.01243, 2021. [20] Yuji Tokozume, Yoshitaka Ushiku, and Tatsuya Harada, “Learning from between-class examples for deep sound recognition,” CoRR, vol. abs/1711.10282, 2017. [21] Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D. Cubuk, and Quoc V. Le, “SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition,” in Proc. Interspeech 2019, 2019, pp. 2613–2617.
Answer: "
"What is Martial Herbert's office building and number?
","['miis-handbook_2023-2024.txt', 'miis-handbook_2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is Martial Herbert's office building and number?

Context: The University Center Information Desk is the location if you want to know about upcoming 
campus events or have questions about Carnegie Mellon in general, call the Information Desk 
at 412-268-2107. The Information Desk not only provides information about campus events, 
but also sells postage stamps, makes copies, sends faxes, distributes campus maps, manages a 
lost & found, and has information brochures about Pittsburgh and the campus. 
9.16  Athletic/Fitness Facilities 
 www.cmu.edu/athletics 
For the participant or the spectator, Carnegie Mellon offers intercollegiate athletics, 
intramural sports, physical education classes and club sports. The Athletics Department also 
offers aerobics classes in the University Center and Skibo Gym as well as occasional 
workshops and instruction related to fitness and health.  The Athletics Office is located in the 
Skibo Gymnasium. 
MIIS Graduate Student Handbook 
Page 38 
 
Skibo Gym facilities include courts for basketball, volleyball, badminton, as well as weight-
training and aerobic equipment. The University Center’s recreational facilities include an 
eight-lane pool, racquetball and squash courts, aerobics room, fitness center and gym for 
basketball and volleyball. All users must present a current Carnegie Mellon Card to use these 
facilities. 
9.17  CMU Alert 
www.cmu.edu/alert  
CMU Alert sends voice and/or text messages to registered phones in the event of a campus 
emergency that threatens public safety or during tests of the system in the spring and fall 
semesters.  Students can register for CMU Alert through the website.   
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
MIIS Graduate Student Handbook 
Page 39 
 
 
A Appendix  
Appendix A 
2023-2024 
Highlighted University Resources for Graduate Students 
 
Note: The following pages are meant to broadly include only some of the resources available to 
graduate students.  It is not an exhaustive appendix of resources, and students are strongly 
encouraged to visit the various websites linked below for the most up-to-date information.
A.1.8 Office of International Education (OIE)  
https://www.cmu.edu/oie/  
Carnegie Mellon hosts international graduate and undergraduate students who come from 
more than 90 countries. The Office of International Education (OIE) is the liaison to the 
University for all non-immigrant students and scholars, as well the repository for study abroad 
opportunities. OIE provides many services including: advising on personal, immigration, study 
abroad, academic, and social and acculturation issues; presenting programs of interest such as 
international career workshops, tax workshops, and cross-cultural and immigration 
workshops; international education and statistics on international students in the United 
MIIS Graduate Student Handbook 
Page 43 
 
States; posting pertinent information to students through email and the OIE website and 
conducting orientation and pre-departure programs.  
 
A.1.9 Veterans and Military Community  
https://www.cmu.edu/veterans/  
Military veterans are a vital part of the Carnegie Mellon University community. Graduate 
students can find information on applying for veteran education benefits, campus services, 
veteran’s groups at CMU, and non-educational resources through the Veterans and Military 
Community website. There are also links and connections to veteran resource in the 
Pittsburgh community. The ROTC and Veteran Affairs Coordinator can be reached at  
urovaedbenefits@andrew.cmu.edu or 412-268-8747.  
A.1.10 Carnegie Mellon Ethics Hotline  
https://www.cmu.edu/hr/resources/ethics-hotline.html 
The health, safety and well-being of the university community are top priorities at Carnegie 
Mellon University. CMU provides a hotline that all members of the university community 
should use to confidentially report suspected unethical activity, violations of university policy, 
or violations of law. Students, faculty and staff can anonymously file a report by calling 1-844-
587-0793 or visiting https://cmu.ethicspoint.com/. All submissions are reported to appropriate 
university personnel and handled discreetly. 
The hotline is NOT an emergency service. For emergencies, call University Police at 412-
268-2323.
Answer: "
"Which LTI faculty are involved in the SPAE paper?
","['alexander hauptmann_376f494126d1ea4f571ea0263c43ac2b6331800a_metadata.txt', 'yonatan bisk_376f494126d1ea4f571ea0263c43ac2b6331800a_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which LTI faculty are involved in the SPAE paper?

Context: Faculty Name: alexander hauptmann
Paperid: 376f494126d1ea4f571ea0263c43ac2b6331800a
Title: SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs
Year: 2023
Abstract: In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.
Authors: Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A. Ross, Irfan Essa, Yonatan Bisk, Ming Yang, K. Murphy, A. Hauptmann, Lu Jiang
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.'}
Url: http://arxiv.org/pdf/2306.17842
Faculty Name: yonatan bisk
Paperid: 376f494126d1ea4f571ea0263c43ac2b6331800a
Title: SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs
Year: 2023
Abstract: In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.
Authors: Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A. Ross, Irfan Essa, Yonatan Bisk, Ming Yang, K. Murphy, A. Hauptmann, Lu Jiang
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.'}
Url: http://arxiv.org/pdf/2306.17842
Answer: "
"In summer 2024, What is the deadline for adding, auditing, and tuition adjustment drop for Mini-5?
","['Meta-Data-Calendar-2324.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In summer 2024, What is the deadline for adding, auditing, and tuition adjustment drop for Mini-5?

Context: Event: 'Mini-5 course drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-07', Day: 'Friday', Event: 'Mini-5 pass/no pass & withdrawal deadline (3)', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-14', Day: 'Friday', Event: 'Mini-5 Faculty Course Evaluations open', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-18', Day: 'Tuesday', Event: 'Semester course drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Summer One 2024 (M24)'
Date: '2022-06-19', Day: 'Wednesday', Event: 'Juneteenth; University Closed & No Classes', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-20', Day: 'Thursday', Event: 'Mini-5 Last Day of Classes', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-20', Day: 'Thursday', Event: 'Mini-5 voucher deadline (4)', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-21', Day: 'Friday', Event: 'Mini-5 Final Exams', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-21', Day: 'Friday', Event: 'Mini-5 Faculty Course Evaluations close ', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-24', Day: 'Monday', Event: 'Mini-6 First Day of Classes', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-25', Day: 'Tuesday', Event: 'Mini-5 Final Grades Due by 4 pm', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-28', Day: 'Friday', Event: 'Mini-6 add, audit, & tuition adjustment drop deadline (1)', Semester: 'Summer One 2024 (M24)'
Date: '2024-07-04', Day: 'Thursday', Event: 'Independence Day; University Closed & No Classes',
On Monday, 05 May, 2025, during the Spring 2025 (S25) semester, Semester & Mini-4 Faculty Course Evaluations close is observed.
On Tuesday, 06 May, 2025, during the Spring 2025 (S25) semester, Make-Up Final Examinations is observed.
On Wednesday, 08 May, 2025, during the Spring 2025 (S25) semester, Graduating Students' Final Grades Due by 4 pm is observed.
On Sunday, 11 May, 2025, during the Spring 2025 (S25) semester, Commencement is observed.
On Tuesday, 13 May, 2025, during the Spring 2025 (S25) semester, Final Grades Due by 4 pm is observed.
On Tuesday, 20 May, 2025, during the Spring 2025 (S25) semester, Spring Deans' Lists Posted is observed.
On Monday, 12 May, 2025, during the Summer 2025 (M25) semester, Summer Session All, One & Mini-5 Classes Begin is observed.
On Friday, 16 May, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 add, audit, & tuition adjustment drop deadline (1) is observed.
On Friday, 23 May, 2025, during the Summer 2025 (M25) semester, Summer All add, audit, & tuition adjustment drop deadline (1) is observed.
On Friday, 26 May, 2025, during the Summer 2025 (M25) semester, Memorial Day is observed, leading to University Closed & No Classes.
On Tuesday, 27 May, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 09 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 pass/no pass & withdrawal deadline (3) is observed.
On Friday, 13 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Faculty Course Evaluations open is observed.
Answer: "
"Who is the PhD Academic Program Manager for the LTI PhD degree?
","['handbook_phd_2023-2024.txt', 'program_info_Dual-DegreePhDinLanguageandInformationTechnologies(PortugalPartnership).txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who is the PhD Academic Program Manager for the LTI PhD degree?

Context: degree are: 
Jamie Callan 
Ph.D. Program Director 
Professor 
GHC 5419 
callan@cs.cmu.edu 
412-268-4525 
Stacey Young 
Ph.D. Academic Program Manager 
LTI Graduate Program Manager 
GHC 6415 
staceyy@cs.cmu.edu  
412-268-2623 
Mona Diab 
LTI Director 
Professor 
GHC 5723 
mdiab@andrew.cmu.edu 
412-268-3669 
 
Joan Axelson 
Office Manager 
GHC 5405 
jaxelson@andrew.cmu.edu 
412-268-7517 
 
Julie Nys 
Employment Processes Manger 
GHC 5405 
jnys@andrew.cmu.edu 
412-268-3515 
1.3 
University Policies and Expectations 
It is the responsibility of each member of the Carnegie Mellon community to be familiar with 
university policies and guidelines. In addition to this departmental graduate student handbook, 
the following resources are available to assist you in understanding community expectations. 
 The Word/Student Handbook: 
https://www.cmu.edu/student-affairs/theword//index.html 
 Academic Integrity Policy: 
 
https://www.cmu.edu/policies/student-and- student-life/academic-
integrity.html 
LTI Ph.D. Graduate Student Handbook 
Page 11 
 
 University Policies Website:  
https://www.cmu.edu/policies/ 
 Office of Graduate and Postdoctoral Affairs: 
https://www.cmu.edu/graduate/policies/index.html 
Due to the changing nature of conditions and expectations surrounding public health and safety 
requirements please visit https://www.cmu.edu/coronavirus/ for the most up to date information.
 
Please see Appendix A for additional information about The Word and University 
resources. 
1.4 
The Academic Calendar  
The Academic calendar can be found at https://www.cmu.edu/hub/calendar/index.html and 
provides information on all deadlines, including registration dates, class start dates, add/drop 
deadlines, exam dates, and more.  
Some doctoral course-sections follow a separate Academic Calendar.
Academic Program Name:
Dual-Degree Ph.D. in Language and Information Technologies (Portugal Partnership)

Website:
https://lti.cs.cmu.edu/academics/phd-programs/dual-degree-phd-lti.html

Overview:
The LTI offers a dual-degree Ph.D. in Language and Information Technologies in cooperation with:
Universidade de Aveiro (Ph.D. in Computer Engineering), Universidade do Minho (Ph.D. in Informatics)  and the Universidade do Porto (FCUP, Ph.D. in Computer Science and FEUP, Ph.D. in Computer Science) as part of MAPi;
Universidade de Lisboa, Faculdade de Ciências – FCUL (Ph.D. in Informatics) 
Universidade de Lisboa, Instituto Superior Técnico – IST  (Ph.D. in Computer Science and Engineering, Ph.D. in Electrical and Computer Engineering, Ph.D. in Information Security)
Universidade Nova de Lisboa, Faculdade de Ciências e Tecnologia – FCTUNL (Ph.D. in Computer Science)
Universidade de Coimbra, Faculdade de Ciências e Tecnologia – FCTUC (Ph.D. in Information Science and Technology)
Students jointly enrolled in the LTI Ph.D program spend a year in Portugal, then two years at Carnegie Mellon taking classes in linguistics, computer science, statistical learning and task orientation.
After completing the majority of their academic requirements, students return to Portugal for the next two years to conduct extensive research, ultimately leading to a dissertation topic that will be publicly defended. One adviser from each institution co-supervises their student’s progress and helps to define their final thesis topic.

Requirements:
Students participating in the dual-degree program will spend their first year in Portugal, followed by two years in Pittsburgh to complete their coursework. They will complete a maximum of eight courses with a proper balance of focus areas (linguistics, computer science, statistical/learning and task orientation). After that, they will return to Portugal for their last two years, pursuing research and completing their dissertation. For more, see the Carnegie Mellon | Portugal page.

Curriculum:
While in the dual Ph.D. program, your schedule may look like this.
Answer: "
"Which LTI professor co-authored the paper titled ""Text Matching Improves Sequential Recommendation by Reducing Popularity Biases""?
","['chenyan xiong_159100c8323fc558e4073a3a006f3f243aca3a60_metadata.txt', 'chenyan xiong_159100c8323fc558e4073a3a006f3f243aca3a60_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which LTI professor co-authored the paper titled ""Text Matching Improves Sequential Recommendation by Reducing Popularity Biases""?

Context: Faculty Name: chenyan xiong
Paperid: 159100c8323fc558e4073a3a006f3f243aca3a60
Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases
Year: 2023
Abstract: This paper proposes Text mAtching based SequenTial rEcommenda-tion model (TASTE), which maps items and users in an embedding space and recommends items by matching their text representations. TASTE verbalizes items and user-item interactions using identifiers and attributes of items. To better characterize user behaviors, TASTE additionally proposes an attention sparsity method, which enables TASTE to model longer user-item interactions by reducing the self-attention computations during encoding. Our experiments show that TASTE outperforms the state-of-the-art methods on widely used sequential recommendation datasets. TASTE alleviates the cold start problem by representing long-tail items using full-text modeling and bringing the benefits of pretrained language models to recommendation systems. Our further analyses illustrate that TASTE significantly improves the recommendation accuracy by reducing the popularity bias of previous item id based recommendation models and returning more appropriate and text-relevant items to satisfy users. All codes are available at https://github.com/OpenMatch/TASTE.
Authors: Zhenghao Liu, Senkun Mei, Chenyan Xiong, Xiaohua Li, Shi Yu, Zhiyuan Liu, Yu Gu, Ge Yu
Venue: International Conference on Information and Knowledge Management
Tldr: {'model': 'tldr@v2.0.0', 'text': None}
Url: https://arxiv.org/pdf/2308.14029
Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases
Authors: Zhenghao Liu, Sen Mei, Chenyan Xiong, Xiaohua Li, Shi Yu, Zhiyuan Liu, Yu Gu, Ge Yu
Section: ACKNOWLEDGMENTS
This work is supported by the Natural Science Foundation of China under Grant No. 62206042, No. 62137001, No. 61991404, and No. 62272093, the Fundamental Research Funds for the Central Universities under Grant No. N2216013, China Postdoctoral Science Foundation under Grant No. 2022M710022, and National Science and Technology Major Project (J2019-IV-0002-0069).
Answer: "
"What is the zero shot top-100 accuracy achieved by the chain-of-skills model on the dev set of HotpotQA?
","['eric nyberg_61354e45bca908ad08f24e44bd507b4e1c958e6f_content_2.txt', 'eric nyberg_61354e45bca908ad08f24e44bd507b4e1c958e6f_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the zero shot top-100 accuracy achieved by the chain-of-skills model on the dev set of HotpotQA?

Context: Title: Chain-of-Skills: A Configurable Model for Open-Domain Question Answering
Authors: Kaixin Ma, Hao Cheng, Yu Zhang, Xiaodong Liu, Eric Nyberg, Jianfeng Gao
Section: B Experimental Details
is optimized using Adam with the initial learning rate of 1e-4. The final checkpoint is used for fine-tuning later. Finetuning When initializing from pretrained COS, the weights mapping for the first 5 experts are illustrated in Figure 3 and the last expert is initialized from BERT-base-uncased. For all experiments, we train models for 40 epochs with the batch size of 192, the learning rate of 2e-5, and the max sequence length of 256. During training, each batch only contains training data for one of the skills from one dataset, thus the model can effectively benefit from the in-batch negatives. To train the entity span proposal skill, we use the same data as entity linking. In particular, we route the data to span proposal experts 20% of the time otherwise the data go through entity linking experts. B.3 Inference Details Zero-shot-evaluation We directly use the single retrieval skill to find the top100 documents and compute the results in Table 1. Supervised and Cross-dataset For NQ, EntityQuestions and SQuAD, the reasoning path has a length of 1, i.e., only single passages. We use both single retrieval and linking skills to find a total of top 1000 passages first, and then reduce the set to top 100 using the reranking skill. Both HotpotQA and OTT-QA have reasoning paths with max length 2.
Title: Chain-of-Skills: A Configurable Model for Open-Domain Question Answering
Authors: Kaixin Ma, Hao Cheng, Yu Zhang, Xiaodong Liu, Eric Nyberg, Jianfeng Gao
Section: C Question Answering Results
extraction loss and Ls is the supporting sentence prediction loss. During training, we sample 0-2 positive passages and 0-2 negative passages from the top 100 chains returned by COS, and the model encodes at most 3 passages, i.e., the passage chain structure is not preserved and the passages are sampled independently. We train the model for 20,000 steps with the batch size of 128, the learning rate of 5e-5, the layer-wise learning rate decay of 0.9, the max answer length of 30, the max question length of 64, and the max sequence length of 512. For inference, the model ranks top 100 passage chains with structure preserved. We sum the scores of the two passages in every chain and subtract the dynamic threshold score and sort the chains based on this final score. Next, we train a reader model that only learns answer extraction and supporting sentence prediction. We only train the model using the two gold passages with the following loss weighting. Lreader = La + 0.5× Ls (11) The model uses the same set of hyperparameters as the path reranker except that the batch size is reduced to 32. At inference time, the model directly read the top 1 prediction returned by the path reranker. Both models here are initialized from Electra-large. C.2 Results The NQ results are presented in Table A2. Overall, our model achieves a similar performance as our own FiE baseline. FiE baseline uses the reader data released by the FiD-KD model, which has an R100 of 89.3 (vs 90.2 of COS). Considering that the gap between our method and FiD-KD model’s top 100 retrieval recall is relatively small, this result is not surprising. The HotpotQA results are shown in Table A3. Overall our results are similar to previous SOTA methods on the dev set. At the time of the paper submission, we have not got the test set results on the leaderboard. We adopted DPR evaluation scripts 2for all the retrieval evaluations and MDR evaluation scripts 3 for all the reader evaluations.
Answer: "
"In the paper ""Surveying (Dis)Parities and Concerns of Compute Hungry NLP Research"", what three topics did the survey investigate regarding concerns about PLMs?
","['emma strubell_667ba2e8f1933b6c32e9672012526904b4c5dc31_metadata.txt', 'emma strubell_667ba2e8f1933b6c32e9672012526904b4c5dc31_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the paper ""Surveying (Dis)Parities and Concerns of Compute Hungry NLP Research"", what three topics did the survey investigate regarding concerns about PLMs?

Context: Faculty Name: emma strubell
Paperid: 667ba2e8f1933b6c32e9672012526904b4c5dc31
Title: Surveying (Dis)Parities and Concerns of Compute Hungry NLP Research
Year: 2023
Abstract: Many recent improvements in NLP stem from the development and use of large pre-trained language models (PLMs) with billions of parameters. Large model sizes makes computational cost one of the main limiting factors for training and evaluating such models; and has raised severe concerns about the sustainability, reproducibility, and inclusiveness for researching PLMs. These concerns are often based on personal experiences and observations. However, there had not been any large-scale surveys that investigate them. In this work, we provide a first attempt to quantify these concerns regarding three topics, namely, environmental impact, equity, and impact on peer reviewing. By conducting a survey with 312 participants from the NLP community, we capture existing (dis)parities between different and within groups with respect to seniority, academia, and industry; and their impact on the peer reviewing process. For each topic, we provide an analysis and devise recommendations to mitigate found disparities, some of which already successfully implemented. Finally, we discuss additional concerns raised by many participants in free-text responses.
Authors: Ji-Ung Lee, Haritz Puerto, Betty van Aken, Yuki Arase, J. Forde, Leon Derczynski, Andreas Ruckl'e, Iryna Gurevych, Roy Schwartz, Emma Strubell, Jesse Dodge
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work captures existing (dis)parities between different and within groups with respect to seniority, academia, and industry; and their impact on the peer reviewing process; and provides an analysis and devise recommendations to mitigate found disparities.'}
Url: http://arxiv.org/pdf/2306.16900
Title: Surveying (Dis)Parities and Concerns of Compute Hungry NLP Research
Authors: Ji-Ung Lee, Haritz Puerto, Betty van Aken, Yuki Arase, Jessica Zosa Forde, Leon Derczynski, Andreas Rücklé, Iryna Gurevych, Roy Schwartz, Emma Strubell, Jesse Dodge
Section: Acknowledgements
This work was initiated at and benefited substantially from the Dagstuhl Seminar 22232: Efficient and Equitable Natural Language Processing in the Age of Deep Learning. We further thank Niranjan Balasubramanian, Jonathan Frankle, Michael Hassid, Kenneth Heafield, Sara Hooker, Alexander Koller, Alexandra Sasha Luccioni, Alexander Löser, André F. T. Martins, Colin Raffel, Nils Reimers, Leonardo Riberio, Anna Rogers, Edwin Simpson, Noam Slonim, Noah A. Smith, and Thomas Wolf for a fruitful discussion and helpful feedback at the seminar. We further thank Leshem Choshen for helpful feedback on this work.
Answer: "
"What two LTI professors were on the ""Making Scalable Meta Learning Practical"" paper?
","['emma strubell_a815c3209e7baff4466dbf6e129129511f842b7e_metadata.txt', 'eric xing_a815c3209e7baff4466dbf6e129129511f842b7e_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What two LTI professors were on the ""Making Scalable Meta Learning Practical"" paper?

Context: Faculty Name: emma strubell
Paperid: a815c3209e7baff4466dbf6e129129511f842b7e
Title: Making Scalable Meta Learning Practical
Year: 2023
Abstract: Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and 2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains.
Authors: Sang Keun Choe, Sanket Vaibhav Mehta, Hwijeen Ahn, W. Neiswanger, Pengtao Xie, Emma Strubell, Eric P. Xing
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients.'}
Url: https://arxiv.org/pdf/2310.05674
Faculty Name: eric xing
Paperid: a815c3209e7baff4466dbf6e129129511f842b7e
Title: Making Scalable Meta Learning Practical
Year: 2023
Abstract: Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and 2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains.
Authors: Sang Keun Choe, Sanket Vaibhav Mehta, Hwijeen Ahn, W. Neiswanger, Pengtao Xie, Emma Strubell, Eric P. Xing
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients.'}
Url: https://arxiv.org/pdf/2310.05674
Answer: "
"What was the previous name for the Language Technology Institute?
","['lori_levin.txt', 'miis-handbook_2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What was the previous name for the Language Technology Institute?

Context: Lori Levin
Research Professor, Language Technologies Institute
Research Area
Corpus Annotation and Resources, Machine Translation, Natural Language Processing and Computational Linguistics
Research
There are more than 6,000 human languages, but less than a hundred of them have robust language technologies such as search engines, spell checkers or speech recognition. Nevertheless, speakers of the technologically poor languages may have uses for language technologies to access information related to education, politics, business, weather and health conditions — not to mention preservation of culture and community relationships. My work focuses on the linguistic aspects of language technologies, specifically the following: 

Language Technologies With Low Resources
Many languages lack sufficient data for supervised or unsupervised machine learning. There may also be low-resource scenarios for technology-rich languages like English in specialized styles or subject areas.  Such cases may call for hybrids of human-engineered knowledge and machine learning. The human engineered knowledge can be in the form of handwritten rules, priors or feature engineering.  

Language Universals and Typology
When there is insufficient time or data to build an NLP system, it is useful to fall back on what is known about human languages in general or what is known about related languages. The field of linguistic typology and universals provides expectations for what languages might be like. We are exploring how to use typology and universals to develop language technologies for new languages on short timelines or in low-resource scenarios. 

Corpus Annotation and Linguistic Resources
When time and data are available, language technologies can be based on supervised learning from annotated data. The annotations may be for any level of linguistic knowledge from sounds to social hierarchies. My approach to annotation is based on the linguistic theory of construction grammar.

Personal Website http://www.cs.cmu.edu/~lsl/
Contact 5717 —Gates & Hillman Centers
Email lsl@cs.cmu.edu
412-268-6193
MIIS Graduate Student Handbook 
Page 8 
 
The Language Technologies Institute is located primarily on the 5th and 6th floors of the Gates 
Hillman Complex (GHC) on Carnegie Mellon’s Pittsburgh campus.  The address and contact 
information of the LTI are: 
Language Technologies Institute 
Carnegie Mellon University 
5000 Forbes Avenue 
Gates Hillman Complex 5404, LTI 
Pittsburgh, PA 15241-3891 
 
412-268-6591 (phone) 
412-268-6298 (fax) 
 
http://www.lti.cs.cmu.edu/ 
1.5 Carnegie Mellon University Statement of Assurance  
Carnegie Mellon University does not discriminate in admission, employment or administration 
of its programs or activities on the basis of race, color, national origin, sex, handicap or disability, 
age, sexual orientation, gender identity, religion, creed, ancestry, belief, veteran status or genetic 
information. Furthermore, Carnegie Mellon University does not discriminate and is required 
not to discriminate in violation of federal, state, or local laws or executive orders. 
  
Inquiries concerning the application of and compliance with this statement should be directed 
to the university ombudsperson, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, 
PA 15213, telephone 412-268-1018.  Obtain general information about Carnegie Mellon 
University by calling 412-268-2000. 
  
Carnegie Mellon University publishes an annual campus security and fire safety report 
describing the university's security, alcohol and drug, sexual assault, and fire safety policies, and 
containing statistics about the number and type of crimes committed on the campus, and the 
number and cause of fires in campus residence facilities during the preceding three years. You 
can obtain a copy by contacting the Carnegie Mellon Police Department at 412-268-2323. The 
annual 
security 
and 
fire 
safety 
report 
also 
is 
available 
online 
at www.cmu.edu/police/annualreports. 
 
Information regarding the application of Title IX, including to admission and employment 
decisions, the sexual misconduct grievance procedures and process, including how to file a 
report or a complaint of sex discrimination, how to file a report of sexual harassment, and how 
the university responds to such reports is available at www.cmu.edu/title-ix.
Answer: "
"In spring 2024, What is the title of course 10735?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, What is the title of course 10735?

Context: In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Independent Study: Research' with Course ID 10620 and Section A offers 612 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Shah located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Generative AI' with Course ID 10623 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Gormley, Li, Lipton located in Building GHC, Room 4401.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Practicum' with Course ID 10635 and Section I offers VAR units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Shah located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Reading and Research' with Course ID 10697 and Section A offers 1-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Shah located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Introduction to Machine Learning (PhD)' with Course ID 10701 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Chai located in Building TEP, Room 1403.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Advanced Deep Learning' with Course ID 10707 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Salakhutdinov located in Building DH, Room 2302.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Independent Study: Research' with Course ID 10620 and Section A offers 612 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Shah located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Generative AI' with Course ID 10623 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Gormley, Li, Lipton located in Building GHC, Room 4401.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Practicum' with Course ID 10635 and Section I offers VAR units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Shah located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Reading and Research' with Course ID 10697 and Section A offers 1-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Shah located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Introduction to Machine Learning (PhD)' with Course ID 10701 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Chai located in Building TEP, Room 1403.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Advanced Deep Learning' with Course ID 10707 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Salakhutdinov located in Building DH, Room 2302.
Answer: "
"In spring 2024, What is the course number for Independent Study: Research?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, What is the course number for Independent Study: Research?

Context: In Semester Spring 2024, from the department of Music, the subject titled 'Independent Study in Theory' with Course ID 57612 and Section J offers 3-9 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Music, the subject titled 'Independent Study in Research' with Course ID 57613 and Section A offers 3-9 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Cardenes located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Music, the subject titled 'Independent Study in Research' with Course ID 57613 and Section B offers 3-9 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Woloshyn located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Music, the subject titled 'Independent Study in Research' with Course ID 57613 and Section C offers 3-9 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Randall located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Music, the subject titled 'Independent Study in Research' with Course ID 57613 and Section D offers 3-9 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Stern located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Music, the subject titled 'Independent Study in Research' with Course ID 57613 and Section E offers 3-9 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Pukinskis located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Music, the subject titled 'Independent Study in Theory' with Course ID 57612 and Section J offers 3-9 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Music, the subject titled 'Independent Study in Research' with Course ID 57613 and Section A offers 3-9 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Cardenes located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Music, the subject titled 'Independent Study in Research' with Course ID 57613 and Section B offers 3-9 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Woloshyn located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Music, the subject titled 'Independent Study in Research' with Course ID 57613 and Section C offers 3-9 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Randall located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Music, the subject titled 'Independent Study in Research' with Course ID 57613 and Section D offers 3-9 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Stern located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Music, the subject titled 'Independent Study in Research' with Course ID 57613 and Section E offers 3-9 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Pukinskis located in Building DNM, Room DNM.
Answer: "
"What 11-6XX courses were not taught by LTI faculty in Spring 2024?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What 11-6XX courses were not taught by LTI faculty in Spring 2024?

Context: In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'LTI Colloquium' with Course ID 11700 and Section P offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Lisbon, Portugal location,led by experienced instructor Instructor TBA located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Advanced Natural Language Processing:' with Course ID 11711 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Advanced Natural Language Processing' with Course ID 11711 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Neubig located in Building TEP, Room 1403.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Lab in Natural Language Processing: Self-Paced' with Course ID 11712 and Section A offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Frederking located in Building TBA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Grammar Formalisms' with Course ID 11722 and Section A offers 12.0 units. The Class meets Monday Wednesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin located in Building WEH, Room 4707.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Linguistics Lab: Self-Paced' with Course ID 11723 and Section A offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin located in Building TBA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'LTI Colloquium' with Course ID 11700 and Section P offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Lisbon, Portugal location,led by experienced instructor Instructor TBA located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Advanced Natural Language Processing:' with Course ID 11711 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Advanced Natural Language Processing' with Course ID 11711 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Neubig located in Building TEP, Room 1403.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Lab in Natural Language Processing: Self-Paced' with Course ID 11712 and Section A offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Frederking located in Building TBA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Grammar Formalisms' with Course ID 11722 and Section A offers 12.0 units. The Class meets Monday Wednesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin located in Building WEH, Room 4707.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Linguistics Lab: Self-Paced' with Course ID 11723 and Section A offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin located in Building TBA, Room None.
Answer: "
"What professor was the last author on ""Deriving Vocal Fold Oscillation Information from Recorded Voice Signals Using Models of Phonation""?
","['rita singh_721b39472c801124b5e3102edffe9d6f0754e1c2_metadata.txt', 'rita singh_papers.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What professor was the last author on ""Deriving Vocal Fold Oscillation Information from Recorded Voice Signals Using Models of Phonation""?

Context: Faculty Name: rita singh
Paperid: 721b39472c801124b5e3102edffe9d6f0754e1c2
Title: Deriving Vocal Fold Oscillation Information from Recorded Voice Signals Using Models of Phonation
Year: 2023
Abstract: During phonation, the vocal folds exhibit a self-sustained oscillatory motion, which is influenced by the physical properties of the speaker’s vocal folds and driven by the balance of bio-mechanical and aerodynamic forces across the glottis. Subtle changes in the speaker’s physical state can affect voice production and alter these oscillatory patterns. Measuring these can be valuable in developing computational tools that analyze voice to infer the speaker’s state. Traditionally, vocal fold oscillations (VFOs) are measured directly using physical devices in clinical settings. In this paper, we propose a novel analysis-by-synthesis approach that allows us to infer the VFOs directly from recorded speech signals on an individualized, speaker-by-speaker basis. The approach, called the ADLES-VFT algorithm, is proposed in the context of a joint model that combines a phonation model (with a glottal flow waveform as the output) and a vocal tract acoustic wave propagation model such that the output of the joint model is an estimated waveform. The ADLES-VFT algorithm is a forward-backward algorithm which minimizes the error between the recorded waveform and the output of this joint model to estimate its parameters. Once estimated, these parameter values are used in conjunction with a phonation model to obtain its solutions. Since the parameters correlate with the physical properties of the vocal folds of the speaker, model solutions obtained using them represent the individualized VFOs for each speaker. The approach is flexible and can be applied to various phonation models. In addition to presenting the methodology, we show how the VFOs can be quantified from a dynamical systems perspective for classification purposes. Mathematical derivations are provided in an appendix for better readability.
List of 2023 Open Access papers by rita singh are:
Implementing International Federation of Gynecology and Obstetrics Nutrition Checklist for Pregnant Women: Opportunities and Challenges in Low- and Middle-income Countries
Mean Platelet Volume in Type 2 Diabetes: Correlation with Poor Glycaemic Control
Gonadotropin Receptor Cross-Talk and Altered Functions in Gonadal and Non-Gonadal Tissues
BASS: Block-wise Adaptation for Speech Summarization
Rethinking Voice-Face Correlation: A Geometry View
A Gene-Based Algorithm for Identifying Factors That May Affect a Speaker’s Voice
Deriving Vocal Fold Oscillation Information from Recorded Voice Signals Using Models of Phonation
Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations
The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features
Pengi: An Audio Language Model for Audio Tasks
GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content
Token Prediction as Implicit Classification to Identify LLM-Generated Text
Effect of myo-inositol and di-chiro inositol plus vitamin D supplementation during pregnancy on prevention of gestational diabetes: a multi-centric, prospective, randomized, double-blind clinical trial
Plant-Avian Frugivory in the Urban Ecosystem of Delhi
LoFT: Local Proxy Fine-tuning For Improving Transferability Of Adversarial Attacks Against Large Language Model
Evaluating Speech Synthesis by Training Recognizers on Synthetic Speech
Rethinking Audiovisual Segmentation with Semantic Quantization and Decomposition
Towards Noise-Tolerant Speech-Referring Video Object Segmentation: Bridging Speech and Text
Importance of negative sampling in weak label learning
Completing Visual Objects via Bridging Generation and Segmentation
Training Audio Captioning Models without Audio
Prompting Audios Using Acoustic Properties For Emotion Representation
Pairwise Similarity Learning is SimPLE
Comparison of freeze-thaw and sonication cycle-based methods for extracting AMR-associated metabolites from Staphylococcus aureus
APPLIED ASPECT OF SATVAVAJAYA CHIKITSA
Utilization of the Whole Cowpea Pod and Barley Husk in The Production of Nutritionally Enriched Composite Flour
Answer: "
"The first doctorate at Carnegie Tech was awarded in 1919. Who was it awarded to and in what discipline was it in?
","['cmuhistory_d402.txt', 'cmuhistory_d402.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: The first doctorate at Carnegie Tech was awarded in 1919. Who was it awarded to and in what discipline was it in?

Context: The first U.S. drama degree was awarded in 1914 at Carnegie Tech. It began offering graduate degrees. In 1919, the first doctorate (in civil engineering) was awarded to Mao Yisheng, a student from China. It laid the groundwork for a research institution, recruiting leading scientists, offering sponsored fellowships with government and industry leaders and pioneering nontraditional interdisciplinary research, which brought together physicists, chemists and metallurgists, for example. Interdisciplinary research would become the hallmark of Carnegie Mellon research. It initiated the 'Carnegie Plan' in 1938, a new curriculum that required science and engineer students to take courses in humanities and social sciences in order to better understand the needs of society. Carnegie died in 1919, but his vision for an educated public lived on after him. Carnegie Tech - Post-war Years With the end of World War II, the latter half of the 20th century brought unprecedented growth to Carnegie Tech. In 1956, the arrival of the first IBM computer to campus was revolutionary, initiating a university culture where information technology pervaded virtually all areas of study. University culture also changed in 1973 when Margaret Morrison closed and women joined their male peers in classrooms and dorms. The times were changing, and Tech positioned itself at the forefront, opening three new schools: 1948: The Graduate School of Industrial Administration, later renamed the David A. Tepper School of Business, focusing on quantitative analysis and pioneering the field of management science. 1968: School of Urban and Public Affairs, later renamed the H. John Heinz III College, providing graduate training for work in the public sector. 1986: School of Computer Science, pioneering computing and artificial intelligence, led by interdisciplinary efforts of Allen Newell and Herbert Simon. Carnegie Mellon University In 1967, Carnegie Tech merged with the Mellon Institute, a science research center founded by the Mellon family of Pittsburgh, to become known as Carnegie Mellon University. The merger built upon a long history of support from the Mellons. It allowed Carnegie Mellon to establish the last of its current pillars: the Mellon College of Science and the College of Humanities and Social Sciences, now known as Marianna Brown Dietrich College of Humanities and Social Sciences.
History - CMU - Carnegie Mellon University Carnegie Mellon University ——— Search Search CMU › About › History Andrew Carnegie A self-educated ""working boy"" who loved books, Andrew Carnegie emigrated from Scotland in 1848 and settled in Pittsburgh, Pa. Attending night school and borrowing books, Carnegie went from factory worker in a textile mill to successful entrepreneur and industrialist. He rose to prominence by founding what became the world's largest steel producing company by the end of the 19th century. Carnegie Technical Schools At one point the richest man in the world, Carnegie believed that ""to die rich is to die disgraced."" He turned his attention to writing, social activism and philanthropy, determined to establish educational opportunities for the general public where few existed. In 1900, he donated $1 million for the creation of a technical institute for the city of Pittsburgh, envisioning a school where working-class men and women of Pittsburgh could learn practical skills, trades and crafts that would enhance their careers, lives and communities. ""My heart is in the work,"" he stated, which would become part of the school's official motto. The Carnegie Technical Schools offered two- and three-year certificates in the arts as well as in engineering disciplines and included a college for women, Margaret Morrison Carnegie College. Carnegie Tech – Early Years Soon faced with the demand for baccalaureate programs, Carnegie Technical Schools began offering bachelor's degrees through its College of Engineering and College of Fine Arts, becoming the Carnegie Institute of Technology, or ""Carnegie Tech."" During the first half of the 20th century, with support from Andrew Carnegie and other funders, Carnegie Tech laid the foundation for a school on the cutting edge. Some key developments were: It expanded from two buildings into an elegant 20th century campus designed in the beaux arts architectural style, housing a wealth of machine shops, studios and laboratories — the hands-on center of learning that persists today. It pioneered conservatory degree programs in music and drama, in addition to visual art and design programs. The first U.S. drama degree was awarded in 1914 at Carnegie Tech. It began offering graduate degrees. In 1919, the first doctorate (in civil engineering) was awarded to Mao Yisheng, a student from China.
Answer: "
"In the paper ""Decoder-only Architecture for Speech Recognition with CTC Prompts and Text Data Augmentation, what is the reduction in word error rates achieved by the proposed models on Switchboard?
","['shinji watanabe_06353e1b7e7c8dc701ac76dcd4db5061b24468c9_metadata.txt', 'shinji watanabe_fa5ebb425c57f6c4f1c36a7200ef1da867346e8c_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the paper ""Decoder-only Architecture for Speech Recognition with CTC Prompts and Text Data Augmentation, what is the reduction in word error rates achieved by the proposed models on Switchboard?

Context: Faculty Name: shinji watanabe
Paperid: 06353e1b7e7c8dc701ac76dcd4db5061b24468c9
Title: Decoder-only Architecture for Speech Recognition with CTC Prompts and Text Data Augmentation
Year: 2023
Abstract: Collecting audio-text pairs is expensive; however, it is much easier to access text-only data. Unless using shallow fusion, end-to-end automatic speech recognition (ASR) models require architecture modifications or additional training schemes to use text-only data. Inspired by recent advances in decoder-only language models (LMs), such as GPT-3 and PaLM adopted for speech-processing tasks, we propose using a decoder-only architecture for ASR with simple text augmentation. To provide audio information, encoder features compressed by CTC prediction are used as prompts for the decoder, which can be regarded as refining CTC prediction using the decoder-only model. Because the decoder architecture is the same as an autoregressive LM, it is simple to enhance the model by leveraging external text data with LM training. An experimental comparison using LibriSpeech and Switchboard shows that our proposed models with text augmentation training reduced word error rates from ordinary CTC by 0.3% and 1.4% on LibriSpeech test-clean and testother set, respectively, and 2.9% and 5.0% on Switchboard and CallHome. The proposed model had advantage on computational efficiency compared with conventional encoder-decoder ASR models with a similar parameter setup, and outperformed them on the LibriSpeech 100h and Switchboard training scenarios.
Authors: E. Tsunoo, Hayato Futami, Yosuke Kashiwagi, Siddhant Arora, Shinji Watanabe
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work proposes using a decoder-only architecture for ASR with simple text augmentation training that had advantage on computational efficiency compared with conventional encoder-decoder ASR models with a similar parameter setup, and outperformed them on the LibriSpeech 100h and Switchboard training scenarios.'}
Url: https://arxiv.org/pdf/2309.08876
Faculty Name: shinji watanabe
Paperid: fa5ebb425c57f6c4f1c36a7200ef1da867346e8c
Title: Speech collage: code-switched audio generation by collaging monolingual corpora
Year: 2023
Abstract: Designing effective automatic speech recognition (ASR) systems for Code-Switching (CS) often depends on the availability of the transcribed CS resources. To address data scarcity, this paper introduces Speech Collage, a method that synthesizes CS data from monolingual corpora by splicing audio segments. We further improve the smoothness quality of audio generation using an overlap-add approach. We investigate the impact of generated data on speech recognition in two scenarios: using in-domain CS text and a zero-shot approach with synthesized CS text. Empirical results highlight up to 34.4% and 16.2% relative reductions in Mixed-Error Rate and Word-Error Rate for in-domain and zero-shot scenarios, respectively. Lastly, we demonstrate that CS augmentation bolsters the model's code-switching inclination and reduces its monolingual bias.
Authors: A. Hussein, Dorsa Zeinali, Ondrej Klejch, Matthew Wiesner, Brian Yan, Shammur A. Chowdhury, Ahmed Ali, Shinji Watanabe, S. Khudanpur
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': ""Speech Collage is introduced, a method that synthesizes CS data from monolingual corpora by splicing audio segments that improves the smoothness quality of audio generation using an overlap-add approach and demonstrates that CS augmentation bolsters the model's code-switching inclination and reduces itsmonolingual bias.""}
Url: https://arxiv.org/pdf/2309.15674
Answer: "
"What are the names of the people from LTI who co-authored the paper COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive Statements?
","['maarten sap_185ace5661963e2e1eb998e739e4110272a6bb43_metadata.txt', 'maarten sap_185ace5661963e2e1eb998e739e4110272a6bb43_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the names of the people from LTI who co-authored the paper COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive Statements?

Context: Faculty Name: maarten sap
Paperid: 185ace5661963e2e1eb998e739e4110272a6bb43
Title: COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive Statements
Year: 2023
Abstract: Warning: This paper contains content that may be offensive or upsetting. Understanding the harms and offensiveness of statements requires reasoning about the social and situational context in which statements are made. For example, the utterance""your English is very good""may implicitly signal an insult when uttered by a white man to a non-white colleague, but uttered by an ESL teacher to their student would be interpreted as a genuine compliment. Such contextual factors have been largely ignored by previous approaches to toxic language detection. We introduce COBRA frames, the first context-aware formalism for explaining the intents, reactions, and harms of offensive or biased statements grounded in their social and situational context. We create COBRACORPUS, a dataset of 33k potentially offensive statements paired with machine-generated contexts and free-text explanations of offensiveness, implied biases, speaker intents, and listener reactions. To study the contextual dynamics of offensiveness, we train models to generate COBRA explanations, with and without access to the context. We find that explanations by context-agnostic models are significantly worse than by context-aware ones, especially in situations where the context inverts the statement's offensiveness (29% accuracy drop). Our work highlights the importance and feasibility of contextualized NLP by modeling social factors.
Authors: Xuhui Zhou, Haojie Zhu, Akhila Yerukola, Thomas Davidson, Jena D. Hwang, Swabha Swayamdipta, Maarten Sap
Venue: Annual Meeting of the Association for Computational Linguistics
Tldr: {'model': 'tldr@v2.0.0', 'text': 'COBRA frames are introduced, the first context-aware formalism for explaining the intents, reactions, and harms of offensive or biased statements grounded in their social and situational context, and the importance and feasibility of contextualized NLP by modeling social factors are highlighted.'}
Url: http://arxiv.org/pdf/2306.01985
Title: COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive Statements
Authors: Xuhui Zhou, Hao Zhu, Akhila Yerukola, Thomas Davidson, Jena D. Hwang, Swabha Swayamdipta, Maarten Sap
Section: C GPT-3 prompts used in this paper
The example prompts for generating likely contexts are in Figure 8. The example prompts for generating adversarial contexts are in Figure 9. The example prompts for generating the likely explanations are in Figure 10. 11https://github.com/huggingface/transformers
Answer: "
"Which two NLP tasks were applied with the NLPositionality framework in the study?
","['maarten sap_a66ff335f5934fe7503a99d3eb3abed493994df1_metadata.txt', 'graham neubig_9bce3661f01825ad56dc9d2b3d254fd9e3792360_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which two NLP tasks were applied with the NLPositionality framework in the study?

Context: Faculty Name: maarten sap
Paperid: a66ff335f5934fe7503a99d3eb3abed493994df1
Title: NLPositionality: Characterizing Design Biases of Datasets and Models
Year: 2023
Abstract: Design biases in NLP systems, such as performance differences for different populations, often stem from their creator’s positionality, i.e., views and lived experiences shaped by identity and background. Despite the prevalence and risks of design biases, they are hard to quantify because researcher, system, and dataset positionality is often unobserved. We introduce NLPositionality, a framework for characterizing design biases and quantifying the positionality of NLP datasets and models. Our framework continuously collects annotations from a diverse pool of volunteer participants on LabintheWild, and statistically quantifies alignment with dataset labels and model predictions. We apply NLPositionality to existing datasets and models for two tasks—social acceptability and hate speech detection. To date, we have collected 16,299 annotations in over a year for 600 instances from 1,096 annotators across 87 countries.We find that datasets and models align predominantly with Western, White, college-educated, and younger populations. Additionally, certain groups, such as non-binary people and non-native English speakers, are further marginalized by datasets and models as they rank least in alignment across all tasks. Finally, we draw from prior literature to discuss how researchers can examine their own positionality and that of their datasets and models, opening the door for more inclusive NLP systems.
Authors: Sebastin Santy, Jenny T Liang, Ronan Le Bras, Katharina Reinecke, Maarten Sap
Venue: Annual Meeting of the Association for Computational Linguistics
Tldr: {'model': 'tldr@v2.0.0', 'text': 'NLPositionality, a framework for characterizing design biases and quantifying the positionality of NLP datasets and models, is introduced and it is found that dataset and models align predominantly with Western, White, college-educated, and younger populations.'}
Url: http://arxiv.org/pdf/2306.01943
Faculty Name: graham neubig
Paperid: 9bce3661f01825ad56dc9d2b3d254fd9e3792360
Title: Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach
Year: 2023
Abstract: Humans work together to solve common problems by having discussions, explaining, and agreeing or disagreeing with each other. Similarly, if a system can have discussions with humans when solving tasks, it can improve the system's performance and reliability. In previous research on explainability, it has only been possible for the system to make predictions and for humans to ask questions about them rather than having a mutual exchange of opinions. This research aims to create a dataset and computational framework for systems that discuss and refine their predictions through dialogue. Through experiments, we show that the proposed system can have beneficial discussions with humans improving the accuracy by up to 25 points in the natural language inference task.
Authors: Masahiro Kaneko, Graham Neubig, Naoaki Okazaki
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This research aims to create a dataset and computational framework for systems that discuss and refine their predictions through dialogue and shows that the proposed system can have beneficial discussions with humans improving the accuracy by up to 25 points in the natural language inference task.'}
Url: http://arxiv.org/pdf/2305.11789
Answer: "
"What are the number of units for 11797?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the number of units for 11797?

Context: In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Project Course: Conversational Systems' with Course ID 11754 and Section A offers 6.0 units. The Class meets Tuesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rudnicky located in Building BH, Room 154A.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Large-Scale Multi-Media Analysis:' with Course ID 11775 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'TBA' with Course ID 11775 and Section A offers 12.0 units. The Class meets Monday Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hauptmann located in Building GHC, Room 4102.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Multimodal Machine Learning' with Course ID 11777 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Bisk located in Building PH, Room 100.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning' with Course ID 11785 and Section Lec 1 offers 12.0 units. The Class meets Monday Wednesday between 08:00AM and 09:20AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ramakrishnan, Singh located in Building BH, Room A51.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning' with Course ID 11785 and Section Lec 2 offers 12.0 units. The Class meets Monday Wednesday between 08:00AM and 09:20AM ET.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Project Course: Conversational Systems' with Course ID 11754 and Section A offers 6.0 units. The Class meets Tuesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rudnicky located in Building BH, Room 154A.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Large-Scale Multi-Media Analysis:' with Course ID 11775 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'TBA' with Course ID 11775 and Section A offers 12.0 units. The Class meets Monday Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hauptmann located in Building GHC, Room 4102.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Multimodal Machine Learning' with Course ID 11777 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Bisk located in Building PH, Room 100.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning' with Course ID 11785 and Section Lec 1 offers 12.0 units. The Class meets Monday Wednesday between 08:00AM and 09:20AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ramakrishnan, Singh located in Building BH, Room A51.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning' with Course ID 11785 and Section Lec 2 offers 12.0 units. The Class meets Monday Wednesday between 08:00AM and 09:20AM ET.
Answer: "
"In the paper ""KIT’s Multilingual Speech Translation System for IWSLT 2023"", what approach was used for effective adaptation in the absence of training data from the target domain?
","['shinji watanabe_b4855ff933fb80846638469a1b43c1766df85d78_metadata.txt', 'shinji watanabe_d43338451cd8676548811e1ff8f9c92ea987c5bd_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the paper ""KIT’s Multilingual Speech Translation System for IWSLT 2023"", what approach was used for effective adaptation in the absence of training data from the target domain?

Context: Faculty Name: shinji watanabe
Paperid: b4855ff933fb80846638469a1b43c1766df85d78
Title: The Pipeline System of ASR and NLU with MLM-based data Augmentation Toward Stop Low-Resource Challenge
Year: 2023
Abstract: This paper describes our system for the low-resource domain adaptation track (Track 3) in Spoken Language Understanding Grand Challenge, which is a part of ICASSP Signal Processing Grand Challenge 2023. In the track, we adopt a pipeline approach of ASR and NLU. For ASR, we fine-tune Whisper for each domain with upsampling. For NLU, we fine-tune BART on all the Track3 data and then on low-resource domain data. We apply masked LM (MLM) -based data augmentation, where some of input tokens and corresponding target labels are replaced using MLM. We also apply a retrieval-based approach, where model input is augmented with similar training samples. As a result, we achieved exact match (EM) accuracy 63.3/75.0 (average: 69.15) for reminder/weather domain, and won the 1st place at the challenge.
Authors: Hayato Futami, Jessica Huynh, Siddhant Arora, Shih-Lun Wu, Yosuke Kashiwagi, Yifan Peng, Brian Yan, E. Tsunoo, Shinji Watanabe
Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This system for the low-resource domain adaptation track (Track 3) in Spoken Language Understanding Grand Challenge, which is a part of ICASSP Signal Processing Grand Challenge 2023, adopts a pipeline approach of ASR and NLU and applies masked LM (MLM) -based data augmentation.'}
Url: https://arxiv.org/pdf/2305.01194
Title: REPRODUCING WHISPER-STYLE TRAINING USING AN OPEN-SOURCE TOOLKIT AND PUBLICLY AVAILABLE DATA
Authors: Yifan Peng, Jinchuan Tian, Brian Yan, Dan Berrebbi, Xuankai Chang, Xinjian Li, Jiatong Shi, Siddhant Arora, William Chen, Roshan Sharma, Wangyou Zhang, Yui Sudo, Muhammad Shakeel, Jee-weon Jung, Soumi Maiti, Shinji Watanabe
Section: 7. REFERENCES
An Evolving, MultiDomain ASR Corpus with 10,000 Hours of Transcribed Audio,” in Proc. Interspeech, 2021. [26] Vassil Panayotov et al., “Librispeech: An ASR corpus based on public domain audio books,” in ICASSP, 2015. [27] Roldano Cattoni et al., “Must-c: A multilingual corpus for end-to-end speech translation,” Computer speech & language, vol. 66, pp. 101155, 2021. [28] Patrick K O’Neill et al., “Spgispeech: 5,000 hours of transcribed financial audio for fully formatted end-to-end speech recognition,” arXiv:2104.02014, 2021. [29] François Hernandez et al., “Ted-lium 3: Twice as much data and corpus repartition for experiments on speaker adaptation,” in Speech & Computer, 2018, pp. 198–208. [30] Rong Ye et al., “Gigast: A 10,000-hour pseudo speech translation corpus,” arXiv:2204.03939, 2022. [31] Vineel Pratap et al., “Mls: A large-scale multilingual dataset for speech research,” arXiv:2012.03411, 2020. [32] Binbin Zhang et al., “Wenetspeech: A 10000+ hours multidomain mandarin corpus for speech recognition,” in Proc. ICASSP, 2022. [33] “aidatatang 200zh, a free Chinese Mandarin speech corpus by Beijing DataTang Technology Co., Ltd,” .
Answer: "
"What is the title of the paper that proposes a novel re-ranker model abbreviated FiT5?
","['chenyan xiong_275da3802142fc42f6fab2ce2104223b2e0ef40d_metadata.txt', 'eric xing_5eceb4d435f1eb86d9b211ffc1f76c87f79aa2c8_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the title of the paper that proposes a novel re-ranker model abbreviated FiT5?

Context: Faculty Name: chenyan xiong
Paperid: 275da3802142fc42f6fab2ce2104223b2e0ef40d
Title: Fusion-in-T5: Unifying Document Ranking Signals for Improved Information Retrieval
Year: 2023
Abstract: Common IR pipelines are typically cascade systems that may involve multiple rankers and/or fusion models to integrate different information step-by-step. In this paper, we propose a novel re-ranker named Fusion-in-T5 (FiT5), which integrates document text information, retrieval features, and global document information into a single unified model using templated-based input and global attention. Experiments on passage ranking benchmarks MS MARCO and TREC DL show that FiT5 significantly improves ranking performance over prior pipelines. Analyses find that through global attention, FiT5 is able to jointly utilize the ranking features via gradually attending to related documents, and thus improve the detection of subtle nuances between them. Our code will be open-sourced.
Authors: S. Yu, Cheng-Chung Fan, Chenyan Xiong, David Jin, Zhiyuan Liu, Zhenghao Liu Tsinghua University, Huazhong University of Science, Technology, Microsoft Research, M. I. O. Technology, N. University
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A novel re-ranker named Fusion-in-T5 (FiT5), which integrates document text information, retrieval features, and global document information into a single unified model using templated-based input and global attention, is proposed.'}
Url: http://arxiv.org/pdf/2305.14685
Faculty Name: eric xing
Paperid: 5eceb4d435f1eb86d9b211ffc1f76c87f79aa2c8
Title: Practical Probabilistic Model-based Deep Reinforcement Learning by Integrating Dropout Uncertainty and Trajectory Sampling
Year: 2023
Abstract: This paper addresses the prediction stability, prediction accuracy and control capability of the current probabilistic model-based reinforcement learning (MBRL) built on neural networks. A novel approach dropout-based probabilistic ensembles with trajectory sampling (DPETS) is proposed where the system uncertainty is stably predicted by combining the Monte-Carlo dropout and trajectory sampling in one framework. Its loss function is designed to correct the fitting error of neural networks for more accurate prediction of probabilistic models. The state propagation in its policy is extended to filter the aleatoric uncertainty for superior control capability. Evaluated by several Mujoco benchmark control tasks under additional disturbances and one practical robot arm manipulation task, DPETS outperforms related MBRL approaches in both average return and convergence velocity while achieving superior performance than well-known model-free baselines with significant sample efficiency. The open source code of DPETS is available at https://github.com/mrjun123/DPETS.
Authors: Wenjun Huang, Yunduan Cui, Huiyun Li, Xin Wu
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'Evaluated by several Mujoco benchmark control tasks under additional disturbances and one practical robot arm manipulation task, DPETS outperforms related MBRL approaches in both average return and convergence velocity while achieving superior performance than well-known model-free baselines with significant sample efficiency.'}
Url: https://arxiv.org/pdf/2309.11089
Answer: "
"Which CMU professor was on the ""Efficiency Pentathlon: A Standardized Arena for Efficiency Evaluation"" paper?
","['emma strubell_84d20ad9f42d80dfd5130a6362d5422be8a6bdc3_metadata.txt', 'emma strubell_84d20ad9f42d80dfd5130a6362d5422be8a6bdc3_content_2.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which CMU professor was on the ""Efficiency Pentathlon: A Standardized Arena for Efficiency Evaluation"" paper?

Context: Faculty Name: emma strubell
Paperid: 84d20ad9f42d80dfd5130a6362d5422be8a6bdc3
Title: Efficiency Pentathlon: A Standardized Arena for Efficiency Evaluation
Year: 2023
Abstract: Rising computational demands of modern natural language processing (NLP) systems have increased the barrier to entry for cutting-edge research while posing serious environmental concerns. Yet, progress on model efficiency has been impeded by practical challenges in model evaluation and comparison. For example, hardware is challenging to control due to disparate levels of accessibility across different institutions. Moreover, improvements in metrics such as FLOPs often fail to translate to progress in real-world applications. In response, we introduce Pentathlon, a benchmark for holistic and realistic evaluation of model efficiency. Pentathlon focuses on inference, which accounts for a majority of the compute in a model's lifecycle. It offers a strictly-controlled hardware platform, and is designed to mirror real-world applications scenarios. It incorporates a suite of metrics that target different aspects of efficiency, including latency, throughput, memory overhead, and energy consumption. Pentathlon also comes with a software library that can be seamlessly integrated into any codebase and enable evaluation. As a standardized and centralized evaluation platform, Pentathlon can drastically reduce the workload to make fair and reproducible efficiency comparisons. While initially focused on natural language processing (NLP) models, Pentathlon is designed to allow flexible extension to other fields. We envision Pentathlon will stimulate algorithmic innovations in building efficient models, and foster an increased awareness of the social and environmental implications in the development of future-generation NLP models.
Authors: Hao Peng, Qingqing Cao, Jesse Dodge, Matthew E. Peters, Jared Fernandez, Tom Sherborne, Kyle Lo, Sam Skjonsberg, Emma Strubell, Darrell Plessas, Iz Beltagy, Pete Walsh, Noah A. Smith, Hannaneh Hajishirzi
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': ""Pentathlon is a benchmark for holistic and realistic evaluation of model efficiency, which focuses on inference, which accounts for a majority of the compute in a model's lifecycle, and is designed to mirror real-world applications scenarios.""}
Url: https://arxiv.org/pdf/2307.09701
Title: Efficiency Pentathlon: A Standardized Arena for Efficiency Evaluation
Authors: Hao Peng, Qingqing Cao, Jesse Dodge, Matthew E. Peters, Jared Fernandez, Tom Sherborne, Kyle Lo, Sam Skjonsberg, Emma Strubell, Darrell Plessas, Iz Beltagy, Evan Pete Walsh, Noah A. Smith, Hannaneh Hajishirzi
Section: 3 Experiments
BLEU score, it takes a substantial hit in efficiency. Interestingly, despite being more than four times larger, WMT19-Meta achieves efficiency performance comparable to OPUS in latency, memory overhead, and energy consumption, and significantly outperforms it in terms of BLEU. However, it falls short of OPUS in throughput. This observation confirms that relying on a single efficiency metric risks oversimplifying the complex performance landscape of efficiency in practical applications. With ONNX, the models achieve over 20% improvements in latency and throughput in the singlestream scenario, accompanied by a significant reduction in memory and energy overhead. However, less efficiency improvement is observed in other scenarios with larger batch sizes. Larger models benefit more from FP16 quantization. By comparing Figures 2a and 2b, we observe that FP16 quantization improves all models’ efficiency performance (except #Params.), particularly memory overhead. Larger models appear to benefit more from quantization. As shown in Figures 2c and 2d, while OPUS experiences minimal efficiency gains from quantization apart from increased throughput, WMT21-Meta’s efficiency dramatically improves with FP16 quantization, nearly doubling throughput and reducing latency, memory overhead, and energy consumption by half or more. These results highlight the promise of advancing quantization techniques for larger models in order to improve the trade-off between accuracy and efficiency.
Answer: "
"In spring 2024, What is the deadline for withdrawing from a semester course with a withdrawal grade assigned?
","['Meta-Data-Calendar-2324.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, What is the deadline for withdrawing from a semester course with a withdrawal grade assigned?

Context: Day: 'Monday', Event: 'Semester course drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Spring 2024 (S24)'
Date: '2024-03-01', Day: 'Friday', Event: 'Mini-3 Last Day of Classes ', Semester: 'Spring 2024 (S24)'
Date: '2024-03-01', Day: 'Friday', Event: 'Mini-3 voucher deadline (4)', Semester: 'Spring 2024 (S24)'
Date: '2024-03-02', Day: 'Saturday', Event: 'Mini-3 Exams', Semester: 'Spring 2024 (S24)'
Date: '2024-03-02', Day: 'Saturday', Event: 'Mini-3 Faculty Course Evaluations close', Semester: 'Spring 2024 (S24)'
Start Date: '2024-03-04', End Date: '2024-03-08', Days: 'Monday to Friday', Event: 'Spring Break; No Classes', Semester: 'Spring 2024 (S24)'
Date: '2024-03-11', Day: 'Monday', Event: 'First day of Mini-4 Classes', Semester: 'Spring 2024 (S24)'
Date: '2024-03-11', Day: 'Monday', Event: 'Mid-Semester & Mini-3 grades due by 4 pm', Semester: 'Spring 2024 (S24)'
Date: '2024-03-12', Day: 'Tuesday', Event: 'Summer 2024 Registration Opens ', Semester: 'Spring 2024 (S24)'
Date: '2024-03-15', Day: 'Friday', Event: 'Mini-4 add, audit & tuition adjustment drop deadline (1)', Semester: 'Spring 2024 (S24)'
Date: '2024-04-01', Day: 'Monday', Event: 'Semester pass/no pass & withdrawal deadline (3)', Semester: 'Spring 2024 (S24)'
Date: '2024-04-03', Day: 'Wednesday', Event: 'Mini-4 course drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Spring 2024 (S24)'
Start Date: '2024-04-11', End Date: '2024-04-13', Days: 'Thursday to Saturday', Event: 'Spring Carnival;
On Monday, 31 March, 2025, during the Spring 2025 (S25) semester, Semester pass/no pass & withdrawal deadline is observed.
On Wednesday, 02 April, 2025, during the Spring 2025 (S25) semester, Mini-4 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 03 April, 2025,Thursday to 05 April, 2023,Saturday marks the Spring Carnival for Spring 2025 (S25) semester which leads to No Classes.
From 07 April, 2025,Monday to 11 April, 2023,Friday marks the Fall 2025 Registration Week for Spring 2025 (S25) semester.
On Monday, 14 April, 2025, during the Spring 2025 (S25) semester, Mini-4 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 14 April, 2025, during the Spring 2025 (S25) semester, Semester & Mini-4 Faculty Course Evaluations open is observed.
On Friday, 25 April, 2025, during the Spring 2025 (S25) semester, Last Day of Classes is observed.
On Friday, 25 April, 2025, during the Spring 2025 (S25) semester, Semester & Mini-4 voucher deadline (4) is observed.
From 28 April, 2025,Monday to 29 April, 2023,Tuesday marks the Final Examinations for Spring 2025 (S25) semester.
On Wednesday, 30 April, 2025, during the Spring 2025 (S25) semester, Reading Day is observed.
From 01 May, 2025,Thursday to 02 May, 2023,Friday marks the Final Examinations for Spring 2025 (S25) semester.
From 03 May, 2025,Saturday to 04 May, 2023,Sunday marks the Reading Days for Spring 2025 (S25) semester.
On Monday, 05 May, 2025, during the Spring 2025 (S25) semester, Final Examinations is observed.
On Monday, 05 May, 2025, during the Spring 2025 (S25) semester, Semester & Mini-4 Faculty Course Evaluations close is observed.
Answer: "
"What are the SCS CMU classes grading standard for max GPA?
","['miis-handbook_2023-2024.txt', 'mcds-student-handbook-2023_2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the SCS CMU classes grading standard for max GPA?

Context: 2.1.4 Computers for MS Students 
Students are expected to provide their own laptop computers that can be used to access 
university resources and complete course assignments.  Laptops running Windows, MacOS, 
and Linux software are all acceptable. 
Master’s students will be given a CS user id.  A CS user id is required to use the LTI computer 
cluster, and other SCS services.  The School of Computer Science has a Help Center located at 
MIIS Graduate Student Handbook 
Page 11 
 
GHC 4201.  They can be contacted at help@cs.cmu.edu, extension 8-4231 from a campus 
phone, or 412-268-4231 from an outside line. 
MS students will be given access to the LTI’s computer cluster on an as-needed basis, to be 
used for course assignments, directed study projects, and/or the capstone project.  The LTI 
cluster provides storage and computation for projects involving large datasets and/or lengthy 
computation.  
3 CMU Degree Completion and Certification 
3.1 Standard Degree Requirements & Degree Certification 
Carnegie Mellon graduate students are expected to complete their degree requirements within 
the standard length of time for their program of study as outlined in the relevant Graduate 
Student Handbook. Standard program lengths for graduate students vary significantly ranging 
from two semesters for some full-time master’s programs to several or more years for doctoral 
programs. Upon completion of the graduate program degree requirements, the degree will be 
certified by the student’s academic program in the semester in which the student completes 
the requirements. 
Early Completion 
Graduate students who consider the completion of all degree requirements in less than the 
standard length of time for their program of study may consult with their degree-granting 
program or department to determine if early degree certification is allowed and under what 
circumstances. 
 
Extended or Longer-than-Standard Completion 
Longer-than-standard degree completion may occur due to academic interruptions in making 
progress toward the degree as defined by the academic program, interruptions of full-time study 
or progress towards the degree due to serious, documented medical issues, or other unusual or 
unforeseen circumstances. 
 
Master’s students who require longer than the standard time to complete their degree 
requirements are expected to remain in close contact with their graduate program, and will be 
certified at the end of the semester in which they have completed their degree requirements.
5. Secure signatures of both the student and the supervisor. Return the form to the 
MCDS administrator in order to obtain approval for the independent study from the 
Director. 
 
Independent study contracts must be submitted no later than on the last day of the first 
week of classes in a given semester. 
3.3.14 Double counting courses 
No course may be used to complete two MCDS degree requirements, nor may a course 
satisfy requirements in two degree programs. 
3.3.15 Courses outside of the School of Computer Science 
Elective courses in other Schools at Carnegie Mellon may be taken with prior permission 
of the Director. 
3.3.16 Grades 
All courses offered by the SCS CMU are graded on the 4.3 grading standard  
http://www.cmu.edu/policies/documents/Grades.html. MCDS students must maintain a 
3.0 overall average each semester to remain in good standing. A student must obtain a B- 
or better grade in all courses, which count towards core requirements. If a student 
 
 
19
receives a C- or better, that course may count as an elective towards the degree 
requirements. All courses must receive a letter grade; courses taken pass/fail do not 
count towards the MCDS degree. 
 
Enrollment Services is the only University office that can provide an official letter of 
enrollment, official transcript and enrollment verification. Enrollment verification can 
be requested online through The HUB at: https://www.cmu.edu/hub/registrar/student-
records/verifications/ 
3.3.17 Student Review, Academic Probation and Academic Actions 
The MCDS program conducts an academic progress review at the conclusion of each 
semester in order to monitor individual student progress towards graduation regarding 
the fulfillment of curricular requirements, course grades, and academic integrity. Should 
a student’s effort fall below the acceptable level of academic performance and/or fail to 
meet standards and policies established by Carnegie Mellon University, the student may 
be dismissed from the program. 
 
Infractions 
 
After each academic progress review, each student will receive a letter indicating the 
result of the review and their standing in the program.
Answer: "
"How much does it cost to apply for the MLT program if an application is submitted on November 20th, 2023?
","['mlt-student-handbook-2023-2024.txt', 'mlt-student-handbook-2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How much does it cost to apply for the MLT program if an application is submitted on November 20th, 2023?

Context: It is to the student's advantage to avoid switching advisors, especially late in 
their graduate studies, because forging a strong student-advisor relationship takes time. 
4.3 Optional Masters Thesis 
MLT students may optionally elect to complete a Masters Thesis; it is not required. Students 
who intend to apply for the PhD program should consider the MLT Thesis as one of several 
ways to demonstrate their research potential. Other possibilities include innovative research in 
a research project or class project work. 
The MLT Thesis must be completed according to the following guidelines. Note in particular the 
October 31 proposal deadline! 
 There are two course components required for students who elect to complete the M.S. 
Thesis: 11-928  Masters Thesis I (with the chosen thesis advisor, typically for 12 units, 
typically in the Fall of their second year) and 11-929  Masters Thesis II in the following 
Spring. 
 Each of these two courses counts towards 6 units of MLT-level LTI course credit; if the 
student goes on to an LTI PhD, the second course (11-929) counts towards one PhD lab 
requirement.   
 The MLT thesis may focus on either a significant research result or a significant 
implementation of a new technology. The student should work with their advisor to 
define an interesting but bounded thesis research project. The MLT thesis culminates in 
MLT Graduate Student Handbook 
Page 16 
 
a publication-quality paper that clearly describes the work in terms of its research 
significance. 
 The MLT thesis committee will be composed of a thesis advisor and two additional 
readers, to be chosen by the student in consultation with the advisor, during the Fall 
Masters Thesis I course. At least 2 of the 3 committee members must be SCS faculty 
working in Language Technologies. 
 To receive a final grade for the first semester's course, the student must have prepared 
a written proposal of at least five pages, describing the scientific thesis to be 
investigated, any relevant research already completed by the student, a comparison 
with related work by others, and a detailed description of the work to be done in the 
next semester. The proposal MUST be accepted by the committee by the end of 
October, or the student will not be allowed to enroll in 11-929  Masters Thesis II!
MLT Graduate Student Handbook 
Page 1 
 
 
 
 
 
 
 
 
 
Master of Language Technologies 
 
Student Handbook 
2023 - 2024 
 
 
 
 
 
 
Revised:  July 26th, 2023 
MLT Graduate Student Handbook 
Page 2 
 
Contents 
1 
Introduction . 6 
1.1 
Welcome . 6 
1.2 
The MLT Degree . 6 
1.3 
MLT Contact Information . 7 
1.4 
University Policies and Expectations . 8 
1.5 
Carnegie Mellon University Statement of Assurance . 8 
1.6 
The Carnegie Mellon Code . 9 
1.7 
Vision . 9 
1.8 
Mission . 10 
1.9 
Consensual Intimate Relationship Policy Regarding Undergraduate Students . 10 
2 
The Language Technologies Institute . 10 
2.1 
Mailboxes & Office Supplies . 10 
2.2 
Photocopies and Printers . 10 
2.3 
Office Space for MS Students . 11 
2.4 
Computers for MS Students . 11 
3 
Masters Degree Completion and Certification . 11 
3.1 
Standard Degree Requirements and Degree Certification . 11 
3.1.1 
Graduate Students . 11 
3.1.2 
Early Completion . 11 
3.1.3 
Extended or Longer-than-Standard Completion (Statute of Limitations) . 12 
3.1.4 
Additional Guidance for Students . 12 
4 
MLT Degree Attainment . 13 
4.1 
Course Requirements . 13 
4.1.1 
Grade Requirements . 14 
4.1.2 
Research Speaking Requirement . 14 
4.1.3 
Withdrawal of a Degree . 14 
4.2 
Advising . 14 
4.3 
Optional Masters Thesis . 15 
4.4 
Definitions of LTI Terminology . 17 
4.5 
Recommended Electives outside of SCS . 17 
4.6 
LTI Orientation . 18 
MLT Graduate Student Handbook 
Page 3 
 
4.7 
End of Semester Evaluation . 18 
4.8 
Enrollment Verification . 18 
4.9 
University Policies on Grades and Grading . 19 
4.9.1 
University Policy on Grades .
Answer: "
"In the Paaploss paper, in which speech enhancement workflows did the proposed method show improvement?
","['bhiksha raj_611f9ee6eef0936462cd78f371798d0699951c59_metadata.txt', 'shinji watanabe_611f9ee6eef0936462cd78f371798d0699951c59_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the Paaploss paper, in which speech enhancement workflows did the proposed method show improvement?

Context: Faculty Name: bhiksha raj
Paperid: 611f9ee6eef0936462cd78f371798d0699951c59
Title: Paaploss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement
Year: 2023
Abstract: Despite rapid advancement in recent years, current speech enhancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics. We identify temporal acoustic parameters – such as spectral tilt, spectral flux, shimmer, etc. – that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-specific weights for each feature, as the acoustic parameters are known to show different behavior in different phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement workflows in both time-domain and time-frequency domain, as measured by standard evaluation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement.
Authors: Muqiao Yang, Joseph Konan, David Bick, YUNYANG ZENG, Shuo Han, Anurag Kumar, Shinji Watanabe, B. Raj
Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A learning objective is proposed that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics, and a neural network estimator is developed that can accurately predict their time-series values across an utterance.'}
Url: https://arxiv.org/pdf/2302.08095
Faculty Name: shinji watanabe
Paperid: 611f9ee6eef0936462cd78f371798d0699951c59
Title: Paaploss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement
Year: 2023
Abstract: Despite rapid advancement in recent years, current speech enhancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics. We identify temporal acoustic parameters – such as spectral tilt, spectral flux, shimmer, etc. – that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-specific weights for each feature, as the acoustic parameters are known to show different behavior in different phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement workflows in both time-domain and time-frequency domain, as measured by standard evaluation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement.
Authors: Muqiao Yang, Joseph Konan, David Bick, YUNYANG ZENG, Shuo Han, Anurag Kumar, Shinji Watanabe, B. Raj
Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A learning objective is proposed that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics, and a neural network estimator is developed that can accurately predict their time-series values across an utterance.'}
Url: https://arxiv.org/pdf/2302.08095
Answer: "
"Which LTI professors wrote ""Rethinking Voice-Face Correlation: A Geometry View""?
","['bhiksha raj_5a3307b2e64bbcaff1202e261b8a83f7d03418a8_content_0.txt', 'rita singh_5a3307b2e64bbcaff1202e261b8a83f7d03418a8_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which LTI professors wrote ""Rethinking Voice-Face Correlation: A Geometry View""?

Context: Title: Rethinking Voice-Face Correlation: A Geometry View
Authors: Xiang Li, Yandong Wen, Muqiao Yang, Jinglu Wang, Rita Singh, Bhiksha Raj, Mohamed bin Zayed
Section: 5 CONCLUSION
In conclusion, this paper presents a novel approach to exploring the voice-face correlation by focusing on the geometric aspects of the face rather than relying on semantic cues such as gender, age, and emotion. The proposed voice-anthropometric measurement (AM)face paradigm identifies predictable facial AMs from the voice to guide 3D face reconstruction, which results in significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium. This approach not only eliminates the influence of unpredictable AMs but also offers a new perspective on voice-face correlation, which can be valuable for anthropometry science. The results of this study open up possibilities for future research in this area, such as developing more accurate voice-guided face synthesis techniques and a better understanding of the relationship between voice and facial geometry. Rethinking Voice-Face Correlation: A Geometry View Conference’23, July 2023, Ottawa, Canada
Title: Rethinking Voice-Face Correlation: A Geometry View
Authors: Xiang Li, Yandong Wen, Muqiao Yang, Jinglu Wang, Rita Singh, Bhiksha Raj, Mohamed bin Zayed
Section: 5 CONCLUSION
In conclusion, this paper presents a novel approach to exploring the voice-face correlation by focusing on the geometric aspects of the face rather than relying on semantic cues such as gender, age, and emotion. The proposed voice-anthropometric measurement (AM)face paradigm identifies predictable facial AMs from the voice to guide 3D face reconstruction, which results in significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium. This approach not only eliminates the influence of unpredictable AMs but also offers a new perspective on voice-face correlation, which can be valuable for anthropometry science. The results of this study open up possibilities for future research in this area, such as developing more accurate voice-guided face synthesis techniques and a better understanding of the relationship between voice and facial geometry. Rethinking Voice-Face Correlation: A Geometry View Conference’23, July 2023, Ottawa, Canada
Answer: "
"What city is the Language Technologies Institute at Carnegie Mellon University located in?
","['handbook_phd_2023-2024.txt', 'miis-handbook_2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What city is the Language Technologies Institute at Carnegie Mellon University located in?

Context: Graduate Student Handbook 
Page 9 
 
1 
 Introduction 
Welcome to the Language Technologies Institute, a graduate department in the School of 
Computer Science at Carnegie Mellon University. You have joined a Ph.D. program that focuses 
on excellence, creativity, and training the next generation of research leaders. While the next few 
years will be a time of hard work and intense concentration, we hope that your time at the LTI 
will also be rewarding professionally and personally. 
Obtaining a Ph.D. is a long and often intense journey. A healthy work-life balance helps to keep it 
in perspective. Carnegie Mellon is located in Pittsburgh, a city with a rich industrial and labor 
heritage. In the past, it was the heart of the U.S. steel industry, a source for quality packaged foods, 
an important financial hub, and an early pioneer in railroads, radio, and nuclear power. This is a 
city where people worked hard and made things that changed the world. Today, Pittsburgh has 
reinvented itself as a leader in Computer Science and medicine, but it is still a place where people 
work hard and make things that change the world. It is also an informal city, where people dont 
take themselves too seriously and remember to have fun. It is home to excellent museums, 
competitive professional sports teams, inexpensive magic shows, and free music during the 
summer, with skiing and magnificent state parks nearby. Your stay here will be more rewarding 
if you make time for exploring the many activities that Pittsburgh and Southwest Pennsylvania 
have to offer. 
There are significant differences between CMU's different departments and degree programs in 
philosophical approach, procedures, policies, and regulations. Each department issues a 
handbook that informs graduate students of their program requirements and procedures and 
ensures that students have written access to standard information. This handbook describes the 
policies, procedures, and requirements for the Ph.D. in Language and Information Technologies. 
All policies not explicitly described in this document conform to School of Computer Science 
(SCS) policies and university policies. For information about Carnegie Mellon requirements and 
policies, please see the universitys handbook The Word, the Office of Graduate and Postdoctoral 
Affairs web page, the Office of the Dean of Students web page, and other resources contained in 
Appendix A of this handbook. 
Welcome!
MIIS Graduate Student Handbook 
Page 8 
 
The Language Technologies Institute is located primarily on the 5th and 6th floors of the Gates 
Hillman Complex (GHC) on Carnegie Mellon’s Pittsburgh campus.  The address and contact 
information of the LTI are: 
Language Technologies Institute 
Carnegie Mellon University 
5000 Forbes Avenue 
Gates Hillman Complex 5404, LTI 
Pittsburgh, PA 15241-3891 
 
412-268-6591 (phone) 
412-268-6298 (fax) 
 
http://www.lti.cs.cmu.edu/ 
1.5 Carnegie Mellon University Statement of Assurance  
Carnegie Mellon University does not discriminate in admission, employment or administration 
of its programs or activities on the basis of race, color, national origin, sex, handicap or disability, 
age, sexual orientation, gender identity, religion, creed, ancestry, belief, veteran status or genetic 
information. Furthermore, Carnegie Mellon University does not discriminate and is required 
not to discriminate in violation of federal, state, or local laws or executive orders. 
  
Inquiries concerning the application of and compliance with this statement should be directed 
to the university ombudsperson, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, 
PA 15213, telephone 412-268-1018.  Obtain general information about Carnegie Mellon 
University by calling 412-268-2000. 
  
Carnegie Mellon University publishes an annual campus security and fire safety report 
describing the university's security, alcohol and drug, sexual assault, and fire safety policies, and 
containing statistics about the number and type of crimes committed on the campus, and the 
number and cause of fires in campus residence facilities during the preceding three years. You 
can obtain a copy by contacting the Carnegie Mellon Police Department at 412-268-2323. The 
annual 
security 
and 
fire 
safety 
report 
also 
is 
available 
online 
at www.cmu.edu/police/annualreports. 
 
Information regarding the application of Title IX, including to admission and employment 
decisions, the sexual misconduct grievance procedures and process, including how to file a 
report or a complaint of sex discrimination, how to file a report of sexual harassment, and how 
the university responds to such reports is available at www.cmu.edu/title-ix.
Answer: "
"Which LTI professor's paper introduced the TASTE algorithm that maps items and users in an embedding space and recommends items by matching their text representations?
","['chenyan xiong_159100c8323fc558e4073a3a006f3f243aca3a60_metadata.txt', 'chenyan xiong_159100c8323fc558e4073a3a006f3f243aca3a60_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which LTI professor's paper introduced the TASTE algorithm that maps items and users in an embedding space and recommends items by matching their text representations?

Context: Faculty Name: chenyan xiong
Paperid: 159100c8323fc558e4073a3a006f3f243aca3a60
Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases
Year: 2023
Abstract: This paper proposes Text mAtching based SequenTial rEcommenda-tion model (TASTE), which maps items and users in an embedding space and recommends items by matching their text representations. TASTE verbalizes items and user-item interactions using identifiers and attributes of items. To better characterize user behaviors, TASTE additionally proposes an attention sparsity method, which enables TASTE to model longer user-item interactions by reducing the self-attention computations during encoding. Our experiments show that TASTE outperforms the state-of-the-art methods on widely used sequential recommendation datasets. TASTE alleviates the cold start problem by representing long-tail items using full-text modeling and bringing the benefits of pretrained language models to recommendation systems. Our further analyses illustrate that TASTE significantly improves the recommendation accuracy by reducing the popularity bias of previous item id based recommendation models and returning more appropriate and text-relevant items to satisfy users. All codes are available at https://github.com/OpenMatch/TASTE.
Authors: Zhenghao Liu, Senkun Mei, Chenyan Xiong, Xiaohua Li, Shi Yu, Zhiyuan Liu, Yu Gu, Ge Yu
Venue: International Conference on Information and Knowledge Management
Tldr: {'model': 'tldr@v2.0.0', 'text': None}
Url: https://arxiv.org/pdf/2308.14029
Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases
Authors: Zhenghao Liu, Sen Mei, Chenyan Xiong, Xiaohua Li, Shi Yu, Zhiyuan Liu, Yu Gu, Ge Yu
Section: 5.3 Effectiveness of Item Verbalization Methods
more to memorize some characteristics of users, such as the taste, preferred cuisines, and active area of users. Besides, the prompt-based modeling method (Prompt) outperforms the embedding-based method (Embed), which illustrates that pretrained language models have the ability to understand the user/item identifiers and establish relevance between users and items via identifiers. It further supports the motivation of TASTE, which fully uses the learned knowledge from pretrained language models to build sequential recommendation systems. Evaluation on Recommendation Behaviors. Finally, we explore the recommendation behaviors of TASTE using different item modeling methods. As shown in Table 7, three models, T5-ID, TASTE w/o ID, and TASTE, are compared. T5-ID randomly initializes item embeddings and directly predicts the item ids. TASTE w/o ID and TASTE employ a two-tower architecture [29] and encode items using attributes and identifiers & attributes, respectively. As shown in our evaluation results, T5-ID returns an average of 49.5% of popular products in the recommendation results of all datasets, showing that it faces the popularity bias problem during recommending items. TASTE alleviates the popularity bias by reducing on average 18.75% popular items in its recommendation results. It represents items using full texts and utilizes text matching to calibrate the popularity-oriented recommendation behavior of T5ID. TASTE demonstrates its effectiveness in recommending more appropriate and text-relevant items by achieving higher Bleu scores and Recall scores. Besides, compared with TASTE w/o ID, TASTE achieves higher Bleu and Dist scores with the help of item ids. It shows that the item ids can serve as a kind of prompt to provide additional matching signals beyond item attributes to better model the relevance between users and items.
Answer: "
"In spring 2024, Who are the instructors for course 17313?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, Who are the instructors for course 17313?

Context: In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Foundations of Software Engineering' with Course ID 17313 and Section A offers 12.0 units. The Class meets Monday between 09:00AM and 09:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hilton, Feo Flushing located in Building POS, Room 147.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Foundations of Software Engineering' with Course ID 17313 and Section B offers 12.0 units. The Class meets Monday between 10:00AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hilton, Feo Flushing located in Building PH, Room 126A.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Foundations of Software Engineering' with Course ID 17313 and Section C offers 12.0 units. The Class meets Monday between 11:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hilton, Feo Flushing located in Building HOA, Room 107.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Foundations of Software Engineering' with Course ID 17313 and Section D offers 12.0 units. The Class meets Monday between 12:00PM and 12:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Feo Flushing, Hilton located in Building POS, Room 147.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Foundations of Software Engineering' with Course ID 17313 and Section E offers 12.0 units. The Class meets Monday between 01:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hilton, Feo Flushing located in Building PH, Room 126A.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Foundations of Software Engineering' with Course ID 17313 and Section F offers 12.0 units.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Foundations of Software Engineering' with Course ID 17313 and Section A offers 12.0 units. The Class meets Monday between 09:00AM and 09:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hilton, Feo Flushing located in Building POS, Room 147.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Foundations of Software Engineering' with Course ID 17313 and Section B offers 12.0 units. The Class meets Monday between 10:00AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hilton, Feo Flushing located in Building PH, Room 126A.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Foundations of Software Engineering' with Course ID 17313 and Section C offers 12.0 units. The Class meets Monday between 11:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hilton, Feo Flushing located in Building HOA, Room 107.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Foundations of Software Engineering' with Course ID 17313 and Section D offers 12.0 units. The Class meets Monday between 12:00PM and 12:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Feo Flushing, Hilton located in Building POS, Room 147.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Foundations of Software Engineering' with Course ID 17313 and Section E offers 12.0 units. The Class meets Monday between 01:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hilton, Feo Flushing located in Building PH, Room 126A.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Foundations of Software Engineering' with Course ID 17313 and Section F offers 12.0 units.
Answer: "
"When was the final application deadline for the PhD program? Give your answer in dd/mm/yyyy.
","['mlt-student-handbook-2023-2024.txt', 'mlt-student-handbook-2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When was the final application deadline for the PhD program? Give your answer in dd/mm/yyyy.

Context: It is to the student's advantage to avoid switching advisors, especially late in 
their graduate studies, because forging a strong student-advisor relationship takes time. 
4.3 Optional Masters Thesis 
MLT students may optionally elect to complete a Masters Thesis; it is not required. Students 
who intend to apply for the PhD program should consider the MLT Thesis as one of several 
ways to demonstrate their research potential. Other possibilities include innovative research in 
a research project or class project work. 
The MLT Thesis must be completed according to the following guidelines. Note in particular the 
October 31 proposal deadline! 
 There are two course components required for students who elect to complete the M.S. 
Thesis: 11-928  Masters Thesis I (with the chosen thesis advisor, typically for 12 units, 
typically in the Fall of their second year) and 11-929  Masters Thesis II in the following 
Spring. 
 Each of these two courses counts towards 6 units of MLT-level LTI course credit; if the 
student goes on to an LTI PhD, the second course (11-929) counts towards one PhD lab 
requirement.   
 The MLT thesis may focus on either a significant research result or a significant 
implementation of a new technology. The student should work with their advisor to 
define an interesting but bounded thesis research project. The MLT thesis culminates in 
MLT Graduate Student Handbook 
Page 16 
 
a publication-quality paper that clearly describes the work in terms of its research 
significance. 
 The MLT thesis committee will be composed of a thesis advisor and two additional 
readers, to be chosen by the student in consultation with the advisor, during the Fall 
Masters Thesis I course. At least 2 of the 3 committee members must be SCS faculty 
working in Language Technologies. 
 To receive a final grade for the first semester's course, the student must have prepared 
a written proposal of at least five pages, describing the scientific thesis to be 
investigated, any relevant research already completed by the student, a comparison 
with related work by others, and a detailed description of the work to be done in the 
next semester. The proposal MUST be accepted by the committee by the end of 
October, or the student will not be allowed to enroll in 11-929  Masters Thesis II!
The proposal MUST be accepted by the committee by the end of 
October, or the student will not be allowed to enroll in 11-929  Masters Thesis II! 
 Sometime during the semester when the student enrolls in 11-929  Masters Thesis II 
(typically, their final semester), the student will distribute a draft of the thesis to the 
committee for initial review. This should be done as early as feasible, to avoid last-
minute surprises that could delay final approval of the thesis. 
 The thesis work culminates in submission of the final version of the thesis document, 
followed by a public presentation of the work in an LTI seminar (or other suitable public 
forum). Since the defense is public, the LTI graduate program administrator must 
receive all the information required for a public announcement at least one week before 
the defense. The Masters Thesis presentation is somewhat less rigorous than a PhD 
thesis defense. The presentation must communicate the research work done, similar to 
a conference paper presentation. The committee will observe the presentation, and 
then decide whether the thesis and presentation were acceptable, or whether further 
work is required. Unlike a PhD defense, only a simple majority vote of the committee is 
required for approval. 
 Although students are required to enroll in the appropriate course sequence of two 
Masters Thesis courses, it is not required that students finish the thesis by the end of 
that second semester. If a student requires more time to revise the thesis to the 
committee's satisfaction, and adequately present the work, an incomplete grade will be 
assessed in the Masters Thesis course, until such time as the work and presentation are 
accepted. The student will still be allowed to walk in Spring Commencement, if all other 
requirements for the MLT degree have been completed. Students should note that any 
financial support beyond the end of the semester will be on a case-by-case basis, and 
must be arranged in advance with the project supporting them. Students are strongly 
encouraged to finish the thesis work within one (1) year following the semester they 
enroll for the first Masters Thesis course.
Answer: "
"In spring 2024, What is the location of course 10716?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, What is the location of course 10716?

Context: In Semester Spring 2024, from the department of Design, the subject titled 'Introduction to Photo Design' with Course ID 51132 and Section B offers 10.0 units. The Class meets Thursday between 02:00PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Vitone located in Building MM, Room 121.
In Semester Spring 2024, from the department of Design, the subject titled 'Design Studies: Futures' with Course ID 51176 and Section A3 offers 5.0 units. The Class meets Tuesday Thursday between 12:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Scupelli, Martin located in Building MM, Room 107.
In Semester Spring 2024, from the department of Design, the subject titled 'Design Studies: Futures' with Course ID 51176 and Section B3 offers 5.0 units. The Class meets Tuesday Thursday between 12:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Scupelli, Martin located in Building MM, Room 107.
In Semester Spring 2024, from the department of Design, the subject titled 'Design Studies: Experience' with Course ID 51178 and Section A4 offers 5.0 units. The Class meets Tuesday Thursday between 12:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Yasko located in Building MM, Room 107.
In Semester Spring 2024, from the department of Design, the subject titled 'Design Studies: Experience' with Course ID 51178 and Section B4 offers 5.0 units. The Class meets Tuesday Thursday between 12:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Yasko located in Building MM, Room 107.
In Semester Spring 2024, from the department of Design, the subject titled 'Communications Studio II: Designing Communications for Interactions' with Course ID 51228 and Section A offers 9.0 units. The Class meets Tuesday Thursday between 08:30AM and 10:20AM ET.
In Semester Spring 2024, from the department of Design, the subject titled 'Introduction to Photo Design' with Course ID 51132 and Section B offers 10.0 units. The Class meets Thursday between 02:00PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Vitone located in Building MM, Room 121.
In Semester Spring 2024, from the department of Design, the subject titled 'Design Studies: Futures' with Course ID 51176 and Section A3 offers 5.0 units. The Class meets Tuesday Thursday between 12:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Scupelli, Martin located in Building MM, Room 107.
In Semester Spring 2024, from the department of Design, the subject titled 'Design Studies: Futures' with Course ID 51176 and Section B3 offers 5.0 units. The Class meets Tuesday Thursday between 12:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Scupelli, Martin located in Building MM, Room 107.
In Semester Spring 2024, from the department of Design, the subject titled 'Design Studies: Experience' with Course ID 51178 and Section A4 offers 5.0 units. The Class meets Tuesday Thursday between 12:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Yasko located in Building MM, Room 107.
In Semester Spring 2024, from the department of Design, the subject titled 'Design Studies: Experience' with Course ID 51178 and Section B4 offers 5.0 units. The Class meets Tuesday Thursday between 12:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Yasko located in Building MM, Room 107.
In Semester Spring 2024, from the department of Design, the subject titled 'Communications Studio II: Designing Communications for Interactions' with Course ID 51228 and Section A offers 9.0 units. The Class meets Tuesday Thursday between 08:30AM and 10:20AM ET.
Answer: "
"The TASTE algorithm was introduced in what paper?
","['chenyan xiong_159100c8323fc558e4073a3a006f3f243aca3a60_metadata.txt', 'kemal oflazer_3b623333145c69cb29c63975213f7b3bac025954_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: The TASTE algorithm was introduced in what paper?

Context: Faculty Name: chenyan xiong
Paperid: 159100c8323fc558e4073a3a006f3f243aca3a60
Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases
Year: 2023
Abstract: This paper proposes Text mAtching based SequenTial rEcommenda-tion model (TASTE), which maps items and users in an embedding space and recommends items by matching their text representations. TASTE verbalizes items and user-item interactions using identifiers and attributes of items. To better characterize user behaviors, TASTE additionally proposes an attention sparsity method, which enables TASTE to model longer user-item interactions by reducing the self-attention computations during encoding. Our experiments show that TASTE outperforms the state-of-the-art methods on widely used sequential recommendation datasets. TASTE alleviates the cold start problem by representing long-tail items using full-text modeling and bringing the benefits of pretrained language models to recommendation systems. Our further analyses illustrate that TASTE significantly improves the recommendation accuracy by reducing the popularity bias of previous item id based recommendation models and returning more appropriate and text-relevant items to satisfy users. All codes are available at https://github.com/OpenMatch/TASTE.
Authors: Zhenghao Liu, Senkun Mei, Chenyan Xiong, Xiaohua Li, Shi Yu, Zhiyuan Liu, Yu Gu, Ge Yu
Venue: International Conference on Information and Knowledge Management
Tldr: {'model': 'tldr@v2.0.0', 'text': None}
Url: https://arxiv.org/pdf/2308.14029
Faculty Name: kemal oflazer
Paperid: 3b623333145c69cb29c63975213f7b3bac025954
Title: Abstractive summarization with deep reinforcement learning using semantic similarity rewards
Year: 2023
Abstract: 
 Abstractive summarization is an approach to document summarization that is not limited to selecting sentences from the document but can generate new sentences as well. We address the two main challenges in abstractive summarization: how to evaluate the performance of a summarization model and what is a good training objective. We first introduce new evaluation measures based on the semantic similarity of the input and corresponding summary. The similarity scores are obtained by the fine-tuned BERTurk model using either the cross-encoder or a bi-encoder architecture. The fine-tuning is done on the Turkish Natural Language Inference and Semantic Textual Similarity benchmark datasets. We show that these measures have better correlations with human evaluations compared to Recall-Oriented Understudy for Gisting Evaluation (ROUGE) scores and BERTScore. We then introduce a deep reinforcement learning algorithm that uses the proposed semantic similarity measures as rewards, together with a mixed training objective, in order to generate more natural summaries in terms of human readability. We show that training with a mixed training objective function compared to only the maximum-likelihood objective improves similarity scores.
Authors: Figen Beken Fikri, Kemal Oflazer, B. Yanikoglu
Venue: Natural Language Engineering
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A deep reinforcement learning algorithm is introduced that uses the proposed semantic similarity measures as rewards, together with a mixed training objective, in order to generate more natural summaries in terms of human readability.'}
Url: https://www.cambridge.org/core/services/aop-cambridge-core/content/view/740B4B5903AE80FE14709F5DAEE7AD41/S1351324923000505a.pdf/div-class-title-abstractive-summarization-with-deep-reinforcement-learning-using-semantic-similarity-rewards-div.pdf
Answer: "
"When was the first three-wheeled buggy introduced?
","['cmubuggy_d403.txt', 'cmubuggy_d403.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When was the first three-wheeled buggy introduced?

Context: Buggy Races Keep Rolling at Carnegie Mellon - News - Carnegie Mellon University Carnegie Mellon University ——— Search Search Search this site only News News › Stories › Archives › 2019 › April › Buggy Races Keep Rolling at Carnegie Mellon April 10, 2019 Buggy Races Keep Rolling at Carnegie Mellon In its 99th year, the tradition is a Spring Carnival treat By Heidi Opdyke opdyke(through)andrew.cmu.edu Media Inquiries Julie Mattera Marketing and Communications jmattera(through)cmu.edu 412-268-2902 Sweepstakes, also known as the Buggy Races, has come a long way at Carnegie Mellon University. The slick, torpedo-like vessels carrying drivers with nerves of steel are a far cry from the two-man teams that once changed places halfway through a race and rode in everything from rain barrels with bicycle wheels to three-wheeled ash cans 99 years ago. Today, it takes six people to maneuver the .84 -mile course around Schenley Park's Flagstaff Hill. But while five pushers and a driver navigate the course's hills, dozens of people are needed to make a successful race happen. A year of planning goes into just over two minutes of racing. The Machine The basics of a buggy are straightforward, but teams are often secretive in how they build the machines, in particular the way they brake, steer and what types of wheels are used. Each has a body, pushbar for runners to move the machine up the hills, wheels, a safety harness and driving and braking mechanisms. Some also include fairings, a type of housing around the wheels that help reduce drag, make the vehicle quieter and just looks cool. Fairings have been a key feature for the Fringe team in recent years, which is celebrating its 50th anniversary of Buggy. ""We have a reputation of being the quietest on the course,"" said Diya Nuxoll, who wrapped up her bachelor's degree in mechanical engineering in December and is working on an advanced studies master's degree in design and manufacturing. Nuxoll leads the Fringe team as one of two head mechanics. An mural celebrating Buggy can be found in CMU's Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions. Sweepstakes Slang Buggy: Vehicle being raced and also a nickname for the competition.
This year's buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It's one of the few times spectators can see buggies up close. Dave Singh explains how Fringe builds its buggies. Each team keeps its actual processes a secret, but each buggy has certain features such as a body, pushbar, wheels and driving and braking mechanisms. Behind Singh are other buggies built by Fringe. Behind the Wheel Drivers, have the closest connection to the vehicles, aside from the mechanics who spent countless hours building and maintaining individual buggies. While races are brief, each driver must log a number of practice runs to qualify to race. Teams practice nights and weekends throughout the fall and spring semesters as weather permits. Buggies are often built around drivers, so the fit can be snug, and drivers are often less than 5 feet, 3 inches in height. Most — but not all — are women. Tishya Girdhar, a junior in neuroscience, is social chair for Fringe. She came to CMU wanting to drive, but first she had to get over being claustrophobic. She started as a mechanic and got behind the wheel for last year's Sweepstakes. ""Our team's philosophy is to teach everyone how to do everything,"" Girdhar said. ""But I came to CMU wanting to drive. I wanted to drive so badly."" Amy Chen demonstrates how a driver is positioned inside of Boson, a Fringe buggy built in 2016. Safety is a top priority for teams. All drivers are required to have five pieces of safety gear: mouth guard, goggles, a harness that includes three points of contact to the body of the buggy, gloves and a helmet. And during practice runs and races, flaggers man the course to let drivers know if it's safe to proceed. The streets of the race course haven't changed, but the condition of the course varies due to potholes, which can make or break a fast run. The last course records were set in 2017, but this year potholes are making for some challenging conditions, said Amy Chen, a senior in psychology who has been driving for four years.
Answer: "
"What is the director of the MSAII program's email address?
","['handbook-msaii-2022-2023.txt', 'handbook-msaii-2022-2023.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the director of the MSAII program's email address?

Context: 6 Academic Calendar 
The Academic Calendar can be found at https://www.cmu.edu/hub/calendar/index.html and 
provides information on all deadlines including registration dates, class start dates, add/drop 
deadlines, exam dates and more. 
7 School and Departmental Information 
The following are key personnel with whom you may interact during your time at Carnegie 
Mellon: 
 
Martial Hebert 
 
 
 
 
 
Dean, School of Computer Science 
9 
 
 
University Professor 
 
GHC 6105 
 
412-268-5704 
 
hebert@cs.cmu.edu  
https://www.cs.cmu.edu/~hebert/  
 
David Garlan 
Associate Dean for Master’s Programs, SCS 
Professor 
TCS 420 
garlan@cs.cmu.edu 
http://www.cs.cmu.edu/~garlan/academics.htm  
 
Carolyn Rosé  
 
 
 
 
Interim Director, Language Technologies Institute 
 
Professor 
 
GHC 5415 
 
412-268-7130 
 
cprose@cs.cmu.edu 
8 MSAII Policies 
The Language Technologies Institute (LTI) has prepared this statement of policies, program 
requirements, guidance, process and procedures for students in the M.S. in Artificial Intelligence 
and Innovation (MSAII) program. A copy of this handbook is also available online at the 
program website.   
The University Student guide, The Word, which details university-wide policies, is also 
available online at www.cmu.edu/student-affairs/theword.     
Additional resources specific to graduate students can be found at 
www.cmu.edu/graduate/policies/    
8.1 The MSAII Degree 
The M.S. in Artificial Intelligence and Innovation (MSAII) is a professional master's program 
offered by the Language Technologies Institute (LTI) in the School of Computer Science at 
Carnegie Mellon University. This program seeks to educate students in the innovative use of 
artificial intelligence to create practical solutions in a wide variety of application areas. As 
artificial intelligence matures, there is a great need for students who can envision, design, plan 
and deliver solutions that integrate AI technologies such as deep learning, natural language 
processing, robotics and big data analytics into new applications.  The program stresses both 
10 
 
intrapreneurship and entrepreneurship and encourages students to develop ideas for startups 
and for introduction into established companies.
See also the ‘Duration of Study’ policy. 
11.7 Residency Requirement 
The MSAII is a full-time, in-residence program conducted only on the Pittsburgh campus.  In 
exceptional circumstances, such as visa complications or medical exigencies, permission may be 
granted by the Director allowing a student to participate in short portions of the program 
remotely.  This is not possible for first-year students. 
12 Financial Issues 
12.1 Graduate Student Funding  
34 
 
The LTI does not provide financial aid or support to students in the professional MS programs.  
Students are encouraged to seek financial aid and support from other sources.  The HUB website 
(https://www.cmu.edu/sfs/financial-aid/graduate/index.html) provides the Graduate Financial 
Aid Guide, information about funding options and how to apply for financial aid and other 
helpful links.  Additional information on financial issues for graduate students can be found on 
the web at www.cmu.edu/hub/new-grad/. 
Students in the professional MS programs are not prohibited from seeking support as Teaching 
Assistants and Research Assistants.  However, typically full-time MS students do not have time 
for these activities.  Typically Research Assistantships are most likely to be awarded to students 
in Carnegie Mellon’s research-oriented degree programs. 
12.2 University Financial Aid 
Graduate students should consult the graduate student financial aid information found on The 
HUB website: www.cmu.edu/sfs/financial-aid/graduate/index.html.  Students will find the 
Graduate Financial Aid Guide, information about funding options and how to apply for financial 
aid and other helpful links. 
12.3 Health Insurance  
Carnegie Mellon has a Student Health Insurance policy requiring full-time, degree-seeking 
students to carry adequate medical insurance. Students must either purchase the plan offered by 
the University or an application for a waiver can be made if the student is “enrolled as the 
dependent, partner/spouse or principal in an employer or government-sponsored insurance 
plan” (see the Carnegie Mellon University Student Health Insurance Policy at 
https://www.cmu.edu/policies/student-and-student-life/student-health-insurance.html).  
It is the responsibility of each student to make arrangements with Student Health Services to 
either pay for their insurance at the beginning of the semester, or elect a payment plan over the 
course of the academic year.
Answer: "
"Which program has an application date of September 30th?
","['handbook-msaii-2022-2023.txt', 'miis-handbook_2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which program has an application date of September 30th?

Context: All students who secure summer internships must register for three units in 11-935 (LTI 
Practicum).  This course can be taken once during the program with Pass/Fail and will be 
counted toward graduation units.  International students must apply for Curricular Practical 
Training (CPT) for the internship by contacting the Office of International Education (OIE).  To 
obtain CPT, students need an offer letter spelling out employment dates, work hours, and wages 
or stipend.  It is the student’s responsibility in 11-935 to give a presentation during the semester 
following the internship to all current MSAII students detailing the student’s internship 
experience. 
International students who are interested in working in the U.S after graduation are required to 
consult with the Office of International Education (OIE) which oversees the OPT registration.  
Note: OPT must only be used after students have completed their degree at Carnegie Mellon. 
NOTE: Students are personally responsible for securing a suitable internship.  The Career and 
Professional Development Center will assist in their search for counseling, workshops, and 
internship opportunity listings in Handshake.  We strongly suggest students begin the 
internship search beginning in the Fall of your first year. Some internship opportunities have 
application deadlines in the Fall, so by starting your search early, you won't miss out on these 
internships.  The Career and Professional Development Center will assist students with the 
internship search, but does not match students with employers; obtaining an internship is the 
students’ responsibility.   
The following steps are recommended to ensure a successful internship outcome.  Many of the 
events below have been modified due to COVID. 
September/October  
 Meet with your advisor to discuss your interests and plan your internship search.  
19 
 
 Review the internships of previous students. We also encourage you to speak informally 
with second-year students who can offer first-hand information about their internships.  
 Attend the Technical Opportunities Conference (TOC) at Carnegie Mellon for exposure 
to potential employers for internships.   
November/December  
 Begin researching organizations and internship opportunities. Pay close attention to 
organizations that have established internship programs and their deadlines.  
 Attend workshops to perfect your resume, hone your interviewing skills, and gain 
knowledge about the internship search process.
Master of Science in 
Intelligent Information Systems 
 
Student Handbook 
2023-2024 
 
 
 
 
 
Revised: August 2023 
 
Last revision date: September 1, 2023 
 
The information contained in this graduate handbook template focuses on the 
resources and locations available at the Carnegie Mellon Pittsburgh Campus. 
MIIS Graduate Student Handbook 
Page 2 
 
Contents 
1 
Welcome 6 
1.1 
Vision . 6 
1.2 
Mission . 6 
1.3 
The MIIS Degree . 6 
1.4 
MIIS Contact Information . 7 
1.5 
Carnegie Mellon University Statement of Assurance . 8 
1.6 
The Carnegie Mellon Code . 9 
1.7 
University Policies and Expectations . 9 
1.8 
Academic Calendar . 10 
2 
The Language Technologies Institute 10 
2.1.1 
Mailboxes & Office Supplies . 10 
2.1.2 
Photocopies and Printers . 10 
2.1.3 
Office Space for MS Students . 10 
2.1.4 
Computers for MS Students . 10 
3 
CMU Degree Completion and Certification 
11 
3.1 
Standard Degree Requirements & Degree Certification . 11 
3.2 
Statute of Limitations . 12 
3.3 
Additional Guidance for Students . 12 
4 
MIIS Degree Requirements and Related Policies/Protocols 
13 
4.1 
Program Options . 13 
4.2 
Required Units for Degree Attainment . 13 
4.3 
Core Requirements . 13 
4.4 
Approved Qualifying Courses . 14 
4.4.1 
Breadth Courses:  Human Language . 15 
4.4.2 
Breadth Courses:  Language Technology Applications . 15 
4.4.3 
Breadth Courses:  Machine Learning . 15 
4.5 
Practice Requirements .16 
4.6 
Registration Process/Procedures . 17 
4.7 
Drop/Add/Withdraw Procedures . 17 
4.8 
Transfer Courses and Pittsburgh Council on Higher Education (PCHE) . 17 
4.9 
Internships . 18 
4.10 
Advising . 18 
MIIS Graduate Student Handbook 
Page 3 
 
4.11 
LTI Orientation .
Answer: "
"What data source do authors in ""Towards Open-Domain Twitter User Profile Inference"" collect their public user profiles from?
","['alexander hauptmann_72cce47fd053bf916314d89a8174726c58c05e02_content_2.txt', 'alexander hauptmann_72cce47fd053bf916314d89a8174726c58c05e02_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What data source do authors in ""Towards Open-Domain Twitter User Profile Inference"" collect their public user profiles from?

Context: Title: Towards Open-Domain Twitter User Profile Inference
Authors: Haoyang Wen, Zhenxin Xiao, Eduard H. Hovy, Alexander G. Hauptmann
Section: 6 Conclusion
it and prevent overestimation of the model. We also provide a complete list of attributes in Table 7 to increase the transparency. We are open to all further explorations that can prevent unintended impacts. • Our constructed dataset for profile inference research is drawn solely from publicly available WikiData and Twitter, where the ethical consideration should be similar to other work using encyclopedia resources such as (Sun and Peng, 2021). Furthermore, according to WikiData: Oversight, non-public personal information are monitored and removed by Wikidata. According to WikiData Term of Use, we can freely reuse and build upon on WikiData. According to the Twitter Developer Agreement and Policy, we will only release IDs instead of actual content for noncommercial research purposes from academic institutions. • To ensure the proper use of this work, we will not release the data via a publicly available access point. Instead, we will release the data based on individual request and we will ask for consent that 1) requesters are from research institutions 2) they will follow all the regulations when using our work 3) they will not use the model to infer non-public users unless obtained proper consent from those users.
Title: Towards Open-Domain Twitter User Profile Inference
Authors: Haoyang Wen, Zhenxin Xiao, Eduard H. Hovy, Alexander G. Hauptmann
Section: 3 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did
you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)? 3.2 Generation-based method 4.1 Experiment Setup D 7 Did you use human annotators (e.g., crowdworkers) or research with human participants? Left blank. D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? No response. D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants’ demographic (e.g., country of residence)? No response. D3. Did you discuss whether and how consent was obtained from people whose data you’re using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? No response. D4. Was the data collection protocol approved (or determined exempt) by an ethics review board? No response. D5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? No response.
Answer: "
"For additional information about the PhD in Language and Information Technology program, who should you contact?
","['handbook_phd_2023-2024.txt', 'mlt-student-handbook-2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: For additional information about the PhD in Language and Information Technology program, who should you contact?

Context: For information about Carnegie Mellon requirements and 
policies, please see the universitys handbook The Word, the Office of Graduate and Postdoctoral 
Affairs web page, the Office of the Dean of Students web page, and other resources contained in 
Appendix A of this handbook. 
Welcome! We hope that your time here is a life-changing experience. 
1.1 
Degrees Offered 
The Language Technologies Institute offers two Ph.D. programs and four Master degrees. 
 Ph.D. in Language and Information Technologies (LTI Ph.D.) 
 Dual-Degree Ph.D. in Language and Information Technologies (CMU-PT Ph.D.) 
 Masters in Language Technologies (MLT) 
 Master of Science in Intelligent Information Systems (MIIS) 
 Master of Computational Data Science (MCDS) 
 Master of Science in Artificial Intelligence and Innovation (MSAII) 
LTI Ph.D. Graduate Student Handbook 
Page 10 
 
This handbook applies to the LTI Ph.D. 
The Ph.D. in Language and Information Technologies (LTI Ph.D.) is focused on understanding 
and extending the state of the art in computational linguistics, natural language processing, 
dialogue systems, information retrieval, machine translation, speech processing, video 
understanding, multimodal systems, automated reasoning, and other topics related to analysis 
and understanding of unstructured information (e.g., machine learning, and software engineering 
of intelligent systems). 
1.2 
Department Personnel 
The people responsible for administering the LTI Ph.D. degree are: 
Jamie Callan 
Ph.D. Program Director 
Professor 
GHC 5419 
callan@cs.cmu.edu 
412-268-4525 
Stacey Young 
Ph.D.
1.2 The MLT Degree 
The Master of Language Technologies (MLT) is a research-oriented Master of Science degree 
offered by the Language Technologies Institute (LTI), a graduate department in the School of 
Computer Science at Carnegie Mellon University.  The MLT program is a 24-month program 
consisting of courses, directed research, and an optional Masters' Thesis.  Typical research 
areas include speech processing, information retrieval, machine translation, natural language 
processing, machine learning, and computational biology.  Many MLT graduates continue on to 
PhD programs in the LTI or other leading universities.  Other graduates go on to work in the 
computer industry, many at major corporate research laboratories. 
There are significant differences between CMU's different departments and degree programs in 
philosophical approach, procedures, policies, and regulations. Each department issues a 
handbook that informs graduate students of their program requirements and procedures and 
ensures that students have written access to the standard information outlined below. This 
handbook describes the policies, procedures, and requirements for the Master of Language 
Technologies (MLT) degree. 
While this handbook is specific to your academic experience in the department, there are 
several other resources and offices graduate students are encouraged to consult during their 
tenure at Carnegie Mellon University.  Information about The Word, Carnegie Mellon University 
Student Handbook, the Office of the Assistant Vice Provost for Graduate Education, the Office 
of the Dean of Student Affairs and others are included in Appendix A of this handbook. 
All policies not explicitly described in this document conform to School of Computer Science 
(SCS) policies and university policies described in The Word, Carnegie Mellon University Student 
Handbook and at the University Policies website.
Answer: "
"What are the four stages of the MultiViz method?
","['louis philippe morency_d01cc51c0d06583b809833a5f7ce71101d278528_metadata.txt', 'shinji watanabe_8bc617c9139648d7a92991d70c671230bac7b2e2_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the four stages of the MultiViz method?

Context: Faculty Name: louis philippe morency
Paperid: d01cc51c0d06583b809833a5f7ce71101d278528
Title: MultiViz: Towards User-Centric Visualizations and Interpretations of Multimodal Models
Year: 2023
Abstract: The nature of human and computer interactions are inherently multimodal, which has led to substantial interest in building interpretable, interactive, and reliable multimodal interfaces. However, modern multimodal models and interfaces are typically black-box neural networks, which makes it challenging to understand their internal mechanics. How can we visualize their internal workings in order to empower stakeholders to visualize model behavior, perform model debugging, and promote trust in these models? Our paper proposes MultiViz, a method for analyzing the behavior of multimodal models via 4 stages: (1) unimodal importance, (2) cross-modal interactions, (3) multimodal representations and (4) multimodal prediction. MultiViz includes modular visualization tools for each stage before combining outputs from all stages through an interactive and human-in-the-loop API. Through user studies with 21 participants on 8 trained models across 6 real-world tasks, we show that the complementary stages in MultiViz together enable users to (1) simulate model predictions, (2) assign interpretable concepts to features, (3) perform error analysis on model misclassifications, and (4) use insights from error analysis to debug models. MultiViz is publicly available at https://github.com/pliang279/MultiViz, will be regularly updated with new visualization tools and metrics, and welcomes input from the community1.
Authors: P. Liang, Yiwei Lyu, Gunjan Chhablani, Nihal Jain, Zihao Deng, Xingbo Wang, Louis-Philippe Morency, R. Salakhutdinov
Venue: CHI Extended Abstracts
Tldr: {'model': 'tldr@v2.0.0', 'text': 'It is shown that the complementary stages in MultiViz together enable users to simulate model predictions, assign interpretable concepts to features, perform error analysis on model misclassifications, and use insights from error analysis to debug models.'}
Url: N/A
Title: AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head
Authors: Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, Yi Ren, Zhou Zhao, Shinji Watanabe
Section: 1 Introduction
the whole process of AudioGPT can be divided into four stages: • Modality Transformation. Using input/output interface for modality transformation between speech and text, bridging the gap between the spoken language LLMs and ChatGPT. • Task Analysis. Utilizing the dialogue engine and prompt manager to help ChatGPT understands the intention of a user to process audio information. • Model Assignment. Receiving the structured arguments for prosody, timbre, and language control, ChatGPT assigns the audio foundation models for understanding and generation. • Response Generation. Generating and returning a final response to users after the execution of audio foundation models. As a blossoming research topics (Wu et al., 2023; Shen et al., 2023; Huang et al., 2023b), there is an increasing demand for evaluating the performance of multi-modal LLMs in understanding human intention and organizing the cooperation of multiple foundation models. In this work, we outline the design principles and process of evaluating AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT for processing complex audio information in multi-round dialogue, covering a series of AI tasks including generating and understanding speech, music, sound, and talking head. Key contributions of the paper include: • We propose AudioGPT, which equips ChatGPT with audio foundation models to handle complex audio tasks. As a general-purpose interface, ChatGPT is connected with a modality transformation interface to enable spoken dialogue. • We outline the design principles and process of evaluating multi-modal LLMs, and test AudioGPT in terms of consistency, capability, and robustness. • Demonstrations present the efficiency of AudioGPT in audio understanding and generation with multiple rounds of dialogue, which empowers humans to create rich and diverse audio content with unprecedented ease.
Answer: "
"In spring 2024, When is the last day of Mini-3 classes?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-2425.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, When is the last day of Mini-3 classes?

Context: On Monday, 22 January, 2024, during the Spring 2024 (S24) semester, Mini-3 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 29 January, 2024, during the Spring 2024 (S24) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Wednesday, 07 February, 2024, during the Spring 2024 (S24) semester, Mini-3 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 19 February, 2024, during the Spring 2024 (S24) semester, Mini-3 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 19 February, 2024, during the Spring 2024 (S24) semester, Mini-3 Faculty Course Evaluations open is observed.
On Monday, 26 February, 2024, during the Spring 2024 (S24) semester, Semester course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 01 March, 2024, during the Spring 2024 (S24) semester, Mini-3 Last Day of Classes is observed.
On Friday, 01 March, 2024, during the Spring 2024 (S24) semester, Mini-3 voucher deadline (4) is observed.
On Saturday, 02 March, 2024, during the Spring 2024 (S24) semester, Mini-3 Exams is observed.
On Saturday, 02 March, 2024, during the Spring 2024 (S24) semester, Mini-3 Faculty Course Evaluations close is observed.
From 04 March, 2024,Monday to 08 March, 2024,Friday marks the Spring Break for Spring 2024 (S24) semester which leads to No Classes.
On Monday, 11 March, 2024, during the Spring 2024 (S24) semester, First day of Mini-4 Classes is observed.
On Monday, 11 March, 2024, during the Spring 2024 (S24) semester, Mid-Semester & Mini-3 grades due by 4 pm is observed.
On Tuesday, 12 March, 2024, during the Spring 2024 (S24) semester, Summer 2024 Registration Opens is observed.
Day: 'Monday', Event: 'Mini-3 Faculty Course Evaluations open', Semester: 'Spring 2025 (S25)'
Date: '2025-02-24', Day: 'Monday', Event: 'Semester course drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Spring 2025 (S25)'
Date: '2025-02-28', Day: 'Friday', Event: 'Mini-3 Last Day of Classes ', Semester: 'Spring 2025 (S25)'
Date: '2025-02-28', Day: 'Friday', Event: 'Mini-3 voucher deadline (4)', Semester: 'Spring 2025 (S25)'
Date: '2025-03-01', Day: 'Saturday', Event: 'Mini-3 Exams', Semester: 'Spring 2025 (S25)'
Date: '2025-03-01', Day: 'Saturday', Event: 'Mini-3 Faculty Course Evaluations close', Semester: 'Spring 2025 (S25)'
Start Date: '2025-03-03', End Date: '2023-03-07', Days: 'Monday to Friday', Event: 'Spring Break; No Classes', Semester: 'Spring 2025 (S25)'
Date: '2025-03-10', Day: 'Monday', Event: 'First day of Mini-4 Classes', Semester: 'Spring 2025 (S25)'
Date: '2025-03-11', Day: 'Tuesday', Event: 'Summer 2025 Registration Opens', Semester: 'Spring 2025 (S25)'
Date: '2025-03-12', Day: 'Wednesday', Event: 'Mid-Semester & Mini-3 grades due by 4 pm', Semester: 'Spring 2025 (S25)'
Date: '2025-03-14', Day: 'Friday', Event: 'Mini-4 add, audit & tuition adjustment drop deadline (1)', Semester: 'Spring 2025 (S25)'
Date: '2025-03-31', Day: 'Monday', Event: 'Semester pass/no pass & withdrawal deadline', Semester: 'Spring 2025 (S25)'
Date: '2025-04-02', Day: 'Wednesday', Event: 'Mini-4 course drop deadline; withdrawal grade assigned after this date (2)',
Answer: "
"In spring 2024, Who are the instructors for course 15210?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, Who are the instructors for course 15210?

Context: In Semester Spring 2024, from the department of Computer Science, the subject titled 'Parallel and Sequential Data Structures and Algorithms' with Course ID 15210 and Section I offers 12.0 units. The Class meets Tuesday between 02:00PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Acar, Sleator located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Parallel and Sequential Data Structures and Algorithms' with Course ID 15210 and Section J offers 12.0 units. The Class meets Tuesday between 02:00PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Acar, Sleator located in Building GHC, Room 5208.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Parallel and Sequential Data Structures and Algorithms' with Course ID 15210 and Section K offers 12.0 units. The Class meets Tuesday between 03:00PM and 03:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Acar, Sleator located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Parallel and Sequential Data Structures and Algorithms' with Course ID 15210 and Section L offers 12.0 units. The Class meets Tuesday between 04:00PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Acar, Sleator located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Parallel and Sequential Data Structures and Algorithms' with Course ID 15210 and Section W offers 12.0 units. The Class meets Sunday Tuesday Thursday between 10:00AM and 11:15AM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Oflazer located in Building CMB, Room 1202.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Introduction to Computer Systems' with Course ID 15213 and Section NA offers 12.0 units.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Parallel and Sequential Data Structures and Algorithms' with Course ID 15210 and Section I offers 12.0 units. The Class meets Tuesday between 02:00PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Acar, Sleator located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Parallel and Sequential Data Structures and Algorithms' with Course ID 15210 and Section J offers 12.0 units. The Class meets Tuesday between 02:00PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Acar, Sleator located in Building GHC, Room 5208.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Parallel and Sequential Data Structures and Algorithms' with Course ID 15210 and Section K offers 12.0 units. The Class meets Tuesday between 03:00PM and 03:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Acar, Sleator located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Parallel and Sequential Data Structures and Algorithms' with Course ID 15210 and Section L offers 12.0 units. The Class meets Tuesday between 04:00PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Acar, Sleator located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Parallel and Sequential Data Structures and Algorithms' with Course ID 15210 and Section W offers 12.0 units. The Class meets Sunday Tuesday Thursday between 10:00AM and 11:15AM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Oflazer located in Building CMB, Room 1202.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Introduction to Computer Systems' with Course ID 15213 and Section NA offers 12.0 units.
Answer: "
"In fall 2023, What is the title of course 05410?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, What is the title of course 05410?

Context: In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'User-Centered Research and Evaluation' with Course ID 05410 and Section D offers 12.0 units. The Class meets Monday Wednesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Musuraca, Eslami located in Building GHC, Room 4101.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'User-Centered Research and Evaluation:' with Course ID 05410 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'User-Centered Research and Evaluation' with Course ID 05410 and Section E offers 12.0 units. The Class meets Monday Wednesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Musuraca, Eslami located in Building PH, Room 125B.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Human Factors' with Course ID 05413 and Section A offers 9.0 units. The Class meets Tuesday Thursday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Dabbish located in Building BH, Room A53.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Programming Usable Interfaces:' with Course ID 05430 and Section NA offers 15.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Programming Usable Interfaces' with Course ID 05430 and Section NA offers 15.0 units. The Class meets Monday Wednesday between 08:00AM and 09:20AM ET.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'User-Centered Research and Evaluation' with Course ID 05410 and Section D offers 12.0 units. The Class meets Monday Wednesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Musuraca, Eslami located in Building GHC, Room 4101.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'User-Centered Research and Evaluation:' with Course ID 05410 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'User-Centered Research and Evaluation' with Course ID 05410 and Section E offers 12.0 units. The Class meets Monday Wednesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Musuraca, Eslami located in Building PH, Room 125B.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Human Factors' with Course ID 05413 and Section A offers 9.0 units. The Class meets Tuesday Thursday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Dabbish located in Building BH, Room A53.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Programming Usable Interfaces:' with Course ID 05430 and Section NA offers 15.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Programming Usable Interfaces' with Course ID 05430 and Section NA offers 15.0 units. The Class meets Monday Wednesday between 08:00AM and 09:20AM ET.
Answer: "
"How many authors contributed to the paper Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation?
","['david mortensen_bf42c0462d1415cdde877c90d58da11545407b8a_content_0.txt', 'lori levin_bf42c0462d1415cdde877c90d58da11545407b8a_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many authors contributed to the paper Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation?

Context: Title: Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation
Authors: David R. Mortensen, Ela Gulsen, Taiqi He, Nathaniel Robinson, Jonathan D. Amith, Lindia Tjuatja, Lori Levin
Section: Acknowledgments
We gratefully acknowledge the support of US National Science Foundation, grant number 2211951, numerous examples Mixtec examples from Rey Castillo Garcia, and generous contributions from three anonymous reviewers.
Title: Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation
Authors: David R. Mortensen, Ela Gulsen, Taiqi He, Nathaniel Robinson, Jonathan D. Amith, Lindia Tjuatja, Lori Levin
Section: Acknowledgments
We gratefully acknowledge the support of US National Science Foundation, grant number 2211951, numerous examples Mixtec examples from Rey Castillo Garcia, and generous contributions from three anonymous reviewers.
Answer: "
"In spring 2024, How many units is course 15112 worth?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, How many units is course 15112 worth?

Context: In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Imperative Computation' with Course ID 15122 and Section NA offers 12.0 units. The Class meets Friday between 05:00PM and 05:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor NA located in Building GHC, Room 4211.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Imperative Computation' with Course ID 15122 and Section R offers 12.0 units. The Class meets Monday between 05:00PM and 05:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Cervesato, Kohlbrenner located in Building GHC, Room CLSTR.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Imperative Computation' with Course ID 15122 and Section NA offers 12.0 units. The Class meets Friday between 05:00PM and 05:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor NA located in Building GHC, Room 4102.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Imperative Computation' with Course ID 15122 and Section W offers 12.0 units. The Class meets Tuesday Thursday between 08:45AM and 09:35AM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Hammoud located in Building CMB, Room 2163.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section Lec 1 offers 12.0 units. The Class meets Tuesday Thursday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building DH, Room 2210.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section Lec 2 offers 12.0 units.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Imperative Computation' with Course ID 15122 and Section NA offers 12.0 units. The Class meets Friday between 05:00PM and 05:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor NA located in Building GHC, Room 4211.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Imperative Computation' with Course ID 15122 and Section R offers 12.0 units. The Class meets Monday between 05:00PM and 05:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Cervesato, Kohlbrenner located in Building GHC, Room CLSTR.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Imperative Computation' with Course ID 15122 and Section NA offers 12.0 units. The Class meets Friday between 05:00PM and 05:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor NA located in Building GHC, Room 4102.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Imperative Computation' with Course ID 15122 and Section W offers 12.0 units. The Class meets Tuesday Thursday between 08:45AM and 09:35AM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Hammoud located in Building CMB, Room 2163.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section Lec 1 offers 12.0 units. The Class meets Tuesday Thursday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building DH, Room 2210.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section Lec 2 offers 12.0 units.
Answer: "
"What is the name of the new class of offline policy gradient algorithms introduced in the paper ""Improving Language Models with Advantage-based Offline Policy Gradients""?
","['maarten sap_9d2dc57903e99f33b9cf727c3903718751d82663_metadata.txt', 'maarten sap_9d2dc57903e99f33b9cf727c3903718751d82663_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the name of the new class of offline policy gradient algorithms introduced in the paper ""Improving Language Models with Advantage-based Offline Policy Gradients""?

Context: Faculty Name: maarten sap
Paperid: 9d2dc57903e99f33b9cf727c3903718751d82663
Title: Improving Language Models with Advantage-based Offline Policy Gradients
Year: 2023
Abstract: Language Models (LMs) achieve substantial language capabilities when finetuned using Reinforcement Learning with Human Feedback (RLHF). However, RLHF is an unstable and data-hungry process that continually requires new high-quality LM-generated data for finetuning. We introduce Advantage-Leftover Lunch RL (A-LoL), a new class of offline policy gradient algorithms that enable RL training on any pre-existing data. By assuming the entire LM output sequence as a single action, A-LoL allows incorporating sequence-level classifiers or human-designed scoring functions as rewards. Subsequently, by using LM's internal sequence-level value estimate, A-LoL filters negative advantage (low-quality) data points during training, making it resilient to noise. Overall, A-LoL is an easy-to-implement LM training recipe that is sample-efficient and stable. We demonstrate the effectiveness of A-LoL and its variants with a set of four different language generation tasks. We compare against both online RL (PPO) and recent preference-based (DPO, PRO) and reward-based (GOLD) offline RL baselines. On the commonly-used RLHF benchmark, Helpful and Harmless Assistant (HHA), LMs trained with A-LoL methods achieve the highest diversity while also being rated more safe and helpful than baselines according to humans. Additionally, in the remaining three tasks, A-LoL could optimize multiple distinct reward functions even when using noisy or suboptimal training data. We also release our experimental code. https://github.com/abaheti95/LoL-RL
Authors: Ashutosh Baheti, Ximing Lu, Faeze Brahman, Ronan Le Bras, Maarten Sap, Mark O. Riedl
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'Advantage-Leftover Lunch RL (A-LoL), a new class of offline policy gradient algorithms that enable RL training on any pre-existing data that assumes the entire LM output sequence as a single action, and allows incorporating sequence-level classifiers or human-designed scoring functions as rewards.'}
Url: https://arxiv.org/pdf/2305.14718
Title: ADVANTAGE-BASED OFFLINE POLICY GRADIENTS
Authors: Ashutosh Baheti, Ximing Lu, Faeze Brahman, Ronan Le Bras, Maarten Sap, Mark Riedl
Section: D LIMITATIONS AND SOCIETAL AND ETHICAL CONSIDERATIONS
We discuss some of the limitations of Advantage-Leftover Lunch RL. First, A-LOL requires that the evaluation metric aligns with the provided rewards. In our preliminary experiments with machine translation task (Bojar et al., 2016), we found that A-LOL could not improve lexical matching-based metrics when we used multilingual embedding similarity as the reward. Furthermore, a single sequence-level reward for each instance will obscure disagreement in how humans would label sequences (i.e., average out value pluralism). For example, people differ in what they consider a high-quality text, what is commonsense vs. domain-specific knowledge, etc. (de Marneffe et al., 2012; Plank, 2022). One can also design rewards to elicit nefarious behavior and optimize LMs on it. Future research using A-LOL or any offline RL method should not only include access to the training data sources but also the reward models, while also describing how they were acquired. Although less than other RL methods, A-LOL is also susceptible to reward hacking (Skalse et al., 2022; Pang et al., 2023) by learning bad sequences as “critical” actions (Kumar et al., 2022). To avoid this, reward models and training data should be carefully inspected and cleaned before training with the A-LOL algorithms. Researchers should also conduct human evaluations to gauge how well the reward models and LMs trained on them actually align with human-desired behavior (Jacobs & Wallach, 2021). On the positive side, LOL RL allows for both stable and sample-efficient training of models on existing language data. Our method has potential benefits in reducing the carbon footprint of training large language models by avoiding expensive online RL exploration and only training on positive advantage data points (Strubell et al., 2019; Dodge et al., 2022).
Answer: "
"How many datasets does GlobalBench currently cover?
","['graham neubig_17605c43ca3eb982c99642052ddc21a93d116594_metadata.txt', 'graham neubig_17605c43ca3eb982c99642052ddc21a93d116594_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many datasets does GlobalBench currently cover?

Context: Faculty Name: graham neubig
Paperid: 17605c43ca3eb982c99642052ddc21a93d116594
Title: GlobalBench: A Benchmark for Global Progress in Natural Language Processing
Year: 2023
Abstract: Despite the major advances in NLP, significant disparities in NLP system performance across languages still exist. Arguably, these are due to uneven resource allocation and sub-optimal incentives to work on less resourced languages. To track and further incentivize the global development of equitable language technology, we introduce GlobalBench. Prior multilingual benchmarks are static and have focused on a limited number of tasks and languages. In contrast, GlobalBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring accuracy, GlobalBench also tracks the estimated per-speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served languages are the ones with a relatively high population, but nonetheless overlooked by composite multilingual benchmarks (like Punjabi, Portuguese, and Wu Chinese). Currently, GlobalBench covers 966 datasets in 190 languages, and has 1,128 system submissions spanning 62 languages.
Authors: Yueqi Song, Catherine Cui, Simran Khanuja, Pengfei Liu, FAHIM FAISAL, Alissa Ostapenko, Genta Indra Winata, Alham Fikri Aji, Samuel Cahyawijaya, Yulia Tsvetkov, Antonios Anastasopoulos, Graham Neubig
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work introduces GlobalBench, an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages and tracks the estimated per-speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world.'}
Url: http://arxiv.org/pdf/2305.14716
Title: GlobalBench: A Benchmark for Global Progress in Natural Language Processing
Authors: Yueqi Song, Catherine Cui, Simran Khanuja, Pengfei Liu, Fahim Faisal, Alissa Ostapenko, Genta Indra Winata, Alham Fikri Aji, Samuel Cahyawijaya, Yulia Tsvetkov, Antonios Anastasopoulos, Graham Neubig
Section: A Appendix
A.1 Datasets and System Outputs In this section, we list all datasets for which GlobalBench has submissions of system outputs. Text Classification GlobalBench covers the following datasets for this task: the QC (Question Classification) dataset (Li and Roth, 2002), the ATIS (Airline Travel Information Systems) dataset (Hemphill et al., 1990), the MR (Movie Review) dataset (Pang and Lee, 2005), the SST-2 (Stanford Sentiment Treebank) Corpus (Socher et al., 2013), datasets from GLUE (the General Language Understanding Evaluation) benchmark (Wang et al., 2018), and the Code-Switching Corpus (Ostapenko et al., 2022). Sequence Labeling GlobalBench covers the following datasets for this task: the MasakhaNER Corpus (Adelani et al., 2021), the CoNLL-2003 dataset (Tjong Kim Sang and De Meulder, 2003), and the PAN-X dataset (Artetxe and Schwenk, 2019). Text Pair Classification GlobalBench covers the following datasets for this task: the Cross-lingual Natural Language Inference (XNLI) corpus (Conneau et al., 2018), the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015), and the Sentences Involving Compositional Knowldedge (SICK) dataset (Marelli et al., 2014). Question Answering GlobalBench covers the following datasets for this task: XQuAD (Artetxe et al., 2019), TyDiQA (Clark et al., 2020), SDQA (Faisal et al., 2021), and MLQA (Lewis et al., 2019).
Answer: "
"What is Carolyn Rose's email address?
","['carolyn_rose.txt', 'carolyn rose_bc936884d358d73d6b514f7e5899e67ad09690d8_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is Carolyn Rose's email address?

Context: Carolyn Rosé
Professor, Language Technologies Institute
Human-Computer Interaction Institute
Contact
5415 —Gates & Hillman Centers
Email
412-268-7130
Research Area
Computer-Supported Collaborative Learning/MOOCs, Information Retrieval, Text Mining and Analytics, Language Technologies for Education, Natural Language Processing and Computational Linguistics

Education
Ph.D. in Language and Information Technology
1997
Employer Upon Graduation: 
Faculty, LTI CMU
Research
My research focuses on better understanding the social and pragmatic nature of conversation, and using this understanding to build computational systems that improve the efficacy of conversation both between people, and between people and computers. In order to pursue these goals, I invoke approaches from computational discourse analysis and text mining, conversational agents, and computer-supported collaborative learning. I ground my research in the fields of language technologies and human-computer interaction, and am fortunate to work closely with students and post-docs from the LTI and the Human-Computer Interaction Institute, as well as to direct my own lab, TELEDIA. My group’s highly interdisciplinary work, published in 160 peer-reviewed publications, is represented in the top venues in five fields: language technologies, learning sciences, cognitive science, educational technology and human-computer interaction.

An exciting direction of my group's work is spearheading a satellite working group to support social interaction for learning in MOOCs with EdX called DANCE. My research toward this end has birthed and substantially contributed to the growth of two thriving interrelated research areas: automated analysis of collaborative learning processes and dynamic support for collaborative learning. Both areas use intelligent conversational agents to support collaborative learning in a context-sensitive way.

All of my work draws insight from rich theoretical models from sociolinguistics and discourse analysis, and pares them down to precise operationalizations that capture the most important essence for achieving impact. I always start by investigating how conversation works and formalizing this understanding in models that are precise enough to be reproducible and that demonstrate explanatory power in connection with outcomes with real-world value. The next step is to adapt, extend and apply machine learning and text mining technologies that leverage this deep understanding to build computational models capable of automatically applying these constructs to naturally occurring language interactions. Finally, with the technology to automatically monitor naturalistic language communication in place, we can build interventions with real-world benefits.
Faculty Name: carolyn rose
Paperid: bc936884d358d73d6b514f7e5899e67ad09690d8
Title: Editorial: Nine elements for robust collaborative learning analytics: A constructive collaborative critique
Year: 2023
Abstract: None
Authors: A. Wise, Carolyn P. Rosé, Sanna Järvelä
Venue: International Journal of Computer-Supported Collaborative Learning
Tldr: {'model': 'tldr@v2.0.0', 'text': 'The four full articles of this March issue offer a view of the kind of work that the CSCL community is engaging in to capture meaningful traces of learning, map them onto valued learning constructs, and discover useful ways to present them back to teachers, students and other educational stakeholders.'}
Url: https://link.springer.com/content/pdf/10.1007/s11412-023-09389-x.pdf
Answer: "
"Which model performed the best in ""Syntax and Semantics Meet in the “Middle”: Probing the Syntax-Semantics Interface of LMs Through Agentivity""?
","['graham neubig_c5207241406586f4263b235667e004b71ea68953_content_0.txt', 'lori levin_c5207241406586f4263b235667e004b71ea68953_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which model performed the best in ""Syntax and Semantics Meet in the “Middle”: Probing the Syntax-Semantics Interface of LMs Through Agentivity""?

Context: Title: Syntax and Semantics Meet in the “Middle”: Probing the Syntax-Semantics Interface of LMs Through Agentivity
Authors: Lindia Tjuatja, Emmy Liu, Lori Levin, Graham Neubig
Section: F Adjusting Threshold for Exp 2
We also considered the possibility that the models may have a bias towards either the “agent” or “patient” label and may actually be correctly classifying nouns given an appropriate non-zero threshold for δ-LL. To account for this, we recalculate accuracies with thresholds that provide the best performance for each model as an “upper bound” for performance, as seen in Figure 8. After this adjustment, all models do at least as well as predicting the majority class, with GPT-2 xl experiencing the largest gain in accuracy. Nevertheless, GPT-3 davinci-003 still outperforms all other models by far.
Title: Syntax and Semantics Meet in the “Middle”: Probing the Syntax-Semantics Interface of LMs Through Agentivity
Authors: Lindia Tjuatja, Emmy Liu, Lori Levin, Graham Neubig
Section: F Adjusting Threshold for Exp 2
We also considered the possibility that the models may have a bias towards either the “agent” or “patient” label and may actually be correctly classifying nouns given an appropriate non-zero threshold for δ-LL. To account for this, we recalculate accuracies with thresholds that provide the best performance for each model as an “upper bound” for performance, as seen in Figure 8. After this adjustment, all models do at least as well as predicting the majority class, with GPT-2 xl experiencing the largest gain in accuracy. Nevertheless, GPT-3 davinci-003 still outperforms all other models by far.
Answer: "
"Is the Wiegand Gymnasium located in the Jared L. Cohon University Center?
","['handbook_phd_2023-2024.txt', 'handbook_phd_2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Is the Wiegand Gymnasium located in the Jared L. Cohon University Center?

Context: The administrative offices are located in the Cohon University Fitness Center. 
The University Centers recreational facilities include an eight-lane pool, racquetball and squash 
courts, aerobics room, fitness center and gym for basketball and volleyball. With renovations to 
Skibo Gym and the new Highmark Center for Health, Wellness, and Athletics scheduled for 
completion in 2024, the strength and conditioning facility has been temporarily placed on the 
lawn next to the outdoor basketball court close to the Donner locker rooms, Gesling Stadium, and 
Weigand Gymnasium. All users must present a valid CMU ID to use these facilities. 
6.18 CMU Alert 
www.cmu.edu/alert  
CMU Alert sends voice and/or text messages to registered phones in the event of a campus 
emergency that threatens public safety or during tests of the system in the spring and fall 
semesters. Students are automatically registered for CMU-Alert using the current contact 
information that has been entered into the Student Information Online (SIO): 
https://www.cmu.edu/hub/sio/about.html. 
 
 
 
 
LTI Ph.D. Graduate Student Handbook 
Page 40 
 
 
Appendix A 
2023-2024 
Highlighted University Resources for Graduate Students 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Note: The following pages are meant to broadly include only some of the resources available to 
graduate students.  It is not an exhaustive appendix of resources, and students are strongly 
encouraged to visit the various websites linked below for the most up-to-date information.   
LTI Ph.D. Graduate Student Handbook 
Page 41 
 
A.1 Key Resources for Graduate Student Support  
A.1.1 Office of Graduate and Postdoctoral Affairs  
https://www.cmu.edu/graduate  graded@cmu.edu  
The Office of Graduate and Postdoctoral Affairs provides university-wide support for all graduate 
students and academic programs, with a focus on supporting graduate student success at 
Carnegie Mellon.
More information regarding these services, business hours, locations and 
contact 
information 
can 
be 
found 
on 
the 
Office 
of 
Tartan 
Ink 
website 
at:  
https://www.cmu.edu/tartanink/aboutus/index.html. 
6.16 University Center 
www.cmu.edu/university-center 
The Jared L. Cohon University Center is a centerpiece of the campus that provides a space for 
special events, physical fitness, student organizations and various activities, as well as 
LTI Ph.D. Graduate Student Handbook 
Page 39 
 
accommodating retail and dining services. As the campus crossroads, the University Center 
functions as a place for students to interact, get involved and enjoy new experiences. Visit the 
University Center website for information about campus eateries, ATMs and PNC Bank, fitness 
rooms and schedules, retail stores, scheduling University Center space, the public prayer room, 
student organizations, and the Wright-Rogal Chapel. 
The University Center Information Desk (first floor of the Cohon Center next to Wean Commons 
and Kirr Commons) is the location if you want to know about upcoming campus events or have 
questions about Carnegie Mellon in general, call the Information Desk at 412-268-2107. The 
Information Desk not only provides information about campus events, but also sells postage 
stamps, makes copies, sends faxes, distributes campus maps, manages a lost & found, and has 
information brochures about Pittsburgh and the campus. 
6.17 Athletic/Fitness Facilities 
 www.cmu.edu/athletics 
For the participant or the spectator, Carnegie Mellon offers intercollegiate athletics, intramural 
sports, physical education classes and club sports. The Athletics Department also offers aerobics 
classes in the University Center as well as occasional workshops and instruction related to fitness 
and health. The administrative offices are located in the Cohon University Fitness Center. 
The University Centers recreational facilities include an eight-lane pool, racquetball and squash 
courts, aerobics room, fitness center and gym for basketball and volleyball.
Answer: "
"What's the cost in us dollars per program for the masters degrees in language technologies if you submit before the early deadline?
","['mcds-student-handbook-2023_2024.txt', 'program_info_LanguageTechnologiesConcentration.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What's the cost in us dollars per program for the masters degrees in language technologies if you submit before the early deadline?

Context: Language Technologies Institute / School of Computer Science 
Graduate Student Handbook 
Academic Year 2023-2024  
Master of Computational Data Science Program 
 
Last revision date: July 20, 2023 
 
The information contained in this graduate handbook template focuses on the 
resources and locations available at the Carnegie Mellon Pittsburgh Campus. 
 
 
1
Table of Contents 
1 Welcome . 6 
1.1 The MCDS Degree . 6 
1.2 Vision . 7 
1.3 Mission . 7 
1.4 MCDS Contact Information. 8 
1.5 University Policies and Expectations . 9 
1.6 Carnegie Mellon University Statement of Assurance . 9 
1.7 The Carnegie Mellon Code . 10 
2 The Language Technologies Institute . 10 
2.1 Main Office . 10 
2.2 Photocopies and Printers . 11 
2.3 Office Space for MS Students . 11 
2.4 Computers for MS Students . 11 
3 MCDS Degree Completion and Certification . 11 
3.1 CMU Degree Completion and Statute of Limitations . 11 
Early Completion 
11 
Extended or Longer-than-Standard Completion 
12 
Policy on Master’s Student Statute of Limitations 
12 
Additional Guidance for Students 
12 
3.2 Full-time Status . 13 
3.3 MCDS Degree Enrollment Process and Related Information . 13 
3.3.1 Duration of the degree program 
13 
3.3.2 Residency requirements 
13 
3.3.3 Degree Certification: Course requirements and related policies/protocols 
13 
3.3.4 Prerequisite Core Course 
14 
3.3.5 Plan of study 
14 
3.3.6.1 MCDS Curriculum 
15 
3.3.6.2 Common MCDS Core Courses 
15 
3.3.6.3 Areas of Concentration 
15 
3.3.6.4 MCDS Capstone Courses 
16 
3.3.10 Capstone project 
17 
3.3.11 Elective courses 
17 
3.3.12 Undergraduate courses 
17 
3.3.13 Independent study course 
18 
 
 
2
3.3.14 Double counting courses 
18 
3.3.15 Courses outside of the School of Computer Science 
18 
3.3.16 Grades 
18 
3.3.17 Student Review,
Academic Program Name:
Language Technologies Concentration

Website:
https://lti.cs.cmu.edu/academics/lt-concentration.html

Overview:
Human language technologies have become an increasingly central component of computer science. Information retrieval, machine translation and speech technology are used daily by the general public, while text mining, natural language processing and language-based tutoring are common within more specialized professional or educational environments. The LTI prepares students for this world by offering a minor that gives you the opportunity to not only learn about language technologies, but to also apply that knowledge through a directed project.

Requirements:
Students interested in the language technologies minor must complete our prerequisite courses with an average grade of B (3.0) or better before applying to the program. (Students who do not meet this average must submit a letter of explanation along with their application.) Prerequisites include:
Principles of Imperative Computation (15-122)
Principles of Functional Programming (15-150)
We also strongly encourage candidates to take:
Differential and Integral Calculus (21-120) and Integration and Approximation (21-122)
Matrices and Linear Transformations (21-241) or Matrix Theory (21-242)
Probability and Computing (15-259) or Probability (21-325) or Probability Theory for Computer Scientists (36-218) or Introduction to Probability Theory (36-225)

Curriculum:
The Language Technologies Concentration requires that SCS students complete one core course and their choice of three elective courses of at least 9 units each. The electives can be chosen from a specific set of stand-alone courses. In addition to the four courses, students are required to do an undergraduate research project for at least 9 units to complete their concentration.
Course Requirements for Undergraduate Minor
Answer: "
"In fall 2024, What is the deadline for Semester add, audit & tuition adjustment drop (deadline 1)?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2024, What is the deadline for Semester add, audit & tuition adjustment drop (deadline 1)?

Context: On Friday, 25 October, 2024, during the Fall 2024 (F24) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
On Tuesday, 05 November, 2024, during the Fall 2024 (F24) semester, Democracy Day is observed, leading to No Classes, except Evening classes after 5 pm will still meet.
On Monday, 11 November, 2024, during the Fall 2024 (F24) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Wednesday, 13 November, 2024, during the Fall 2024 (F24) semester, Mini-2 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 18 November, 2024,Monday to 22 November, 2024,Friday marks the Spring 2025 Registration Week for Fall 2024 (F24) semester.
On Monday, 25 November, 2024, during the Fall 2024 (F24) semester, Mini-2 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 25 November, 2024, during the Fall 2024 (F24) semester, Semester & Mini-2 Faculty Course Evaluations open is observed.
From 27 November, 2024,Wednesday to 29 November, 2023,Friday marks the Thanksgiving Break for Fall 2024 (F24) semester which leads to No Classes.
On Friday, 06 December, 2024, during the Fall 2024 (F24) semester, Semester & Mini-2 Last Day of Classes is observed.
On Friday, 06 December, 2024, during the Fall 2024 (F24) semester, Semester & Mini-2 voucher deadline (4) is observed.
From 09 December, 2024,Monday to 10 December, 2023,Tuesday marks the Final Exams for Fall 2024 (F24) semester.
On Monday, 11 December, 2024, during the Fall 2024 (F24) semester, Reading Day is observed.
From 12 December, 2024,Thursday to 13 December, 2023,Friday marks the Final Exams for Fall 2024 (F24) semester.
On Monday, 02 September, 2024, during the Fall 2024 (F24) semester, Labor Day is observed, leading to No Classes & University Closed.
On Monday, 09 September, 2024, during the Fall 2024 (F24) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 18 September, 2024, during the Fall 2024 (F24) semester, Mini-1 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 30 September, 2024, during the Fall 2024 (F24) semester, Mini-1 Pass/no pass & withdrawal deadline (3) is observed.
On Monday, 30 September, 2024, during the Fall 2024 (F24) semester, Mini-1 Faculty Course Evaluations open is observed.
On Monday, 07 October, 2024, during the Fall 2024 (F24) semester, Semester drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 11 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Last Day of Classes is observed.
On Friday, 11 October, 2024, during the Fall 2024 (F24) semester, Mini-1 voucher election deadline (4) is observed.
On Saturday, 12 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Exams is observed.
On Saturday, 12 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Faculty Course Evaluations close is observed.
From 14 October, 2024,Monday to 18 October, 2023,Friday marks the Fall Break for Fall 2024 (F24) semester which leads to No Classes.
On Monday, 21 October, 2024, during the Fall 2024 (F24) semester, Mini-2 Classes Begin is observed.
On Wednesday, 23 October, 2024, during the Fall 2024 (F24) semester, Mid-Semester & Mini-1 grades due by 4 pm is observed.
On Friday, 25 October, 2024, during the Fall 2024 (F24) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
Answer: "
"How many authors are on the paper ""Pragmatic Inference with a CLIP Listener for Contrastive Captioning""?
","['daniel fried_19f59c14b3d79e3203c696128a135d33eb35e468_content_1.txt', 'daniel fried_19f59c14b3d79e3203c696128a135d33eb35e468_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many authors are on the paper ""Pragmatic Inference with a CLIP Listener for Contrastive Captioning""?

Context: Title: Pragmatic Inference with a CLIP Listener for Contrastive Captioning
Authors: Jiefu Ou, Benno Krojer, Daniel Fried
Section: 2 Related Work
of image captions requires multifaceted evaluation. Prior work on contrastive/discriminative captioning investigates the tradeoff of model performance between discriminativeness and accuracy/fluency (Wang et al., 2021; Liu et al., 2019; Honda et al., 2022; Cho et al., 2022; Vedantam et al., 2017; Andreas and Klein, 2016). In this paper, we also perform an extensive study on the tradeoff between informativeness and fluency. Specifically, we focus on analyzing the robustness of the proposed and baseline methods in the tradeoff according to the selection of hyperparameters.
Title: Pragmatic Inference with a CLIP Listener for Contrastive Captioning
Authors: Jiefu Ou, Benno Krojer, Daniel Fried
Section: Acknowledgments
We would like to thank Google for providing funding for this work through a gift on Action, Task, and User Journey modeling, and Samsung Electronics Co., Ltd. for providing funding for BK. Limitations We evaluate only on the “static” image partition of the ImageCoDe dataset. ImageCoDe contains another more challenging partition, containing frames from short temporal intervals in videos, which remains extremely difficult for all current discriminative captioning methods, including our PICL approach. (This partition, along with the static image partition that we use, has previously only been used in contrastive retrieval tasks, not in discriminative captioning.) While we made a substantial effort to explore the tradeoff between informativity and fluency, we were limited in the number of human evaluations that we were able to do and could only evaluate a few settings of the informativity parameter for each method. We complement these human evaluations with automated evaluations on a much wider range of parameters, and analyze the correlations between human performance and judgements and the automated metrics.
Answer: "
"In spring 2024, What is the title of course 15195?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, What is the title of course 15195?

Context: In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section T offers 12.0 units. The Class meets Wednesday between 07:00PM and 08:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5210.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section U offers 12.0 units. The Class meets Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5208.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Mathematical Foundations for Computer Science' with Course ID 15151 and Section NA offers 12.0 units. The Class meets Monday Wednesday Friday between 10:00AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Peng located in Building GHC, Room 4307.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Mathematical Foundations for Computer Science' with Course ID 15151 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 01:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Peng located in Building GHC, Room 4301.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Mathematical Foundations for Computer Science' with Course ID 15151 and Section B offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Peng located in Building GHC, Room 4301.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Competition Programming I' with Course ID 15195 and Section A offers 5.0 units. The Class meets Wednesday between 07:00PM and 09:50PM ET.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section T offers 12.0 units. The Class meets Wednesday between 07:00PM and 08:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5210.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section U offers 12.0 units. The Class meets Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5208.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Mathematical Foundations for Computer Science' with Course ID 15151 and Section NA offers 12.0 units. The Class meets Monday Wednesday Friday between 10:00AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Peng located in Building GHC, Room 4307.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Mathematical Foundations for Computer Science' with Course ID 15151 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 01:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Peng located in Building GHC, Room 4301.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Mathematical Foundations for Computer Science' with Course ID 15151 and Section B offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Peng located in Building GHC, Room 4301.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Competition Programming I' with Course ID 15195 and Section A offers 5.0 units. The Class meets Wednesday between 07:00PM and 09:50PM ET.
Answer: "
"Who generates ValuePrism's contextualized values?
","['maarten sap_d655f652d02251b45db43181c5e3c73dfc59cd51_metadata.txt', 'eric xing_e6dbe34d154591618ef78d56d5e8a50583b5f9d1_content_32.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who generates ValuePrism's contextualized values?

Context: Faculty Name: maarten sap
Paperid: d655f652d02251b45db43181c5e3c73dfc59cd51
Title: Value Kaleidoscope: Engaging AI with Pluralistic Human Values, Rights, and Duties
Year: 2023
Abstract: Human values are crucial to human decision-making. Value pluralism is the view that multiple correct values may be held in tension with one another (e.g., when considering lying to a friend to protect their feelings, how does one balance honesty with friendship?). As statistical learners, AI systems fit to averages by default, washing out these potentially irreducible value conflicts. To improve AI systems to better reflect value pluralism, the first-order challenge is to explore the extent to which AI systems can model pluralistic human values, rights, and duties as well as their interaction. We introduce ValuePrism, a large-scale dataset of 218k values, rights, and duties connected to 31k human-written situations. ValuePrism's contextualized values are generated by GPT-4 and deemed high-quality by human annotators 91% of the time. We conduct a large-scale study with annotators across diverse social and demographic backgrounds to try to understand whose values are represented. With ValuePrism, we build Kaleido, an open, light-weight, and structured language-based multi-task model that generates, explains, and assesses the relevance and valence (i.e., support or oppose) of human values, rights, and duties within a specific context. Humans prefer the sets of values output by our system over the teacher GPT-4, finding them more accurate and with broader coverage. In addition, we demonstrate that Kaleido can help explain variability in human decision-making by outputting contrasting values. Finally, we show that Kaleido's representations transfer to other philosophical frameworks and datasets, confirming the benefit of an explicit, modular, and interpretable approach to value pluralism. We hope that our work will serve as a step to making more explicit the implicit values behind human decision-making and to steering AI systems to make decisions that are more in accordance with them.
Title: Contextualized Networks Reveal Heterogeneous Transcriptomic Regulation in Tumors at Sample-Specific Resolution
Authors: Caleb N. Ellington, Benjamin J. Lengerich, Thomas B.K. Watkins, Jiekun Yang, Manolis Kellis, Eric P. Xing
Section: 
1 13 6 0 2 6 0 67 2 0 26 4 0 14 6 0 2 6 (a) Known subtypes survival function 0 500 1000 1500 2000 2500 3000 3500 4000 timeline 0.0 0.2 0.4 0.6 0.8 1.0 ['KIRP'] network_subtypes Survival Function plotting shared samples only p-value 6.2327202098523915e-16 KIRP.Net.1 (13/13) KIRP.Net.2 (58/60) KIRP.Net.3 (3/9) KIRP.Net.4 (35/45) KIRP.Net.1 (13/13) At risk 13 Censored 0 Events 0 KIRP.Net.2 (58/60) At risk 60 Censored 0 Events 0 KIRP.Net.3 (3/9) At risk 9 Censored 0 Events 0 KIRP.Net.
Answer: "
"In fall 2024, What is the deadline for Mini-2 add, audit & tuition adjustment drop (deadline 1)?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2024, What is the deadline for Mini-2 add, audit & tuition adjustment drop (deadline 1)?

Context: On Friday, 25 October, 2024, during the Fall 2024 (F24) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
On Tuesday, 05 November, 2024, during the Fall 2024 (F24) semester, Democracy Day is observed, leading to No Classes, except Evening classes after 5 pm will still meet.
On Monday, 11 November, 2024, during the Fall 2024 (F24) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Wednesday, 13 November, 2024, during the Fall 2024 (F24) semester, Mini-2 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 18 November, 2024,Monday to 22 November, 2024,Friday marks the Spring 2025 Registration Week for Fall 2024 (F24) semester.
On Monday, 25 November, 2024, during the Fall 2024 (F24) semester, Mini-2 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 25 November, 2024, during the Fall 2024 (F24) semester, Semester & Mini-2 Faculty Course Evaluations open is observed.
From 27 November, 2024,Wednesday to 29 November, 2023,Friday marks the Thanksgiving Break for Fall 2024 (F24) semester which leads to No Classes.
On Friday, 06 December, 2024, during the Fall 2024 (F24) semester, Semester & Mini-2 Last Day of Classes is observed.
On Friday, 06 December, 2024, during the Fall 2024 (F24) semester, Semester & Mini-2 voucher deadline (4) is observed.
From 09 December, 2024,Monday to 10 December, 2023,Tuesday marks the Final Exams for Fall 2024 (F24) semester.
On Monday, 11 December, 2024, during the Fall 2024 (F24) semester, Reading Day is observed.
From 12 December, 2024,Thursday to 13 December, 2023,Friday marks the Final Exams for Fall 2024 (F24) semester.
On Monday, 28 August, 2023, during the Fall 2023 (F23) semester, Semester & Mini-1 Classes Begin is observed.
On Friday, 01 September, 2023, during the Fall 2023 (F23) semester, Mini-1 add, audit & tuition adjustment drop deadline  (1) is observed.
On Monday, 04 September, 2023, during the Fall 2023 (F23) semester, Labor Day is observed, leading to No Classes & University Closed.
On Monday, 11 September, 2023, during the Fall 2023 (F23) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Wednesday, 20 September, 2023, during the Fall 2023 (F23) semester, Mini-1 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 02 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Pass/no pass & withdrawal deadline (3) is observed.
On Monday, 02 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Faculty Course Evaluations open is observed.
On Monday, 09 October, 2023, during the Fall 2023 (F23) semester, Semester drop deadline is observed, leading to withdrawal grade assigned after this date.
On Friday, 13 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Last Day of Classes is observed.
On Friday, 13 October, 2023, during the Fall 2023 (F23) semester, Mini-1 voucher election deadline (4) is observed.
From 13 October, 2023,Friday to 14 October, 2023,Saturday marks the Family Weekend for Fall 2023 (F23) semester.
On Saturday, 14 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Exams is observed.
On Saturday, 14 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Faculty Course Evaluations close is observed.
From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
Answer: "
"What course did Lanni teach in Spring 2023?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What course did Lanni teach in Spring 2023?

Context: In Semester Spring 2024, from the department of Biological Sciences, the subject titled 'Biosensors' with Course ID 03737 and Section A3 offers 6.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Zhao located in Building SH, Room 236.
In Semester Spring 2024, from the department of Biological Sciences, the subject titled 'Advanced Biochemistry' with Course ID 03740 and Section A offers 12.0 units. The Class meets Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Linstedt, Lanni located in Building MI, Room SOCIAL.
In Semester Spring 2024, from the department of Biological Sciences, the subject titled 'Advanced Cell Biology' with Course ID 03741 and Section A offers 12.0 units. The Class meets Friday between 02:00PM and 04:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Lee, Linstedt, Campbell, Zhang located in Building MI, Room 355.
In Semester Spring 2024, from the department of Biological Sciences, the subject titled 'Membrane Trafficking' with Course ID 03744 and Section A offers 9.0 units. The Class meets Tuesday between 12:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Lee, Linstedt located in Building TBA, Room None.
In Semester Spring 2024, from the department of Biological Sciences, the subject titled 'Graduate Seminar' with Course ID 03750 and Section A offers 1.0 units. The Class meets Wednesday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Woolford located in Building MI, Room 348.
In Semester Spring 2024, from the department of Biological Sciences, the subject titled 'Graduate Research Club' with Course ID 03755 and Section A offers 3.0 units. The Class meets Thursday between 12:00PM and 12:50PM ET.
In Semester Spring 2024, from the department of Biological Sciences, the subject titled 'Biosensors' with Course ID 03737 and Section A3 offers 6.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Zhao located in Building SH, Room 236.
In Semester Spring 2024, from the department of Biological Sciences, the subject titled 'Advanced Biochemistry' with Course ID 03740 and Section A offers 12.0 units. The Class meets Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Linstedt, Lanni located in Building MI, Room SOCIAL.
In Semester Spring 2024, from the department of Biological Sciences, the subject titled 'Advanced Cell Biology' with Course ID 03741 and Section A offers 12.0 units. The Class meets Friday between 02:00PM and 04:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Lee, Linstedt, Campbell, Zhang located in Building MI, Room 355.
In Semester Spring 2024, from the department of Biological Sciences, the subject titled 'Membrane Trafficking' with Course ID 03744 and Section A offers 9.0 units. The Class meets Tuesday between 12:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Lee, Linstedt located in Building TBA, Room None.
In Semester Spring 2024, from the department of Biological Sciences, the subject titled 'Graduate Seminar' with Course ID 03750 and Section A offers 1.0 units. The Class meets Wednesday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Woolford located in Building MI, Room 348.
In Semester Spring 2024, from the department of Biological Sciences, the subject titled 'Graduate Research Club' with Course ID 03755 and Section A offers 3.0 units. The Class meets Thursday between 12:00PM and 12:50PM ET.
Answer: "
"In fall 2023, When is the Spring 2024 Registration Week?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, When is the Spring 2024 Registration Week?

Context: On Tuesday, 12 March, 2024, during the Spring 2024 (S24) semester, Summer 2024 Registration Opens is observed.
On Friday, 15 March, 2024, during the Spring 2024 (S24) semester, Mini-4 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 01 April, 2024, during the Spring 2024 (S24) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Wednesday, 03 April, 2024, during the Spring 2024 (S24) semester, Mini-4 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 11 April, 2024,Thursday to 13 April, 2024,Saturday marks the Spring Carnival for Spring 2024 (S24) semester which leads to No Classes.
From 15 April, 2024,Monday to 19 April, 2024,Friday marks the Fall 2024 Registration Week for Spring 2024 (S24) semester.
On Monday, 15 April, 2024, during the Spring 2024 (S24) semester, Mini-4 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 15 April, 2024, during the Spring 2024 (S24) semester, Semester & Mini-4 Faculty Course Evaluations open is observed.
On Friday, 26 April, 2024, during the Spring 2024 (S24) semester, Last Day of Classes is observed.
On Friday, 26 April, 2024, during the Spring 2024 (S24) semester, Semester & Mini-4 voucher deadline (4) is observed.
From 29 April, 2024,Monday to 30 April, 2024,Tuesday marks the Final Examinations for Spring 2024 (S24) semester.
On Wednesday, 01 May, 2024, during the Spring 2024 (S24) semester, Reading Day is observed.
From 02 May, 2024,Thursday to 03 May, 2024,Friday marks the Final Examinations for Spring 2024 (S24) semester.
From 04 May, 2024,Saturday to 05 May, 2024,Sunday marks the Reading Days for Spring 2024 (S24) semester.
From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mini-2 Classes Begin is observed.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mid-Semester & Mini-1 grades due by 4 pm is observed.
On Friday, 27 October, 2023, during the Fall 2023 (F23) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
On Tuesday, 07 November, 2023, during the Fall 2023 (F23) semester, Democracy Day is observed, leading to No Classes, except Evening classes after 5 pm will still meet.
On Saturday, 11 November, 2023, during the Fall 2023 (F23) semester, Homecoming is observed.
From 13 November, 2023,Monday to 17 November, 2023,Friday marks the Spring 2024 Registration Week for Fall 2023 (F23) semester.
On Monday, 13 November, 2023, during the Fall 2023 (F23) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Wednesday, 15 November, 2023, during the Fall 2023 (F23) semester, Mini-2 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 22 November, 2023,Wednesday to 24 November, 2023,Friday marks the Thanksgiving Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Mini-2 pass/no pass & withdrawal deadline (2) is observed.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Faculty Course Evaluations open is observed.
On Friday, 08 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Last Day of Classes is observed.
Answer: "
"In spring 2024, Who is the instructor for Dissertation Research?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, Who is the instructor for Dissertation Research?

Context: In Semester Spring 2024, from the department of Architecture, the subject titled 'Ph.D. Thesis' with Course ID 48793 and Section X offers 0-99 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Krishnamurti located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Architecture, the subject titled 'LEED:' with Course ID 48795 and Section NA offers 6.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Architecture, the subject titled 'LEED, Green Design and Building Ratings' with Course ID 48795 and Section A3 offers 6.0 units. The Class meets Monday Wednesday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Baird located in Building CFA, Room 102.
In Semester Spring 2024, from the department of Architecture, the subject titled 'PhD Dissertation Defense' with Course ID 48797 and Section A offers 5.0,36 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Sawyer located in Building TBA, Room None.
In Semester Spring 2024, from the department of Architecture, the subject titled 'PhD Dissertation Defense' with Course ID 48797 and Section B offers 5.0,36 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Byrne located in Building TBA, Room None.
In Semester Spring 2024, from the department of Architecture, the subject titled 'PhD Dissertation Defense' with Course ID 48797 and Section C offers 5.0,36 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Cardoso Llach located in Building TBA, Room None.
In Semester Spring 2024, from the department of Architecture, the subject titled 'Ph.D. Thesis' with Course ID 48793 and Section X offers 0-99 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Krishnamurti located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Architecture, the subject titled 'LEED:' with Course ID 48795 and Section NA offers 6.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Architecture, the subject titled 'LEED, Green Design and Building Ratings' with Course ID 48795 and Section A3 offers 6.0 units. The Class meets Monday Wednesday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Baird located in Building CFA, Room 102.
In Semester Spring 2024, from the department of Architecture, the subject titled 'PhD Dissertation Defense' with Course ID 48797 and Section A offers 5.0,36 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Sawyer located in Building TBA, Room None.
In Semester Spring 2024, from the department of Architecture, the subject titled 'PhD Dissertation Defense' with Course ID 48797 and Section B offers 5.0,36 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Byrne located in Building TBA, Room None.
In Semester Spring 2024, from the department of Architecture, the subject titled 'PhD Dissertation Defense' with Course ID 48797 and Section C offers 5.0,36 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Cardoso Llach located in Building TBA, Room None.
Answer: "
"What is Carolyn Penstein Rose's phone number?
","['mcds-student-handbook-2023_2024.txt', 'carolyn_rose.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is Carolyn Penstein Rose's phone number?

Context: To impact society in a transformative way — regionally, nationally, and 
globally — by engaging with partners outside the traditional borders of the 
university campus. 
1.4 MCDS Contact Information 
The people responsible for administering the MCDS degree are: 
 
Jennifer M Lucas 
Academic Program Manager 
Master of Computational Data Science  
Language Technologies Institute 
School of Computer Science  
Carnegie Mellon University 
Gates-Hillman Center 6415 
5000 Forbes Avenue, Pgh, PA 15213 
Phone: (412) 268-9870 
Fax: (412) 268-7287 
 
 
Carolyn Penstein Rosé, Director 
Master of Computational Data Science  
Language Technologies Institute 
School of Computer Science 
Carnegie Mellon University 
Gates-Hillman Center 5419 
5000 Forbes Avenue, Pgh, PA 15213 
Phone: (412) 268-4525 
Fax: (412) 268-7287 
Robert Frederking 
Graduate Program Chair 
Language Technologies Institute 
School of Computer Science  
Carnegie Mellon University 
Gates-Hillman Center 6515 
5000 Forbes Avenue, Pgh, PA 15213 
Phone: (412) 268-6656 
Mona Diab, LTI Director 
Language Technologies Institute 
School of Computer Science  
Carnegie Mellon University 
Gates-Hillman Center 5723 
5000 Forbes Avenue, Pgh, PA 15213 
Phone: (412) 268-3669 
 
The Language Technologies Institute is located primarily on the 5 th and 6th floors of the 
Gates Hillman Complex (GHC) on Carnegie Mellon’s Pittsburgh campus: 
 
Language Technologies Institute 
Carnegie Mellon University 
5000 Forbes Avenue 
Gates Hillman Complex 5402, LTI 
Pittsburgh, PA 15241-3891 
 
412-268-6591 (phone) 
412-268-6298 (fax) 
http://www.lti.cs.cmu.edu/ 
 
 
9
1.5 University Policies and Expectations 
Each member of the Carnegie Mellon community must be familiar with university 
policies and guidelines.
Carolyn Rosé
Professor, Language Technologies Institute
Human-Computer Interaction Institute
Contact
5415 —Gates & Hillman Centers
Email
412-268-7130
Research Area
Computer-Supported Collaborative Learning/MOOCs, Information Retrieval, Text Mining and Analytics, Language Technologies for Education, Natural Language Processing and Computational Linguistics

Education
Ph.D. in Language and Information Technology
1997
Employer Upon Graduation: 
Faculty, LTI CMU
Research
My research focuses on better understanding the social and pragmatic nature of conversation, and using this understanding to build computational systems that improve the efficacy of conversation both between people, and between people and computers. In order to pursue these goals, I invoke approaches from computational discourse analysis and text mining, conversational agents, and computer-supported collaborative learning. I ground my research in the fields of language technologies and human-computer interaction, and am fortunate to work closely with students and post-docs from the LTI and the Human-Computer Interaction Institute, as well as to direct my own lab, TELEDIA. My group’s highly interdisciplinary work, published in 160 peer-reviewed publications, is represented in the top venues in five fields: language technologies, learning sciences, cognitive science, educational technology and human-computer interaction.

An exciting direction of my group's work is spearheading a satellite working group to support social interaction for learning in MOOCs with EdX called DANCE. My research toward this end has birthed and substantially contributed to the growth of two thriving interrelated research areas: automated analysis of collaborative learning processes and dynamic support for collaborative learning. Both areas use intelligent conversational agents to support collaborative learning in a context-sensitive way.

All of my work draws insight from rich theoretical models from sociolinguistics and discourse analysis, and pares them down to precise operationalizations that capture the most important essence for achieving impact. I always start by investigating how conversation works and formalizing this understanding in models that are precise enough to be reproducible and that demonstrate explanatory power in connection with outcomes with real-world value. The next step is to adapt, extend and apply machine learning and text mining technologies that leverage this deep understanding to build computational models capable of automatically applying these constructs to naturally occurring language interactions. Finally, with the technology to automatically monitor naturalistic language communication in place, we can build interventions with real-world benefits.
Answer: "
"When was Human-Computer Interaction Institute formed?
","['history_d401.txt', 'carolyn_rose.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When was Human-Computer Interaction Institute formed?

Context: As the 1970s progressed, Newell became interested in human-computer interaction, and began a long relationship with Xerox’s Palo Alto Research Center, or PARC, which released the Xerox Alto in 1973. Considered a forerunner to many of the computing environments that followed, Alto featured a graphical-user interface and was among the first commercially available workstations controlled with a mouse. Inspired by Alto, Reddy launched a drive for development of CMU’s own “three-M” machine—a personal workstation with a megabyte of memory, a megapixel display and at least one million instructions per second of processing power.Launching a Robotics InstituteIn 1979, an executive at Pittsburgh’s Westinghouse Electric Corp., Tom Murrin, collaborated with Jordan and Reddy to create the Robotics Institute, with Reddy as its first director. By 1982, the Computer Science Department included more than 30 faculty members and 100 graduate students.The best-wired campus in the worldWorking with IBM in the early 1980s, the university and the Computer Science Department established another new research frontier: Development of a high-speed computer network that would reach virtually every room on campus, along with a GUI-based computing environment, and providing networked PCs or workstations for 7,000 students, faculty members and employees. Called the Andrew Project, it turned Carnegie Mellon into the best-connected, most-wired university in the world—a process Newell called “greening up the campus with computer science.” CMU also became home to a new Software Engineering Institute, funded by the Defense Department, to study computer security and develop best practices in the design of operating systems. Between 1982 and 1985, the amount of sponsored research in the Computer Science Department doubled, from $7.2 million to $15.3 million—more than the other four departments in the Mellon College of Science combined.A “school of computer science” is proposedFeeling that CSD’s needs were inadequately represented in MCS, CSD head A. Nico Habermann and then-CMU provost Angel Jordan in 1986 wrote a white paper proposing the creation of “a School of Computer Science.” Responding to concerns from the faculty that the change might be taking place too quickly, the university first established a free-floating Department of Computer Science. The experiment, which lasted two years, was an unqualified success.
Carolyn Rosé
Professor, Language Technologies Institute
Human-Computer Interaction Institute
Contact
5415 —Gates & Hillman Centers
Email
412-268-7130
Research Area
Computer-Supported Collaborative Learning/MOOCs, Information Retrieval, Text Mining and Analytics, Language Technologies for Education, Natural Language Processing and Computational Linguistics

Education
Ph.D. in Language and Information Technology
1997
Employer Upon Graduation: 
Faculty, LTI CMU
Research
My research focuses on better understanding the social and pragmatic nature of conversation, and using this understanding to build computational systems that improve the efficacy of conversation both between people, and between people and computers. In order to pursue these goals, I invoke approaches from computational discourse analysis and text mining, conversational agents, and computer-supported collaborative learning. I ground my research in the fields of language technologies and human-computer interaction, and am fortunate to work closely with students and post-docs from the LTI and the Human-Computer Interaction Institute, as well as to direct my own lab, TELEDIA. My group’s highly interdisciplinary work, published in 160 peer-reviewed publications, is represented in the top venues in five fields: language technologies, learning sciences, cognitive science, educational technology and human-computer interaction.

An exciting direction of my group's work is spearheading a satellite working group to support social interaction for learning in MOOCs with EdX called DANCE. My research toward this end has birthed and substantially contributed to the growth of two thriving interrelated research areas: automated analysis of collaborative learning processes and dynamic support for collaborative learning. Both areas use intelligent conversational agents to support collaborative learning in a context-sensitive way.

All of my work draws insight from rich theoretical models from sociolinguistics and discourse analysis, and pares them down to precise operationalizations that capture the most important essence for achieving impact. I always start by investigating how conversation works and formalizing this understanding in models that are precise enough to be reproducible and that demonstrate explanatory power in connection with outcomes with real-world value. The next step is to adapt, extend and apply machine learning and text mining technologies that leverage this deep understanding to build computational models capable of automatically applying these constructs to naturally occurring language interactions. Finally, with the technology to automatically monitor naturalistic language communication in place, we can build interventions with real-world benefits.
Answer: "
"Which professors at LTI are on leave?
","['faculty_overview.txt', 'mona_diab.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which professors at LTI are on leave?

Context: yonatan bisk is an assistant professor, language technologies institute at CMU.
ralf brown is the senior systems scientist/chair of admissions, language technologies institute at CMU.
jamie callan is a professor, language technologies institute at CMU.
justine cassel is a professor (on leave), language technologies institute at CMU.
mona diab is the lti director and tenured professor, language technologies institute at CMU.
fernando diaz is an associate professor, language technologies institute at CMU.
scott fahlman is the professor emeritus, language technologies institute at CMU.
robert frederking is the associate dean for PhD programs and chair for MLT program, language technologies institute at CMU.
daniel fried is an assistant professor, language technologies institute at CMU.
alexander hauptmann is a research professor, language technologies institute at CMU.
daphne ippolito is an assistant professor, language technologies institute at CMU.
lori levin is a research professor, language technologies institute at CMU.
lei li is an assistant professor, language technologies institute at CMU.
teruko mitamura is a research professor, language technologies institute at CMU.
louis-philippe morency is the leonardo associate professor of computer science (on leave), language technologies institute at CMU.
david mortensen is an assistant research professor, language technologies institute at CMU.
graham neubig is an associate professor, language technologies institute at CMU.
eric nyberg is a professor, language technologies institute at CMU.
kemal oflazer is a teaching professor, language technologies institute at CMU.
bhiksha raj is a professor, language technologies institute at CMU.
carolyn rose is a professor, language technologies institute at CMU.
alexander rudnicky is the ""research professor emeritus"", language technologies institute at CMU.
maarten sap is an assistant professor, language technologies institute at CMU.
michael shamos is the ""distinguished career professor"", language technologies institute at CMU.
rita singh is an associate research professor, language technologies institute at CMU.
emma strubell is an assistant professor, language technologies institute at CMU.
alexander waibel is a professor (on leave), language technologies institute at CMU.
shinji watanabe is an associate professor, language technologies institute at CMU.
Mona Diab
LTI Director and Tenured Professor, Language Technologies Institute
Research Area : Fairness and Ethics in Language Technology
Contact: 5723 Gates & Hillman Centers
Email : mdiab@andrew.cmu.edu
Phone : 412-268-3669
Address
5000 Forbes Avenue
Pittsburgh, PA 15213
Answer: "
"Which LTI faculty member is an author on ""Aligning Large Multimodal Models with Factually Augmented RLHF""?
","['daphne ippolito_8724579d3f126e753a0451d98ff57b165f722e72_metadata.txt', 'bhiksha raj_f5a7a4fda49c657742072a2758f43b1cbcde3886_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which LTI faculty member is an author on ""Aligning Large Multimodal Models with Factually Augmented RLHF""?

Context: Faculty Name: daphne ippolito
Paperid: 8724579d3f126e753a0451d98ff57b165f722e72
Title: Are aligned neural networks adversarially aligned?
Year: 2023
Abstract: Large language models are now tuned to align with the goals of their creators, namely to be""helpful and harmless.""These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study to what extent these models remain aligned, even when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs. However the recent trend in large-scale ML models is multimodal models that allow users to provide images that influence the text that is generated. We show these models can be easily attacked, i.e., induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image. We conjecture that improved NLP attacks may demonstrate this same level of adversarial control over text-only models.
Authors: Nicholas Carlini, Milad Nasr, Christopher A. Choquette-Choo, Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tramèr, Ludwig Schmidt
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'It is shown that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models, and conjecture that improved NLP attacks may demonstrate this same level of adversarial control over text-only models.'}
Url: http://arxiv.org/pdf/2306.15447
Faculty Name: bhiksha raj
Paperid: f5a7a4fda49c657742072a2758f43b1cbcde3886
Title: Continual Contrastive Spoken Language Understanding
Year: 2023
Abstract: Recently, neural networks have shown impressive progress across diverse fields, with speech processing being no exception. However, recent breakthroughs in this area require extensive offline training using large datasets and tremendous computing resources. Unfortunately, these models struggle to retain their previously acquired knowledge when learning new tasks continually, and retraining from scratch is almost always impractical. In this paper, we investigate the problem of learning sequence-to-sequence models for spoken language understanding in a class-incremental learning (CIL) setting and we propose COCONUT, a CIL method that relies on the combination of experience replay and contrastive learning. Through a modified version of the standard supervised contrastive loss applied only to the rehearsal samples, COCONUT preserves the learned representations by pulling closer samples from the same class and pushing away the others. Moreover, we leverage a multimodal contrastive loss that helps the model learn more discriminative representations of the new data by aligning audio and text features. We also investigate different contrastive designs to combine the strengths of the contrastive loss with teacher-student architectures used for distillation. Experiments on two established SLU datasets reveal the effectiveness of our proposed approach and significant improvements over the baselines. We also show that COCONUT can be combined with methods that operate on the decoder side of the model, resulting in further metrics improvements.
Authors: Umberto Cappellazzo, Enrico Fini, Muqiao Yang, Daniele Falavigna, A. Brutti, Bhiksha Raj
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper investigates the problem of learning sequence-to-sequence models for spoken language understanding in a class-incremental learning (CIL) setting and proposes COCONUT, a CIL method that relies on the combination of experience replay and contrastive learning.'}
Url: https://arxiv.org/pdf/2310.02699
Answer: "
"What does CodeBERTScore encode in addition to the generated tokens?
","['graham neubig_31366ff634fc905affd78dbd8ddc9a872c006a87_metadata.txt', 'lei li_75b68d0903af9d9f6e47ce3cf7e1a7d27ec811dc_content_2.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What does CodeBERTScore encode in addition to the generated tokens?

Context: Faculty Name: graham neubig
Paperid: 31366ff634fc905affd78dbd8ddc9a872c006a87
Title: CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code
Year: 2023
Abstract: Since the rise of neural natural-language-to-code models (NL->Code) that can generate long expressions and statements rather than a single next-token, one of the major problems has been reliably evaluating their generated output. In this paper, we propose CodeBERTScore: an evaluation metric for code generation, which builds on BERTScore (Zhang et al., 2020). Instead of encoding only the generated tokens as in BERTScore, CodeBERTScore also encodes the natural language input preceding the generated code, thus modeling the consistency between the generated code and its given natural language context as well. We perform an extensive evaluation of CodeBERTScore across four programming languages. We find that CodeBERTScore achieves a higher correlation with human preference and with functional correctness than all existing metrics. That is, generated code that receives a higher score by CodeBERTScore is more likely to be preferred by humans, as well as to function correctly when executed. We release five language-specific pretrained models to use with our publicly available code. Our language-specific models have been downloaded more than 1,000,000 times from the Huggingface Hub. Our code and data are available at https://github.com/neulab/code-bert-score
Authors: Shuyan Zhou, Uri Alon, Sumit Agarwal, Graham Neubig
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'It is found that generated code that receives a higher score by CodeBERTScore is more likely to be preferred by humans, as well as to function correctly when executed, than all existing metrics.'}
Url: http://arxiv.org/pdf/2302.05527
Title: Provable Robust Watermarking for AI-Generated Text
Authors: Xuandong Zhao, Prabhanjan Ananth, Lei Li, Yu-Xiang Wang
Section: 1. Assume m ≥ 1, then
tokens is mγ/2. Stegnography attack One may extend the attack even further by asking the language model to encode a message, which swaps each token in the vocabulary with another token through a secret codebook. For example, whenever you want to output Token i, output Token mod(i+1, N) instead. If the “code book” is supplied in the prompt with an instruction for the LM to follow the code book when generating the text, then it really breaks all watermarks including ours, while allowing the user who knows the code book to easily revert it to the original text. The issue of such an attack is that it requires significantly heavy-lifting for the language model to predict outside the typical distribution it is trained on. There is no real risk of such an attack being employed as it is likely to significantly reduce the quality of the generated text. To be clear, these attacks are, in fact, not post-processing-based evasion attacks, but rather hacks into prompts. Nevertheless, our watermark that is robust to edits turns out to be quite resilient to them.
Answer: "
"In which month and year the Mascot Identity Task Force was formed?
","['mascot_d405.txt', 'tartanfacts_d404.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In which month and year the Mascot Identity Task Force was formed?

Context: About Scotty
The Scottish terrier has long been a familiar figure around Carnegie Mellon's campus. For years students have suited up in an unofficial Scottish terrier costume to excite the fans at athletic events. But the relationship between the Scottish terrier breed and Carnegie Mellon far precedes anybody doing somersaults in a dog costume. Andrew Carnegie, founder of the university, kept a Scottish terrier as his pet.
Scotty's road from popular icon to official mascot of the university began in 2006. Carnegie Mellon formed a Mascot Identity Task Force in November 2006, which consisted of students, faculty, staff and alumni. The Task Force was co-chaired by Director of Athletics Susan Bassett and Dean of Student Affairs Jennifer Church.
The mascot selection process included a series of surveys and a university Town Hall meeting. Nearly 78 percent of 2,370 students surveyed in February 2007 voted for the Scottish terrier, and approximately 25 percent of 400 alumni surveyed thought the Scottish terrier was already the mascot.
In the spring, the Task Force partnered with SME Branding — a firm with more than 17 years of experience creating mascots for professional sports teams and universities — to develop the graphics for the mascot. During October, students and alumni reviewed potential mascot images in focus groups.
Carnegie Mellon's official mascot debuted at the Nov. 10, 2007 home football game. The graphic features a profile of a distinguished, bold Scottish terrier sporting a plaid scarf around his neck. The dog is contained in a shield, representing Carnegie Mellon's Scottish heritage.
The Task Force then partnered with a mascot costume company to design our Scottish terrier in the winter of 2007. The official Scotty costume was unveiled at the 2008 Spring Carnival.
The
Scottish terrier breed
is known for its keen, alert and intelligent expression. Its temperament is described as determined and thoughtful while its physical aspects exemplify strength, power and agility in a small package. Many of these traits are also apparent throughout the university, making the Scottish terrier a natural choice for Carnegie Mellon's mascot.
Fun Fact
In keeping with tradition, students, faculty, staff and alumni voted to name the new official mascot Scotty. But it's not just the costumed mascot that voters named. The live dog is also known as Scotty to the Carnegie Mellon community.
About Scotty
|
Celebrating Scotty
|
Performer Requests
Department Quick Facts
Name of School: Carnegie Mellon University
City/Zip: Pittsburgh, PA 15213
Founded: 1900
Enrollment (undergrad): 6,982- Fall 2021
Enrollment (graduate and doctoral): 7,062 - Fall 2021
Alumni: 102,577
Nickname: Tartans
Mascot: Scottie Dog
School Colors: Cardinal and Gray
Football Stadium and Track and Field: Gesling Stadium
Capacity: 3,500
Surface: FieldTurf
Basketball/Volleyball Gym: Wiegand Gymnasium
Capacity: 500
Soccer Stadium: CMU Soccer Field
Capacity: 250
Surface: FieldTurf
Affiliation: NCAA Division III
Conference: University Athletic Association | Presidents' Athletic Conference (Football Only)
President: Dr. Farnam Jahanian
Alma Mater, Year: University of Texas at Austin
Athletic Director: Dr. Josh Centor
Alma Mater, Year: Brandeis, 2004
Athletic Department Phone: 412-268-8054
Mailing Address:  5000 Forbes Avenue / Pittsburgh, PA 15213
Answer: "
"What's the URL for the code and data of InPars-light
","['eric nyberg_3a30217c4115777fb30c182c97cc77d34d065556_metadata.txt', 'eric nyberg_3a30217c4115777fb30c182c97cc77d34d065556_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What's the URL for the code and data of InPars-light

Context: (2022). We believe that InPars-light is the first truly cost-effective prompt-based unsupervised recipe to train and deploy neural ranking models that outperform BM25. Our code and data is publicly available. https://github.com/searchivarius/inpars_light/
Authors: Leonid Boytsov, Preksha Patel, Vivek Sourabh, Riddhi Nisar, Sayan Kundu, R. Ramanathan, Eric Nyberg
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'InPars-light is the first truly cost-effective prompt-based unsupervised recipe to train and deploy neural ranking models that outperform BM25.'}
Url: http://arxiv.org/pdf/2301.02998
Faculty Name: eric nyberg
Paperid: 3a30217c4115777fb30c182c97cc77d34d065556
Title: InPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers
Year: 2023
Abstract: We carried out a reproducibility study of InPars, which is a method for unsupervised training of neural rankers (Bonifacio et al., 2022). As a by-product, we developed InPars-light, which is a simple-yet-effective modification of InPars. Unlike InPars, InPars-light uses 7x-100x smaller ranking models and only a freely available language model BLOOM, which -- as we found out -- produced more accurate rankers compared to a proprietary GPT-3 model. On all five English retrieval collections (used in the original InPars study) we obtained substantial (7%-30%) and statistically significant improvements over BM25 (in nDCG and MRR) using only a 30M parameter six-layer MiniLM-30M ranker and a single three-shot prompt. In contrast, in the InPars study only a 100x larger monoT5-3B model consistently outperformed BM25, whereas their smaller monoT5-220M model (which is still 7x larger than our MiniLM ranker) outperformed BM25 only on MS MARCO and TREC DL 2020. In the same three-shot prompting scenario, our 435M parameter DeBERTA v3 ranker was at par with the 7x larger monoT5-3B (average gain over BM25 of 1.3 vs 1.32): In fact, on three out of five datasets, DeBERTA slightly outperformed monoT5-3B. Finally, these good results were achieved by re-ranking only 100 candidate documents compared to 1000 used by Bonifacio et al. (2022). We believe that InPars-light is the first truly cost-effective prompt-based unsupervised recipe to train and deploy neural ranking models that outperform BM25. Our code and data is publicly available.
Answer: "
"Is a valid CMU ID needed to use the tennis court?
","['handbook_phd_2023-2024.txt', 'mlt-student-handbook-2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Is a valid CMU ID needed to use the tennis court?

Context: The administrative offices are located in the Cohon University Fitness Center. 
The University Centers recreational facilities include an eight-lane pool, racquetball and squash 
courts, aerobics room, fitness center and gym for basketball and volleyball. With renovations to 
Skibo Gym and the new Highmark Center for Health, Wellness, and Athletics scheduled for 
completion in 2024, the strength and conditioning facility has been temporarily placed on the 
lawn next to the outdoor basketball court close to the Donner locker rooms, Gesling Stadium, and 
Weigand Gymnasium. All users must present a valid CMU ID to use these facilities. 
6.18 CMU Alert 
www.cmu.edu/alert  
CMU Alert sends voice and/or text messages to registered phones in the event of a campus 
emergency that threatens public safety or during tests of the system in the spring and fall 
semesters. Students are automatically registered for CMU-Alert using the current contact 
information that has been entered into the Student Information Online (SIO): 
https://www.cmu.edu/hub/sio/about.html. 
 
 
 
 
LTI Ph.D. Graduate Student Handbook 
Page 40 
 
 
Appendix A 
2023-2024 
Highlighted University Resources for Graduate Students 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Note: The following pages are meant to broadly include only some of the resources available to 
graduate students.  It is not an exhaustive appendix of resources, and students are strongly 
encouraged to visit the various websites linked below for the most up-to-date information.   
LTI Ph.D. Graduate Student Handbook 
Page 41 
 
A.1 Key Resources for Graduate Student Support  
A.1.1 Office of Graduate and Postdoctoral Affairs  
https://www.cmu.edu/graduate  graded@cmu.edu  
The Office of Graduate and Postdoctoral Affairs provides university-wide support for all graduate 
students and academic programs, with a focus on supporting graduate student success at 
Carnegie Mellon.
Affiliate ID Cards are available for spouses and domestic partners of graduate students that 
allow them to access Carnegie Mellons campus. These cards are available through The HUB to 
spouses and partners of graduate students who are enrolled for the current academic year in a 
full-time graduate degree program. The card is valid for one year. For more information about 
student and affiliate ID cards, please visit: http://www.cmu.edu/idplus/idcards/cardtypes.html.  
7.12 Domestic Partner Registration 
Carnegie Mellon extends certain benefits to domestic partners of students. Eligible students 
may elect benefits for their domestic partners through the registration process orchestrated by 
the Office of the Dean of Student Affairs, 3rd floor Warner Hall. Information regarding the 
benefits available for domestic partners, eligibility for domestic partner benefits, registration 
instructions, and forms can be located at: 
www.studentaffairs.cmu.edu/dean/domestic_partner/index.html.  
MLT Graduate Student Handbook 
Page 34 
 
7.13 Housing 
The university does not currently offer housing to graduate students. The Office of Housing and 
Dining Services does provide community housing information to assist graduate students who 
are seeking housing in the communities surrounding the university, including information on 
the legal aspects of renting an apartment, moving checklists, and the off-campus housing 
database. This information can be located at: www.cmu.edu/housing/community-
housing/index.html.  
7.14 Dining 
www.cmu.edu/dining/  
Dining services and operations are offered through the Office of Housing and Dining Services. 
The office operates dining locations open around campus in academic buildings, Hunt Library, 
and the University Center. These locations offer flexible hours with options from the early 
morning through late night. The Dining Service website contains information about dining 
locations, hours of operation, graduate student dining plans forms, nutritional information, and 
weekly menus for dining locations. 
7.15 Parking and Transportation 
www.cmu.edu/parking/ 
Graduate students will find information about parking and availability, parking policies, 
transportation options and Port Authority Transit usage with a valid university ID on the Parking 
and Transportation Services site. The Parking and Transportation Services office is located in 
the lower level of the East Campus Garage. There is limited parking on campus and the varying 
permit rates can be found on the website.
Answer: "
"What city is CMU LTI located in?
","['handbook_phd_2023-2024.txt', 'mlt-student-handbook-2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What city is CMU LTI located in?

Context: The administrative offices are located in the Cohon University Fitness Center. 
The University Centers recreational facilities include an eight-lane pool, racquetball and squash 
courts, aerobics room, fitness center and gym for basketball and volleyball. With renovations to 
Skibo Gym and the new Highmark Center for Health, Wellness, and Athletics scheduled for 
completion in 2024, the strength and conditioning facility has been temporarily placed on the 
lawn next to the outdoor basketball court close to the Donner locker rooms, Gesling Stadium, and 
Weigand Gymnasium. All users must present a valid CMU ID to use these facilities. 
6.18 CMU Alert 
www.cmu.edu/alert  
CMU Alert sends voice and/or text messages to registered phones in the event of a campus 
emergency that threatens public safety or during tests of the system in the spring and fall 
semesters. Students are automatically registered for CMU-Alert using the current contact 
information that has been entered into the Student Information Online (SIO): 
https://www.cmu.edu/hub/sio/about.html. 
 
 
 
 
LTI Ph.D. Graduate Student Handbook 
Page 40 
 
 
Appendix A 
2023-2024 
Highlighted University Resources for Graduate Students 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Note: The following pages are meant to broadly include only some of the resources available to 
graduate students.  It is not an exhaustive appendix of resources, and students are strongly 
encouraged to visit the various websites linked below for the most up-to-date information.   
LTI Ph.D. Graduate Student Handbook 
Page 41 
 
A.1 Key Resources for Graduate Student Support  
A.1.1 Office of Graduate and Postdoctoral Affairs  
https://www.cmu.edu/graduate  graded@cmu.edu  
The Office of Graduate and Postdoctoral Affairs provides university-wide support for all graduate 
students and academic programs, with a focus on supporting graduate student success at 
Carnegie Mellon.
LTIs printers 
are located in GHC 5404 and GHC 6604. The School of Computer Science provides a number of 
black-and-white and color printers for use by students.  The SCS Computer Facilities publishes a 
list of printers online at http://www.cs.cmu.edu/~help/printing/.  
 
MLT Graduate Student Handbook 
Page 11 
 
2.3 Office Space for MS Students 
To help them create a sense of community, full time students in the LTIs MLT program have 
access to a shared office space. 
2.4 Computers for MS Students 
Students are expected to provide their own laptop computers that can be used to access 
university resources and complete course assignments. Laptops running Windows, MacOS, and 
Linux software are all acceptable. 
MS students will be given a CS user ID. A CS user ID is required to use the LTI computer cluster, 
department printers, and other SCS services. The School of Computer Science has a Help Center 
located at 4203 GHC.  They can be contacted at help@cs.cmu.edu, extension 8-4231 from a 
campus phone, or 412-268-4231 from an outside line. 
MS students will be given access to the LTIs computer cluster on an as-needed basis, to be 
used for course assignments, directed study projects, and/or capstone projects. The LTI cluster 
provides storage and computation for projects involving large datasets and/or lengthy 
computation.  
3 Masters Degree Completion and Certification 
3.1 Standard Degree Requirements and Degree Certification 
3.1.1 Graduate Students 
Carnegie Mellon graduate students are expected to complete their degree requirements within 
the standard length of time for their program of study as outlined in the relevant Graduate 
Student Handbook. Standard program lengths for graduate students vary significantly-- ranging 
from two semesters for some full-time masters programs to several or more years for doctoral 
programs. Upon completion of the graduate program degree requirements, the degree will be 
certified by the students academic program in the semester in which the student completes 
the requirements. 
3.1.2 Early Completion 
Graduate students who consider the completion of all degree requirements in less than the 
standard length of time for their program of study may consult with their degree-granting 
program or department to determine if early degree certification is allowed and under what 
circumstances.
Answer: "
"When did independent organizations, other than fraternities, enter Buggy for the first time?
","['Apr-11_Eventno_26_BuggyAlumniAssociationWelcomeEvent.txt', 'Apr-13_Eventno_2_BuggyAlumniAssociationInformationTent.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When did independent organizations, other than fraternities, enter Buggy for the first time?

Context: Event: Buggy Alumni Association Welcome Event
Date: 4/11/24
Time: 7:00 PM-9:00 PM ET
Participants/Audience: Open to Buggy community 
Event Details: 
Join the Buggy Alumni Association for food and drinks and make your predictions for the Sweepstakes prelims. 

Note: Pay on your own at restaurant. Open to members of the Buggy community.
Event: Buggy Alumni Association Information Tent
Date: 4/13/24
Time: 8:00 AM-12:00 PM ET
Participants/Audience: Open to entire CMU community 
Event Details: 
Come by the Buggy Alumni Association tent during Sweepstakes races to meet your BAA officers and learn about our work. 

Note: No registration required. No event fee. This event is open to the entire CMU community and their guests.
Answer: "
"What is Mona Diab's phone number according to the MCDS handbook?
","['mcds-student-handbook-2023_2024.txt', 'mona_diab.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is Mona Diab's phone number according to the MCDS handbook?

Context: To impact society in a transformative way — regionally, nationally, and 
globally — by engaging with partners outside the traditional borders of the 
university campus. 
1.4 MCDS Contact Information 
The people responsible for administering the MCDS degree are: 
 
Jennifer M Lucas 
Academic Program Manager 
Master of Computational Data Science  
Language Technologies Institute 
School of Computer Science  
Carnegie Mellon University 
Gates-Hillman Center 6415 
5000 Forbes Avenue, Pgh, PA 15213 
Phone: (412) 268-9870 
Fax: (412) 268-7287 
 
 
Carolyn Penstein Rosé, Director 
Master of Computational Data Science  
Language Technologies Institute 
School of Computer Science 
Carnegie Mellon University 
Gates-Hillman Center 5419 
5000 Forbes Avenue, Pgh, PA 15213 
Phone: (412) 268-4525 
Fax: (412) 268-7287 
Robert Frederking 
Graduate Program Chair 
Language Technologies Institute 
School of Computer Science  
Carnegie Mellon University 
Gates-Hillman Center 6515 
5000 Forbes Avenue, Pgh, PA 15213 
Phone: (412) 268-6656 
Mona Diab, LTI Director 
Language Technologies Institute 
School of Computer Science  
Carnegie Mellon University 
Gates-Hillman Center 5723 
5000 Forbes Avenue, Pgh, PA 15213 
Phone: (412) 268-3669 
 
The Language Technologies Institute is located primarily on the 5 th and 6th floors of the 
Gates Hillman Complex (GHC) on Carnegie Mellon’s Pittsburgh campus: 
 
Language Technologies Institute 
Carnegie Mellon University 
5000 Forbes Avenue 
Gates Hillman Complex 5402, LTI 
Pittsburgh, PA 15241-3891 
 
412-268-6591 (phone) 
412-268-6298 (fax) 
http://www.lti.cs.cmu.edu/ 
 
 
9
1.5 University Policies and Expectations 
Each member of the Carnegie Mellon community must be familiar with university 
policies and guidelines.
Mona Diab
LTI Director and Tenured Professor, Language Technologies Institute
Research Area : Fairness and Ethics in Language Technology
Contact: 5723 Gates & Hillman Centers
Email : mdiab@andrew.cmu.edu
Phone : 412-268-3669
Address
5000 Forbes Avenue
Pittsburgh, PA 15213
Answer: "
"When was the department of Computer Science (CSD) established at CMU?
","['history_d401.txt', 'history_d401.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When was the department of Computer Science (CSD) established at CMU?

Context: The experiment, which lasted two years, was an unqualified success. Separately, and also in 1986, the Pittsburgh Supercomputing Center was created as a joint effort between CMU, the University of Pittsburgh and Westinghouse Electric Corp.SCS is officially formedCMU’s Faculty Senate in the fall of 1988 agreed to President Richard Cyert’s plan to elevate the Department of Computer Science to college status. In addition to the Computer Science Department, SCS also incorporated the Robotics Institute, the Center for Machine Translation, and researchers from the Information Technology Center, which had developed Andrew. On Dec. 13, 1988, Cyert (H’89) told faculty and staff that Habermann had been appointed CMU’s first Dean of Computer Science, effective Dec. 1, and that the School of Computer Science would soon begin operations. SCS made its formal debut on Dec. 22, 1988, with a reception in the Wherrett Room of Skibo Hall, CMU’s student union. The official announcement of CMU’s new “graduate School of Computer Science” was made Jan. 3, 1989.Undergraduate degrees beginFor several years, undergraduates interested in computer science pursued an “applied math/CS” bachelor’s degree offered by the Mathematics Department. CSD professor Mary Shaw (CS’72) led CMU’s first effort to design an undergraduate curriculum solely in computer science. She and her colleagues were guided by the Carnegie Plan—guidelines established in 1938 under Carnegie Tech President Robert Doherty (A’40, E’48, H’50), outlining the principles of a sound professional education. Drawing on Shaw’s plan and also on the work of other faculty members, an undergraduate program in computer science was created during the 1989-90 academic year. Seven CS majors were admitted to the program as sophomores. Another 73 undergraduates were admitted in 1990–91.
The university’s first computer science Ph.D.s were graduates of this program.Computer Science Department establishedIn 1965, Carnegie Tech established its Computer Science Department, or CSD, with a $5 million grant from the R.K. Mellon Foundation. Perlis was the first department head. There were no undergraduates; only Ph.D. students were admitted, and the department’s focus was on research, much of it funded by the federal government through the Defense Department’s Advanced Research Projects Agency, or DARPA. In 1967, Carnegie Institute of Technology merged with the nearby Mellon Institute of Industrial Research to form Carnegie-Mellon University. The Department of Computer Science moved into the newly created Mellon Institute of Science, later renamed Mellon College of Science, or MCS. Future SCS dean Raj Reddy joined the CSD in 1969 after three years as an assistant professor at Stanford. He brought with him research in speech, language and computer vision. But in 1970 and 1971, the new Computer Science Department faced its first crisis, as half of its tenured faculty members—including department head Perlis—left for other universities. Joe Traub was recruited from Bell Labs to CMU to become the new department head.Multi-processor machines emergeCSD emerged from the brief crisis as a highly agile, interdisciplinary entity, with many new faculty members taking joint appointments with other CMU departments. Several large projects emerged in the Computer Science Department, including C.mmp, the first shared-memory multiprocessor computer, with 16 processing units, and Cm*, a 50-processor computer. These computers were the forerunners of today’s ubiquitous multi-core desktops and laptops.Turing Awards and a Nobel PrizeIn 1975, Simon and Newell were awarded the A.M. Turing Award for their work in artificial intelligence. (As of 2014, 12 CMU alumni or faculty have been awarded Turings, sometimes considered the Nobel Prize of computing.) Three years later, Simon received the Nobel Prize in Economics for his work on decision-making theory. As the 1970s progressed, Newell became interested in human-computer interaction, and began a long relationship with Xerox’s Palo Alto Research Center, or PARC, which released the Xerox Alto in 1973.
Answer: "
"In spring 2024, Who is the instructor for course 10403?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, Who is the instructor for course 10403?

Context: In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Deep Reinforcement Learning & Control' with Course ID 10403 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Fragkiadaki located in Building GHC, Room 4215.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Machine Learning with Large Datasets (Undergraduate)' with Course ID 10405 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Gordon, Talwalkar located in Building DH, Room 2210.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Foundations of Learning, Game Theory, and Their Connections' with Course ID 10422 and Section A offers 12.0 units. The Class meets Monday Wednesday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Balcan located in Building GHC, Room 4211.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Generative AI' with Course ID 10423 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Gormley, Li, Lipton located in Building GHC, Room 4401.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Senior Research Project' with Course ID 10500 and Section A offers 612 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Gormley located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Independent Study' with Course ID 10520 and Section A offers VAR units. The Class meets Schedule will be added between NA and NA ET.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Deep Reinforcement Learning & Control' with Course ID 10403 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Fragkiadaki located in Building GHC, Room 4215.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Machine Learning with Large Datasets (Undergraduate)' with Course ID 10405 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Gordon, Talwalkar located in Building DH, Room 2210.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Foundations of Learning, Game Theory, and Their Connections' with Course ID 10422 and Section A offers 12.0 units. The Class meets Monday Wednesday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Balcan located in Building GHC, Room 4211.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Generative AI' with Course ID 10423 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Gormley, Li, Lipton located in Building GHC, Room 4401.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Senior Research Project' with Course ID 10500 and Section A offers 612 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Gormley located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Independent Study' with Course ID 10520 and Section A offers VAR units. The Class meets Schedule will be added between NA and NA ET.
Answer: "
"According the OUTDOOR paper, what is one of the challenges of navigating in outdoor environments compared to indoor environments?
","['yonatan bisk_8035a247980cb18abf2bb7b9d96e7d4c63622ef2_metadata.txt', 'yonatan bisk_8035a247980cb18abf2bb7b9d96e7d4c63622ef2_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: According the OUTDOOR paper, what is one of the challenges of navigating in outdoor environments compared to indoor environments?

Context: Faculty Name: yonatan bisk
Paperid: 8035a247980cb18abf2bb7b9d96e7d4c63622ef2
Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation
Year: 2023
Abstract: Robots should exist anywhere humans do: indoors, outdoors, and even unmapped environments. In contrast, the focus of recent advancements in Object Goal Navigation(OGN) has targeted navigating in indoor environments by leveraging spatial and semantic cues that do not generalize outdoors. While these contributions provide valuable insights into indoor scenarios, the broader spectrum of real-world robotic applications often extends to outdoor settings. As we transition to the vast and complex terrains of outdoor environments, new challenges emerge. Unlike the structured layouts found indoors, outdoor environments lack clear spatial delineations and are riddled with inherent semantic ambiguities. Despite this, humans navigate with ease because we can reason about the unseen. We introduce a new task OUTDOOR, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain. Additionally, we show impressive results on both a simulated drone and physical quadruped in outdoor environments. Our agent has no premapping and our formalism outperforms naive LLM-based approaches
Authors: Quanting Xie, Tianyi Zhang, Kedi Xu, M. Johnson-Roberson, Yonatan Bisk
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A new task OUTDOOR is introduced, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain are introduced.'}
Url: https://arxiv.org/pdf/2309.10103
Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation
Authors: Quanting Xie, Tianyi Zhang, Kedi Xu, Matthew Johnson-Roberson, Yonatan Bisk
Section: VII. CONCLUSIONS
The emergence of LLMs has opened new avenues in the embodied agent domain. This paper introduced the OUTDOOR task, a pioneering approach aimed at propelling embodied agents into challenging outdoor settings. Further, we introduced a novel, general, mechanism for using LLMs to reason about robot plans in unseen environments, and proposed the CASR, the first metric to assess balance between reasoning and action for embodied agents. Our formulation more closely mirrors how humans navigate and explore, trading off between thinking and acting to both leverage what we know in general and can see in the specific.
Answer: "
"Who are the authors of the book ""The Last Lecture""?
","['fact_sheet_d407.txt', 'combined_metadata_final.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who are the authors of the book ""The Last Lecture""?

Context: 13%  International
SCHOOLS AND COLLEGES
College of Engineering 
College of Fine Arts
Dietrich College of Humanities  
and Social Sciences
Heinz College of Information Systems 
and Public Policy
Mellon College of Science
School of Computer Science
Tepper School of Business
cmu.edu/assets/pdfs/cmu-fact-sheet.pdf    |   cmu.edu/about/awards.html
March 2020
BIG IDEAS THAT SHAPE YOUR WORLD START HERE
1,000+ companies greenlighted 
across the U.S. and internationally 
Cognitive tutors, which improve 
the math skills of 500,000+ 
students each year 
The first U.S. school to award  
a degree in drama in 1914
James Gosling (SCS 1983) 
invented the Java computer 
programming language.
The first exclusive higher 
education partner of the Tony 
Awards®, co-creating the first, 
national recognition program 
honoring K-12 theatre educators 
The first smile in an email, created 
in 1982 by professor Scott Fahlman
Kevlar Fiber, invented by alumna 
Stephanie Kwolek (MM 1946)
The Last Lecture, written  
by alumni Randy Pausch (SCS 1988)  
and Jeffrey Zaslow (DC 1980)
In 1919, CMU’s first Ph.D. was 
awarded to Mao Yisheng, father  
of Chinese bridge construction. 
Duolingo, invented by professor  
Luis von Ahn (CS 2003, 2005), had  
10 million downloads in 12 months 
and was named Apple’s 2013 app  
of the year.
:-)
LEADERSHIP
Farnam Jahanian  
President
David Coulter 
Chair, Board of Trustees
James H. Garrett Jr.
The Class meets Tuesday Thursday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Sandage located in Building BH, Room 255A.
In Semester Fall 2023, from the department of History, the subject titled 'U.S. Constitution & the Presidency' with Course ID 79248 and Section A offers 9.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Sandage located in Building PH, Room 126A.
In Semester Fall 2023, from the department of History, the subject titled 'Voting Rights: An Introduction' with Course ID 79250 and Section A offers 9.0 units. The Class meets Monday Wednesday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Tetrault located in Building WEH, Room 4709.
In Semester Fall 2023, from the department of History, the subject titled 'The Last Emperors: Chinese History and Society, 1600-1900' with Course ID 79261 and Section A offers 9.0 units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Weiner located in Building BH, Room 255A.
In Semester Fall 2023, from the department of History, the subject titled 'Russian History and Revolutionary Socialism' with Course ID 79266 and Section A offers 9.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Storella located in Building PH, Room 225B.
In Semester Fall 2023, from the department of History, the subject titled 'Anti-Semitism Then and Now: Perspectives from the Middle Ages to the Present' with Course ID 79270 and Section A offers 9.0 units. The Class meets Monday Wednesday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Friedman located in Building PH, Room 226B.
Answer: "
"Which LTI faculty member focuses on embodiment?
","['yonatan_bisk.txt', 'shinji watanabe_8f0a24d1678e4d0e584b0932196cd257d5c53c7d_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which LTI faculty member focuses on embodiment?

Context: Faculty Bio: Yonatan Bisk
Assistant Professor, Language Technologies Institute
Research Area : Embodiment, Grounding, RoboNLP, Unsupervised Learning, Vision and Language
Research: My work broadly falls into: 1. Uncovering the latent structures of natural language, 2. Modeling the semantics of the physical world, and 3. Connecting language to perception and control.
Personal Website : https://talkingtorobots.com/yonatanbisk.html
Contact: 6703 Gates & Hillman Centers
Email : ybisk@cs.cmu.edu
Faculty Name: shinji watanabe
Paperid: 8f0a24d1678e4d0e584b0932196cd257d5c53c7d
Title: Improving Audio Captioning Models with Fine-grained Audio Features, Text Embedding Supervision, and LLM Mix-up Augmentation
Year: 2023
Abstract: Automated audio captioning (AAC) aims to generate informative descriptions for various sounds from nature and/or human activities. In recent years, AAC has quickly attracted research interest, with state-of-the-art systems now relying on a sequence-to-sequence (seq2seq) backbone powered by strong models such as Transformers. Following the macro-trend of applied machine learning research, in this work, we strive to improve the performance of seq2seq AAC models by extensively leveraging pretrained models and large language models (LLMs). Specifically, we utilize BEATs to extract fine-grained audio features. Then, we employ Instructor LLM to fetch text embeddings of captions, and infuse their language-modality knowledge into BEATs audio features via an auxiliary InfoNCE loss function. Moreover, we propose a novel data augmentation method that uses ChatGPT to produce caption mix-ups (i.e., grammatical and compact combinations of two captions) which, together with the corresponding audio mixtures, increase not only the amount but also the complexity and diversity of training data. During inference, we propose to employ nucleus sampling and a hybrid reranking algorithm, which has not been explored in AAC research. Combining our efforts, our model achieves a new state-of-the-art 32.6 SPIDEr-FL score on the Clotho evaluation split, and wins the 2023 DCASE AAC challenge.
Authors: Shih-Lun Wu, Xuankai Chang, G. Wichern, Jee-weon Jung, Franccois G. Germain, Jonathan Le Roux, Shinji Watanabe
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work utilizes BEATs to extract fine-grained audio features and proposes a novel data augmentation method that uses ChatGPT to produce caption mix-ups which increase not only the amount but also the complexity and diversity of training data.'}
Url: https://arxiv.org/pdf/2309.17352
Answer: "
"What is the full name of the workshop where the paper Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation, got published?
","['david mortensen_bf42c0462d1415cdde877c90d58da11545407b8a_metadata.txt', 'lori levin_bf42c0462d1415cdde877c90d58da11545407b8a_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the full name of the workshop where the paper Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation, got published?

Context: Faculty Name: david mortensen
Paperid: bf42c0462d1415cdde877c90d58da11545407b8a
Title: Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation
Year: 2023
Abstract: Interlinear glossing provides a vital type of morphosyntactic annotation, both for linguists and language revitalists, and numerous conventions exist for representing it formally and computationally. Some of these formats are human readable; others are machine readable. Some are easy to edit with general-purpose tools. Few represent non-concatentative processes like infixation, reduplication, mutation, truncation, and tonal overwriting in a consistent and formally rigorous way (on par with affixation). We propose an annotation conventionâ€”Generalized Glossing Guidelines (GGG) that combines all of these positive properties using an Item-and-Process (IP) framework. We describe the format, demonstrate its linguistic adequacy, and compare it with two other interlinear glossed text annotation schemes.
Authors: David R. Mortensen, Ela Gulsen, Taiqi He, Nathaniel R. Robinson, Jonathan D. Amith, Lindia Tjuatja, L. Levin
Venue: Special Interest Group on Computational Morphology and Phonology Workshop
Tldr: {'model': 'tldr@v2.0.0', 'text': 'An annotation convention is proposed that combines all of these positive properties using an Item-and-Process (IP) framework, and its linguistic adequacy is demonstrated, and it is compared with two other interlinear glossed text annotation schemes.'}
Url: https://aclanthology.org/2023.sigmorphon-1.7.pdf
Faculty Name: lori levin
Paperid: bf42c0462d1415cdde877c90d58da11545407b8a
Title: Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation
Year: 2023
Abstract: Interlinear glossing provides a vital type of morphosyntactic annotation, both for linguists and language revitalists, and numerous conventions exist for representing it formally and computationally. Some of these formats are human readable; others are machine readable. Some are easy to edit with general-purpose tools. Few represent non-concatentative processes like infixation, reduplication, mutation, truncation, and tonal overwriting in a consistent and formally rigorous way (on par with affixation). We propose an annotation conventionâ€”Generalized Glossing Guidelines (GGG) that combines all of these positive properties using an Item-and-Process (IP) framework. We describe the format, demonstrate its linguistic adequacy, and compare it with two other interlinear glossed text annotation schemes.
Authors: David R. Mortensen, Ela Gulsen, Taiqi He, Nathaniel R. Robinson, Jonathan D. Amith, Lindia Tjuatja, L. Levin
Venue: Special Interest Group on Computational Morphology and Phonology Workshop
Tldr: {'model': 'tldr@v2.0.0', 'text': 'An annotation convention is proposed that combines all of these positive properties using an Item-and-Process (IP) framework, and its linguistic adequacy is demonstrated, and it is compared with two other interlinear glossed text annotation schemes.'}
Url: https://aclanthology.org/2023.sigmorphon-1.7.pdf
Answer: "
"Who was the first director of the Robotics Institute?
","['fact_sheet_d407.txt', 'history_d401.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who was the first director of the Robotics Institute?

Context: :-)
LEADERSHIP
Farnam Jahanian  
President
David Coulter 
Chair, Board of Trustees
James H. Garrett Jr. 
Provost
2 National Academy of Engineering       3 National Academy of Sciences        4 National Academy of Medicine
Won by alumni and current/former faculty
The leading university center 
for cybersecurity, providing 
support to more than 110 
centers around the world 
The world’s first university 
robotics department,  
founded in 1979
Alumnus Andy Warhol  
(CFA1949), pop artist pioneer  
and cultural icon
One of only 29 universities 
invited to be a member of the 
World Economic Forum’s 
Global University  
Leaders Forum
CMU.EDU
ECONOMIC IMPACT
• Attracting major companies —  
including Google, Intel, Uber and GE —  
to locate operations and create new  
jobs in Pittsburgh
• To date, the CMU community has 
launched more than 400 startups 
and created more than 152 spinoff 
companies.
• Contributing to the cultural and civic life of  
the city with performances, exhibitions 
and research collaborations
13
ACADEMY  
AWARDS
65
MEMBERS  
OF NAE2
146
EMMY 
AWARDS
20
MEMBERS 
OF NAS3
6
MEMBERS 
OF NAM4
13
TURING 
AWARDS
20
NOBEL 
LAUREATES
58
TONY 
AWARDS
May 2023
As the 1970s progressed, Newell became interested in human-computer interaction, and began a long relationship with Xerox’s Palo Alto Research Center, or PARC, which released the Xerox Alto in 1973. Considered a forerunner to many of the computing environments that followed, Alto featured a graphical-user interface and was among the first commercially available workstations controlled with a mouse. Inspired by Alto, Reddy launched a drive for development of CMU’s own “three-M” machine—a personal workstation with a megabyte of memory, a megapixel display and at least one million instructions per second of processing power.Launching a Robotics InstituteIn 1979, an executive at Pittsburgh’s Westinghouse Electric Corp., Tom Murrin, collaborated with Jordan and Reddy to create the Robotics Institute, with Reddy as its first director. By 1982, the Computer Science Department included more than 30 faculty members and 100 graduate students.The best-wired campus in the worldWorking with IBM in the early 1980s, the university and the Computer Science Department established another new research frontier: Development of a high-speed computer network that would reach virtually every room on campus, along with a GUI-based computing environment, and providing networked PCs or workstations for 7,000 students, faculty members and employees. Called the Andrew Project, it turned Carnegie Mellon into the best-connected, most-wired university in the world—a process Newell called “greening up the campus with computer science.” CMU also became home to a new Software Engineering Institute, funded by the Defense Department, to study computer security and develop best practices in the design of operating systems. Between 1982 and 1985, the amount of sponsored research in the Computer Science Department doubled, from $7.2 million to $15.3 million—more than the other four departments in the Mellon College of Science combined.A “school of computer science” is proposedFeeling that CSD’s needs were inadequately represented in MCS, CSD head A. Nico Habermann and then-CMU provost Angel Jordan in 1986 wrote a white paper proposing the creation of “a School of Computer Science.” Responding to concerns from the faculty that the change might be taking place too quickly, the university first established a free-floating Department of Computer Science. The experiment, which lasted two years, was an unqualified success.
Answer: "
"What are the two courses that are prerequisities for the undergraduate concentration termed the LT concentration? Include the title and course number in parentheses.
","['program_info_LanguageTechnologiesConcentration.txt', 'mlt-student-handbook-2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the two courses that are prerequisities for the undergraduate concentration termed the LT concentration? Include the title and course number in parentheses.

Context: Academic Program Name:
Language Technologies Concentration

Website:
https://lti.cs.cmu.edu/academics/lt-concentration.html

Overview:
Human language technologies have become an increasingly central component of computer science. Information retrieval, machine translation and speech technology are used daily by the general public, while text mining, natural language processing and language-based tutoring are common within more specialized professional or educational environments. The LTI prepares students for this world by offering a minor that gives you the opportunity to not only learn about language technologies, but to also apply that knowledge through a directed project.

Requirements:
Students interested in the language technologies minor must complete our prerequisite courses with an average grade of B (3.0) or better before applying to the program. (Students who do not meet this average must submit a letter of explanation along with their application.) Prerequisites include:
Principles of Imperative Computation (15-122)
Principles of Functional Programming (15-150)
We also strongly encourage candidates to take:
Differential and Integral Calculus (21-120) and Integration and Approximation (21-122)
Matrices and Linear Transformations (21-241) or Matrix Theory (21-242)
Probability and Computing (15-259) or Probability (21-325) or Probability Theory for Computer Scientists (36-218) or Introduction to Probability Theory (36-225)

Curriculum:
The Language Technologies Concentration requires that SCS students complete one core course and their choice of three elective courses of at least 9 units each. The electives can be chosen from a specific set of stand-alone courses. In addition to the four courses, students are required to do an undergraduate research project for at least 9 units to complete their concentration.
Course Requirements for Undergraduate Minor
Students are strongly 
encouraged to finish the thesis work within one (1) year following the semester they 
enroll for the first Masters Thesis course. 
 
 
 
 
MLT Graduate Student Handbook 
Page 17 
 
4.4 Definitions of LTI Terminology 
We define here some of the terms as used in this handbook: 
 An LTI course is any 12-unit course with a number of 11-XXX; a 6-unit course with an 
11-XXX counts as 1/2 of an LTI course. Unless otherwise specified, ""course"" means an 
actual classroom course, not credit given for research or independent study. Note that 
we will allow any one MLD (10-xxx) graduate-level course to count as an LTI course. 
 An SCS course is any 12-unit course with a course number indicating a unit of the 
School of Computer Science (including LTI); a 6-unit course with such a number counts 
as 1/2 of an SCS course. Unless otherwise specified, ""course"" means an actual classroom 
course, not credit given for research or independent study. Note: Recommended 
electives that are technically outside of the SCS now count towards this requirement; for 
example, Digital Signal Processing in ECE. Please see the Program Director for approval 
of electives as SCS. 
 LTI Focus Areas are sets of courses defined on the LTI course webpage under Course 
Categories. If a student believes a new course should be added to a Focus Area, they 
should notify the Chair of the LTI Graduate Programs. He will decide, with advice from 
faculty in the appropriate area, whether it should be in the Focus Area, and if approved 
it will be added to the LTI Focus Area webpage. 
o A Task-Orientation Focus Course is simply a course belonging to that LTI Focus 
Area, as listed on the Course Categories webpage. 
 An LTI lab course is simply a course in the list of lab courses defined in the LTI Course 
Categories webpage. 
4.5 Recommended Electives outside of SCS  
Students are free to take elective courses outside the SCS, at Carnegie Mellon or cross-
registered at the University of Pittsburgh, as long as the student fulfills the requirements of 
their program as described above.
Answer: "
"What is the publicly available website for WebArena
","['daniel fried_e41482f4ee984f17382f6cdd900df094d928be06_content_0.txt', 'graham neubig_e41482f4ee984f17382f6cdd900df094d928be06_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the publicly available website for WebArena

Context: Title: WEBARENA: A REALISTIC WEB ENVIRONMENT FOR BUILDING AUTONOMOUS AGENTS
Authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig
Section: A.8 ADDITIONAL ERROR ANALYSIS
Observation Bias Realistic websites frequently present information on similar topics across various sections to ensure optimal user accessibility. However, a GPT-4 agent often demonstrates a tendency to latch onto the first related piece of information it encounters without sufficiently verifying its relevance or accuracy. For instance, the homepage of the E-Commerce CMS displays the best-selling items based on recent purchases, while historical best-seller data is typically accessed via a separate report. Presented with the task of “What is the top-1 best-selling product in 2022”, the GPT-4 agent defaults to leveraging the readily available information on the homepage, bypassing the necessary step of generating the report to obtain the accurate data. Failures in Observation Interpretation Interestingly, while GPT-4 is capable of summarizing the observations, it occasionally overlooks more granular information, such as the previously entered input. As in the right-hand example of Figure 11, [5172] StaticText indicates that the search term “DMV area” has already been entered. However, the agent disregards this detail and continuously issues the command type [2430] [DMV area] until it reaches the maximum step limit. Furthermore, the agent often neglects the previous action information that is provided alongside the observation. We hypothesize that these observed failures are related to the current pretraining and supervised fine-tuning on dialogues employed in GPT models Ouyang et al. (2022). These models are primarily trained to execute instructions given immediate observations (i.e.,, the dialogue history); thereby, they may exhibit a lack of explorations. Furthermore, in dialogue scenarios, subtle differences in NL expressions often have less impact on the overall conversation. As a result, models may tend to overlook minor variations in their observations.
Title: WEBARENA: A REALISTIC WEB ENVIRONMENT FOR BUILDING AUTONOMOUS AGENTS
Authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig
Section: A.8 ADDITIONAL ERROR ANALYSIS
Observation Bias Realistic websites frequently present information on similar topics across various sections to ensure optimal user accessibility. However, a GPT-4 agent often demonstrates a tendency to latch onto the first related piece of information it encounters without sufficiently verifying its relevance or accuracy. For instance, the homepage of the E-Commerce CMS displays the best-selling items based on recent purchases, while historical best-seller data is typically accessed via a separate report. Presented with the task of “What is the top-1 best-selling product in 2022”, the GPT-4 agent defaults to leveraging the readily available information on the homepage, bypassing the necessary step of generating the report to obtain the accurate data. Failures in Observation Interpretation Interestingly, while GPT-4 is capable of summarizing the observations, it occasionally overlooks more granular information, such as the previously entered input. As in the right-hand example of Figure 11, [5172] StaticText indicates that the search term “DMV area” has already been entered. However, the agent disregards this detail and continuously issues the command type [2430] [DMV area] until it reaches the maximum step limit. Furthermore, the agent often neglects the previous action information that is provided alongside the observation. We hypothesize that these observed failures are related to the current pretraining and supervised fine-tuning on dialogues employed in GPT models Ouyang et al. (2022). These models are primarily trained to execute instructions given immediate observations (i.e.,, the dialogue history); thereby, they may exhibit a lack of explorations. Furthermore, in dialogue scenarios, subtle differences in NL expressions often have less impact on the overall conversation. As a result, models may tend to overlook minor variations in their observations.
Answer: "
"In spring 2024, Who are the instructors for course 15112?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, Who are the instructors for course 15112?

Context: The Class meets Friday between 04:00PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor NA located in Building GHC, Room CLSTR.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Fundamentals of Programming and Computer Science' with Course ID 15112 and Section R offers 12.0 units. The Class meets Wednesday between 05:00PM and 05:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Taylor, Kosbie located in Building GHC, Room 4301.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Fundamentals of Programming and Computer Science' with Course ID 15112 and Section NA offers 12.0 units. The Class meets Friday between 05:00PM and 05:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor NA located in Building GHC, Room CLSTR.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Fundamentals of Programming and Computer Science' with Course ID 15112 and Section W offers 12.0 units. The Class meets Sunday Tuesday Thursday between 02:30PM and 03:45PM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Riley, Gedawy located in Building CMB, Room 2152.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Imperative Computation' with Course ID 15122 and Section Lec 1 offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Cervesato, Kohlbrenner located in Building CUC, Room MCCNMY.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Imperative Computation' with Course ID 15122 and Section Lec 2 offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET.
The Class meets Friday between 04:00PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor NA located in Building GHC, Room CLSTR.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Fundamentals of Programming and Computer Science' with Course ID 15112 and Section R offers 12.0 units. The Class meets Wednesday between 05:00PM and 05:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Taylor, Kosbie located in Building GHC, Room 4301.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Fundamentals of Programming and Computer Science' with Course ID 15112 and Section NA offers 12.0 units. The Class meets Friday between 05:00PM and 05:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor NA located in Building GHC, Room CLSTR.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Fundamentals of Programming and Computer Science' with Course ID 15112 and Section W offers 12.0 units. The Class meets Sunday Tuesday Thursday between 02:30PM and 03:45PM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Riley, Gedawy located in Building CMB, Room 2152.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Imperative Computation' with Course ID 15122 and Section Lec 1 offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Cervesato, Kohlbrenner located in Building CUC, Room MCCNMY.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Imperative Computation' with Course ID 15122 and Section Lec 2 offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET.
Answer: "
"In summer 2024, When do Mini-5 Final Exams take place?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In summer 2024, When do Mini-5 Final Exams take place?

Context: On Friday, 07 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 pass/no pass & withdrawal deadline (3) is observed.
On Friday, 14 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Faculty Course Evaluations open is observed.
On Tuesday, 18 June, 2024, during the Summer One 2024 (M24) semester, Semester course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Wednesday, 19 June, 2022, during the Summer One 2024 (M24) semester, Juneteenth is observed, leading to University Closed & No Classes.
On Thursday, 20 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Last Day of Classes is observed.
On Thursday, 20 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 voucher deadline (4) is observed.
On Friday, 21 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Final Exams is observed.
On Friday, 21 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Faculty Course Evaluations close is observed.
On Monday, 24 June, 2024, during the Summer One 2024 (M24) semester, Mini-6 First Day of Classes is observed.
On Tuesday, 25 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Final Grades Due by 4 pm is observed.
On Friday, 28 June, 2024, during the Summer One 2024 (M24) semester, Mini-6 add, audit, & tuition adjustment drop deadline (1) is observed.
On Thursday, 04 July, 2024, during the Summer One 2024 (M24) semester, Independence Day is observed, leading to University Closed & No Classes.
On Friday, 05 July, 2024, during the Summer One 2024 (M24) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Friday, 13 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Faculty Course Evaluations open is observed.
On Tuesday, 17 June, 2025, during the Summer 2025 (M25) semester, Summer All course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Wednesday, 18 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Last Day of Classes is observed.
On Wednesday, 18 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 voucher deadline is observed.
On Thursday, 19 June, 2025, during the Summer 2025 (M25) semester, Juneteenth is observed, leading to University Closed & No Classes.
On Friday, 20 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Final Exams is observed.
On Friday, 20 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Faculty Course Evaluations close is observed.
On Monday, 23 June, 2025, during the Summer 2025 (M25) semester, Summer Two & Mini-6 First Day of Classes is observed.
On Tuesday, 24 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Final Grades Due by 4 pm is observed.
On Friday, 27 June, 2025, during the Summer 2025 (M25) semester, Summer Two & Mini-6 add, audit, & tuition adjustment drop deadline (1) is observed.
On Thursday, 03 July, 2025, during the Summer 2025 (M25) semester, Summer All pass/no pass & withdrawal grade deadline (3) is observed.
On Friday, 04 July, 2025, during the Summer 2025 (M25) semester, Independence Day is observed, leading to University Closed & No Classes.
On Monday, 07 July, 2025, during the Summer 2025 (M25) semester, Summer Two & Mini-6 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
Answer: "
"Where should robots ideally exist according to the OUTDOOR paper?
","['yonatan bisk_8035a247980cb18abf2bb7b9d96e7d4c63622ef2_metadata.txt', 'yonatan bisk_8035a247980cb18abf2bb7b9d96e7d4c63622ef2_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Where should robots ideally exist according to the OUTDOOR paper?

Context: Faculty Name: yonatan bisk
Paperid: 8035a247980cb18abf2bb7b9d96e7d4c63622ef2
Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation
Year: 2023
Abstract: Robots should exist anywhere humans do: indoors, outdoors, and even unmapped environments. In contrast, the focus of recent advancements in Object Goal Navigation(OGN) has targeted navigating in indoor environments by leveraging spatial and semantic cues that do not generalize outdoors. While these contributions provide valuable insights into indoor scenarios, the broader spectrum of real-world robotic applications often extends to outdoor settings. As we transition to the vast and complex terrains of outdoor environments, new challenges emerge. Unlike the structured layouts found indoors, outdoor environments lack clear spatial delineations and are riddled with inherent semantic ambiguities. Despite this, humans navigate with ease because we can reason about the unseen. We introduce a new task OUTDOOR, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain. Additionally, we show impressive results on both a simulated drone and physical quadruped in outdoor environments. Our agent has no premapping and our formalism outperforms naive LLM-based approaches
Authors: Quanting Xie, Tianyi Zhang, Kedi Xu, M. Johnson-Roberson, Yonatan Bisk
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A new task OUTDOOR is introduced, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain are introduced.'}
Url: https://arxiv.org/pdf/2309.10103
Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation
Authors: Quanting Xie, Tianyi Zhang, Kedi Xu, Matthew Johnson-Roberson, Yonatan Bisk
Section: VII. CONCLUSIONS
The emergence of LLMs has opened new avenues in the embodied agent domain. This paper introduced the OUTDOOR task, a pioneering approach aimed at propelling embodied agents into challenging outdoor settings. Further, we introduced a novel, general, mechanism for using LLMs to reason about robot plans in unseen environments, and proposed the CASR, the first metric to assess balance between reasoning and action for embodied agents. Our formulation more closely mirrors how humans navigate and explore, trading off between thinking and acting to both leverage what we know in general and can see in the specific.
Answer: "
"What number do all of the Chemical Engineering classes start with?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What number do all of the Chemical Engineering classes start with?

Context: In Semester Spring 2024, from the department of Chemical Engineering, the subject titled 'Junior Research Project' with Course ID 06300 and Section A offers 3-36 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Laird located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Chemical Engineering, the subject titled 'Unit Operations of Chemical Engineering' with Course ID 06361 and Section A offers 9.0 units. The Class meets Monday Wednesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Whitehead located in Building GHC, Room 4215.
In Semester Spring 2024, from the department of Chemical Engineering, the subject titled 'Transport Process Laboratory' with Course ID 06363 and Section NA offers 9.0 units. The Class meets Monday Wednesday Friday between 11:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Beckwith, Cline located in Building DH, Room 1212.
In Semester Spring 2024, from the department of Chemical Engineering, the subject titled 'Transport Process Laboratory' with Course ID 06363 and Section A offers 9.0 units. The Class meets Tuesday between 08:00AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Beckwith, Cline located in Building DH, Room A100.
In Semester Spring 2024, from the department of Chemical Engineering, the subject titled 'Transport Process Laboratory' with Course ID 06363 and Section B offers 9.0 units. The Class meets Thursday between 08:00AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Beckwith, Cline located in Building DH, Room A100.
In Semester Spring 2024, from the department of Chemical Engineering, the subject titled 'Transport Process Laboratory' with Course ID 06363 and Section C offers 9.0 units. The Class meets Thursday between 02:00PM and 04:50PM ET.
In Semester Spring 2024, from the department of Chemical Engineering, the subject titled 'Junior Research Project' with Course ID 06300 and Section A offers 3-36 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Laird located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Chemical Engineering, the subject titled 'Unit Operations of Chemical Engineering' with Course ID 06361 and Section A offers 9.0 units. The Class meets Monday Wednesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Whitehead located in Building GHC, Room 4215.
In Semester Spring 2024, from the department of Chemical Engineering, the subject titled 'Transport Process Laboratory' with Course ID 06363 and Section NA offers 9.0 units. The Class meets Monday Wednesday Friday between 11:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Beckwith, Cline located in Building DH, Room 1212.
In Semester Spring 2024, from the department of Chemical Engineering, the subject titled 'Transport Process Laboratory' with Course ID 06363 and Section A offers 9.0 units. The Class meets Tuesday between 08:00AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Beckwith, Cline located in Building DH, Room A100.
In Semester Spring 2024, from the department of Chemical Engineering, the subject titled 'Transport Process Laboratory' with Course ID 06363 and Section B offers 9.0 units. The Class meets Thursday between 08:00AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Beckwith, Cline located in Building DH, Room A100.
In Semester Spring 2024, from the department of Chemical Engineering, the subject titled 'Transport Process Laboratory' with Course ID 06363 and Section C offers 9.0 units. The Class meets Thursday between 02:00PM and 04:50PM ET.
Answer: "
"In fall 2024, What is the deadline for Mini-1 add, audit & tuition adjustment drop (deadline 1)?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-2324.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2024, What is the deadline for Mini-1 add, audit & tuition adjustment drop (deadline 1)?

Context: On Monday, 28 August, 2023, during the Fall 2023 (F23) semester, Semester & Mini-1 Classes Begin is observed.
On Friday, 01 September, 2023, during the Fall 2023 (F23) semester, Mini-1 add, audit & tuition adjustment drop deadline  (1) is observed.
On Monday, 04 September, 2023, during the Fall 2023 (F23) semester, Labor Day is observed, leading to No Classes & University Closed.
On Monday, 11 September, 2023, during the Fall 2023 (F23) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Wednesday, 20 September, 2023, during the Fall 2023 (F23) semester, Mini-1 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 02 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Pass/no pass & withdrawal deadline (3) is observed.
On Monday, 02 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Faculty Course Evaluations open is observed.
On Monday, 09 October, 2023, during the Fall 2023 (F23) semester, Semester drop deadline is observed, leading to withdrawal grade assigned after this date.
On Friday, 13 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Last Day of Classes is observed.
On Friday, 13 October, 2023, during the Fall 2023 (F23) semester, Mini-1 voucher election deadline (4) is observed.
From 13 October, 2023,Friday to 14 October, 2023,Saturday marks the Family Weekend for Fall 2023 (F23) semester.
On Saturday, 14 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Exams is observed.
On Saturday, 14 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Faculty Course Evaluations close is observed.
From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
Date: '2023-08-28', Day: 'Monday', Event: 'Semester & Mini-1 Classes Begin', Semester: 'Fall 2023 (F23)'
Date: '2023-09-01', Day: 'Friday', Event: 'Mini-1 add, audit & tuition adjustment drop deadline  (1)', Semester: 'Fall 2023 (F23)'
Date: '2023-09-04', Day: 'Monday', Event: 'Labor Day; No Classes & University Closed', Semester: 'Fall 2023 (F23)'
Date: '2023-09-11', Day: 'Monday', Event: 'Semester add, audit & tuition adjustment drop deadline (1)', Semester: 'Fall 2023 (F23)'
Date: '2023-09-20', Day: 'Wednesday', Event: 'Mini-1 drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Fall 2023 (F23)'
Date: '2023-10-02', Day: 'Monday', Event: 'Mini-1 Pass/no pass & withdrawal deadline (3)', Semester: 'Fall 2023 (F23)'
Date: '2023-10-02', Day: 'Monday', Event: 'Mini-1 Faculty Course Evaluations open', Semester: 'Fall 2023 (F23)'
Date: '2023-10-09', Day: 'Monday', Event: 'Semester drop deadline; withdrawal grade assigned after this date', Semester: 'Fall 2023 (F23)'
Date: '2023-10-13', Day: 'Friday', Event: 'Mini-1 Last Day of Classes', Semester: 'Fall 2023 (F23)'
Date: '2023-10-13', Day: 'Friday', Event: 'Mini-1 voucher election deadline (4)', Semester: 'Fall 2023 (F23)'
Start Date: '2023-10-13', End Date: '2023-10-14', Days: 'Friday to Saturday', Event: 'Family Weekend', Semester: 'Fall 2023 (F23)'
Date: '2023-10-14', Day: 'Saturday', Event: 'Mini-1 Exams', Semester: 'Fall 2023 (F23)'
Date: '2023-10-14', Day: 'Saturday', Event: 'Mini-1 Faculty Course Evaluations close',
Answer: "
"The first authors of the paper NLPositionality: Characterizing Design Biases of Datasets and Models, are from which university/institute?
","['maarten sap_a66ff335f5934fe7503a99d3eb3abed493994df1_metadata.txt', 'maarten sap_a66ff335f5934fe7503a99d3eb3abed493994df1_content_2.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: The first authors of the paper NLPositionality: Characterizing Design Biases of Datasets and Models, are from which university/institute?

Context: Faculty Name: maarten sap
Paperid: a66ff335f5934fe7503a99d3eb3abed493994df1
Title: NLPositionality: Characterizing Design Biases of Datasets and Models
Year: 2023
Abstract: Design biases in NLP systems, such as performance differences for different populations, often stem from their creator’s positionality, i.e., views and lived experiences shaped by identity and background. Despite the prevalence and risks of design biases, they are hard to quantify because researcher, system, and dataset positionality is often unobserved. We introduce NLPositionality, a framework for characterizing design biases and quantifying the positionality of NLP datasets and models. Our framework continuously collects annotations from a diverse pool of volunteer participants on LabintheWild, and statistically quantifies alignment with dataset labels and model predictions. We apply NLPositionality to existing datasets and models for two tasks—social acceptability and hate speech detection. To date, we have collected 16,299 annotations in over a year for 600 instances from 1,096 annotators across 87 countries.We find that datasets and models align predominantly with Western, White, college-educated, and younger populations. Additionally, certain groups, such as non-binary people and non-native English speakers, are further marginalized by datasets and models as they rank least in alignment across all tasks. Finally, we draw from prior literature to discuss how researchers can examine their own positionality and that of their datasets and models, opening the door for more inclusive NLP systems.
Authors: Sebastin Santy, Jenny T Liang, Ronan Le Bras, Katharina Reinecke, Maarten Sap
Venue: Annual Meeting of the Association for Computational Linguistics
Tldr: {'model': 'tldr@v2.0.0', 'text': 'NLPositionality, a framework for characterizing design biases and quantifying the positionality of NLP datasets and models, is introduced and it is found that dataset and models align predominantly with Western, White, college-educated, and younger populations.'}
Url: http://arxiv.org/pdf/2306.01943
Title: NLPositionality: Characterizing Design Biases of Datasets and Models
Authors: Sebastin Santy, Jenny T. Liang, Ronan Le Bras, Katharina Reinecke, Maarten Sap, Aditya Sharma
Section: 5 Discussion
Hanna et al., 2020; Bender et al., 2021). This can be done using approaches such as participatory design (Spinuzzi, 2005), including interactive storyboarding (Madsen and Aiken, 1993), as well as value-sensitive design (Friedman, 1996), including panels of experiential experts (Madsen and Aiken, 1993). Building datasets and models with large global teams such as BigBench (Srivastava et al., 2022) and NL-Augmenter (Dhole et al., 2021) could also reduce design biases by having diverse teams (Li, 2020). To account for annotator subjectivity (Aroyo and Welty, 2015), researchers should make concerted efforts to recruit annotators from diverse backgrounds. Websites like LabintheWild can be platforms where these annotators are recruited. Since new design biases could be introduced in this process, we recommend following the practice of documenting the demographics of annotators as in prior works (e.g., Forbes et al., 2020; Vidgen et al., 2021) to record a dataset’s positionality. We urge considering research through the lens of perspectivism (Basile et al., 2021), i.e. being mindful of different perspectives by sharing datasets with disaggregated annotations and finding modeling techniques that can handle inherent disagreements or distributions (Plank, 2022), instead of forcing a single answer in the data (e.g., by majority vote; Davani et al., 2022) or model (e.g., by classification to one label; Costanza-Chock, 2018). Researchers also should carefully consider how they aggregate labels from diverse annotators during modeling so their perspectives are represented, such as not averaging annotations to avoid the “tyranny of the mean” (Talat et al., 2022). Finally, we argue that the notion of “inclusive NLP” does not mean that all language technologies have to work for everyone.
Answer: "
"How many people co-authored the paper Learning to Ask Questions for Zero-shot Dialogue State Tracking?
","['alexander rudnicky_f743324682d5d50db9b114fa60b908f09c10c9a0_metadata.txt', 'alexander rudnicky_papers.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many people co-authored the paper Learning to Ask Questions for Zero-shot Dialogue State Tracking?

Context: Faculty Name: alexander rudnicky
Paperid: f743324682d5d50db9b114fa60b908f09c10c9a0
Title: Learning to Ask Questions for Zero-shot Dialogue State Tracking
Year: 2023
Abstract: We present a method for performing zero-shot Dialogue State Tracking (DST) by casting the task as a learning-to-ask-questions framework. The framework learns to pair the best question generation (QG) strategy with in-domain question answering (QA) methods to extract slot values from a dialogue without any human intervention. A novel self-supervised QA pretraining step using in-domain data is essential to learn the structure without requiring any slot-filling annotations. Moreover, we show that QG methods need to be aligned with the same grammatical person used in the dialogue. Empirical evaluation on the MultiWOZ 2.1 dataset demonstrates that our approach, when used alongside robust QA models, outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation-given a comparable amount of domain knowledge during data creation. Finally, we analyze the impact of the types of questions used, and demonstrate that the algorithmic approach outperforms template-based question generation.
Authors: Diogo Tavares, David Semedo, Alexander I. Rudnicky, João Magalhães
Venue: Annual International ACM SIGIR Conference on Research and Development in Information Retrieval
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work presents a method for performing zero-shot Dialogue State Tracking by casting the task as a learning-to-ask-questions framework that outperforms template-based question generation and shows that QG methods need to be aligned with the same grammatical person used in the dialogue.'}
Url: https://dl.acm.org/doi/pdf/10.1145/3539618.3592010
List of 2023 Open Access papers by alexander rudnicky are:
Advancing Regular Language Reasoning in Linear Recurrent Neural Networks
Structured Dialogue Discourse Parsing
A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech
Overview of Robust and Multilingual Automatic Evaluation Metrics

for Open-Domain Dialogue Systems at DSTC 11 Track 4
Learning to Ask Questions for Zero-shot Dialogue State Tracking
Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings
Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation
Answer: "
"What are the number of credits MCDS students must complete to graduate?
","['program_info_MasterofComputationalDataScience.txt', 'mcds-student-handbook-2023_2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the number of credits MCDS students must complete to graduate?

Context: Academic Program Name:
Master of Computational Data Science

Website:
https://lti.cs.cmu.edu/academics/masters-programs/mcds.html

Overview:
The MCDS degree focuses on engineering and deploying large-scale information systems. Our comprehensive curriculum equips you with the skills and knowledge to develop the layers of technology involved in the next generation of massive information system deployments and analyze the data these systems generate. When you graduate, you’ll have a unified vision of these systems from your core courses; internship experience; and semester-long, group-oriented capstone project. MCDS graduates are sought-after software engineers, data scientists and project managers at leading information technology, software services and social media companies.

Requirements:
The MCDS program offers three majors: Systems, Analytics, and Human-Centered Data Science. All three require the same total number of course credits, split among required core courses, electives, data science seminar and capstone courses specifically defined for each major. The degree can also be earned in two different ways, depending on the length of time you spend working on it. Regardless of the timing option, all MCDS students must complete a minimum of 144 units to graduate.
Here are the options:
Standard Timing — a 16-month degree consisting of study for fall and spring semesters, a summer internship, and fall semester of study. Each semester comprises a minimum of 48 units. This timing is typical for most students. Students graduate in December.
Extended Timing — a 20-month degree consisting of study for fall and spring semesters, a summer internship, and a second year of fall and spring study. Each semester comprises a minimum of 36 units. Students graduate in May.
For a complete overview of the MCDS requirements read the MCDS Handbook.

Curriculum:
To earn an MCDS degree, students must pass courses in the core curriculum, the MCDS seminar, a concentration area, and electives. Students must also complete a capstone project in which they work on a research project at CMU or on an industry-sponsored project.
In total, students must complete 144 eligible units of study, including eight 12-unit courses, two 12-unit seminar courses, and one 24-unit capstone course. Students must choose at minimum five core courses. The remainder of the 12-unit courses with course numbers 600 or greater can be electives chosen from the SCS course catalog.
Note that the per-semester course load is lower, but the 
total cost is higher since four semesters of tuition are paid. This timing is also 
 
 
15
recommended for students interested in pursuing a PhD after graduation. The 
student graduates in May. 
3.3.6.1 MCDS Curriculum 
All MCDS students must complete 144 units of graduate study which satisfy the 
following curriculum: 
 
11-637 - Foundations of Computational Data Science 
 
Four (4) additional MCDS Core Courses (10-601 Introduction to Machine 
Learning; 05-839 Interactive Data Science; 15-619 Cloud Computing; 11-631 Data 
Science Seminar; 48 units) 
 
Three courses (3) from one area of concentration curriculum (36 units) 
 
Three (3) MCDS Capstone courses (11-634, 11-635 and 11-632) (36 units) 
 
One (1) Elective: any graduate level course 600 and above in the School of 
Computer Science (12 units) 
 
3.3.6.2 Common MCDS Core Courses 
All MCDS students are required to complete four common core courses in their first two 
semesters: 
 
10-601 - Machine Learning 
 
15-619 - Cloud Computing 
 
05-839 - Interactive Data Science 
 
11-631 - Data Science Seminar 
 
3.3.6.3 Areas of Concentration 
In addition to the common MCDS core, all students must complete at least one area of 
concentration, which consists of three courses in Analytics, Systems, or Human-
Centered Data Science. Students consult with their academic advisor and choose one or 
more areas of concentration during their first semester, in preparation for enrolling in 
Spring classes.
Answer: "
"Where will the The President’s Reception in honor of CMU’s Doctoral Candidates be held?
","['Commencement.txt', 'Commencement.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Where will the The President’s Reception in honor of CMU’s Doctoral Candidates be held?

Context: Invitation, along with registration details, will be sent in late April.

First Gen Graduation Recognition
Reception: 5-5:30 p.m.
Alumni Concert Hall, College of Fine Arts
Ceremony: 5:30-6:30p.m.
Kresge Theater, College of Fine Arts

Contact:
M. Shernell Smith
mssmith@andrew.cmu.edu 

Sam Colavecchio
scolavec@andrew.cmu.edu 
412-268-7733


Friday, May 10
Diploma Ceremonies
Various times

Senior Leadership Recognition Ceremony
4–5:30 p.m.
Wiegand Gym, Cohon University Center
Undergraduate students and their guests by invitation only. This ceremony recognizes nominated seniors who have reflected upon their specific leadership contributions during their time at CMU.


Saturday, May 11
Diploma Ceremonies
Various times

Center for Student Diversity and Inclusion Ceremony
Reception: 12:30 p.m.
Program: 1-2:15 p.m.
Simmons Auditorium, Tepper Building

Contact:
M. Shernell Smith
mssmith@andrew.cmu.edu 

Rowshan Lang
rowshan@andrew.cmu.edu
Naval ROTC Commissioning
Ceremony: 1:30-2:30 p.m.
Auditorium, Soldiers & Sailors Memorial Hall & Museum *
4141 Fifth Avenue, Pittsburgh, PA 15213
Contact:
Mike Danko
mdanko@andrew.cmu.edu 

The President’s Reception in honor of CMU’s Doctoral Candidates
4–6 p.m.
Tepper Building Atrium


Sunday, May 12
Gesling Stadium opens to guests
8 a.m.

Robing and procession for graduates
9–10 a.m.
Various locations across campus

Student procession begins
9:15 a.m.
All guests in stadium must be seated. Access to guest seating will be restricted once the procession begins.

Commencement Ceremony
10–11:30 a.m.
Gesling Stadium, CMU’s campus

Diploma Ceremonies
Various times
Main Commencement Ceremony
Bachelor’s, master’s and doctoral degree candidates and their guests are invited to join the main commencement ceremony on Sunday, May 12, for the conferral of all degrees.
The main ceremony will include remarks from the president, keynote speaker, student speaker and academic deans, in addition to recognition of the honorary degree recipients.
There is no limit on number of guests who can attend the main commencement ceremony and tickets are not needed.
The ceremony will take place on CMU’s campus beginning at 10 a.m. and will be approximately 1.5 hours long. All guests must be seated by 9:15 a.m. for the start of the student procession. Access to guest seating will be restricted once the student procession begins.

Diploma Ceremonies
In addition to the main commencement ceremony, each college/school/department will host a diploma ceremony to recognize all graduating students. Diploma ceremonies will be held over the course of the weekend (Friday, May 10–Sunday, May 12).
Diploma ceremonies will include the presentation of diplomas to graduates, hooding of doctoral candidates and remarks from their college/school/department leadership. Each ceremony is organized and customized by their college/school/department.
Diploma ceremonies will take place both on and off campus. Locations and times for diploma ceremonies will be provided soon.
Diploma ceremonies typically include a reception and the length of time for each ceremony varies based on the number of graduates.
More details on the weekend schedule, including a diploma ceremony schedule, will be provided in the coming weeks. 

Thursday, May 9
Phi Beta Kappa Initiation Ceremony
Ceremony: 2–3 p.m.
McConomy Auditorium, Cohon University Center
Reception: 3–4 p.m.
Connan Room, Cohon University Center

Contact:
Joseph Devine
jd0x@andrew.cmu.edu 

Joanne Ursenbach
joanneu@andrew.cmu.edu 

President's Graduates Toast (bachelor's students)
3:30–4:30 p.m.
Location TBD
Registration required. Invitation, along with registration details, will be sent in late April.

First Gen Graduation Recognition
Reception: 5-5:30 p.m.
Alumni Concert Hall, College of Fine Arts
Ceremony: 5:30-6:30p.m.
Answer: "
"How many Emmy Awards have alumni and current/former faculty won so far?
","['fact_sheet_d407.txt', 'fact_sheet_d407.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many Emmy Awards have alumni and current/former faculty won so far?

Context: :-)
LEADERSHIP
Farnam Jahanian  
President
David Coulter 
Chair, Board of Trustees
James H. Garrett Jr. 
Provost
2 National Academy of Engineering       3 National Academy of Sciences        4 National Academy of Medicine
Won by alumni and current/former faculty
The leading university center 
for cybersecurity, providing 
support to more than 110 
centers around the world 
The world’s first university 
robotics department,  
founded in 1979
Alumnus Andy Warhol  
(CFA1949), pop artist pioneer  
and cultural icon
One of only 29 universities 
invited to be a member of the 
World Economic Forum’s 
Global University  
Leaders Forum
CMU.EDU
ECONOMIC IMPACT
• Attracting major companies —  
including Google, Intel, Uber and GE —  
to locate operations and create new  
jobs in Pittsburgh
• To date, the CMU community has 
launched more than 400 startups 
and created more than 152 spinoff 
companies.
• Contributing to the cultural and civic life of  
the city with performances, exhibitions 
and research collaborations
13
ACADEMY  
AWARDS
65
MEMBERS  
OF NAE2
146
EMMY 
AWARDS
20
MEMBERS 
OF NAS3
6
MEMBERS 
OF NAM4
13
TURING 
AWARDS
20
NOBEL 
LAUREATES
58
TONY 
AWARDS
May 2023
Carnegie Mellon University has been a birthplace 
of innovation since its founding in 1900.
Today, CMU is a global leader bringing groundbreaking 
ideas to market and creating successful startup 
businesses. Our award-winning faculty are renowned for 
working closely with students to solve major scientific, 
technological and societal challenges. We put a strong 
emphasis on creating things — from art to robots. We 
have become a model for economic development in 
forming partnerships with companies such as Uber, 
Google and Disney. Our students are recruited by some 
of the world’s most innovative companies.
MANAGEMENT 
INFORMATION 
SYSTEMS
U.S. News & World Report, 2022
#1
ARTIFICIAL 
INTELLIGENCE
U.S. News & World Report, 2022
#1
AMONG U.S. 
UNIVERSITIES
Times Higher Education  
World University 
Ranking, 2023
#19
SOFTWARE 
ENGINEERING
U.S. News & World Report, 2022
COLLEGE OF 
ENGINEERING
U.S. News & World Report, 2022
#1
#4
#1
SCHOOL OF 
COMPUTER 
SCIENCE
U.S. News & World Report, 2022
UNIVERSITY  
IN THE WORLD
Times Higher Education  
World University  
Ranking, 2023
#28
TIME-BASED/ 
NEW MEDIA
U.S. News & World Report, 2019
#1
1,529  
FACULTY
16,779 
STUDENTS
Graduate
Undergraduate
117,257 
ALUMNI (LIVING)
85%  U.S.
15%  International
35%  U.S. 
65%  International
76%  U.S. 
24%  International
GLOBAL COMMUNITY
Students representing
126 countries
Faculty representing
56 countries
Alumni representing 
148 countries
OF COMPUTER 
SCIENCE’S FIRST-
YEAR STUDENTS  
WERE WOMEN  
IN 2019
Nearly triple the  
national average
“ My Heart is in the Work.” 
 
Andrew Carnegie, Founder 
 
November 15, 1900
49.8%
87%  U.S.
Answer: "
"What are the three concentrations in the MCDS program?
","['mcds-student-handbook-2023_2024.txt', 'mcds-student-handbook-2023_2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the three concentrations in the MCDS program?

Context: Students consult with their academic advisor and choose one or 
more areas of concentration during their first semester, in preparation for enrolling in 
Spring classes. 
 
 
 
Analytics concentration: 
o One (1) Machine Learning course 
o One (1) Software Systems course 
o One (1) big data course 
 
Systems concentration: 
o 15-513 Introduction to Computer Systems (elective, prerequisite for many 
advanced Systems courses) 
o Three (3) systems project courses 
 
Human-Centered Data Science concentration: 
o One (1) Methods course 
o Two (2) HCI courses 
 
 
16
 
A detailed list of courses satisfying each concentration is contained in the MCDS Program 
FAQ. 
 
3.3.6.4 MCDS Capstone Courses 
All MCDS students complete three Capstone courses: 
 
11-634 - Capstone Planning Seminar (12 units) 
 
11-635 - Capstone Research (12 units) 
 
11-632 - Data Science Capstone (12 units) 
 
MCDS Program Learning Outcomes 
 
 
Design, implement and evaluate the use of analytic algorithms on sample 
datasets.  
 
Explain how a machine-learning model is developed for and evaluated on real 
world datasets. 
 
Design and execute experimental data collection and present resulting analyses 
using appropriate user experience (UX) techniques including interactive data 
visualizations. 
 
Apply and customize analytics, systems and human-centered data science 
techniques to application-specific data science requirements and objectives. 
 
Identify tradeoffs among data science techniques (analytics, systems and/or 
human-centered) and contrast design alternatives, within the context of specific 
data science application domains. 
 
Survey, interpret and comparatively criticize state of the art research talks and 
papers, with emphasis on constructive improvements. 
 
Organize, execute, report on, and present a real world data science project in 
collaboration with other researchers/programmers. 
 
Depending on the concentration, additional learning outcomes are emphasized: 
 
Analytics. Students electing to complete the Analytics concentration will also learn to:  
 
 
Design, implement and evaluate a software system and machine-learning model 
on real world datasets at real world scale.
Note that the per-semester course load is lower, but the 
total cost is higher since four semesters of tuition are paid. This timing is also 
 
 
15
recommended for students interested in pursuing a PhD after graduation. The 
student graduates in May. 
3.3.6.1 MCDS Curriculum 
All MCDS students must complete 144 units of graduate study which satisfy the 
following curriculum: 
 
11-637 - Foundations of Computational Data Science 
 
Four (4) additional MCDS Core Courses (10-601 Introduction to Machine 
Learning; 05-839 Interactive Data Science; 15-619 Cloud Computing; 11-631 Data 
Science Seminar; 48 units) 
 
Three courses (3) from one area of concentration curriculum (36 units) 
 
Three (3) MCDS Capstone courses (11-634, 11-635 and 11-632) (36 units) 
 
One (1) Elective: any graduate level course 600 and above in the School of 
Computer Science (12 units) 
 
3.3.6.2 Common MCDS Core Courses 
All MCDS students are required to complete four common core courses in their first two 
semesters: 
 
10-601 - Machine Learning 
 
15-619 - Cloud Computing 
 
05-839 - Interactive Data Science 
 
11-631 - Data Science Seminar 
 
3.3.6.3 Areas of Concentration 
In addition to the common MCDS core, all students must complete at least one area of 
concentration, which consists of three courses in Analytics, Systems, or Human-
Centered Data Science. Students consult with their academic advisor and choose one or 
more areas of concentration during their first semester, in preparation for enrolling in 
Spring classes.
Answer: "
"How many de-biased training examples were used for fine-tuning the pre-trained model to significantly reduce the tendency to favor any gender?
","['louis philippe morency_f891e9eeedbf20cdc54429ffcc0402a10f48494e_metadata.txt', 'louis philippe morency_f891e9eeedbf20cdc54429ffcc0402a10f48494e_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many de-biased training examples were used for fine-tuning the pre-trained model to significantly reduce the tendency to favor any gender?

Context: Faculty Name: louis philippe morency
Paperid: f891e9eeedbf20cdc54429ffcc0402a10f48494e
Title: Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions
Year: 2023
Abstract: Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 debiased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, we argue that our few-shot de-biasing approach is highly feasible and practical. Through extensive experimentation, we show that our de-biasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability.
Authors: Himanshu Thakur, Atishay Jain, Praneetha Vaddamanu, P. Liang, Louis-Philippe Morency
Venue: Annual Meeting of the Association for Computational Linguistics
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper empirically shows that by fine-tuning a pre-trained model on only 10 debiased (intervened) training examples, the tendency to favor any gender is significantly reduced, and argues that the few-shot de-biasing approach is highly feasible and practical.'}
Url: http://arxiv.org/pdf/2306.04597
Title: Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions
Authors: Himanshu Thakur, Atishay Jain, Praneetha Vaddamanu, Paul Pu Liang, Louis-Philippe Morency
Section: A Appendix
A.1 Dataset Bias Analysis To gauge the feasibility of using a wordlist based intervention approach, we first analyze our datasets for occurrences of gender words. As shown in the word cloud 4, gender pronouns are the mostfrequent word in our datasets. Moreover, as per Figure 1, ""she,"" ""he,"" and ""her"" are the top three most frequently occurring words in our dataset. This suggests that we can definitely detect gender words in our corpus and apply our interventions. A.2 Sensitivity to Choice of Dataset To understand the effectiveness of our proposed data-interventions, we study apply our methods to two datasets under varying number of training samples (10, 50 and 100) and selection strategies (most biased first and random) as per Table 6. Our methods obtain better results on StereoSet (dev) dataset. One reason this could happen is due to the fact that StereoSet has explicit gender bias, thus it would be less likely for a sentence like ""She needs a gynaecologist"" to appear on it. Because our interventions perform blunt substitutions, this sentence might become incorrect due to our method - ""Either he or she needs a gynaecologist"". A.3 Sensitivity to Number of Training Samples and Sampling Strategy As per Figure 5, When we vary the number of training samples, we observe that the difference in performance is not huge when we transition from 10 to 100 samples, thus suggesting that our method is capable of few-shot fine-tuning. Moreover, sampling the most biased data points helps our methods achieve better performance consistently, as shown in Figure 5 and Table 6. Table ?? shows some top three most gender biased entries found in the StereoSet dataset. A.4 Ablations of interventions We study the effects of choosing different ways of replacement for name and non-name words. In addition to our three interventions proposed previously, we also experimented with a couple of others. In female-first-random-phrase-masking, we always keep the female gendered word before a male word.
Answer: "
"What is the full name of the conference where the paper NLPositionality: Characterizing Design Biases of Datasets and Models, got published?
","['maarten sap_a66ff335f5934fe7503a99d3eb3abed493994df1_metadata.txt', 'maarten sap_a66ff335f5934fe7503a99d3eb3abed493994df1_content_2.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the full name of the conference where the paper NLPositionality: Characterizing Design Biases of Datasets and Models, got published?

Context: Faculty Name: maarten sap
Paperid: a66ff335f5934fe7503a99d3eb3abed493994df1
Title: NLPositionality: Characterizing Design Biases of Datasets and Models
Year: 2023
Abstract: Design biases in NLP systems, such as performance differences for different populations, often stem from their creator’s positionality, i.e., views and lived experiences shaped by identity and background. Despite the prevalence and risks of design biases, they are hard to quantify because researcher, system, and dataset positionality is often unobserved. We introduce NLPositionality, a framework for characterizing design biases and quantifying the positionality of NLP datasets and models. Our framework continuously collects annotations from a diverse pool of volunteer participants on LabintheWild, and statistically quantifies alignment with dataset labels and model predictions. We apply NLPositionality to existing datasets and models for two tasks—social acceptability and hate speech detection. To date, we have collected 16,299 annotations in over a year for 600 instances from 1,096 annotators across 87 countries.We find that datasets and models align predominantly with Western, White, college-educated, and younger populations. Additionally, certain groups, such as non-binary people and non-native English speakers, are further marginalized by datasets and models as they rank least in alignment across all tasks. Finally, we draw from prior literature to discuss how researchers can examine their own positionality and that of their datasets and models, opening the door for more inclusive NLP systems.
Authors: Sebastin Santy, Jenny T Liang, Ronan Le Bras, Katharina Reinecke, Maarten Sap
Venue: Annual Meeting of the Association for Computational Linguistics
Tldr: {'model': 'tldr@v2.0.0', 'text': 'NLPositionality, a framework for characterizing design biases and quantifying the positionality of NLP datasets and models, is introduced and it is found that dataset and models align predominantly with Western, White, college-educated, and younger populations.'}
Url: http://arxiv.org/pdf/2306.01943
Title: NLPositionality: Characterizing Design Biases of Datasets and Models
Authors: Sebastin Santy, Jenny T. Liang, Ronan Le Bras, Katharina Reinecke, Maarten Sap, Aditya Sharma
Section: 5 Discussion
Hanna et al., 2020; Bender et al., 2021). This can be done using approaches such as participatory design (Spinuzzi, 2005), including interactive storyboarding (Madsen and Aiken, 1993), as well as value-sensitive design (Friedman, 1996), including panels of experiential experts (Madsen and Aiken, 1993). Building datasets and models with large global teams such as BigBench (Srivastava et al., 2022) and NL-Augmenter (Dhole et al., 2021) could also reduce design biases by having diverse teams (Li, 2020). To account for annotator subjectivity (Aroyo and Welty, 2015), researchers should make concerted efforts to recruit annotators from diverse backgrounds. Websites like LabintheWild can be platforms where these annotators are recruited. Since new design biases could be introduced in this process, we recommend following the practice of documenting the demographics of annotators as in prior works (e.g., Forbes et al., 2020; Vidgen et al., 2021) to record a dataset’s positionality. We urge considering research through the lens of perspectivism (Basile et al., 2021), i.e. being mindful of different perspectives by sharing datasets with disaggregated annotations and finding modeling techniques that can handle inherent disagreements or distributions (Plank, 2022), instead of forcing a single answer in the data (e.g., by majority vote; Davani et al., 2022) or model (e.g., by classification to one label; Costanza-Chock, 2018). Researchers also should carefully consider how they aggregate labels from diverse annotators during modeling so their perspectives are represented, such as not averaging annotations to avoid the “tyranny of the mean” (Talat et al., 2022). Finally, we argue that the notion of “inclusive NLP” does not mean that all language technologies have to work for everyone.
Answer: "
"What is the office number for Joan Axelson?
","['handbook_phd_2023-2024.txt', 'eric xing_84a36e19f9394f22b34f79756fa9628a795e02ea_content_13.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the office number for Joan Axelson?

Context: degree are: 
Jamie Callan 
Ph.D. Program Director 
Professor 
GHC 5419 
callan@cs.cmu.edu 
412-268-4525 
Stacey Young 
Ph.D. Academic Program Manager 
LTI Graduate Program Manager 
GHC 6415 
staceyy@cs.cmu.edu  
412-268-2623 
Mona Diab 
LTI Director 
Professor 
GHC 5723 
mdiab@andrew.cmu.edu 
412-268-3669 
 
Joan Axelson 
Office Manager 
GHC 5405 
jaxelson@andrew.cmu.edu 
412-268-7517 
 
Julie Nys 
Employment Processes Manger 
GHC 5405 
jnys@andrew.cmu.edu 
412-268-3515 
1.3 
University Policies and Expectations 
It is the responsibility of each member of the Carnegie Mellon community to be familiar with 
university policies and guidelines. In addition to this departmental graduate student handbook, 
the following resources are available to assist you in understanding community expectations. 
 The Word/Student Handbook: 
https://www.cmu.edu/student-affairs/theword//index.html 
 Academic Integrity Policy: 
 
https://www.cmu.edu/policies/student-and- student-life/academic-
integrity.html 
LTI Ph.D. Graduate Student Handbook 
Page 11 
 
 University Policies Website:  
https://www.cmu.edu/policies/ 
 Office of Graduate and Postdoctoral Affairs: 
https://www.cmu.edu/graduate/policies/index.html 
Due to the changing nature of conditions and expectations surrounding public health and safety 
requirements please visit https://www.cmu.edu/coronavirus/ for the most up to date information.
 
Please see Appendix A for additional information about The Word and University 
resources. 
1.4 
The Academic Calendar  
The Academic calendar can be found at https://www.cmu.edu/hub/calendar/index.html and 
provides information on all deadlines, including registration dates, class start dates, add/drop 
deadlines, exam dates, and more.  
Some doctoral course-sections follow a separate Academic Calendar.
Title: LMSYS-CHAT-1M: A LARGE-SCALE REAL-WORLD LLM CONVERSATION DATASET
Authors: Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric P. Xing, Joseph E. Gonzalez, Ion Stoica, Hao Zhang
Section: B SAMPLE CONVERSATIONS
the following prompt message written by user delimited in triple backticks and determine if the prompt contains any sensitive data that a company would not want to be leaked. Follow these instruction: 1. Extract all sensitive data strings in the prompt and insert them into a JSON object. The keys for each string will be the data type that they are followed by incremental counter. 2. The keys that were used in the JSON object to store the sensitive string should be used as placeholders in the original prompt and replace the sensitive data strings. 3. Come up with in place replacements of the same type for each sensitive data point and replace the key in the json of the sensitive data point with it, then replace the placeholder in the original prompt with the made up replacement. “I just moved to 123 Elm Street, Springfield, IL 62704. Can you recommend any good restaurants in my neighborhood?” User: Write an ethical and practical guideline for employees consisting of 10 rules and the resulting recommendations for action in dealing with AI and AI-generated content for a privately organized Swiss company. Important: This guide should address employees and not the company. Take into account Swiss legal circumstances. Close possible gaps due to missing rules in advance by integrating them into the 10 rules. Avoid duplications and use a minimum of 5000 characters and a maximum of 7000 characters.
Answer: "
"What sort of credentials are required to print something from an LTI printer?
","['mlt-student-handbook-2023-2024.txt', 'handbook-msaii-2022-2023.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What sort of credentials are required to print something from an LTI printer?

Context: LTIs printers 
are located in GHC 5404 and GHC 6604. The School of Computer Science provides a number of 
black-and-white and color printers for use by students.  The SCS Computer Facilities publishes a 
list of printers online at http://www.cs.cmu.edu/~help/printing/.  
 
MLT Graduate Student Handbook 
Page 11 
 
2.3 Office Space for MS Students 
To help them create a sense of community, full time students in the LTIs MLT program have 
access to a shared office space. 
2.4 Computers for MS Students 
Students are expected to provide their own laptop computers that can be used to access 
university resources and complete course assignments. Laptops running Windows, MacOS, and 
Linux software are all acceptable. 
MS students will be given a CS user ID. A CS user ID is required to use the LTI computer cluster, 
department printers, and other SCS services. The School of Computer Science has a Help Center 
located at 4203 GHC.  They can be contacted at help@cs.cmu.edu, extension 8-4231 from a 
campus phone, or 412-268-4231 from an outside line. 
MS students will be given access to the LTIs computer cluster on an as-needed basis, to be 
used for course assignments, directed study projects, and/or capstone projects. The LTI cluster 
provides storage and computation for projects involving large datasets and/or lengthy 
computation.  
3 Masters Degree Completion and Certification 
3.1 Standard Degree Requirements and Degree Certification 
3.1.1 Graduate Students 
Carnegie Mellon graduate students are expected to complete their degree requirements within 
the standard length of time for their program of study as outlined in the relevant Graduate 
Student Handbook. Standard program lengths for graduate students vary significantly-- ranging 
from two semesters for some full-time masters programs to several or more years for doctoral 
programs. Upon completion of the graduate program degree requirements, the degree will be 
certified by the students academic program in the semester in which the student completes 
the requirements. 
3.1.2 Early Completion 
Graduate students who consider the completion of all degree requirements in less than the 
standard length of time for their program of study may consult with their degree-granting 
program or department to determine if early degree certification is allowed and under what 
circumstances.
9 The Language Technologies Institute 
9.1 Working Space for MS Students 
Except for restrictions due to COVID, full-time students in the LTI’s MS degree programs on the 
Pittsburgh campus have access to a shared working space to create a sense of community and 
provide space for working when on campus. 
9.2 Photocopies and Printers 
Mailboxes, printers, copiers, and other departmental resources are located in GHC 5404 
The use of a photocopier requires only a CMU ID card.  The School of Computer Science 
provides a number of black-and-white and color printers for use by students.  The SCS 
Computer Facilities publishes a list of printers online at 
https://computing.cs.cmu.edu/desktop/printing.  When you send a file to the print queue, you 
may go to any of the listed printers, swipe your CMU ID, and the file will be printed on that 
printer. 
9.3 Computers for MS Students 
Students are expected to provide their own laptop computers that can be used to access 
university resources and complete course assignments.  Laptops running Windows, MacOS, and 
Linux software are all acceptable. 
MS students will be given a CS user id.  A CS user id is required to use the LTI computer cluster, 
department printers, and other SCS services.  The School of Computer Science has a Help 
Center located at 4203 GHC.  They can be contacted at help@cs.cmu.edu, extension 8-4231 from 
a campus phone, or 412-268-4231 from an outside line. 
MS students will be given access to the LTI’s computer cluster on an as-needed basis, to be used 
for course assignments, directed study projects, and/or the capstone project.  The LTI cluster 
provides storage and computation for projects involving large datasets and/or lengthy 
computation.  
12 
 
10 Degree Requirements 
MSAII students are expected to complete their degree requirements within five consecutive semesters, 
fall, spring, summer internship, second fall, second spring.  Under exceptional conditions, such as for 
medical reasons, a student may be granted a leave of absence.  See Section 11.1.10, below.   Upon 
completion of the graduate program degree requirements, the degree will be certified by the student’s 
academic program in the semester in which the student completes the requirements.
Answer: "
"Which LTI faculty are involved in the framework tax paper?
","['emma strubell_b777aa86b5a1d49ce8eababc5c2ee56d3562801e_metadata.txt', 'yonatan bisk_b777aa86b5a1d49ce8eababc5c2ee56d3562801e_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which LTI faculty are involved in the framework tax paper?

Context: Faculty Name: emma strubell
Paperid: b777aa86b5a1d49ce8eababc5c2ee56d3562801e
Title: The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment
Year: 2023
Abstract: Increased focus on the computational efficiency of NLP systems has motivated the design of efficient model architectures and improvements to underlying hardware accelerators. However, the resulting increases in computational throughput and reductions in floating point operations have not directly translated to improvements in wall-clock inference latency. We demonstrate that these discrepancies can be largely attributed to bottlenecks introduced by deep learning frameworks. We denote this phenomenon as the \textit{framework tax}, and observe that the disparity is growing as hardware speed increases over time. In this work, we examine this phenomenon through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency. Code is available at https://github.com/JaredFern/Framework-Tax.
Authors: Jared Fernandez, Jacob Kahn, Clara Na, Yonatan Bisk, Emma Strubell
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work examines the effects of model design decisions, framework paradigms, and hardware platforms on total model latency through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency.'}
Url: http://arxiv.org/pdf/2302.06117
Faculty Name: yonatan bisk
Paperid: b777aa86b5a1d49ce8eababc5c2ee56d3562801e
Title: The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment
Year: 2023
Abstract: Increased focus on the computational efficiency of NLP systems has motivated the design of efficient model architectures and improvements to underlying hardware accelerators. However, the resulting increases in computational throughput and reductions in floating point operations have not directly translated to improvements in wall-clock inference latency. We demonstrate that these discrepancies can be largely attributed to bottlenecks introduced by deep learning frameworks. We denote this phenomenon as the \textit{framework tax}, and observe that the disparity is growing as hardware speed increases over time. In this work, we examine this phenomenon through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency. Code is available at https://github.com/JaredFern/Framework-Tax.
Authors: Jared Fernandez, Jacob Kahn, Clara Na, Yonatan Bisk, Emma Strubell
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work examines the effects of model design decisions, framework paradigms, and hardware platforms on total model latency through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency.'}
Url: http://arxiv.org/pdf/2302.06117
Answer: "
"What is the full name of the metric used to evaluate the performance of the models on the Squad test set in the paper PROMPT2MODEL: Generating Deployable Models from Natural Language Instructions?
","['graham neubig_e69684fb06a7b1fe621d7ef0c97fc2ca0e122c43_content_1.txt', 'graham neubig_e69684fb06a7b1fe621d7ef0c97fc2ca0e122c43_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the full name of the metric used to evaluate the performance of the models on the Squad test set in the paper PROMPT2MODEL: Generating Deployable Models from Natural Language Instructions?

Context: Title: PROMPT2MODEL: Generating Deployable Models from Natural Language Instructions
Authors: Vijay Viswanathan, Chenyang Zhao, Amanda Bertsch, Tongshuang Wu, Graham Neubig
Section: 6 Discussion and Conclusion
reference implementation. This model is believed to be similar to GPT-3 (Brown et al., 2020), which was trained on 93% English documents, 1% German documents, 1% French documents, and <5% documents in any other language. Our use of this model may exacerbate existing disparities in language technologies between highresource languages and low-resource languages. One potential limitation is that we have only tested our approach on 3 tasks, each with a single dataset and a single evaluation metric. We justify this decision because our focus is on providing an extensible software system rather than establishing state-of-the-art results on many datasets, but we believe that our results suggest broader applicability. Ethics Statement Any system which makes powerful technology more accessible to the public has ethical implications. Widder et al. (2022) discuss ethical issues with open-source packages in relation to software libraries for deepfaking, including the possibility of enabling malicious actors to use technology that they would otherwise not have the technical skills to leverage. This is also a risk for an AutoML system such as Prompt2Model; however, we believe this risk is outweighed by the benefits of greater accessibility, especially given that a low barrier to entry for generating harmful data already exists in the form of prompted, web-interface models. While Prompt2Model could, if given harmful inputs, generate toxic, offensive, or inaccurate synthetic data, this is no more of a risk with Prompt2Model than it is with the underlying prompted model (Bender et al., 2021); indeed, the use of models and supplementary datasets retrieved from Hugging Face may lessen the likelihood of a downstream model replicating harms from the prompted model’s outputs, though more investigation is needed. Like all ML models, the models that Prompt2Model returns can make mistakes, and we aim to be transparent in our documentation about potential limitations of the system. We hope that Prompt2Model will be broadly useful.
Faculty Name: graham neubig
Paperid: e69684fb06a7b1fe621d7ef0c97fc2ca0e122c43
Title: Prompt2Model: Generating Deployable Models from Natural Language Instructions
Year: 2023
Abstract: Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment. This is done through a multi-step process of retrieval of existing datasets and pretrained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains models that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model reliability before deployment. Prompt2Model is available open-source at https://github.com/neulab/prompt2model.
Authors: Vijay Viswanathan, Chenyang Zhao, Amanda Bertsch, Tongshuang Sherry Wu, Graham Neubig
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment.'}
Url: https://arxiv.org/pdf/2308.12261
Answer: "
"Which conference is DIFFERENCE-MASKING published in?
","['eric nyberg_8d0c37eee7162f33178979b4183f0211e2dcae0d_metadata.txt', 'louis philippe morency_8d0c37eee7162f33178979b4183f0211e2dcae0d_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which conference is DIFFERENCE-MASKING published in?

Context: Faculty Name: eric nyberg
Paperid: 8d0c37eee7162f33178979b4183f0211e2dcae0d
Title: Difference-Masking: Choosing What to Mask in Continued Pretraining
Year: 2023
Abstract: The self-supervised objective of masking-and-predicting has led to promising performance gains on a variety of downstream tasks. However, while most approaches randomly mask tokens, there is strong intuition that deciding what to mask can substantially improve learning outcomes. We investigate this in continued pretraining setting in which pretrained models continue to pretrain on domain-specific data before performing some downstream task. We introduce Difference-Masking, a masking strategy that automatically chooses what to mask during continued pretraining by considering what makes a task domain different from the pretraining domain. Empirically, we find that Difference-Masking outperforms baselines on continued pretraining settings across four diverse language-only and multimodal video tasks.
Authors: Alex Wilf, Syeda Nahida Akter, Leena Mathur, P. Liang, Sheryl Mathew, Mengrou Shou, Eric Nyberg, Louis-Philippe Morency
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'Difference-Masking is introduced, a masking strategy that automatically chooses what to mask during continued pretraining by considering what makes a task domain different from the pretraining domain.'}
Url: http://arxiv.org/pdf/2305.14577
Faculty Name: louis philippe morency
Paperid: 8d0c37eee7162f33178979b4183f0211e2dcae0d
Title: Difference-Masking: Choosing What to Mask in Continued Pretraining
Year: 2023
Abstract: The self-supervised objective of masking-and-predicting has led to promising performance gains on a variety of downstream tasks. However, while most approaches randomly mask tokens, there is strong intuition that deciding what to mask can substantially improve learning outcomes. We investigate this in continued pretraining setting in which pretrained models continue to pretrain on domain-specific data before performing some downstream task. We introduce Difference-Masking, a masking strategy that automatically chooses what to mask during continued pretraining by considering what makes a task domain different from the pretraining domain. Empirically, we find that Difference-Masking outperforms baselines on continued pretraining settings across four diverse language-only and multimodal video tasks.
Authors: Alex Wilf, Syeda Nahida Akter, Leena Mathur, P. Liang, Sheryl Mathew, Mengrou Shou, Eric Nyberg, Louis-Philippe Morency
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'Difference-Masking is introduced, a masking strategy that automatically chooses what to mask during continued pretraining by considering what makes a task domain different from the pretraining domain.'}
Url: http://arxiv.org/pdf/2305.14577
Answer: "
"What is the name of the proposed recommendation model in the paper ""Text Matching Improves Sequential Recommendation by Reducing Popularity Biases""?
","['chenyan xiong_159100c8323fc558e4073a3a006f3f243aca3a60_metadata.txt', 'chenyan xiong_159100c8323fc558e4073a3a006f3f243aca3a60_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the name of the proposed recommendation model in the paper ""Text Matching Improves Sequential Recommendation by Reducing Popularity Biases""?

Context: Faculty Name: chenyan xiong
Paperid: 159100c8323fc558e4073a3a006f3f243aca3a60
Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases
Year: 2023
Abstract: This paper proposes Text mAtching based SequenTial rEcommenda-tion model (TASTE), which maps items and users in an embedding space and recommends items by matching their text representations. TASTE verbalizes items and user-item interactions using identifiers and attributes of items. To better characterize user behaviors, TASTE additionally proposes an attention sparsity method, which enables TASTE to model longer user-item interactions by reducing the self-attention computations during encoding. Our experiments show that TASTE outperforms the state-of-the-art methods on widely used sequential recommendation datasets. TASTE alleviates the cold start problem by representing long-tail items using full-text modeling and bringing the benefits of pretrained language models to recommendation systems. Our further analyses illustrate that TASTE significantly improves the recommendation accuracy by reducing the popularity bias of previous item id based recommendation models and returning more appropriate and text-relevant items to satisfy users. All codes are available at https://github.com/OpenMatch/TASTE.
Authors: Zhenghao Liu, Senkun Mei, Chenyan Xiong, Xiaohua Li, Shi Yu, Zhiyuan Liu, Yu Gu, Ge Yu
Venue: International Conference on Information and Knowledge Management
Tldr: {'model': 'tldr@v2.0.0', 'text': None}
Url: https://arxiv.org/pdf/2308.14029
Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases
Authors: Zhenghao Liu, Sen Mei, Chenyan Xiong, Xiaohua Li, Shi Yu, Zhiyuan Liu, Yu Gu, Ge Yu
Section: 5.3 Effectiveness of Item Verbalization Methods
more to memorize some characteristics of users, such as the taste, preferred cuisines, and active area of users. Besides, the prompt-based modeling method (Prompt) outperforms the embedding-based method (Embed), which illustrates that pretrained language models have the ability to understand the user/item identifiers and establish relevance between users and items via identifiers. It further supports the motivation of TASTE, which fully uses the learned knowledge from pretrained language models to build sequential recommendation systems. Evaluation on Recommendation Behaviors. Finally, we explore the recommendation behaviors of TASTE using different item modeling methods. As shown in Table 7, three models, T5-ID, TASTE w/o ID, and TASTE, are compared. T5-ID randomly initializes item embeddings and directly predicts the item ids. TASTE w/o ID and TASTE employ a two-tower architecture [29] and encode items using attributes and identifiers & attributes, respectively. As shown in our evaluation results, T5-ID returns an average of 49.5% of popular products in the recommendation results of all datasets, showing that it faces the popularity bias problem during recommending items. TASTE alleviates the popularity bias by reducing on average 18.75% popular items in its recommendation results. It represents items using full texts and utilizes text matching to calibrate the popularity-oriented recommendation behavior of T5ID. TASTE demonstrates its effectiveness in recommending more appropriate and text-relevant items by achieving higher Bleu scores and Recall scores. Besides, compared with TASTE w/o ID, TASTE achieves higher Bleu and Dist scores with the help of item ids. It shows that the item ids can serve as a kind of prompt to provide additional matching signals beyond item attributes to better model the relevance between users and items.
Answer: "
"In spring 2024, What is the title of course 17416?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, What is the title of course 17416?

Context: In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Software Engineering for Startups' with Course ID 17356 and Section B offers 12.0 units. The Class meets Friday between 11:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Wright, Brown located in Building GHC, Room 4301.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Software Engineering Practicum' with Course ID 17413 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Timperley located in Building WEH, Room 6403.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'AI Governance: Identifying & Mitigating Risks in Design & Dev of AI Solutions' with Course ID 17416 and Section A offers 6.0 units. The Class meets Wednesday between 10:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Sadeh located in Building HH, Room B131.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'AI Governance: Identifying & Mitigating Risks in Design & Dev of AI Solutions' with Course ID 17416 and Section B offers 6.0 units. The Class meets Wednesday between 10:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Sadeh located in Building CMU, Room REMOTE.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Building User-Focused Sensing Systems' with Course ID 17422 and Section A offers 12.0 units. The Class meets Monday Wednesday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Agarwal, Goel located in Building WEH, Room 5403.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Software Engineering for Startups' with Course ID 17356 and Section B offers 12.0 units. The Class meets Friday between 11:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Wright, Brown located in Building GHC, Room 4301.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Software Engineering Practicum' with Course ID 17413 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Timperley located in Building WEH, Room 6403.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'AI Governance: Identifying & Mitigating Risks in Design & Dev of AI Solutions' with Course ID 17416 and Section A offers 6.0 units. The Class meets Wednesday between 10:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Sadeh located in Building HH, Room B131.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'AI Governance: Identifying & Mitigating Risks in Design & Dev of AI Solutions' with Course ID 17416 and Section B offers 6.0 units. The Class meets Wednesday between 10:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Sadeh located in Building CMU, Room REMOTE.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Building User-Focused Sensing Systems' with Course ID 17422 and Section A offers 12.0 units. The Class meets Monday Wednesday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Agarwal, Goel located in Building WEH, Room 5403.
Answer: "
"For the MCDS degree do you need to do a capstone project? Yes/no
","['program_info_MasterofComputationalDataScience.txt', 'mcds-student-handbook-2023_2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: For the MCDS degree do you need to do a capstone project? Yes/no

Context: Academic Program Name:
Master of Computational Data Science

Website:
https://lti.cs.cmu.edu/academics/masters-programs/mcds.html

Overview:
The MCDS degree focuses on engineering and deploying large-scale information systems. Our comprehensive curriculum equips you with the skills and knowledge to develop the layers of technology involved in the next generation of massive information system deployments and analyze the data these systems generate. When you graduate, you’ll have a unified vision of these systems from your core courses; internship experience; and semester-long, group-oriented capstone project. MCDS graduates are sought-after software engineers, data scientists and project managers at leading information technology, software services and social media companies.

Requirements:
The MCDS program offers three majors: Systems, Analytics, and Human-Centered Data Science. All three require the same total number of course credits, split among required core courses, electives, data science seminar and capstone courses specifically defined for each major. The degree can also be earned in two different ways, depending on the length of time you spend working on it. Regardless of the timing option, all MCDS students must complete a minimum of 144 units to graduate.
Here are the options:
Standard Timing — a 16-month degree consisting of study for fall and spring semesters, a summer internship, and fall semester of study. Each semester comprises a minimum of 48 units. This timing is typical for most students. Students graduate in December.
Extended Timing — a 20-month degree consisting of study for fall and spring semesters, a summer internship, and a second year of fall and spring study. Each semester comprises a minimum of 36 units. Students graduate in May.
For a complete overview of the MCDS requirements read the MCDS Handbook.

Curriculum:
To earn an MCDS degree, students must pass courses in the core curriculum, the MCDS seminar, a concentration area, and electives. Students must also complete a capstone project in which they work on a research project at CMU or on an industry-sponsored project.
In total, students must complete 144 eligible units of study, including eight 12-unit courses, two 12-unit seminar courses, and one 24-unit capstone course. Students must choose at minimum five core courses. The remainder of the 12-unit courses with course numbers 600 or greater can be electives chosen from the SCS course catalog.
While some MCDS graduates continue on to PhD programs in the 
LTI or other leading universities, most graduates go on to jobs in 
 
 
7
corporate research and development laboratories. 
 
The program consists entirely of coursework and a Capstone Project, and 
no Master’s Thesis is required. All Capstone projects are structured as 
research activities and may lead to a publication. There is no Doctoral 
program in Computational Data Science. Because of the highly selective 
nature of the MCDS program and quality of the MCDS curriculum, 
performing well in the program will give a boost to a PhD application. MS 
graduates are welcome to apply to CMU PhD programs but will not receive 
preferential treatment. 
 
There are significant differences between CMU's different departments 
and degree programs in philosophical approach, procedures, policies and 
regulations. Each department issues a handbook that informs graduate 
students of their program requirements and procedures and ensures that 
students have written access to the standard information outlined below. 
This handbook describes the policies, procedures, and requirements for 
the Master of Computational Data Science (MCDS) degree. 
All policies not explicitly described in this document conform to School of 
Computer Science (SCS) policies and university policies described in The 
Word, Carnegie Mellon University Student Handbook and at the 
University Policies website. 
1.2 Vision 
Carnegie Mellon University will have a transformative impact on society 
through continual innovation in education, research, creativity, and 
entrepreneurship. 
1.3 Mission 
To create a transformative educational experience for students focused on 
deep disciplinary knowledge; problem solving; leadership, 
communication, and interpersonal skills; and personal health and well-
being. 
To cultivate a transformative university community committed to (a) 
attracting and retaining diverse, world-class talent; (b) creating a 
collaborative environment open to the free exchange of ideas, where 
 
 
8
research, creativity, innovation, and entrepreneurship can flourish; and 
(c) ensuring individuals can achieve their full potential. 
To impact society in a transformative way — regionally, nationally, and 
globally — by engaging with partners outside the traditional borders of the 
university campus.
Answer: "
"How many turing awards recipients have been from Carnegie Mellon University?
","['history_d401.txt', '25things_d400.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many turing awards recipients have been from Carnegie Mellon University?

Context: The university’s first computer science Ph.D.s were graduates of this program.Computer Science Department establishedIn 1965, Carnegie Tech established its Computer Science Department, or CSD, with a $5 million grant from the R.K. Mellon Foundation. Perlis was the first department head. There were no undergraduates; only Ph.D. students were admitted, and the department’s focus was on research, much of it funded by the federal government through the Defense Department’s Advanced Research Projects Agency, or DARPA. In 1967, Carnegie Institute of Technology merged with the nearby Mellon Institute of Industrial Research to form Carnegie-Mellon University. The Department of Computer Science moved into the newly created Mellon Institute of Science, later renamed Mellon College of Science, or MCS. Future SCS dean Raj Reddy joined the CSD in 1969 after three years as an assistant professor at Stanford. He brought with him research in speech, language and computer vision. But in 1970 and 1971, the new Computer Science Department faced its first crisis, as half of its tenured faculty members—including department head Perlis—left for other universities. Joe Traub was recruited from Bell Labs to CMU to become the new department head.Multi-processor machines emergeCSD emerged from the brief crisis as a highly agile, interdisciplinary entity, with many new faculty members taking joint appointments with other CMU departments. Several large projects emerged in the Computer Science Department, including C.mmp, the first shared-memory multiprocessor computer, with 16 processing units, and Cm*, a 50-processor computer. These computers were the forerunners of today’s ubiquitous multi-core desktops and laptops.Turing Awards and a Nobel PrizeIn 1975, Simon and Newell were awarded the A.M. Turing Award for their work in artificial intelligence. (As of 2014, 12 CMU alumni or faculty have been awarded Turings, sometimes considered the Nobel Prize of computing.) Three years later, Simon received the Nobel Prize in Economics for his work on decision-making theory. As the 1970s progressed, Newell became interested in human-computer interaction, and began a long relationship with Xerox’s Palo Alto Research Center, or PARC, which released the Xerox Alto in 1973.
25 Great Things About SCS | SCS25 - Carnegie Mellon University School of Computer Science Skip to main content Legal Events Register 25 Things History Video 25 Great Things About SCS What’s so great about computer science at Carnegie Mellon?We're glad you asked! Here are 25 great ideas from CMU computer scientists to think about as we celebrate the birthday of the School of Computer Science.1. Artificial intelligence, 1955-56 Can you write a working computer program without a computer? Herb Simon (H’90), at left, Allen Newell (IA’57), at right, and Cliff Shaw did. The team created the first artificial intelligence program, Logic Theorist, which could solve logic puzzles in the same way that a human might solve them. Newell demonstrated that it worked by writing the instructions on 3-by-5 index cards that were manipulated on the kitchen table by Newell, his wife, and a group of Carnegie Tech grad students. 2. Multi-core processors, 1971 Multi-core processors are common in today’s computers, but they were still science fiction in the early 1970s. But when CMU researchers found their existing machines too slow to keep pace with the advance of speech and graphics programs, they knew they had to do something. They solved the problem by ganging together 16 processors to build a pioneering computer called C.mmp—then topped the feat by linking 50 processors into Cm*. 3. Tutoring machines, 1973Games and drills, such as flash cards, have long been used to help students learn tough subjects. But the cognitive tutoring programs developed at Carnegie Mellon, beginning in the 1970s, did more than simply drill students on math problems. Cognitive tutors were able to adapt, presenting harder or easier problems as students learned or stumbled. Today, cognitive tutors teach subjects such as algebra to hundreds of thousands of students every year. 4. Speech recognition, 1976If you have an iPhone, ask Siri to look up “Hearsay I,” the first computer system capable of continuous speech recognition. It was developed by future Turing Award winner and future SCS dean Raj Reddy along with his students. Their work on subsequent systems established many of the principles that still underlie speech recognition software. 5.
Answer: "
"In summer 2024, When is the deadline for withdrawing from a Mini-5 course and receiving a withdrawal grade?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-2324.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In summer 2024, When is the deadline for withdrawing from a Mini-5 course and receiving a withdrawal grade?

Context: On Friday, 07 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 pass/no pass & withdrawal deadline (3) is observed.
On Friday, 14 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Faculty Course Evaluations open is observed.
On Tuesday, 18 June, 2024, during the Summer One 2024 (M24) semester, Semester course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Wednesday, 19 June, 2022, during the Summer One 2024 (M24) semester, Juneteenth is observed, leading to University Closed & No Classes.
On Thursday, 20 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Last Day of Classes is observed.
On Thursday, 20 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 voucher deadline (4) is observed.
On Friday, 21 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Final Exams is observed.
On Friday, 21 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Faculty Course Evaluations close is observed.
On Monday, 24 June, 2024, during the Summer One 2024 (M24) semester, Mini-6 First Day of Classes is observed.
On Tuesday, 25 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Final Grades Due by 4 pm is observed.
On Friday, 28 June, 2024, during the Summer One 2024 (M24) semester, Mini-6 add, audit, & tuition adjustment drop deadline (1) is observed.
On Thursday, 04 July, 2024, during the Summer One 2024 (M24) semester, Independence Day is observed, leading to University Closed & No Classes.
On Friday, 05 July, 2024, during the Summer One 2024 (M24) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
Event: 'Mini-5 course drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-07', Day: 'Friday', Event: 'Mini-5 pass/no pass & withdrawal deadline (3)', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-14', Day: 'Friday', Event: 'Mini-5 Faculty Course Evaluations open', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-18', Day: 'Tuesday', Event: 'Semester course drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Summer One 2024 (M24)'
Date: '2022-06-19', Day: 'Wednesday', Event: 'Juneteenth; University Closed & No Classes', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-20', Day: 'Thursday', Event: 'Mini-5 Last Day of Classes', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-20', Day: 'Thursday', Event: 'Mini-5 voucher deadline (4)', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-21', Day: 'Friday', Event: 'Mini-5 Final Exams', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-21', Day: 'Friday', Event: 'Mini-5 Faculty Course Evaluations close ', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-24', Day: 'Monday', Event: 'Mini-6 First Day of Classes', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-25', Day: 'Tuesday', Event: 'Mini-5 Final Grades Due by 4 pm', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-28', Day: 'Friday', Event: 'Mini-6 add, audit, & tuition adjustment drop deadline (1)', Semester: 'Summer One 2024 (M24)'
Date: '2024-07-04', Day: 'Thursday', Event: 'Independence Day; University Closed & No Classes',
Answer: "
"What does SenteCon do to a given passage of text?
","['louis philippe morency_47a4ac301820c3ea7da4efb8e2466cc6468ad631_metadata.txt', 'louis philippe morency_47a4ac301820c3ea7da4efb8e2466cc6468ad631_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What does SenteCon do to a given passage of text?

Context: Faculty Name: louis philippe morency
Paperid: 47a4ac301820c3ea7da4efb8e2466cc6468ad631
Title: SenteCon: Leveraging Lexicons to Learn Human-Interpretable Language Representations
Year: 2023
Abstract: Although deep language representations have become the dominant form of language featurization in recent years, in many settings it is important to understand a model's decision-making process. This necessitates not only an interpretable model but also interpretable features. In particular, language must be featurized in a way that is interpretable while still characterizing the original text well. We present SenteCon, a method for introducing human interpretability in deep language representations. Given a passage of text, SenteCon encodes the text as a layer of interpretable categories in which each dimension corresponds to the relevance of a specific category. Our empirical evaluations indicate that encoding language with SenteCon provides high-level interpretability at little to no cost to predictive performance on downstream tasks. Moreover, we find that SenteCon outperforms existing interpretable language representations with respect to both its downstream performance and its agreement with human characterizations of the text.
Authors: Victoria Lin, Louis-Philippe Morency
Venue: Annual Meeting of the Association for Computational Linguistics
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work presents SenteCon, a method for introducing human interpretability in deep language representations that outperforms existing interpretable language representations with respect to both its downstream performance and its agreement with human characterizations of the text.'}
Url: http://arxiv.org/pdf/2305.14728
Title: SENTECON: Leveraging Lexicons to Learn Human-Interpretable Language Representations
Authors: Victoria Lin, Louis-Philippe Morency
Section: B Experimental Details
• Not expressed: Out of all possible interpretations of the sentence above, you cannot imagine a scenario in which the speaker of the sentence was expressing the topic. • Potentially expressed: You can imagine at least one scenario in which the speaker of the sentence was expressing the topic. • Most likely expressed: The most natural interpretation of the sentence clearly expresses the topic. Category batches. As mentioned in the main paper, the 52 LIWC categories were randomly split into 5 sets of roughly equal size to avoid annotator fatigue. The splits were as follows: • Batch 1: netspeak, differ, cause, nonflu, discrep, drivers, relig, swear, feel, home, family • Batch 2: leisure, sexual, see, bio, certain, money, percept, female, death, anger, cogproc • Batch 3: filler, sad, posemo, friend, relativ, ingest, body, work, time, social, informal • Batch 4: focusfuture, anx, affiliation, motion, power, reward, space, tentat, risk, focuspresent, affect • Batch 5: negemo, hear, male, health, insight, achiev, focuspast, assent Inter-rater reliability. To assess the reliability of our annotations, we calculated intraclass correlation coefficients (ICCs) using the agreement software package (Girard, 2020). For each batch of sentences, we computed the ICC and its 95% confidence interval, then averaged these across category batches (Table 7). We averaged ICCs over all batches to obtain the overall ICC. Annotators. Annotators were required to be fluent in English and to be nationals of one of the following countries: the United States, the United Kingdom, Ireland, Australia, or Canada. Annotators were further required to have a prior approval rating of ≥ 95%, and an attention check question was included in every sentence batch. All annotators passed the attention check. We took care to compensate annotators at a rate above the local minimum wage. Annotators received an average hourly wage of 8.00 USD.
Answer: "
"What year was Exploration on HuBERT with Multiple Resolutions published?
","['shinji watanabe_fa75ef55e04e3b25b8af56435478c2fd17403ce8_content_1.txt', 'shinji watanabe_fa75ef55e04e3b25b8af56435478c2fd17403ce8_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What year was Exploration on HuBERT with Multiple Resolutions published?

Context: Title: Exploration on HuBERT with Multiple Resolutions
Authors: Jiatong Shi, Yun Tang, Hirofumi Inaguma, Hongyu Gong, Juan Pino, Shinji Watanabe
Section: 6. References
on sa-unet,” in 2019 4th International Conference on Mechanical, Control and Computer Engineering (ICMCCE), 2019, pp. 818–8183. [21] T. Zhao et al., “Unet++-based multi-channel speech dereverberation and distant speech recognition,” in 2021 12th International Symposium on Chinese Spoken Language Processing (ISCSLP), 2021, pp. 1–5. [22] R. Li et al., “Unet-tts: Improving unseen speaker and style transfer in one-shot voice cloning,” in Proc. ICASSP, 2022, pp. 8327– 8331. [23] X. Xiang, X. Zhang, and H. Chen, “A nested u-net with selfattention and dense connectivity for monaural speech enhancement,” IEEE Signal Processing Letters, vol. 29, pp. 105–109, 2021. [24] Y. Xian et al., “A multi-scale feature recalibration network for end-to-end single channel speech enhancement,” IEEE Journal of Selected Topics in Signal Processing, vol. 15, no. 1, pp. 143– 155, 2020. [25] G. Liu et al., “Cp-GAN: Context pyramid generative adversarial network for speech enhancement,” in Proc. ICASSP, 2020, pp. 6624–6628. [26] X. Xiang, X. Zhang, and H. Chen, “A convolutional network with multi-scale and attention mechanisms for end-to-end single-channel speech enhancement,” IEEE Signal Processing Letters, vol. 28, pp. 1455–1459, 2021. [27] J. Shi et al., “Bridging speech and textual pre-trained models with unsupervised ASR,” arXiv preprint arXiv:2211.03025, 2022. [28] V. Panayotov et al., “Librispeech: An asr corpus based on public domain audio books,” in Proc.
Faculty Name: shinji watanabe
Paperid: fa75ef55e04e3b25b8af56435478c2fd17403ce8
Title: Exploration on HuBERT with Multiple Resolutions
Year: 2023
Abstract: Hidden-unit BERT (HuBERT) is a widely-used self-supervised learning (SSL) model in speech processing. However, we argue that its fixed 20ms resolution for hidden representations would not be optimal for various speech-processing tasks since their attributes (e.g., speaker characteristics and semantics) are based on different time scales. To address this limitation, we propose utilizing HuBERT representations at multiple resolutions for downstream tasks. We explore two approaches, namely the parallel and hierarchical approaches, for integrating HuBERT features with different resolutions. Through experiments, we demonstrate that HuBERT with multiple resolutions outperforms the original model. This highlights the potential of utilizing multiple resolutions in SSL models like HuBERT to capture diverse information from speech signals.
Authors: Jiatong Shi, Yun Tang, H. Inaguma, Hongyu Gong, J. Pino, Shinji Watanabe
Venue: Interspeech
Tldr: {'model': 'tldr@v2.0.0', 'text': 'Through experiments, it is demonstrated that HuBERT with multiple resolutions outperforms the original model, highlighting the potential of utilizing multiple resolutions in SSL models like HuberT to capture diverse information from speech signals.'}
Url: http://arxiv.org/pdf/2306.01084
Answer: "
"How many Chemical Engineering courses are going to be held in Summer 2024?
","['metadata_course_summer_one_all_24.txt', 'metadata_course_summer_one_all_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many Chemical Engineering courses are going to be held in Summer 2024?

Context: In Semester Summer One(All) 2024, from the department of Chemical Engineering, the subject titled 'Independent Study' with Course ID 06405 and Section A offers VAR units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Laird located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Chemical Engineering, the subject titled 'Masters Chemical Engineering Project' with Course ID 06600 and Section R offers 1-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Laird located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Chemical Engineering, the subject titled 'Graduate Reading and Research' with Course ID 06803 and Section R offers 51836 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Laird located in Building TBA, Room None.
In Semester Summer One(All) 2024, from the department of Chemistry, the subject titled 'Study Abroad' with Course ID 09050 and Section S offers 0.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Stump located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Chemistry, the subject titled 'Modern Chemistry II' with Course ID 09106 and Section S offers 10.0 units. The Class meets Monday Tuesday Wednesday Thursday Friday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Sherwood located in Building DH, Room 2302.
In Semester Summer One(All) 2024, from the department of Chemistry, the subject titled 'Modern Chemistry II' with Course ID 09106 and Section NA offers 10.0 units. The Class meets Tuesday between 02:00PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor NA located in Building DH, Room 2302.
In Semester Summer One(All) 2024, from the department of Chemistry, the subject titled 'Doctoral Dissertation' with Course ID 09871 and Section S offers 5.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Noonan located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Chemistry, the subject titled 'Internship:' with Course ID 09990 and Section NA offers 0-99 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Summer One(All) 2024, from the department of Chemistry, the subject titled 'TBA' with Course ID 09990 and Section S offers 0-99 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Noonan located in Building TBA, Room None.
In Semester Summer One(All) 2024, from the department of Civil & Environmental Engineering, the subject titled 'Graduate Project' with Course ID 12791 and Section R offers 0-99 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Civil & Environmental Engineering, the subject titled 'Advanced Independent Study' with Course ID 12792 and Section R offers 0-99 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Akinci located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Civil & Environmental Engineering, the subject titled 'Ph.D. Thesis' with Course ID 12799 and Section A offers 0-99 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building DNM, Room DNM.
Answer: "
"When was the deadline for the MLT program applications?
","['mlt-student-handbook-2023-2024.txt', 'mlt-student-handbook-2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When was the deadline for the MLT program applications?

Context: Students must begin their study at CMU in the program that admitted them; this is a university 
policy. 
Students may request to transfer into the MLT program after completing their first semester. 
The student must make the request in writing (or email) to the MLT/PhD Admissions Chair. The 
MLT/PhD Admissions Chair will inform the student about what application materials are 
required; for example, an explanation of why a transfer is desired, a proposed plan of study, a 
proposed advisor, and CMU transcripts. Students that are already enrolled in an LTI degree 
program are not required to retake GRE and TOEFL exams or to produce new transcripts from 
other universities. The MLT program will conduct an expedited admissions process after 
receiving such a request.   
5.1.8 Transferring Out of the MLT Program 
The MLT program does not prevent students from transferring to another degree program. 
Each degree program has its own rules about whether and when transfers into the program are 
permitted. A student that is interested in transferring out of the MLT degree program should 
consult the handbook and Program Director of the desired degree program to learn whether 
transfers are permitted, and if so, how and when to request such a transfer. 
5.1.9 Drop/Add/Withdraw Procedures 
Students taking undergraduate and Masters level courses must follow the procedures and 
deadlines for adding, dropping, and withdrawing from courses as identified on the academic 
calendar. Information can be found at https://www.cmu.edu/hub/registrar/course-
changes.index. Please note that there is a separate calendar for doctoral courses that does not 
apply to Masters students. 
5.1.10  Statute of Limitations 
As outlined in the Masters Students Statute of Limitations, 
https://www.cmu.edu/policies/student-and-student-life/masters-students-statute-of- 
limitations.html. Students will complete all requirements for the masters degree within a 
maximum of seven years from original matriculation as a masters student, or less if required by 
a more restrictive department, school, or college policy. Once this time-to-degree limit has 
lapsed, the person may resume work towards a masters degree only if newly admitted to a 
currently offered masters degree program under criteria determined by that program. 
See also the Duration of Study policy.
It is to the student's advantage to avoid switching advisors, especially late in 
their graduate studies, because forging a strong student-advisor relationship takes time. 
4.3 Optional Masters Thesis 
MLT students may optionally elect to complete a Masters Thesis; it is not required. Students 
who intend to apply for the PhD program should consider the MLT Thesis as one of several 
ways to demonstrate their research potential. Other possibilities include innovative research in 
a research project or class project work. 
The MLT Thesis must be completed according to the following guidelines. Note in particular the 
October 31 proposal deadline! 
 There are two course components required for students who elect to complete the M.S. 
Thesis: 11-928  Masters Thesis I (with the chosen thesis advisor, typically for 12 units, 
typically in the Fall of their second year) and 11-929  Masters Thesis II in the following 
Spring. 
 Each of these two courses counts towards 6 units of MLT-level LTI course credit; if the 
student goes on to an LTI PhD, the second course (11-929) counts towards one PhD lab 
requirement.   
 The MLT thesis may focus on either a significant research result or a significant 
implementation of a new technology. The student should work with their advisor to 
define an interesting but bounded thesis research project. The MLT thesis culminates in 
MLT Graduate Student Handbook 
Page 16 
 
a publication-quality paper that clearly describes the work in terms of its research 
significance. 
 The MLT thesis committee will be composed of a thesis advisor and two additional 
readers, to be chosen by the student in consultation with the advisor, during the Fall 
Masters Thesis I course. At least 2 of the 3 committee members must be SCS faculty 
working in Language Technologies. 
 To receive a final grade for the first semester's course, the student must have prepared 
a written proposal of at least five pages, describing the scientific thesis to be 
investigated, any relevant research already completed by the student, a comparison 
with related work by others, and a detailed description of the work to be done in the 
next semester. The proposal MUST be accepted by the committee by the end of 
October, or the student will not be allowed to enroll in 11-929  Masters Thesis II!
Answer: "
"In spring 2025, Is there class on Martin Luther King Day?
","['Meta-Data-Calendar-2324.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2025, Is there class on Martin Luther King Day?

Context: Day: 'Monday', Event: 'Semester & Mini-2 Faculty Course Evaluations close', Semester: 'Fall 2023 (F23)'
Date: '2023-12-20', Day: 'Wednesday', Event: 'Final Grades Due by 4 pm ', Semester: 'Fall 2023 (F23)'
Start Date: '2023-12-23', End Date: '2024-01-02', Days: 'Saturday to Tuesday', Event: 'Winter Break; University Closed', Semester: 'Fall 2023 (F23)'
Date: '2023-01-08', Day: 'Monday', Event: 'Fall Deans' Lists Posted', Semester: 'Fall 2023 (F23)'
Date: '2024-01-15', Day: 'Monday', Event: 'Martin Luther King Day; No Classes', Semester: 'Spring 2024 (S24)'
Date: '2024-01-16', Day: 'Tuesday', Event: 'First Day of Class', Semester: 'Spring 2024 (S24)'
Date: '2024-01-22', Day: 'Monday', Event: 'Mini-3 add, audit & tuition adjustment drop deadline (1)', Semester: 'Spring 2024 (S24)'
Date: '2024-01-29', Day: 'Monday', Event: 'Semester add, audit & tuition adjustment drop deadline (1)', Semester: 'Spring 2024 (S24)'
Date: '2024-02-07', Day: 'Wednesday', Event: 'Mini-3 course drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Spring 2024 (S24)'
Date: '2024-02-19', Day: 'Monday', Event: 'Mini-3 pass/no pass & withdrawal deadline (3)', Semester: 'Spring 2024 (S24)'
Date: '2024-02-19', Day: 'Monday', Event: 'Mini-3 Faculty Course Evaluations open', Semester: 'Spring 2024 (S24)'
Date: '2024-02-26', Day: 'Monday', Event: 'Semester course drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Spring 2024 (S24)'
Date: '2024-03-01', Day: 'Friday', Event: 'Mini-3 Last Day of Classes ',
On Friday, 08 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Last Day of Classes is observed.
On Friday, 08 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 voucher deadline (4) is observed.
From 11 December, 2023,Monday to 12 December, 2023,Tuesday marks the Final Exams for Fall 2023 (F23) semester.
On Wednesday, 13 December, 2023, during the Fall 2023 (F23) semester, Reading Day is observed.
From 14 December, 2023,Thursday to 15 December, 2023,Friday marks the Final Exams for Fall 2023 (F23) semester.
On Saturday, 16 December, 2023, during the Fall 2023 (F23) semester, Reading Day is observed.
On Sunday, 17 December, 2023, during the Fall 2023 (F23) semester, Final Exams is observed.
On Monday, 18 December, 2023, during the Fall 2023 (F23) semester, Make-Up Final Exams is observed.
On Monday, 18 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Faculty Course Evaluations close is observed.
On Wednesday, 20 December, 2023, during the Fall 2023 (F23) semester, Final Grades Due by 4 pm is observed.
From 23 December, 2023,Saturday to 02 January, 2024,Tuesday marks the Winter Break for Fall 2023 (F23) semester which leads to University Closed.
On Monday, 08 January, 2023, during the Fall 2023 (F23) semester, Fall Deans' Lists Posted is observed.
On Monday, 15 January, 2024, during the Spring 2024 (S24) semester, Martin Luther King Day is observed, leading to No Classes.
On Tuesday, 16 January, 2024, during the Spring 2024 (S24) semester, First Day of Class is observed.
On Monday, 22 January, 2024, during the Spring 2024 (S24) semester, Mini-3 add, audit & tuition adjustment drop deadline (1) is observed.
Answer: "
"Which LTI prof co-authored the paper titled ""Self-Refine: Iterative Refinement with Self-Feedback""?
","['sean welleck_papers.txt', 'bhiksha raj_0a8d38686b18f28aae1222529e6b9e8a60cab1c2_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which LTI prof co-authored the paper titled ""Self-Refine: Iterative Refinement with Self-Feedback""?

Context: List of 2023 Open Access papers by sean welleck are:
Self-Refine: Iterative Refinement with Self-Feedback
Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning
Faculty Name: bhiksha raj
Paperid: 0a8d38686b18f28aae1222529e6b9e8a60cab1c2
Title: UTOPIA: Unconstrained Tracking Objects without Preliminary Examination via Cross-Domain Adaptation
Year: 2023
Abstract: Multiple Object Tracking (MOT) aims to find bounding boxes and identities of targeted objects in consecutive video frames. While fully-supervised MOT methods have achieved high accuracy on existing datasets, they cannot generalize well on a newly obtained dataset or a new unseen domain. In this work, we first address the MOT problem from the cross-domain point of view, imitating the process of new data acquisition in practice. Then, a new cross-domain MOT adaptation from existing datasets is proposed without any pre-defined human knowledge in understanding and modeling objects. It can also learn and update itself from the target data feedback. The intensive experiments are designed on four challenging settings, including MOTSynth to MOT17, MOT17 to MOT20, MOT17 to VisDrone, and MOT17 to DanceTrack. We then prove the adaptability of the proposed self-supervised learning strategy. The experiments also show superior performance on tracking metrics MOTA and IDF1, compared to fully supervised, unsupervised, and self-supervised state-of-the-art methods.
Authors: Pha Nguyen, Kha Gia Quach, J. Gauch, S. Khan, B. Raj, Khoa Luu
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A new cross-domain MOT adaptation from existing datasets is proposed without any pre-defined human knowledge in understanding and modeling objects, and it can also learn and update itself from the target data feedback.'}
Url: http://arxiv.org/pdf/2306.09613
Answer: "
"How many candidate documents were re-ranked using InPars-light compared to InPars?
","['eric nyberg_3a30217c4115777fb30c182c97cc77d34d065556_metadata.txt', 'eric nyberg_3a30217c4115777fb30c182c97cc77d34d065556_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many candidate documents were re-ranked using InPars-light compared to InPars?

Context: Faculty Name: eric nyberg
Paperid: 3a30217c4115777fb30c182c97cc77d34d065556
Title: InPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers
Year: 2023
Abstract: We carried out a reproducibility study of InPars, which is a method for unsupervised training of neural rankers (Bonifacio et al., 2022). As a by-product, we developed InPars-light, which is a simple-yet-effective modification of InPars. Unlike InPars, InPars-light uses 7x-100x smaller ranking models and only a freely available language model BLOOM, which -- as we found out -- produced more accurate rankers compared to a proprietary GPT-3 model. On all five English retrieval collections (used in the original InPars study) we obtained substantial (7%-30%) and statistically significant improvements over BM25 (in nDCG and MRR) using only a 30M parameter six-layer MiniLM-30M ranker and a single three-shot prompt. In contrast, in the InPars study only a 100x larger monoT5-3B model consistently outperformed BM25, whereas their smaller monoT5-220M model (which is still 7x larger than our MiniLM ranker) outperformed BM25 only on MS MARCO and TREC DL 2020. In the same three-shot prompting scenario, our 435M parameter DeBERTA v3 ranker was at par with the 7x larger monoT5-3B (average gain over BM25 of 1.3 vs 1.32): In fact, on three out of five datasets, DeBERTA slightly outperformed monoT5-3B. Finally, these good results were achieved by re-ranking only 100 candidate documents compared to 1000 used by Bonifacio et al. (2022). We believe that InPars-light is the first truly cost-effective prompt-based unsupervised recipe to train and deploy neural ranking models that outperform BM25. Our code and data is publicly available.
Title: InPars-Light: Cost-Effective Unsupervised Training of Effi- cient Rankers
Authors: Leonid Boytsov, Preksha Patel, Vivek Sourabh, Riddhi Nisar, Sayani Kundu, Ramya Ramanathan, Eric Nyberg
Section: A Appendix
compared to InPars. In contrast, it takes only about 15 hours to generate 100K queries using RTX 3090 GPU. Extrapolating this estimate to A100, which is about 2x faster than RTX 309015, and using the pricing of Lambda GPU cloud, we estimate the cost of generation in our InPars-light study to be under $10 per collection. 16 Efficiency of Re-ranking. A rather common opinion (in particular expressed by anonymous reviewers on multiple occasions) is that using cross-encoders is not a practical option. This might be true for extremely constrained latency environments or very large models, but we think it is totally practical to use small models such as MiniLM-L6-30M for applications such as enterprise search. In particular, on a reasonably modern GPU (such as RTX 3090) and MinLm-L6-30M re-ranking throughput exceeds 500 passages per second (assuming truncation to the first 477 characters). Thus re-ranking 100 documents has an acceptable sub-second latency. In fact, Cohere AI provides re-ranking with neural models as a cloud service.17 Cost of Model Training. Here, all training times are given with respect to a single RTX 3090 GPU. Training and evaluating MiniLM6-30M models had negligible costs dominated by all-domain pretraining, which took about two hours per seed. In contrast, the all-domain pretraining of DeBERTA-v3-435M took 28 hours. However, without all-domain pretraining, the training time itself was rather small, in particular, because we used only a fraction of all generated queries (10K queries in the original InPars training and about 20K queries in the follow-up fine-tuning using consistency checked data).
Answer: "
"What is the room number for the Advanced Natural Language Processing course?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the room number for the Advanced Natural Language Processing course?

Context: Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Introduction to Question Answering' with Course ID 11697 and Section A offers 12.0 units. The Class meets Monday Wednesday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Nyberg, Mitamura located in Building PH, Room A18A.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'LTI Colloquium' with Course ID 11700 and Section A offers 6.0 units. The Class meets Friday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Bisk, Diaz located in Building POS, Room A35.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'LTI Colloquium' with Course ID 11700 and Section PP offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Lisbon, Portugal location,led by experienced instructor Diaz, Bisk located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Advanced Natural Language Processing' with Course ID 11711 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Frederking, Fried located in Building TEP, Room 1403.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Graduate Seminar on Dialog Processing' with Course ID 11716 and Section A offers 6.0 units. The Class meets Tuesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rudnicky located in Building PH, Room A21A.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Linguistics Lab' with Course ID 11723 and Section A offers 6.0 units.
Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Introduction to Question Answering' with Course ID 11697 and Section A offers 12.0 units. The Class meets Monday Wednesday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Nyberg, Mitamura located in Building PH, Room A18A.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'LTI Colloquium' with Course ID 11700 and Section A offers 6.0 units. The Class meets Friday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Bisk, Diaz located in Building POS, Room A35.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'LTI Colloquium' with Course ID 11700 and Section PP offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Lisbon, Portugal location,led by experienced instructor Diaz, Bisk located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Advanced Natural Language Processing' with Course ID 11711 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Frederking, Fried located in Building TEP, Room 1403.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Graduate Seminar on Dialog Processing' with Course ID 11716 and Section A offers 6.0 units. The Class meets Tuesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rudnicky located in Building PH, Room A21A.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Linguistics Lab' with Course ID 11723 and Section A offers 6.0 units.
Answer: "
"How much increase in throughput (multi GPU setup) does SAMA showcase in large-scale meta learning benchmarks?
","['emma strubell_a815c3209e7baff4466dbf6e129129511f842b7e_metadata.txt', 'eric xing_a815c3209e7baff4466dbf6e129129511f842b7e_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How much increase in throughput (multi GPU setup) does SAMA showcase in large-scale meta learning benchmarks?

Context: Faculty Name: emma strubell
Paperid: a815c3209e7baff4466dbf6e129129511f842b7e
Title: Making Scalable Meta Learning Practical
Year: 2023
Abstract: Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and 2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains.
Authors: Sang Keun Choe, Sanket Vaibhav Mehta, Hwijeen Ahn, W. Neiswanger, Pengtao Xie, Emma Strubell, Eric P. Xing
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients.'}
Url: https://arxiv.org/pdf/2310.05674
Faculty Name: eric xing
Paperid: a815c3209e7baff4466dbf6e129129511f842b7e
Title: Making Scalable Meta Learning Practical
Year: 2023
Abstract: Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and 2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains.
Authors: Sang Keun Choe, Sanket Vaibhav Mehta, Hwijeen Ahn, W. Neiswanger, Pengtao Xie, Emma Strubell, Eric P. Xing
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients.'}
Url: https://arxiv.org/pdf/2310.05674
Answer: "
"What is the location of the courses that will be taught by Affara in Summer 2024?
","['combined_metadata_final.txt', 'combined_metadata_final.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the location of the courses that will be taught by Affara in Summer 2024?

Context: In Semester Fall 2023, from the department of Biological Sciences, the subject titled 'Undergraduate Research' with Course ID 03445 and Section YY offers 1-18 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Affara, Maksoud located in Building TBA, Room None.
In Semester Fall 2023, from the department of Biological Sciences, the subject titled 'Undergraduate Research' with Course ID 03445 and Section Z offers 1-18 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Affara located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Biological Sciences, the subject titled 'Advanced Developmental Biology and Human Health' with Course ID 03451 and Section A offers 9.0 units. The Class meets Tuesday between 02:00PM and 03:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ettensohn, Mccartney located in Building DH, Room 2122.
In Semester Fall 2023, from the department of Biological Sciences, the subject titled 'Computational Molecular Biology and Genomics' with Course ID 03511 and Section A offers 9.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Durand located in Building WEH, Room 5409.
In Semester Fall 2023, from the department of Biological Sciences, the subject titled 'Biotechnology or Biopharmaceutical Engineering Internship' with Course ID 03600 and Section A offers 3.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Brasier located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Biological Sciences, the subject titled 'Computational Biology Internship' with Course ID 03601 and Section A offers 3.0 units. The Class meets Schedule will be added between NA and NA ET.
In Semester Spring 2024, from the department of Biological Sciences, the subject titled 'Systems Neuroscience' with Course ID 03363 and Section W offers 9.0 units. The Class meets Monday Wednesday between 02:30PM and 03:45PM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Affara located in Building CMB, Room 3048.
In Semester Spring 2024, from the department of Biological Sciences, the subject titled 'Neural Correlates of Learning and Memory' with Course ID 03365 and Section A offers 9.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor McGuier located in Building DH, Room 1211.
In Semester Spring 2024, from the department of Biological Sciences, the subject titled 'Neuropharmacology: Drugs, Brain and Behavior' with Course ID 03366 and Section A offers 9.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hong located in Building WEH, Room 5415.
In Semester Spring 2024, from the department of Biological Sciences, the subject titled 'Molecular and Cellular Immunology' with Course ID 03390 and Section A offers 9.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Giannoukakis located in Building GHC, Room 4101.
In Semester Spring 2024, from the department of Biological Sciences, the subject titled 'Microbiology' with Course ID 03391 and Section A offers 9.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hiller located in Building POS, Room 151.
In Semester Spring 2024, from the department of Biological Sciences, the subject titled 'Topics in Research' with Course ID 03412 and Section A offers 1-2 units. The Class meets Wednesday between 12:30PM and 01:50PM ET.
Answer: "
"When should the guests be seated for the start of the student procession on May 12?
","['Commencement.txt', 'Commencement.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When should the guests be seated for the start of the student procession on May 12?

Context: Main Commencement Ceremony
Bachelor’s, master’s and doctoral degree candidates and their guests are invited to join the main commencement ceremony on Sunday, May 12, for the conferral of all degrees.
The main ceremony will include remarks from the president, keynote speaker, student speaker and academic deans, in addition to recognition of the honorary degree recipients.
There is no limit on number of guests who can attend the main commencement ceremony and tickets are not needed.
The ceremony will take place on CMU’s campus beginning at 10 a.m. and will be approximately 1.5 hours long. All guests must be seated by 9:15 a.m. for the start of the student procession. Access to guest seating will be restricted once the student procession begins.

Diploma Ceremonies
In addition to the main commencement ceremony, each college/school/department will host a diploma ceremony to recognize all graduating students. Diploma ceremonies will be held over the course of the weekend (Friday, May 10–Sunday, May 12).
Diploma ceremonies will include the presentation of diplomas to graduates, hooding of doctoral candidates and remarks from their college/school/department leadership. Each ceremony is organized and customized by their college/school/department.
Diploma ceremonies will take place both on and off campus. Locations and times for diploma ceremonies will be provided soon.
Diploma ceremonies typically include a reception and the length of time for each ceremony varies based on the number of graduates.
More details on the weekend schedule, including a diploma ceremony schedule, will be provided in the coming weeks. 

Thursday, May 9
Phi Beta Kappa Initiation Ceremony
Ceremony: 2–3 p.m.
McConomy Auditorium, Cohon University Center
Reception: 3–4 p.m.
Connan Room, Cohon University Center

Contact:
Joseph Devine
jd0x@andrew.cmu.edu 

Joanne Ursenbach
joanneu@andrew.cmu.edu 

President's Graduates Toast (bachelor's students)
3:30–4:30 p.m.
Location TBD
Registration required. Invitation, along with registration details, will be sent in late April.

First Gen Graduation Recognition
Reception: 5-5:30 p.m.
Alumni Concert Hall, College of Fine Arts
Ceremony: 5:30-6:30p.m.
Invitation, along with registration details, will be sent in late April.

First Gen Graduation Recognition
Reception: 5-5:30 p.m.
Alumni Concert Hall, College of Fine Arts
Ceremony: 5:30-6:30p.m.
Kresge Theater, College of Fine Arts

Contact:
M. Shernell Smith
mssmith@andrew.cmu.edu 

Sam Colavecchio
scolavec@andrew.cmu.edu 
412-268-7733


Friday, May 10
Diploma Ceremonies
Various times

Senior Leadership Recognition Ceremony
4–5:30 p.m.
Wiegand Gym, Cohon University Center
Undergraduate students and their guests by invitation only. This ceremony recognizes nominated seniors who have reflected upon their specific leadership contributions during their time at CMU.


Saturday, May 11
Diploma Ceremonies
Various times

Center for Student Diversity and Inclusion Ceremony
Reception: 12:30 p.m.
Program: 1-2:15 p.m.
Simmons Auditorium, Tepper Building

Contact:
M. Shernell Smith
mssmith@andrew.cmu.edu 

Rowshan Lang
rowshan@andrew.cmu.edu
Naval ROTC Commissioning
Ceremony: 1:30-2:30 p.m.
Auditorium, Soldiers & Sailors Memorial Hall & Museum *
4141 Fifth Avenue, Pittsburgh, PA 15213
Contact:
Mike Danko
mdanko@andrew.cmu.edu 

The President’s Reception in honor of CMU’s Doctoral Candidates
4–6 p.m.
Tepper Building Atrium


Sunday, May 12
Gesling Stadium opens to guests
8 a.m.

Robing and procession for graduates
9–10 a.m.
Various locations across campus

Student procession begins
9:15 a.m.
All guests in stadium must be seated. Access to guest seating will be restricted once the procession begins.

Commencement Ceremony
10–11:30 a.m.
Gesling Stadium, CMU’s campus

Diploma Ceremonies
Various times
Answer: "
"Which year did The Kiltie Band began?
","['kiltieband_d406.txt', 'Apr-11_Eventno_21_KiltieBandAlumniStudentReception.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which year did The Kiltie Band began?

Context: The Kiltie Band
The Kiltie Band began in 1908 with a group of just seven students dedicated to supporting Carnegie Tech football, and today's Kiltie Band continues a tradition of excellence originated over a century ago. The Band took the field for its first official performance on November 25th, 1922, on what would have been Andrew Carnegie's 87th birthday.
The Kiltie Marching Band consists of musicians and colorguard and plays at all Carnegie Mellon home football games. There is a new show every week with music ranging from traditional marches and oldies to current pop tunes and jazz standards.
After football season, the Kiltie Concert Band plays a holiday concert and two spring concerts, including a performance at Carnegie Mellon’s Spring Carnival. Additionally, a part of the band gets together and acts as a pep band for the basketball teams.
The Kiltie Band is a campus organization recognized by Student Senate and administered by the Department of Athletics, Physical Education and Recreation. Membership is open to all members of the campus/community without audition. Participants must do their best to promote the success of the ensemble through attendance, attitude, and dependability.
FAQs
Q: When are rehearsals?
A: Football season: Mondays and Thursdays from 5:30 p.m.-7:30 p.m. and Game Day 9:30-11:00a.m. Concert Season: Mondays and Thursdays from 5:30-6:30 p.m.
Location is the CUC Studio Theater.
Q: When are performances?
A: The Kiltie Band performs at all home football games, a holiday concert, and two spring concerts. Times and dates for all performances are announced at the first practice.
Q: Are there auditions?
A: No, any member of the campus community with music experience is able to join the Kiltie Band!
Q: Do I have to memorize music?
A: No, the music is changed for every show. You should invest in a good and trusty lyre.
Q: When is the first rehearsal?
A: On the first day of class (Monday) at 5:30 p.m. in the CUC Studio Theater. If you have questions, please email the Direcator at
jolisar@andrew.cmu.edu
.
Q: Can I borrow a school instrument?
A: There are several Kiltie-owned instruments available for your use.
Event: Kiltie Band Alumni & Student Reception
Date: 4/11/24
Time: 4:30 PM-6:30 PM ET
Participants/Audience: Open to alumni and families of the Kiltie community 
Event Details: 
After the Kiltie Band Spring Carnival kick-off concert, current and former Kilties are invited for food and fun!

Note: Registration required. Walk-ins are welcome if space permits. No event fee. Open to the Kiltie community and guests.
Answer: "
"In what year did Carnegie Tech became Carnegie Mellon University?
","['cmuhistory_d402.txt', 'cmuhistory_d402.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In what year did Carnegie Tech became Carnegie Mellon University?

Context: The first U.S. drama degree was awarded in 1914 at Carnegie Tech. It began offering graduate degrees. In 1919, the first doctorate (in civil engineering) was awarded to Mao Yisheng, a student from China. It laid the groundwork for a research institution, recruiting leading scientists, offering sponsored fellowships with government and industry leaders and pioneering nontraditional interdisciplinary research, which brought together physicists, chemists and metallurgists, for example. Interdisciplinary research would become the hallmark of Carnegie Mellon research. It initiated the 'Carnegie Plan' in 1938, a new curriculum that required science and engineer students to take courses in humanities and social sciences in order to better understand the needs of society. Carnegie died in 1919, but his vision for an educated public lived on after him. Carnegie Tech - Post-war Years With the end of World War II, the latter half of the 20th century brought unprecedented growth to Carnegie Tech. In 1956, the arrival of the first IBM computer to campus was revolutionary, initiating a university culture where information technology pervaded virtually all areas of study. University culture also changed in 1973 when Margaret Morrison closed and women joined their male peers in classrooms and dorms. The times were changing, and Tech positioned itself at the forefront, opening three new schools: 1948: The Graduate School of Industrial Administration, later renamed the David A. Tepper School of Business, focusing on quantitative analysis and pioneering the field of management science. 1968: School of Urban and Public Affairs, later renamed the H. John Heinz III College, providing graduate training for work in the public sector. 1986: School of Computer Science, pioneering computing and artificial intelligence, led by interdisciplinary efforts of Allen Newell and Herbert Simon. Carnegie Mellon University In 1967, Carnegie Tech merged with the Mellon Institute, a science research center founded by the Mellon family of Pittsburgh, to become known as Carnegie Mellon University. The merger built upon a long history of support from the Mellons. It allowed Carnegie Mellon to establish the last of its current pillars: the Mellon College of Science and the College of Humanities and Social Sciences, now known as Marianna Brown Dietrich College of Humanities and Social Sciences.
History - CMU - Carnegie Mellon University Carnegie Mellon University ——— Search Search CMU › About › History Andrew Carnegie A self-educated ""working boy"" who loved books, Andrew Carnegie emigrated from Scotland in 1848 and settled in Pittsburgh, Pa. Attending night school and borrowing books, Carnegie went from factory worker in a textile mill to successful entrepreneur and industrialist. He rose to prominence by founding what became the world's largest steel producing company by the end of the 19th century. Carnegie Technical Schools At one point the richest man in the world, Carnegie believed that ""to die rich is to die disgraced."" He turned his attention to writing, social activism and philanthropy, determined to establish educational opportunities for the general public where few existed. In 1900, he donated $1 million for the creation of a technical institute for the city of Pittsburgh, envisioning a school where working-class men and women of Pittsburgh could learn practical skills, trades and crafts that would enhance their careers, lives and communities. ""My heart is in the work,"" he stated, which would become part of the school's official motto. The Carnegie Technical Schools offered two- and three-year certificates in the arts as well as in engineering disciplines and included a college for women, Margaret Morrison Carnegie College. Carnegie Tech – Early Years Soon faced with the demand for baccalaureate programs, Carnegie Technical Schools began offering bachelor's degrees through its College of Engineering and College of Fine Arts, becoming the Carnegie Institute of Technology, or ""Carnegie Tech."" During the first half of the 20th century, with support from Andrew Carnegie and other funders, Carnegie Tech laid the foundation for a school on the cutting edge. Some key developments were: It expanded from two buildings into an elegant 20th century campus designed in the beaux arts architectural style, housing a wealth of machine shops, studios and laboratories — the hands-on center of learning that persists today. It pioneered conservatory degree programs in music and drama, in addition to visual art and design programs. The first U.S. drama degree was awarded in 1914 at Carnegie Tech. It began offering graduate degrees. In 1919, the first doctorate (in civil engineering) was awarded to Mao Yisheng, a student from China.
Answer: "
"What are the four common domains of websites in the WebArena environment?
","['graham neubig_e41482f4ee984f17382f6cdd900df094d928be06_metadata.txt', 'daniel fried_e41482f4ee984f17382f6cdd900df094d928be06_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the four common domains of websites in the WebArena environment?

Context: Faculty Name: graham neubig
Paperid: e41482f4ee984f17382f6cdd900df094d928be06
Title: WebArena: A Realistic Web Environment for Building Autonomous Agents
Year: 2023
Abstract: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.
Authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper builds an environment for language-guided agents that is highly realistic and reproducible, and creates an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.'}
Url: https://arxiv.org/pdf/2307.13854
Faculty Name: daniel fried
Paperid: e41482f4ee984f17382f6cdd900df094d928be06
Title: WebArena: A Realistic Web Environment for Building Autonomous Agents
Year: 2023
Abstract: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.
Authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper builds an environment for language-guided agents that is highly realistic and reproducible, and creates an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.'}
Url: https://arxiv.org/pdf/2307.13854
Answer: "
"Which LLMs were used for validation of the SPAE method?
","['alexander hauptmann_376f494126d1ea4f571ea0263c43ac2b6331800a_content_0.txt', 'yonatan bisk_376f494126d1ea4f571ea0263c43ac2b6331800a_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which LLMs were used for validation of the SPAE method?

Context: Title: SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs
Authors: Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A. Ross, Irfan Essa, Yonatan Bisk, Ming-Hsuan Yang, Kevin Murphy, Alexander G. Hauptmann, Lu Jiang
Section: D Additional Qualitative Examples
Token pyramid visualization. Fig. 13 shows tokenization and reconstruction samples by a 6-layer SPAE from ImageNet validation set. Key concepts are captured in the first few layers, whereas the later layers focus on the visual appearance. In the coffee machine example, many keywords are present to describe various aspects from the stove to the thermometer. In the parrot case, a single unified concept is repeatedly highlighted. Coarse-to-fine reconstruction. Fig. 14 shows reconstruction samples by SPAE-8 from ImageNet validation set. We compare the reconstructed images from layer 5 to layer 8 to demonstrate the coarse-to-fine progress. Conditional image interpolation. To the best of our knowledge, there have been no successful attempts that demonstrate generic image generation capability using a frozen LLM. To this end, we define a very simple setup to explore the interpolation capability of LLM, where the conditions are integers from 1 to 9. The target images are created with different pixel-space transformations detailed in . As shown in Fig. 15, images 1-4 and 6-9 are fed as context to produce image 5, where the model interpolates the variable property. Fig. 16 shows generated samples at 256×256 resolution under the same setup. Conditional image denoising. We use PAR decoding to produce the first 5 token layers with taskspecific conditions, followed by task-agnostic PNAR decoding to fill in layer 6. Fig. 17 visualizes the input pairs for the image-to-image generation examples in Figs. 7 and 9, with more examples in Fig. 18. Under the in-context denoising setup, the LLM generates novel images based on the provided context, where multiple different generations can be obtained. Multimodal outputs. Fig.
Title: SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs
Authors: Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A. Ross, Irfan Essa, Yonatan Bisk, Ming-Hsuan Yang, Kevin Murphy, Alexander G. Hauptmann, Lu Jiang
Section: D Additional Qualitative Examples
Token pyramid visualization. Fig. 13 shows tokenization and reconstruction samples by a 6-layer SPAE from ImageNet validation set. Key concepts are captured in the first few layers, whereas the later layers focus on the visual appearance. In the coffee machine example, many keywords are present to describe various aspects from the stove to the thermometer. In the parrot case, a single unified concept is repeatedly highlighted. Coarse-to-fine reconstruction. Fig. 14 shows reconstruction samples by SPAE-8 from ImageNet validation set. We compare the reconstructed images from layer 5 to layer 8 to demonstrate the coarse-to-fine progress. Conditional image interpolation. To the best of our knowledge, there have been no successful attempts that demonstrate generic image generation capability using a frozen LLM. To this end, we define a very simple setup to explore the interpolation capability of LLM, where the conditions are integers from 1 to 9. The target images are created with different pixel-space transformations detailed in . As shown in Fig. 15, images 1-4 and 6-9 are fed as context to produce image 5, where the model interpolates the variable property. Fig. 16 shows generated samples at 256×256 resolution under the same setup. Conditional image denoising. We use PAR decoding to produce the first 5 token layers with taskspecific conditions, followed by task-agnostic PNAR decoding to fill in layer 6. Fig. 17 visualizes the input pairs for the image-to-image generation examples in Figs. 7 and 9, with more examples in Fig. 18. Under the in-context denoising setup, the LLM generates novel images based on the provided context, where multiple different generations can be obtained. Multimodal outputs. Fig.
Answer: "
"What do the KALE vocabulary semantic concepts perform better than?
","['jamie callan_1e0a4ff0c5d2d850ff5907e310ffcedc9cad9718_metadata.txt', 'jamie callan_1e0a4ff0c5d2d850ff5907e310ffcedc9cad9718_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What do the KALE vocabulary semantic concepts perform better than?

Context: Faculty Name: jamie callan
Paperid: 1e0a4ff0c5d2d850ff5907e310ffcedc9cad9718
Title: KALE: Using a K-Sparse Projector for Lexical Expansion
Year: 2023
Abstract: Recent research has proposed retrieval approaches based on sparse representations and inverted indexes, with terms produced by neural language models and leveraging the advantages from both neural retrieval and lexical matching. This paper proposes KALE, a new lightweight method of this family that uses a small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary. The KALE vocabulary captures semantic concepts than perform well when used in isolation, and perform better when extending the original lexical vocabulary, this way improving first-stage retrieval accuracy. Experiments with the MSMARCOv1 passage retrieval dataset, the TREC Deep Learning dataset, and BEIR datasets, examined the effectiveness of KALE under varying conditions. Results show that the KALE terms can replace the original lexical vocabulary, with gains in accuracy and efficiency. Combining KALE with the original lexical vocabulary, or with other learned terms, can further improve retrieval accuracy with only a modest increase in computational cost.
Authors: Luís Borges, Bruno Martins, Jamie Callan
Venue: International Conference on the Theory of Information Retrieval
Tldr: {'model': 'tldr@v2.0.0', 'text': 'KALE is a new lightweight method that uses a small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary, which can replace the original lexical vocabulary with gains in accuracy and efficiency.'}
Url: https://dl.acm.org/doi/pdf/10.1145/3578337.3605131
Title: KALE: Using a K-Sparse Projector for Lexical Expansion
Authors: Luís Borges, Bruno Martins, Jamie Callan
Section: 5.4 Assessing Posting List Size Distribution
of the DFs, which aligns with the previous expectation that regularization helped balance the DFs, and consequently, decrease search latency. The previous experiments showed that the equipartitioning component was useful in balancing the posting list sizes of the generated KALE vocabulary, and therefore improve search efficiency. Not considering the regularization component led not only to worse MRR@10, but also to an increase in query latency.
Answer: "
"In fall 2024, When is the Semester drop deadline and withdrawal grade assigned after this date?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2024, When is the Semester drop deadline and withdrawal grade assigned after this date?

Context: On Monday, 02 September, 2024, during the Fall 2024 (F24) semester, Labor Day is observed, leading to No Classes & University Closed.
On Monday, 09 September, 2024, during the Fall 2024 (F24) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 18 September, 2024, during the Fall 2024 (F24) semester, Mini-1 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 30 September, 2024, during the Fall 2024 (F24) semester, Mini-1 Pass/no pass & withdrawal deadline (3) is observed.
On Monday, 30 September, 2024, during the Fall 2024 (F24) semester, Mini-1 Faculty Course Evaluations open is observed.
On Monday, 07 October, 2024, during the Fall 2024 (F24) semester, Semester drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 11 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Last Day of Classes is observed.
On Friday, 11 October, 2024, during the Fall 2024 (F24) semester, Mini-1 voucher election deadline (4) is observed.
On Saturday, 12 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Exams is observed.
On Saturday, 12 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Faculty Course Evaluations close is observed.
From 14 October, 2024,Monday to 18 October, 2023,Friday marks the Fall Break for Fall 2024 (F24) semester which leads to No Classes.
On Monday, 21 October, 2024, during the Fall 2024 (F24) semester, Mini-2 Classes Begin is observed.
On Wednesday, 23 October, 2024, during the Fall 2024 (F24) semester, Mid-Semester & Mini-1 grades due by 4 pm is observed.
On Friday, 25 October, 2024, during the Fall 2024 (F24) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 08 July, 2024, during the Summer Two 2024 (N24) semester, Summer Semester  Two drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 22 July, 2024, during the Summer Two 2024 (N24) semester, Summer Semester  Two pass/no pass & withdrawal deadline (3) is observed.
On Monday, 29 July, 2024, during the Summer Two 2024 (N24) semester, Summer Semester  Two Faculty Course Evaulations open is observed.
On Thursday, 01 August, 2024, during the Summer Two 2024 (N24) semester, Summer Semester  Two Last Day of Classes is observed.
On Thursday, 01 August, 2024, during the Summer Two 2024 (N24) semester, Summer Semester  Two voucher deadline (4) is observed.
On Friday, 02 August, 2024, during the Summer Two 2024 (N24) semester, Summer Semester  Two Final Exams*** is observed.
On Friday, 02 August, 2024, during the Summer Two 2024 (N24) semester, Summer Semester  Two Faculty Course Evaluations close is observed.
On Tuesday, 06 August, 2024, during the Summer Two 2024 (N24) semester, Summer Semester  Two Final Grades Due by 4 pm is observed.
From 18 August, 2024,Saturday to 23 August, 2024,Friday marks the First-Year Orientation for Fall 2024 (F24) semester.
On Thursday, 22 August, 2024, during the Fall 2024 (F24) semester, Convocation is observed.
On Monday, 26 August, 2024, during the Fall 2024 (F24) semester, Semester & Mini-1 Classes Begin is observed.
On Friday, 30 August, 2024, during the Fall 2024 (F24) semester, Mini-1 add, audit & tuition adjustment drop deadline  (1) is observed.
On Monday, 02 September, 2024, during the Fall 2024 (F24) semester, Labor Day is observed, leading to No Classes & University Closed.
Answer: "
"What does SHAP mean in SHAP-based Prediction of Mother's History of Depression to Understand the Influence on Child Behavior?
","['louis philippe morency_6838c43e702a3f995967ba2e3edd5f65ff5f5511_content_0.txt', 'louis philippe morency_6838c43e702a3f995967ba2e3edd5f65ff5f5511_content_3.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What does SHAP mean in SHAP-based Prediction of Mother's History of Depression to Understand the Influence on Child Behavior?

Context: Title: SHAP-based Prediction of Mother’s History of Depression to Understand the Influence on Child Behavior
Authors: Maneesh Bilalpur, Laura Cariola, Lisa Sheeber, Saurabh Hinduja, Nicholas Allen, Louis-Philippe Morency, Jefrey F. Cohn
Section: ACKNOWLEDGMENTS
This research was supported in part by the U.S. National Institutes of Health through U.S. National Institute of Mental Health award MH096951.
Title: SHAP-based Prediction of Mother’s History of Depression to Understand the Influence on Child Behavior
Authors: Maneesh Bilalpur, Laura Cariola, Lisa Sheeber, Saurabh Hinduja, Nicholas Allen, Louis-Philippe Morency, Jefrey F. Cohn
Section: 5 DISCUSSION
classifer leaves room for exploring ensemble approaches such as random forest and XGBoost. Four, generalization of the proposed feature selection approach across diferent prediction frameworks remains as future work.
Answer: "
"When did amusing buggies like Delta Upsilon's ""Fish"" and Printing Management's Bathtub disappear?
","['cmubuggy_d403.txt', 'cmubuggy_d403.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When did amusing buggies like Delta Upsilon's ""Fish"" and Printing Management's Bathtub disappear?

Context: The last course records were set in 2017, but this year potholes are making for some challenging conditions, said Amy Chen, a senior in psychology who has been driving for four years. With the vehicles just inches from the ground, ""even poorly filled potholes makes it dangerous to drive,"" she added. Depending on Friday and Saturday's conditions, most heats may run just two lanes instead of three. But still, despite some of the challenges, Chen said she wouldn't miss it. ""Being a driver is really fun,"" Chen said. ""I love going fast and going around the course."" — #CMUcarnival — Powered by Curator.io — Related Content — Media Advisory: Carnegie Mellon Celebrates Spring Carnival Buggy Races Keep Rolling at Carnegie Mellon Enjoy a World of Fun at Spring Carnival Student Architects Design Carnival Archway Take a Ride on The Old Mill at CMU MoBot Turns 25 Witchner, Wood Roll Into Buggy Royalty The Piper: Campus & Community News Official Events Calendar Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA 15213 412-268-2900 Legal Info www.cmu.edu © 2021 Carnegie Mellon University CMU on Facebook CMU on Twitter CMU on LinkedIn CMU YouTube Channel CMU RSS Feed CMU on Instagram CMU Social Media Directory Stories College of Engineering College of Fine Arts Dietrich College of Humanities & Social Sciences Heinz College of Information Systems and Public Policy Mellon College of Science School of Computer Science Tepper School of Business Archives 2021 March February January 2020 December November October September August July June May April March February January 2019 December November October September August July June May April March February January 2018 December November October September August July June May April March February January 2017 January February March April May June July August September October November December 2016 January February March April May June July August September October November December 2015 January February March April May June July August September October November December 2014 January February March April May June July August September October November December 2013 January February March April May June July August September October November December 2012 January February March April May June July August September October November December 2011 January February March April May June July August September October November December Media Highlights Media Resources Experts (Alphabetical) Experts (by Topic) Contact Us The Piper: Campus & Community News
This year's buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It's one of the few times spectators can see buggies up close. Dave Singh explains how Fringe builds its buggies. Each team keeps its actual processes a secret, but each buggy has certain features such as a body, pushbar, wheels and driving and braking mechanisms. Behind Singh are other buggies built by Fringe. Behind the Wheel Drivers, have the closest connection to the vehicles, aside from the mechanics who spent countless hours building and maintaining individual buggies. While races are brief, each driver must log a number of practice runs to qualify to race. Teams practice nights and weekends throughout the fall and spring semesters as weather permits. Buggies are often built around drivers, so the fit can be snug, and drivers are often less than 5 feet, 3 inches in height. Most — but not all — are women. Tishya Girdhar, a junior in neuroscience, is social chair for Fringe. She came to CMU wanting to drive, but first she had to get over being claustrophobic. She started as a mechanic and got behind the wheel for last year's Sweepstakes. ""Our team's philosophy is to teach everyone how to do everything,"" Girdhar said. ""But I came to CMU wanting to drive. I wanted to drive so badly."" Amy Chen demonstrates how a driver is positioned inside of Boson, a Fringe buggy built in 2016. Safety is a top priority for teams. All drivers are required to have five pieces of safety gear: mouth guard, goggles, a harness that includes three points of contact to the body of the buggy, gloves and a helmet. And during practice runs and races, flaggers man the course to let drivers know if it's safe to proceed. The streets of the race course haven't changed, but the condition of the course varies due to potholes, which can make or break a fast run. The last course records were set in 2017, but this year potholes are making for some challenging conditions, said Amy Chen, a senior in psychology who has been driving for four years.
Answer: "
"What type of requests will aligned text models refuse to answer?
","['daphne ippolito_8724579d3f126e753a0451d98ff57b165f722e72_metadata.txt', 'mona diab_99bfe503743c5ec8e16e50ab8438159cdb533a89_content_15.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What type of requests will aligned text models refuse to answer?

Context: Faculty Name: daphne ippolito
Paperid: 8724579d3f126e753a0451d98ff57b165f722e72
Title: Are aligned neural networks adversarially aligned?
Year: 2023
Abstract: Large language models are now tuned to align with the goals of their creators, namely to be""helpful and harmless.""These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study to what extent these models remain aligned, even when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs. However the recent trend in large-scale ML models is multimodal models that allow users to provide images that influence the text that is generated. We show these models can be easily attacked, i.e., induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image. We conjecture that improved NLP attacks may demonstrate this same level of adversarial control over text-only models.
Authors: Nicholas Carlini, Milad Nasr, Christopher A. Choquette-Choo, Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tramèr, Ludwig Schmidt
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'It is shown that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models, and conjecture that improved NLP attacks may demonstrate this same level of adversarial control over text-only models.'}
Url: http://arxiv.org/pdf/2306.15447
2023a. How language model hallucinations can snowball. Shuo Zhang, Liangming Pan, Junzhou Zhao, and William Yang Wang. 2023b. Mitigating language model hallucination with interactive questionknowledge alignment. Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettlemoyer. 2022. Opt: Open pretrained transformer language models. Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul F. Christiano, and Geoffrey Irving. 2019. Fine-tuning language models from human preferences. CoRR, abs/1909.08593. Frequently Asked Questions (FAQs) ✽ This study explores the unintended, negative aspects of hallucination; how about the useful effects that arise as a result of hallucination? ➠ While hallucinating has beneficiary effects in some computer vision use cases, where a generative vision model could perform in-painting of an occluded content in an image or generate an image of a scenario it hasn’t seen in its training set (for example, a generated image corresponding to the prompt, “water on Mars”), but it is usually undesirable in the context of the text. The downstream impact as a result of the model’s is exacerbated by the fact that there is a lack of a programmatic method in the research community to distinguish the hallucinated vs. factually correct output. For this reason, this study focuses on characterizing the problem of hallucination particularly in the context of text. ✽ Why do you select those 15 large language models? ➠ We want to select several language models with varying parameter sizes for our experiments - ranging from large to small. Hence, the above chosen
Answer: "
"What is Yonatan Bisk's job title?
","['yonatan bisk_5ce2f1dff23a5620f77f9b11f1e534422ab8ff3f_metadata.txt', 'yonatan bisk_376f494126d1ea4f571ea0263c43ac2b6331800a_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is Yonatan Bisk's job title?

Context: Faculty Name: yonatan bisk
Paperid: 5ce2f1dff23a5620f77f9b11f1e534422ab8ff3f
Title: Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents
Year: 2023
Abstract: Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accomplished each sub-task. On the AlfWorld instruction following benchmark, the PET framework leads to a significant 15% improvement over SOTA for generalization to human goal specifications.
Authors: Yue Wu, So Yeon Min, Yonatan Bisk, R. Salakhutdinov, A. Azaria, Yuan-Fang Li, Tom M. Mitchell, Shrimai Prabhumoye
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A framework to use the knowledge in LLMs to simplify the control problem, rather than solving it is proposed, which leads to a significant 15% improvement over SOTA for generalization to human goal specifications.'}
Url: http://arxiv.org/pdf/2305.02412
Faculty Name: yonatan bisk
Paperid: 376f494126d1ea4f571ea0263c43ac2b6331800a
Title: SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs
Year: 2023
Abstract: In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.
Authors: Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A. Ross, Irfan Essa, Yonatan Bisk, Ming Yang, K. Murphy, A. Hauptmann, Lu Jiang
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.'}
Url: http://arxiv.org/pdf/2306.17842
Answer: "
"Who taught human language for AI in the fall of 2023?
","['carolyn rose_27ca2d927421035e10b48c96a96db32224f1f8e6_metadata.txt', 'combined_metadata_final.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who taught human language for AI in the fall of 2023?

Context: Faculty Name: carolyn rose
Paperid: 27ca2d927421035e10b48c96a96db32224f1f8e6
Title: Exploring Artificial Intelligence in English Language Arts with StoryQ
Year: 2023
Abstract: Exploring Artificial Intelligence (AI) in English Language Arts (ELA) with StoryQ is a 10-hour curriculum module designed for high school ELA classes. The module introduces students to fundamental AI concepts and essential machine learning workflow using StoryQ, a web-based GUI environment for Grades 6-12 learners. In this module, students work with unstructured text data and learn to train, test, and improve text classification models such as intent recognition, clickbait filter, and sentiment analysis. As they interact with machine-learning language models deeply, students also gain a nuanced understanding of language and how to wield it, not just as a data structure, but as a tool in our human-human encounters as well. The current version contains eight lessons, all delivered through a full-featured online learning and teaching platform. Computers and Internet access are required to implement the module. The module was piloted in an ELA class in the Spring of 2022, and the student learning outcomes were positive. The module is currently undergoing revision and will be further tested and improved in Fall 2022.
Authors: J. Chao, Rebecca Ellis, Shiyan Jiang, C. Rosé, W. Finzer, Can Tatar, James Fiacco, Kenia Wiedemann
Venue: AAAI Conference on Artificial Intelligence
Tldr: {'model': 'tldr@v2.0.0', 'text': None}
Url: https://ojs.aaai.org/index.php/AAAI/article/download/26899/26671
The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Zimmerman, Musuraca located in Building 4SC, Room 104.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Human AI Interaction' with Course ID 05618 and Section A offers 12.0 units. The Class meets Monday Wednesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Zhu located in Building SH, Room 234.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Programming Usable Interfaces:' with Course ID 05630 and Section NA offers 15.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Programming Usable Interfaces' with Course ID 05630 and Section NA offers 15.0 units. The Class meets Monday Wednesday between 08:00AM and 09:20AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ion, Levin-Decanini located in Building TEP, Room 1403.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Programming Usable Interfaces:' with Course ID 05630 and Section NA offers 15.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Programming Usable Interfaces' with Course ID 05630 and Section A offers 15.0 units. The Class meets Monday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ion, Levin-Decanini located in Building WEH, Room 5310.
Answer: "
"How does CSurF address sparse lexicon-based retrieval?
","['jamie callan_6b7eefa15c0a461afeab4fa13cf862c5340fdc2a_content_0.txt', 'jamie callan_6b7eefa15c0a461afeab4fa13cf862c5340fdc2a_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How does CSurF address sparse lexicon-based retrieval?

Context: Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms
Authors: Zhen Fan, Luyu Gao, Jamie Callan
Section: 6 CONCLUSION
This paper proposes CSurF, which performs sparse lexicon-based retrieval through constructing and matching Contextualized Surface Forms. Its retrieval process combines efficient surface form exact match and fine-grained contextualized semantic scoring, which leads to maximized model capacity while maintaining the simplicity and efficiency of exact-match-based retrieval systems. CSurF extends current term-weight based learned sparse retrieval approaches with vector term representations. On experiments across multiple datasets and retrieval settings, CSurF is able to simultaneously bridge the vocabulary and semantic mismatch in exact-match retrieval, and achieve state-of-the-art retrieval performance for lexical exact-match systems. Ablation studies and analysis further demonstrate CSurF’s ability to jointly expandmeaningful surface forms and ground surface forms to underlying semantics, which leads to increased model capacity. We also propose a simple interpolation approach in out-of-domain retrieval settings, to analyze the effect of original text vs. expanded surface forms as well as the quality of lexical form expansion on different retrieval tasks. Compared to all-to-all soft-match retrievers, CSurF achieves comparable performance across all retrieval tasks as an exact-matchbased retrieval system. CSurF is able to learn sparse connections of the original query and document terms, resolving the key efficiency issue of lexical soft-match. The retrieval efficiency of CSurF can also be further optimized with different approaches including training regularization adjustment, post-hoc index pruning, and vector representation approximation or dimension control, without significantly affecting retrieval accuracy. We hope this work encourages more research on building effective, efficient, robust and knowledge-enhanced sparse retrieval systems in the real world, as well as exploring the connection and distinction among current retrieval frameworks and systems.
Faculty Name: jamie callan
Paperid: 6b7eefa15c0a461afeab4fa13cf862c5340fdc2a
Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms
Year: 2023
Abstract: Lexical exact-match systems perform text retrieval efficiently with sparse matching signals and fast retrieval through inverted lists, but naturally suffer from the mismatch between lexical surface form and implicit term semantics. This paper proposes to directly bridge the surface form space and the term semantics space in lexical exact-match retrieval via contextualized surface forms (CSF). Each CSF pairs a lexical surface form with a context source, and is represented by a lexical form weight and a contextualized semantic vector representation. This framework is able to perform sparse lexicon-based retrieval by learning to represent each query and document as a ""bag-of-CSFs"", simultaneously addressing two key factors in sparse retrieval: vocabulary expansion of surface form and semantic representation of term meaning. At retrieval time, it efficiently matches CSFs through exact-match of learned surface forms, and effectively scores each CSF pair via contextual semantic representations, leading to joint improvement in both term match and term scoring. Multiple experiments show that this approach successfully resolves the main mismatch issues in lexical exact-match retrieval and outperforms state-of-the-art lexical exact-match systems, reaching comparable accuracy as lexical all-to-all soft match systems as an efficient exact-match-based system.
Authors: Zhen Fan, Luyu Gao, Jamie Callan
Venue: International Conference on the Theory of Information Retrieval
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes to directly bridge the surface form space and the term semantics space in lexical exact-match retrieval via contextualized surface forms (CSF), reaching comparable accuracy as lexical all-to-all soft match systems as an efficient exact- match-based system.'}
Url: https://dl.acm.org/doi/pdf/10.1145/3578337.3605126
Answer: "
"What does SPAE convert between?
","['yonatan bisk_376f494126d1ea4f571ea0263c43ac2b6331800a_metadata.txt', 'alexander hauptmann_376f494126d1ea4f571ea0263c43ac2b6331800a_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What does SPAE convert between?

Context: Faculty Name: yonatan bisk
Paperid: 376f494126d1ea4f571ea0263c43ac2b6331800a
Title: SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs
Year: 2023
Abstract: In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.
Authors: Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A. Ross, Irfan Essa, Yonatan Bisk, Ming Yang, K. Murphy, A. Hauptmann, Lu Jiang
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.'}
Url: http://arxiv.org/pdf/2306.17842
Faculty Name: alexander hauptmann
Paperid: 376f494126d1ea4f571ea0263c43ac2b6331800a
Title: SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs
Year: 2023
Abstract: In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.
Authors: Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A. Ross, Irfan Essa, Yonatan Bisk, Ming Yang, K. Murphy, A. Hauptmann, Lu Jiang
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.'}
Url: http://arxiv.org/pdf/2306.17842
Answer: "
"In the IWSLT 2023 paper titled ""Evaluating Multilingual Speech Translation Under Realistic Conditions with Resegmentation and Terminology"", authors provided a speech translation dataset covering ACL technical presentations. How many target languages were included in the final dataset?
","['mona diab_c5849f406e8263806a84e1a407ec0e0fe131bd5c_metadata.txt', 'mona diab_c5849f406e8263806a84e1a407ec0e0fe131bd5c_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the IWSLT 2023 paper titled ""Evaluating Multilingual Speech Translation Under Realistic Conditions with Resegmentation and Terminology"", authors provided a speech translation dataset covering ACL technical presentations. How many target languages were included in the final dataset?

Context: Faculty Name: mona diab
Paperid: c5849f406e8263806a84e1a407ec0e0fe131bd5c
Title: Evaluating Multilingual Speech Translation under Realistic Conditions with Resegmentation and Terminology
Year: 2023
Abstract: We present the ACL 60/60 evaluation sets for multilingual translation of ACL 2022 technical presentations into 10 target languages. This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics.
Authors: Elizabeth Salesky, Kareem Darwish, Mohamed Al-Badrashiny, Mona T. Diab, J. Niehues
Venue: International Workshop on Spoken Language Translation
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics.'}
Url: https://aclanthology.org/2023.iwslt-1.2.pdf
Title: Evaluating Multilingual Speech Translation Under Realistic Conditions with Resegmentation and Terminology
Authors: Elizabeth Salesky, Kareem Darwish, Mohamed Al-Badrashiny, Mona Diab, Jan Niehues
Section: 4.1 Segmentation
making the dataset standard scoring with resegmentation. 12chrF for individual languages is shown in Table 6.
Answer: "
"How many system submissions does GlobalBench currently cover?
","['graham neubig_17605c43ca3eb982c99642052ddc21a93d116594_metadata.txt', 'graham neubig_17605c43ca3eb982c99642052ddc21a93d116594_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many system submissions does GlobalBench currently cover?

Context: Faculty Name: graham neubig
Paperid: 17605c43ca3eb982c99642052ddc21a93d116594
Title: GlobalBench: A Benchmark for Global Progress in Natural Language Processing
Year: 2023
Abstract: Despite the major advances in NLP, significant disparities in NLP system performance across languages still exist. Arguably, these are due to uneven resource allocation and sub-optimal incentives to work on less resourced languages. To track and further incentivize the global development of equitable language technology, we introduce GlobalBench. Prior multilingual benchmarks are static and have focused on a limited number of tasks and languages. In contrast, GlobalBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring accuracy, GlobalBench also tracks the estimated per-speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served languages are the ones with a relatively high population, but nonetheless overlooked by composite multilingual benchmarks (like Punjabi, Portuguese, and Wu Chinese). Currently, GlobalBench covers 966 datasets in 190 languages, and has 1,128 system submissions spanning 62 languages.
Authors: Yueqi Song, Catherine Cui, Simran Khanuja, Pengfei Liu, FAHIM FAISAL, Alissa Ostapenko, Genta Indra Winata, Alham Fikri Aji, Samuel Cahyawijaya, Yulia Tsvetkov, Antonios Anastasopoulos, Graham Neubig
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work introduces GlobalBench, an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages and tracks the estimated per-speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world.'}
Url: http://arxiv.org/pdf/2305.14716
Title: GlobalBench: A Benchmark for Global Progress in Natural Language Processing
Authors: Yueqi Song, Catherine Cui, Simran Khanuja, Pengfei Liu, Fahim Faisal, Alissa Ostapenko, Genta Indra Winata, Alham Fikri Aji, Samuel Cahyawijaya, Yulia Tsvetkov, Antonios Anastasopoulos, Graham Neubig
Section: A Appendix
A.1 Datasets and System Outputs In this section, we list all datasets for which GlobalBench has submissions of system outputs. Text Classification GlobalBench covers the following datasets for this task: the QC (Question Classification) dataset (Li and Roth, 2002), the ATIS (Airline Travel Information Systems) dataset (Hemphill et al., 1990), the MR (Movie Review) dataset (Pang and Lee, 2005), the SST-2 (Stanford Sentiment Treebank) Corpus (Socher et al., 2013), datasets from GLUE (the General Language Understanding Evaluation) benchmark (Wang et al., 2018), and the Code-Switching Corpus (Ostapenko et al., 2022). Sequence Labeling GlobalBench covers the following datasets for this task: the MasakhaNER Corpus (Adelani et al., 2021), the CoNLL-2003 dataset (Tjong Kim Sang and De Meulder, 2003), and the PAN-X dataset (Artetxe and Schwenk, 2019). Text Pair Classification GlobalBench covers the following datasets for this task: the Cross-lingual Natural Language Inference (XNLI) corpus (Conneau et al., 2018), the Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015), and the Sentences Involving Compositional Knowldedge (SICK) dataset (Marelli et al., 2014). Question Answering GlobalBench covers the following datasets for this task: XQuAD (Artetxe et al., 2019), TyDiQA (Clark et al., 2020), SDQA (Faisal et al., 2021), and MLQA (Lewis et al., 2019).
Answer: "
"When was the Center for Machine Translation established at CMU?
","['history_d401.txt', 'alexander_waibel.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When was the Center for Machine Translation established at CMU?

Context: The experiment, which lasted two years, was an unqualified success. Separately, and also in 1986, the Pittsburgh Supercomputing Center was created as a joint effort between CMU, the University of Pittsburgh and Westinghouse Electric Corp.SCS is officially formedCMU’s Faculty Senate in the fall of 1988 agreed to President Richard Cyert’s plan to elevate the Department of Computer Science to college status. In addition to the Computer Science Department, SCS also incorporated the Robotics Institute, the Center for Machine Translation, and researchers from the Information Technology Center, which had developed Andrew. On Dec. 13, 1988, Cyert (H’89) told faculty and staff that Habermann had been appointed CMU’s first Dean of Computer Science, effective Dec. 1, and that the School of Computer Science would soon begin operations. SCS made its formal debut on Dec. 22, 1988, with a reception in the Wherrett Room of Skibo Hall, CMU’s student union. The official announcement of CMU’s new “graduate School of Computer Science” was made Jan. 3, 1989.Undergraduate degrees beginFor several years, undergraduates interested in computer science pursued an “applied math/CS” bachelor’s degree offered by the Mathematics Department. CSD professor Mary Shaw (CS’72) led CMU’s first effort to design an undergraduate curriculum solely in computer science. She and her colleagues were guided by the Carnegie Plan—guidelines established in 1938 under Carnegie Tech President Robert Doherty (A’40, E’48, H’50), outlining the principles of a sound professional education. Drawing on Shaw’s plan and also on the work of other faculty members, an undergraduate program in computer science was created during the 1989-90 academic year. Seven CS majors were admitted to the program as sophomores. Another 73 undergraduates were admitted in 1990–91.
Alexander Waibel
Professor (On Leave), Language Technologies Institute
Contact
205 —407 South Craig Street
Email waibel@cs.cmu.edu
412-268-7676
Research Area
Machine Learning, Machine Translation, Multimodal Computing and Interaction, Neural Networks, Speech Processing, Spoken Interfaces and Dialogue Processing, Spoken Language Translation

Bio
Dr. Alexander Waibel is a Professor of Computer Science at Carnegie Mellon University, Pittsburgh and at the Karlsruhe Institute of Technology, Germany. He is the director of the International Center for Advanced Communication Technologies (interACT). The Center works in a network with eight of the world’s top research institutions. The Center’s mission is to develop advanced machine learning algorithms to improve human-human and human-machine communication technologies.  Prof. Waibel and his team developed many statistical and neural network learning  algorithms that made such communication breakthroughs possible. Most notably, the “Time-Delay Neural Network” (1987) (the first “convolutional” neural network) now is at the heart of many of today’s AI technologies. System breakthroughs at Waibel’s lab included early multimodal interfaces, speech and language interfaces, the first speech translation system in Europe & USA (1990/1991), the first simultaneous lecture translation system (2005), and Jibbigo, the first commercial speech translator on a phone (2009).

Dr. Waibel founded and served as chairmen of C-STAR, the Consortium for Speech Translation Advanced Research in 1991. He directed many research programs in speech, translation, multimodal interfaces and machine learning in the US, Europe and Asia. He served as director of EU-Bridge (2012-2015) and CHIL (2004-2007), two large European multi-site Integrated Project initiatives on intelligent assistants and speech translation services. He also was co-director of IMMI, a joint venture between KIT, CNRS & RWTH.

Dr. Waibel is an IEEE Fellow and received many awards for his work on multilingual and multimodal communication  and translation. He published extensively (>750 publications, >25,000 citations, h-index 80) in the field and received/filed numerous patents. Waibel was elected to the National Academy of Sciences of Germany in 2017, and was named Honorary Senator of the Hochschulrektorenkonferenz (Representation of German Universities).

During his career, Dr.
Answer: "
"In the paper titled ""WebArena: A Realistic Web Environment for Building Autonomous Agents"", what is the success rate of the best performing GPT-3.5 model?
","['yonatan bisk_e41482f4ee984f17382f6cdd900df094d928be06_metadata.txt', 'graham neubig_e41482f4ee984f17382f6cdd900df094d928be06_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the paper titled ""WebArena: A Realistic Web Environment for Building Autonomous Agents"", what is the success rate of the best performing GPT-3.5 model?

Context: Faculty Name: yonatan bisk
Paperid: e41482f4ee984f17382f6cdd900df094d928be06
Title: WebArena: A Realistic Web Environment for Building Autonomous Agents
Year: 2023
Abstract: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.
Authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper builds an environment for language-guided agents that is highly realistic and reproducible, and creates an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.'}
Url: https://arxiv.org/pdf/2307.13854
Faculty Name: graham neubig
Paperid: e41482f4ee984f17382f6cdd900df094d928be06
Title: WebArena: A Realistic Web Environment for Building Autonomous Agents
Year: 2023
Abstract: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.
Authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper builds an environment for language-guided agents that is highly realistic and reproducible, and creates an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.'}
Url: https://arxiv.org/pdf/2307.13854
Answer: "
"At what conference was ""The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features"" published at?
","['bhiksha raj_a6e3a10a6286967413e3406374bbeea533640030_content_1.txt', 'rita singh_a6e3a10a6286967413e3406374bbeea533640030_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: At what conference was ""The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features"" published at?

Context: Title: The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features
Authors: Liao Qu, Xianwei Zou, Xiang Li, Yandong Wen, Rita Singh, Bhiksha Raj
Section: 6. References
Z.-Q. Wang and I. Tashev, “Learning utterance-level representations for speech emotion and age/gender recognition using deep neural networks,” 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5150–5154, 2017. [18] T. Riede, E. Bronson, H. Hatzikirou, and K. Zuberbühler, “Vocal production mechanisms in a non-human primate: morphological data and a model,” Journal of Human Evolution, vol. 48, no. 1, pp. 85–96, 2005. [19] R. Singh, B. Raj, and D. Gençaga, “Forensic anthropometry from voice: An articulatory-phonetic approach,” 2016 39th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO), pp. 1375–1380, 2016. [20] M. T. Ghiselin, P. Ekman, and H. E. Gruber, “Darwin and facial expression: A century of research in review.@@@darwin on man: A psychological study of scientific creativity.” Systematic Biology, vol. 23, p. 562, 1974. [21] M. Tan, B. Chen, R. Pang, V. Vasudevan, and Q. V. Le, “Mnasnet: Platform-aware neural architecture search for mobile,” 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2815–2823, 2018. [22] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778, 2015.
Title: The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features
Authors: Liao Qu, Xianwei Zou, Xiang Li, Yandong Wen, Rita Singh, Bhiksha Raj
Section: 6. References
Z.-Q. Wang and I. Tashev, “Learning utterance-level representations for speech emotion and age/gender recognition using deep neural networks,” 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5150–5154, 2017. [18] T. Riede, E. Bronson, H. Hatzikirou, and K. Zuberbühler, “Vocal production mechanisms in a non-human primate: morphological data and a model,” Journal of Human Evolution, vol. 48, no. 1, pp. 85–96, 2005. [19] R. Singh, B. Raj, and D. Gençaga, “Forensic anthropometry from voice: An articulatory-phonetic approach,” 2016 39th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO), pp. 1375–1380, 2016. [20] M. T. Ghiselin, P. Ekman, and H. E. Gruber, “Darwin and facial expression: A century of research in review.@@@darwin on man: A psychological study of scientific creativity.” Systematic Biology, vol. 23, p. 562, 1974. [21] M. Tan, B. Chen, R. Pang, V. Vasudevan, and Q. V. Le, “Mnasnet: Platform-aware neural architecture search for mobile,” 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2815–2823, 2018. [22] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778, 2015.
Answer: "
"What is the code URL for the case studies presented in the framework tax paper?
","['emma strubell_b777aa86b5a1d49ce8eababc5c2ee56d3562801e_metadata.txt', 'yonatan bisk_b777aa86b5a1d49ce8eababc5c2ee56d3562801e_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the code URL for the case studies presented in the framework tax paper?

Context: Faculty Name: emma strubell
Paperid: b777aa86b5a1d49ce8eababc5c2ee56d3562801e
Title: The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment
Year: 2023
Abstract: Increased focus on the computational efficiency of NLP systems has motivated the design of efficient model architectures and improvements to underlying hardware accelerators. However, the resulting increases in computational throughput and reductions in floating point operations have not directly translated to improvements in wall-clock inference latency. We demonstrate that these discrepancies can be largely attributed to bottlenecks introduced by deep learning frameworks. We denote this phenomenon as the \textit{framework tax}, and observe that the disparity is growing as hardware speed increases over time. In this work, we examine this phenomenon through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency. Code is available at https://github.com/JaredFern/Framework-Tax.
Authors: Jared Fernandez, Jacob Kahn, Clara Na, Yonatan Bisk, Emma Strubell
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work examines the effects of model design decisions, framework paradigms, and hardware platforms on total model latency through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency.'}
Url: http://arxiv.org/pdf/2302.06117
Faculty Name: yonatan bisk
Paperid: b777aa86b5a1d49ce8eababc5c2ee56d3562801e
Title: The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment
Year: 2023
Abstract: Increased focus on the computational efficiency of NLP systems has motivated the design of efficient model architectures and improvements to underlying hardware accelerators. However, the resulting increases in computational throughput and reductions in floating point operations have not directly translated to improvements in wall-clock inference latency. We demonstrate that these discrepancies can be largely attributed to bottlenecks introduced by deep learning frameworks. We denote this phenomenon as the \textit{framework tax}, and observe that the disparity is growing as hardware speed increases over time. In this work, we examine this phenomenon through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency. Code is available at https://github.com/JaredFern/Framework-Tax.
Authors: Jared Fernandez, Jacob Kahn, Clara Na, Yonatan Bisk, Emma Strubell
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work examines the effects of model design decisions, framework paradigms, and hardware platforms on total model latency through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency.'}
Url: http://arxiv.org/pdf/2302.06117
Answer: "
"In Prof. Fernando Diaz's paper on best-case retrieval evaluation, what is the name of the proposed metric for preference-based evaluation?
","['fernando diaz_55704caaf3d31e1795a1ca0c3bed9e77ae3a3c95_content_0.txt', 'fernando diaz_55704caaf3d31e1795a1ca0c3bed9e77ae3a3c95_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In Prof. Fernando Diaz's paper on best-case retrieval evaluation, what is the name of the proposed metric for preference-based evaluation?

Context: Title: Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision
Authors: Fernando Diaz
Section: 7 CONCLUSION
Motivated by ceiling effects in evaluation with reciprocal rank, we have attempted to increase our understanding of the metric and designed a well-grounded mitigation to conducting best-case retrieval evaluation. We have shown that lexiprecision can effectively address the limitations of reciprocal rank in retrieval evaluation. Our results highlight the importance of considering the effects of tie-breaking in the evaluation process and provide a method for conducting more reliable best-case retrieval evaluation. Given the use of retrieval metrics—including reciprocal rank—outside of information retrieval contexts, we believe these contributions will be relevant to a researchers in the broader research community.
Faculty Name: fernando diaz
Paperid: 55704caaf3d31e1795a1ca0c3bed9e77ae3a3c95
Title: Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision
Year: 2023
Abstract: Across a variety of ranking tasks, researchers use reciprocal rank to measure the effectiveness for users interested in exactly one relevant item. Despite its widespread use, evidence suggests that reciprocal rank is brittle when discriminating between systems. This brittleness, in turn, is compounded in modern evaluation settings where current, high-precision systems may be difficult to distinguish. We address the lack of sensitivity of reciprocal rank by introducing and connecting it to the concept of best-case retrieval, an evaluation method focusing on assessing the quality of a ranking for the most satisfied possible user across possible recall requirements. This perspective allows us to generalize reciprocal rank and define a new preference-based evaluation we call lexicographic precision or lexiprecision. By mathematical construction, we ensure that lexiprecision preserves differences detected by reciprocal rank, while empirically improving sensitivity and robustness across a broad set of retrieval and recommendation tasks.
Authors: Fernando Diaz
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work addresses the lack of sensitivity of reciprocal rank by introducing and connecting it to the concept of best-case retrieval, an evaluation method focusing on assessing the quality of a ranking for the most satisfied possible user across possible recall requirements.'}
Url: http://arxiv.org/pdf/2306.07908
Answer: "
"Where was ""End-to-End Speech Recognition: A Survey"" published?
","['shinji watanabe_f80c354908efd4d5617878e36e35446016534190_content_1.txt', 'shinji watanabe_331af9b7193e563b021e8e6892e7cb3030decd38_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Where was ""End-to-End Speech Recognition: A Survey"" published?

Context: 7. REFERENCES [1] R. Prabhavalkar et al., “End-to-end speech recognition: A survey,” arXiv preprint arXiv:2303.03329, 2023. [2] J. Li et al., “Recent advances in end-to-end automatic speech recognition,” APSIPA Transactions on Signal and Information Processing, vol. 11, no. 1, [3] S. Watanabe et al., “ESPnet: End-to-end speech processing toolkit,” in Proc. Interspeech, 2018. [4] A. Graves et al., “Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks,” in ICML 2006, vol. 148, 2006, pp. 369–376. [5] A. Graves, “Sequence transduction with recurrent neural networks,” CoRR, vol. abs/1211.3711, 2012. [6] J. Chorowski et al., “Attention-based models for speech recognition,” in Proc. NeurIPS, 2015, pp. 577–585. [7] A. Graves, A. Mohamed, and G. E. Hinton, “Speech recognition with deep recurrent neural networks,” in Proc. ICASSP, 2013. [8] G. Saon et al., “Advancing RNN transducer technology for speech recognition,” in Proc. ICASSP, 2021, pp. 5654–5658. [9] W. Chan et al., “Listen,
Mag., vol. 29, no. 6, pp. 82–97, 2012. [2] Alex Graves, Abdelrahman Mohamed, and Geoffrey Hinton, “Speech recognition with deep recurrent neural networks,” in Proc. ICASSP, 2013, pp. 6645–6649. [3] Rohit Prabhavalkar, Takaaki Hori, Tara N Sainath, Ralf Schlüter, and Shinji Watanabe, “End-to-end speech recognition: A survey,” arXiv preprint arXiv:2303.03329, 2023. [4] Alex Graves, “Sequence transduction with recurrent neural networks,” in Proc. ICML, 2012. [5] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang, “Conformer: Convolution-augmented transformer for speech recognition,” in Proc. Interspeech, 2020, pp. 5036–5040. [6] Kwangyoun Kim, Felix Wu, Yifan Peng, Jing Pan, Prashant
Answer: "
"In spring 2024, What is the title of course 17437?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, What is the title of course 17437?

Context: In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Foundations of Software Engineering' with Course ID 17313 and Section F offers 12.0 units. The Class meets Monday between 02:00PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hilton, Feo Flushing located in Building PH, Room 126A.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Usable Privacy and Security' with Course ID 17334 and Section A offers 9.0 units. The Class meets Monday Wednesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Cranor, Agarwal located in Building HH, Room B103.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Usable Privacy and Security' with Course ID 17334 and Section B offers 9.0 units. The Class meets Monday Wednesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Cranor, Agarwal located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Software Engineering for Startups' with Course ID 17356 and Section NA offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Wright, Brown located in Building SH, Room 238.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Software Engineering for Startups' with Course ID 17356 and Section A offers 12.0 units. The Class meets Friday between 09:00AM and 09:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Wright, Brown located in Building PH, Room A19.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Software Engineering for Startups' with Course ID 17356 and Section B offers 12.0 units.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Foundations of Software Engineering' with Course ID 17313 and Section F offers 12.0 units. The Class meets Monday between 02:00PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hilton, Feo Flushing located in Building PH, Room 126A.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Usable Privacy and Security' with Course ID 17334 and Section A offers 9.0 units. The Class meets Monday Wednesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Cranor, Agarwal located in Building HH, Room B103.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Usable Privacy and Security' with Course ID 17334 and Section B offers 9.0 units. The Class meets Monday Wednesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Cranor, Agarwal located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Software Engineering for Startups' with Course ID 17356 and Section NA offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Wright, Brown located in Building SH, Room 238.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Software Engineering for Startups' with Course ID 17356 and Section A offers 12.0 units. The Class meets Friday between 09:00AM and 09:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Wright, Brown located in Building PH, Room A19.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Software Engineering for Startups' with Course ID 17356 and Section B offers 12.0 units.
Answer: "
"What number do all of the CFA Interdisciplinary classes start with?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What number do all of the CFA Interdisciplinary classes start with?

Context: In Semester Spring 2024, from the department of CFA Interdisciplinary, the subject titled 'TBA' with Course ID 62347 and Section B offers 3.0 units. The Class meets Saturday between 12:30PM and 03:30PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Mcgowan located in Building OFF, Room CAMPUS.
In Semester Spring 2024, from the department of CFA Interdisciplinary, the subject titled 'Interdisciplinary Independent Study' with Course ID 62348 and Section A offers 36912 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Oresick located in Building TBA, Room None.
In Semester Spring 2024, from the department of CFA Interdisciplinary, the subject titled 'Interdisciplinary Independent Study' with Course ID 62348 and Section B offers 36912 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Cupkova located in Building TBA, Room None.
In Semester Spring 2024, from the department of CFA Interdisciplinary, the subject titled 'Photographers and Photography Since World War II' with Course ID 62360 and Section A offers 9.0 units. The Class meets Monday between 07:00PM and 09:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor May located in Building PH, Room A18B.
In Semester Spring 2024, from the department of CFA Interdisciplinary, the subject titled 'Large Format Photography: The Antiquarian Avant-Garde' with Course ID 62375 and Section A offers 10.0 units. The Class meets Tuesday Thursday between 08:00AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Blum located in Building MM, Room B10.
In Semester Spring 2024, from the department of CFA Interdisciplinary, the subject titled 'Independent Study: Topics in Photography' with Course ID 62397 and Section A4 offers 2.0 units. The Class meets Schedule will be added between NA and NA ET.
In Semester Spring 2024, from the department of CFA Interdisciplinary, the subject titled 'TBA' with Course ID 62347 and Section B offers 3.0 units. The Class meets Saturday between 12:30PM and 03:30PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Mcgowan located in Building OFF, Room CAMPUS.
In Semester Spring 2024, from the department of CFA Interdisciplinary, the subject titled 'Interdisciplinary Independent Study' with Course ID 62348 and Section A offers 36912 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Oresick located in Building TBA, Room None.
In Semester Spring 2024, from the department of CFA Interdisciplinary, the subject titled 'Interdisciplinary Independent Study' with Course ID 62348 and Section B offers 36912 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Cupkova located in Building TBA, Room None.
In Semester Spring 2024, from the department of CFA Interdisciplinary, the subject titled 'Photographers and Photography Since World War II' with Course ID 62360 and Section A offers 9.0 units. The Class meets Monday between 07:00PM and 09:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor May located in Building PH, Room A18B.
In Semester Spring 2024, from the department of CFA Interdisciplinary, the subject titled 'Large Format Photography: The Antiquarian Avant-Garde' with Course ID 62375 and Section A offers 10.0 units. The Class meets Tuesday Thursday between 08:00AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Blum located in Building MM, Room B10.
In Semester Spring 2024, from the department of CFA Interdisciplinary, the subject titled 'Independent Study: Topics in Photography' with Course ID 62397 and Section A4 offers 2.0 units. The Class meets Schedule will be added between NA and NA ET.
Answer: "
"What are the names of the people from CMU who contributed to the paper RIVETER Measuring Power and Social Dynamics Between Entities?
","['maarten sap_27553f8bd9cbee90f6e65b9cdecadff0e7cc55ee_metadata.txt', 'maarten sap_27553f8bd9cbee90f6e65b9cdecadff0e7cc55ee_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the names of the people from CMU who contributed to the paper RIVETER Measuring Power and Social Dynamics Between Entities?

Context: Faculty Name: maarten sap
Paperid: 27553f8bd9cbee90f6e65b9cdecadff0e7cc55ee
Title: Riveter: Measuring Power and Social Dynamics Between Entities
Year: 2023
Abstract: Riveter provides a complete easy-to-use pipeline for analyzing verb connotations associated with entities in text corpora. We prepopulate the package with connotation frames of sentiment, power, and agency, which have demonstrated usefulness for capturing social phenomena, such as gender bias, in a broad range of corpora. For decades, lexical frameworks have been foundational tools in computational social science, digital humanities, and natural language processing, facilitating multifaceted analysis of text corpora. But working with verb-centric lexica specifically requires natural language processing skills, reducing their accessibility to other researchers. By organizing the language processing pipeline, providing complete lexicon scores and visualizations for all entities in a corpus, and providing functionality for users to target specific research questions, Riveter greatly improves the accessibility of verb lexica and can facilitate a broad range of future research.
Authors: Maria Antoniak, Anjalie Field, Jimin Mun, Melanie Walsh, Lauren F. Klein, Maarten Sap
Venue: Annual Meeting of the Association for Computational Linguistics
Tldr: {'model': 'tldr@v2.0.0', 'text': 'Riveter greatly improves the accessibility of verb lexica and can facilitate a broad range of future research by organizing the language processing pipeline, providing complete lexicon scores and visualizations for all entities in a corpus, and providing functionality for users to target specific research questions.'}
Url: https://aclanthology.org/2023.acl-demo.36.pdf
Title: Measuring Power and Social Dynamics Between Entities
Authors: Maria Antoniak, Anjalie Field, Jimin Mun, Melanie Walsh, Lauren F. Klein, Maarten Sap
Section: 4.1 Machine Stories with Power Dyads
shows the entities identified by RIVETER that have the highest and lowest power scores, using the lexicon from Sap et al. (2017). Characters like Miss Lucas have higher power scores, while characters like Elizabeth have lower power scores. By using RIVETER’s functionality to pull out the documents contributing to an entity’s score, we find that a mistaken entity, “her eyes,” indeed often occurs in low power roles, as in “Miss Bingley fixed her eyes on face,” providing an intuitive validation of the results. This plot also shows the standard deviation across bootstrapped samples of the novel, indicating the overlapping instability of many of the power scores for this single novel. Figure 7 shows the lexicon verbs contributing most frequently to each pronoun group’s power score. For example, we see that feminine pronouns are frequently used as subjects of the verb “hear”— emphasizing women’s low-power role of waiting to hear news. We also observe that while feminine pronouns are often placed in high-power positions at rates similar to masculine pronouns, they have higher frequencies for low-power positions. In other words, in Austen’s world, masculine and feminine entities both engage in high-power actions, but feminine entities engage in more low-power actions. Arguably, though, some of the low-power positions are used by the feminine entities to obtain power, e.g., by “hearing” news or eavesdropping on others, the feminine entities can learn information that informs their future decisions and strategies.
Answer: "
"In which two task families does the document demonstrate MOSAIC's versatility?
","['yonatan bisk_69b8cd15966c4c9c3e44e71769e557f1c87fb3f9_metadata.txt', 'yonatan bisk_69b8cd15966c4c9c3e44e71769e557f1c87fb3f9_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In which two task families does the document demonstrate MOSAIC's versatility?

Context: Faculty Name: yonatan bisk
Paperid: 69b8cd15966c4c9c3e44e71769e557f1c87fb3f9
Title: MOSAIC: Learning Unified Multi-Sensory Object Property Representations for Robot Perception
Year: 2023
Abstract: A holistic understanding of object properties across diverse sensory modalities (e.g., visual, audio, and haptic) is essential for tasks ranging from object categorization to complex manipulation. Drawing inspiration from cognitive science studies that emphasize the significance of multi-sensory integration in human perception, we introduce MOSAIC (Multimodal Object property learning with Self-Attention and Interactive Comprehension), a novel framework designed to facilitate the learning of unified multi-sensory object property representations. While it is undeniable that visual information plays a prominent role, we acknowledge that many fundamental object properties extend beyond the visual domain to encompass attributes like texture, mass distribution, or sounds, which significantly influence how we interact with objects. In MOSAIC, we leverage this profound insight by distilling knowledge from multimodal foundation models and aligning these representations not only across vision but also haptic and auditory sensory modalities. Through extensive experiments on a dataset where a humanoid robot interacts with 100 objects across 10 exploratory behaviors, we demonstrate the versatility of MOSAIC in two task families: object categorization and object-fetching tasks. Our results underscore the efficacy of MOSAIC's unified representations, showing competitive performance in category recognition through a simple linear probe setup and excelling in the fetch object task under zero-shot transfer conditions. This work pioneers the application of sensory grounding in foundation models for robotics, promising a significant leap in multi-sensory perception capabilities for autonomous systems. We have released the code, datasets, and additional results: https://github.com/gtatiya/MOSAIC.
Authors: Gyan Tatiya, Jonathan M Francis, Ho-Hsiang Wu, Yonatan Bisk, Jivko Sinapov
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work pioneers the application of sensory grounding in foundation models for robotics, promising a significant leap in multi-sensory perception capabilities for autonomous systems.'}
Url: https://arxiv.org/pdf/2309.08508
Title: MOSAIC: Learning Unified Multi-Sensory Object Property Representations for Robot Learning via Interactive Perception
Authors: Gyan Tatiya, Jonathan Francis, Ho-Hsiang Wu, Yonatan Bisk, Jivko Sinapov
Section: VI. CONCLUSION AND FUTURE WORK
We introduced the MOSAIC framework to enable robots to generate versatile, multimodal representations through interactive object perception and to leverage these unified representations across various downstream robot learning tasks. Through extensive performance evaluation, we have showcased the effectiveness of these unified representations in tasks such as category recognition, using a simple linear probe setup, and the fetch object task under zero-shot conditions. Moving forward, there are several exciting directions for future research. Firstly, we plan to consider the transfer of unified representations across different robot morphologies, enabling a broader range of robots to benefit from this technology. Furthermore, we envision settings where interactive behaviors are learned and composed, alongside the tasks we considered in this paper, thereby further increasing the efficacy of object exploration. These future endeavors hold the potential to further enhance the utility of unified representations in robotics and expand their applications across a multitude of scenarios and environments. One limitation in our current study is that, for the fetch object task, we evaluated using a zero-shot transfer condition rather than a learning-based approach to find the target object. For future work, it would be important to explore learningbased policies for solving the fetch object task, potentially increasing the versatility and adaptability of our framework.
Answer: "
"Who is the Office Manager for LTI who is listed in the LTI handbook?
","['handbook_phd_2023-2024.txt', 'handbook_phd_2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who is the Office Manager for LTI who is listed in the LTI handbook?

Context: degree are: 
Jamie Callan 
Ph.D. Program Director 
Professor 
GHC 5419 
callan@cs.cmu.edu 
412-268-4525 
Stacey Young 
Ph.D. Academic Program Manager 
LTI Graduate Program Manager 
GHC 6415 
staceyy@cs.cmu.edu  
412-268-2623 
Mona Diab 
LTI Director 
Professor 
GHC 5723 
mdiab@andrew.cmu.edu 
412-268-3669 
 
Joan Axelson 
Office Manager 
GHC 5405 
jaxelson@andrew.cmu.edu 
412-268-7517 
 
Julie Nys 
Employment Processes Manger 
GHC 5405 
jnys@andrew.cmu.edu 
412-268-3515 
1.3 
University Policies and Expectations 
It is the responsibility of each member of the Carnegie Mellon community to be familiar with 
university policies and guidelines. In addition to this departmental graduate student handbook, 
the following resources are available to assist you in understanding community expectations. 
 The Word/Student Handbook: 
https://www.cmu.edu/student-affairs/theword//index.html 
 Academic Integrity Policy: 
 
https://www.cmu.edu/policies/student-and- student-life/academic-
integrity.html 
LTI Ph.D. Graduate Student Handbook 
Page 11 
 
 University Policies Website:  
https://www.cmu.edu/policies/ 
 Office of Graduate and Postdoctoral Affairs: 
https://www.cmu.edu/graduate/policies/index.html 
Due to the changing nature of conditions and expectations surrounding public health and safety 
requirements please visit https://www.cmu.edu/coronavirus/ for the most up to date information.
 
Please see Appendix A for additional information about The Word and University 
resources. 
1.4 
The Academic Calendar  
The Academic calendar can be found at https://www.cmu.edu/hub/calendar/index.html and 
provides information on all deadlines, including registration dates, class start dates, add/drop 
deadlines, exam dates, and more.  
Some doctoral course-sections follow a separate Academic Calendar.
students in residence on the Pittsburgh campus are given an office in which to study 
and do research. Typically, offices are shared with other Ph.D. students, but they may also be 
shared with staff, visitors, or other members of the LTI. 
Offices are assigned by the LTIs Office Manager (see Section 1.2, Department Personnel, for 
contact information). 
2.3 
Mailboxes and Office Supplies 
Mailboxes and office supplies are in GHC 5404. 
2.4 
Photocopies and Printers 
Printers and photocopies are available to LTI students. The use of a photocopier or printer 
requires you to log in with your CMU ID card. LTI students may use printers/photocopiers 
scattered throughout the School of Computer Science buildings, but the machines in GHC 5404 
and GHC 6604 are the most convenient. The SCS Computing Facilities publishes a list of printers 
online at https://computing.cs.cmu.edu/desktop/printer-list. 
2.5 
Computers for LTI Ph.D. Students 
Ph.D. students are responsible for having their own laptop computers to support their education 
and research. Students are free to choose their own operating system (e.g., Linux, MacOs, 
Windows). 
Many Ph.D. advisors also provide access to computer clusters, cloud computing, or other 
resources to support computationally-intense research. 
Ph.D. students are given access to the LTIs computer cluster on an as-needed basis, to be used for 
course assignments, directed study projects, and/or capstone projects.  The LTI cluster provides 
storage and computation for projects involving large datasets and/or lengthy computation. 
Ph.D. students receive two types of user ids: An Andrew id and a CS id. All CMU students have 
an Andrew id. Computer Science students also have a CS id that provides access to SCS-specific 
resources (e.g., computer clusters). CS ids are being phased out very slowly, so it is likely that you 
will need both types of user id. 
LTI Ph.D. Graduate Student Handbook 
Page 14 
 
The School of Computer Science has a Help Center in GHC 4201.
Answer: "
"In spring 2024, What is the day and time of course 17445-A?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, What is the day and time of course 17445-A?

Context: On Monday, 31 March, 2025, during the Spring 2025 (S25) semester, Semester pass/no pass & withdrawal deadline is observed.
On Wednesday, 02 April, 2025, during the Spring 2025 (S25) semester, Mini-4 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 03 April, 2025,Thursday to 05 April, 2023,Saturday marks the Spring Carnival for Spring 2025 (S25) semester which leads to No Classes.
From 07 April, 2025,Monday to 11 April, 2023,Friday marks the Fall 2025 Registration Week for Spring 2025 (S25) semester.
On Monday, 14 April, 2025, during the Spring 2025 (S25) semester, Mini-4 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 14 April, 2025, during the Spring 2025 (S25) semester, Semester & Mini-4 Faculty Course Evaluations open is observed.
On Friday, 25 April, 2025, during the Spring 2025 (S25) semester, Last Day of Classes is observed.
On Friday, 25 April, 2025, during the Spring 2025 (S25) semester, Semester & Mini-4 voucher deadline (4) is observed.
From 28 April, 2025,Monday to 29 April, 2023,Tuesday marks the Final Examinations for Spring 2025 (S25) semester.
On Wednesday, 30 April, 2025, during the Spring 2025 (S25) semester, Reading Day is observed.
From 01 May, 2025,Thursday to 02 May, 2023,Friday marks the Final Examinations for Spring 2025 (S25) semester.
From 03 May, 2025,Saturday to 04 May, 2023,Sunday marks the Reading Days for Spring 2025 (S25) semester.
On Monday, 05 May, 2025, during the Spring 2025 (S25) semester, Final Examinations is observed.
On Monday, 05 May, 2025, during the Spring 2025 (S25) semester, Semester & Mini-4 Faculty Course Evaluations close is observed.
On Monday, 05 May, 2025, during the Spring 2025 (S25) semester, Semester & Mini-4 Faculty Course Evaluations close is observed.
On Tuesday, 06 May, 2025, during the Spring 2025 (S25) semester, Make-Up Final Examinations is observed.
On Wednesday, 08 May, 2025, during the Spring 2025 (S25) semester, Graduating Students' Final Grades Due by 4 pm is observed.
On Sunday, 11 May, 2025, during the Spring 2025 (S25) semester, Commencement is observed.
On Tuesday, 13 May, 2025, during the Spring 2025 (S25) semester, Final Grades Due by 4 pm is observed.
On Tuesday, 20 May, 2025, during the Spring 2025 (S25) semester, Spring Deans' Lists Posted is observed.
On Monday, 12 May, 2025, during the Summer 2025 (M25) semester, Summer Session All, One & Mini-5 Classes Begin is observed.
On Friday, 16 May, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 add, audit, & tuition adjustment drop deadline (1) is observed.
On Friday, 23 May, 2025, during the Summer 2025 (M25) semester, Summer All add, audit, & tuition adjustment drop deadline (1) is observed.
On Friday, 26 May, 2025, during the Summer 2025 (M25) semester, Memorial Day is observed, leading to University Closed & No Classes.
On Tuesday, 27 May, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 09 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 pass/no pass & withdrawal deadline (3) is observed.
On Friday, 13 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Faculty Course Evaluations open is observed.
Answer: "
"What does SYNTACC stand for from Alexander Waibel's paper?
","['alexander waibel_papers.txt', 'shinji watanabe_d24d60719e90e69749a75c160cb760d1d9fca44a_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What does SYNTACC stand for from Alexander Waibel's paper?

Context: List of 2023 Open Access papers by alexander waibel are:
AdapITN: A Fast, Reliable, and Dynamic Adaptive Inverse Text Normalization
Train Global, Tailor Local: Minimalist Multilingual Translation into Endangered Languages
Towards Efficient Simultaneous Speech Translation: CUNI-KIT System for Simultaneous Track at IWSLT 2023
SYNTACC : Synthesizing Multi-Accent Speech By Weight Factorization
KIT’s Multilingual Speech Translation System for IWSLT 2023
Convoifilter: A case study of doing cocktail party speech recognition
Continually learning new languages
Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff
End-to-End Evaluation for Low-Latency Simultaneous Speech Translation
FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN
Incremental Learning of Humanoid Robot Behavior from Natural Interaction and Large Language Models
Title: Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff
Authors: Peter Polák, Brian Yan, Shinji Watanabe, Alex Waibel, Ondřej Bojar
Section: 6. References
[1] C. Fügen, A. Waibel, and M. Kolss, “Simultaneous translation of lectures and speeches,” Machine translation, vol. 21, pp. 209– 252, 2007. [2] A. Anastasopoulos et al., “FINDINGS OF THE IWSLT 2021 EVALUATION CAMPAIGN,” in Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021), 2021, pp. 1–29. [3] A. Anastasopoulos et al., “FINDINGS OF THE IWSLT 2022 EVALUATION CAMPAIGN,” in Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022), 2022. [4] M. Ma et al., “Stacl: Simultaneous translation with implicit anticipation and controllable latency using prefix-to-prefix framework,” in Proc. ACL, 2019, pp. 3025–3036. [5] X. Ma, J. Pino, and P. Koehn, “SimulMT to SimulST: Adapting simultaneous text translation to end-to-end simultaneous speech translation,” in Proc. ACL, 2020, pp. 582–587. [6] D. Liu, G. Spanakis, and J. Niehues, “Low-Latency Sequenceto-Sequence Speech Recognition and Translation by Partial Hypothesis Selection,” in Proc. Interspeech, 2020, pp. 3620–3624. [7] P. Polák et al., “CUNI-KIT System for Smultaneous Speech Translation Task at IWSLT 2022,” in Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022), 2022. [8] S. Watanabe et al., “Hybrid ctc/attention architecture for endto-end speech recognition,” IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp.
Answer: "
"In spring 2024, What is the title of course 15210?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, What is the title of course 15210?

Context: In Semester Spring 2024, from the department of Computer Science, the subject titled 'Parallel and Sequential Data Structures and Algorithms' with Course ID 15210 and Section I offers 12.0 units. The Class meets Tuesday between 02:00PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Acar, Sleator located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Parallel and Sequential Data Structures and Algorithms' with Course ID 15210 and Section J offers 12.0 units. The Class meets Tuesday between 02:00PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Acar, Sleator located in Building GHC, Room 5208.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Parallel and Sequential Data Structures and Algorithms' with Course ID 15210 and Section K offers 12.0 units. The Class meets Tuesday between 03:00PM and 03:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Acar, Sleator located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Parallel and Sequential Data Structures and Algorithms' with Course ID 15210 and Section L offers 12.0 units. The Class meets Tuesday between 04:00PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Acar, Sleator located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Parallel and Sequential Data Structures and Algorithms' with Course ID 15210 and Section W offers 12.0 units. The Class meets Sunday Tuesday Thursday between 10:00AM and 11:15AM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Oflazer located in Building CMB, Room 1202.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Introduction to Computer Systems' with Course ID 15213 and Section NA offers 12.0 units.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Parallel and Sequential Data Structures and Algorithms' with Course ID 15210 and Section I offers 12.0 units. The Class meets Tuesday between 02:00PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Acar, Sleator located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Parallel and Sequential Data Structures and Algorithms' with Course ID 15210 and Section J offers 12.0 units. The Class meets Tuesday between 02:00PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Acar, Sleator located in Building GHC, Room 5208.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Parallel and Sequential Data Structures and Algorithms' with Course ID 15210 and Section K offers 12.0 units. The Class meets Tuesday between 03:00PM and 03:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Acar, Sleator located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Parallel and Sequential Data Structures and Algorithms' with Course ID 15210 and Section L offers 12.0 units. The Class meets Tuesday between 04:00PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Acar, Sleator located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Parallel and Sequential Data Structures and Algorithms' with Course ID 15210 and Section W offers 12.0 units. The Class meets Sunday Tuesday Thursday between 10:00AM and 11:15AM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Oflazer located in Building CMB, Room 1202.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Introduction to Computer Systems' with Course ID 15213 and Section NA offers 12.0 units.
Answer: "
"What does FACTORCL mean?
","['louis philippe morency_e1b2a35a000ca296c32284b323c7e36a28fe0693_metadata.txt', 'louis philippe morency_e1b2a35a000ca296c32284b323c7e36a28fe0693_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What does FACTORCL mean?

Context: Faculty Name: louis philippe morency
Paperid: e1b2a35a000ca296c32284b323c7e36a28fe0693
Title: Factorized Contrastive Learning: Going Beyond Multi-view Redundancy
Year: 2023
Abstract: In a wide range of multimodal tasks, contrastive learning has become a particularly appealing approach since it can successfully learn representations from abundant unlabeled data with only pairing information (e.g., image-caption or video-audio pairs). Underpinning these approaches is the assumption of multi-view redundancy - that shared information between modalities is necessary and sufficient for downstream tasks. However, in many real-world settings, task-relevant information is also contained in modality-unique regions: information that is only present in one modality but still relevant to the task. How can we learn self-supervised multimodal representations to capture both shared and unique information relevant to downstream tasks? This paper proposes FactorCL, a new multimodal representation learning method to go beyond multi-view redundancy. FactorCL is built from three new contributions: (1) factorizing task-relevant information into shared and unique representations, (2) capturing task-relevant information via maximizing MI lower bounds and removing task-irrelevant information via minimizing MI upper bounds, and (3) multimodal data augmentations to approximate task relevance without labels. On large-scale real-world datasets, FactorCL captures both shared and unique information and achieves state-of-the-art results on six benchmarks
Authors: P. Liang, Zihao Deng, Martin Q. Ma, James Y. Zou, Louis-Philippe Morency, R. Salakhutdinov
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'FactorCL is a new multimodal representation learning method to go beyond multi-view redundancy and captures both shared and unique information and achieves state-of-the-art results on six benchmarks.'}
Url: http://arxiv.org/pdf/2306.05268
Title: FACTORIZED CONTRASTIVE LEARNING: Going Beyond Multi-view Redundancy
Authors: Paul Pu Liang, Zihao Deng, Martin Q. Ma, James Zou, Louis-Philippe Morency, Ruslan Salakhutdinov
Section: D.3 Additional analysis and results
Fusion experiments: In Table 6 and 7 we present more detailed results on the Multibench [44] and IRFL [87] datasets computed from 5 independent runs. FACTORCL significantly outperforms the baselines that do not capture both shared and unique information in both supervised and selfsupervised settings, particularly on MUSTARD (where unique information expresses sarcasm, such as sardonic facial expressions or ironic tone of voice), and on MIMIC (with unique health indicators and sensor readings). There are also big improvements on the two sentiment analysis datasets MOSEI and MOSI, with 6.8% and 21.9% increases respectively when compared to SupCon [40]. In Table 7, we also see that FACTORCL substantially improves the state-of-the-art in classifying images and figurative captions which are not literally descriptive of the image on IRFL, outperforming zero-shot and fine-tuned CLIP [60] as well as continued pre-training baselines on top of CLIP. While the supervised version gives the best results overall, the self-supervised version with our proposed unique augmentations also performs better than independent augmentations, indicating that in the case without label information, we should always try to find and use unique augmentations when possible. In our experiments, we use word masking for text augmentations. For independent image augmentations, we use cropping, flipping, and color jittering. The unique augmentation simply removes the cropping operation, as illustrated in Figure 4 in the main text. Additional experiments on high shared information and low unique information: In Table 8 we include additional results using our method on the CIFAR10 [42] and MNIST [19] datasets. Our method outperforms the self-supervised contrastive learning on both datasets as expected, and roughly maintains the same performance as supervised contrastive learning. Therefore, in cases with abundant shared information (two modalities with high shared information or two different views generated from augmentations), our method recovers the performance of existing methods that do not capture unique information.
Answer: "
"What is the title for 11737?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the title for 11737?

Context: In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Linguistics Lab' with Course ID 11723 and Section A offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin, Mortensen located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Human Language for Artificial Intelligence' with Course ID 11724 and Section A offers 12.0 units. The Class meets Monday Wednesday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin located in Building WEH, Room 5409.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Meaning in Language Lab (Self Paced)' with Course ID 11726 and Section A offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rose located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Multilingual Natural Language Processing.' with Course ID 11737 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Li located in Building DH, Room 1212.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Machine Learning for Text and Graph-based Mining' with Course ID 11741 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Yang located in Building HH, Room B131.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Machine Learning for Text and Graph-based Mining' with Course ID 11741 and Section PP offers 12.0 units. The Class meets Schedule will be added between NA and NA ET.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Linguistics Lab' with Course ID 11723 and Section A offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin, Mortensen located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Human Language for Artificial Intelligence' with Course ID 11724 and Section A offers 12.0 units. The Class meets Monday Wednesday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin located in Building WEH, Room 5409.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Meaning in Language Lab (Self Paced)' with Course ID 11726 and Section A offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rose located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Multilingual Natural Language Processing.' with Course ID 11737 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Li located in Building DH, Room 1212.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Machine Learning for Text and Graph-based Mining' with Course ID 11741 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Yang located in Building HH, Room B131.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Machine Learning for Text and Graph-based Mining' with Course ID 11741 and Section PP offers 12.0 units. The Class meets Schedule will be added between NA and NA ET.
Answer: "
"In fall 2024, When are Mid-Semester & Mini-1 grades due by 4 pm?
","['Meta-Data-Calendar-2425.txt', 'Meta-Data-Calendar-2324.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2024, When are Mid-Semester & Mini-1 grades due by 4 pm?

Context: Day: 'Friday', Event: 'Mini-1 voucher election deadline (4)', Semester: 'Fall 2024 (F24)'
Date: '2024-10-12', Day: 'Saturday', Event: 'Mini-1 Exams', Semester: 'Fall 2024 (F24)'
Date: '2024-10-12', Day: 'Saturday', Event: 'Mini-1 Faculty Course Evaluations close', Semester: 'Fall 2024 (F24)'
Start Date: '2024-10-14', End Date: '2023-10-18', Days: 'Monday to Friday', Event: 'Fall Break; No Classes', Semester: 'Fall 2024 (F24)'
Date: '2024-10-21', Day: 'Monday', Event: 'Mini-2 Classes Begin ', Semester: 'Fall 2024 (F24)'
Date: '2024-10-23', Day: 'Wednesday', Event: 'Mid-Semester & Mini-1 grades due by 4 pm', Semester: 'Fall 2024 (F24)'
Date: '2024-10-25', Day: 'Friday', Event: 'Mini-2 add, audit & tuition adjustment drop deadline (1)', Semester: 'Fall 2024 (F24)'
Date: '2024-11-05', Day: 'Tuesday', Event: 'Democracy Day; No Classes, except Evening classes after 5 pm will still meet', Semester: 'Fall 2024 (F24)'
Date: '2024-11-11', Day: 'Monday', Event: 'Semester pass/no pass & withdrawal deadline (3)', Semester: 'Fall 2024 (F24)'
Date: '2024-11-13', Day: 'Wednesday', Event: 'Mini-2 drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Fall 2024 (F24)'
Start Date: '2024-11-18', End Date: '2024-11-22', Days: 'Monday to Friday', Event: 'Spring 2025 Registration Week', Semester: 'Fall 2024 (F24)'
Date: '2024-11-25', Day: 'Monday', Event: 'Mini-2 pass/no pass & withdrawal deadline (3)', Semester: 'Fall 2024 (F24)'
Date: '2024-11-25', Day: 'Monday',
Day: 'Saturday', Event: 'Mini-1 Exams', Semester: 'Fall 2023 (F23)'
Date: '2023-10-14', Day: 'Saturday', Event: 'Mini-1 Faculty Course Evaluations close', Semester: 'Fall 2023 (F23)'
Start Date: '2023-10-16', End Date: '2023-10-20', Days: 'Monday to Friday', Event: 'Fall Break; No Classes', Semester: 'Fall 2023 (F23)'
Date: '2023-10-23', Day: 'Monday', Event: 'Mini-2 Classes Begin ', Semester: 'Fall 2023 (F23)'
Date: '2023-10-23', Day: 'Monday', Event: 'Mid-Semester & Mini-1 grades due by 4 pm', Semester: 'Fall 2023 (F23)'
Date: '2023-10-27', Day: 'Friday', Event: 'Mini-2 add, audit & tuition adjustment drop deadline (1)', Semester: 'Fall 2023 (F23)'
Date: '2023-11-07', Day: 'Tuesday', Event: 'Democracy Day; No Classes, except Evening classes after 5 pm will still meet', Semester: 'Fall 2023 (F23)'
Date: '2023-11-11', Day: 'Saturday', Event: 'Homecoming', Semester: 'Fall 2023 (F23)'
Start Date: '2023-11-13', End Date: '2023-11-17', Days: 'Monday to Friday', Event: 'Spring 2024 Registration Week', Semester: 'Fall 2023 (F23)'
Date: '2023-11-13', Day: 'Monday', Event: 'Semester pass/no pass & withdrawal deadline (3)', Semester: 'Fall 2023 (F23)'
Date: '2023-11-15', Day: 'Wednesday', Event: 'Mini-2 drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Fall 2023 (F23)'
Start Date: '2023-11-22', End Date: '2023-11-24', Days: 'Wednesday to Friday', Event: 'Thanksgiving Break; No Classes', Semester: 'Fall 2023 (F23)'
Date: '2023-11-27', Day: 'Monday',
Answer: "
"Which paper proposed style radiance fields?
","['eric xing_8cc1cd002bfc36a8cba8bcbe63d32eacc656097f_metadata.txt', 'eric xing_papers.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which paper proposed style radiance fields?

Context: Faculty Name: eric xing
Paperid: 8cc1cd002bfc36a8cba8bcbe63d32eacc656097f
Title: StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields
Year: 2023
Abstract: 3D style transfer aims to render stylized novel views of a 3D scene with multiview consistency. However, most existing work suffers from a three-way dilemma over accurate geometry reconstruction, high-quality stylization, and being generalizable to arbitrary new styles. We propose StyleRF (Style Radiance Fields), an innovative 3D style transfer technique that resolves the three-way dilemma by performing style transformation within the feature space of a radiance field. StyleRF employs an explicit grid of high-level features to represent 3D scenes, with which highfidelity geometry can be reliably restored via volume rendering. In addition, it transforms the grid features according to the reference style which directly leads to high-quality zero-shot style transfer. StyleRF consists of two innovative designs. The first is sampling-invariant content transformation that makes the transformation invariant to the holistic statistics of the sampled 3D points and accordingly ensures multi-view consistency. The second is deferred style transformation of 2D feature maps which is equivalent to the transformation of 3D points but greatly reduces memory footprint without degrading multi-view consistency. Extensive experiments show that StyleRF achieves superior 3D stylization quality with precise geometry reconstruction and it can generalize to various new styles in a zero-shot manner. Project website: https://kunhao-liu.github.io/StyleRF/
Authors: Kunhao Liu, Fangneng Zhan, Yiwen Chen, Jiahui Zhang, Yingchen Yu, Abdulmotaleb El Saddik, Shijian Lu, E. Xing
Venue: Computer Vision and Pattern Recognition
Tldr: {'model': 'tldr@v2.0.0', 'text': 'StyleRF (Style Radiance Fields), an innovative 3D style transfer technique that resolves the three-way dilemma by performing style transformation within the feature space of a radiance field, achieves superior 3D stylization quality with precise geometry reconstruction and it can generalize to various new styles in a zero-shot manner.'}
Url: https://arxiv.org/pdf/2303.10598
List of 2023 Open Access papers by eric xing are:
SlimPajama-DC: Understanding Data Combinations for LLM Training
Fusing Models with Complementary Expertise
Making Scalable Meta Learning Practical
3D Open-vocabulary Segmentation with Foundation Models
Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective
One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning
Defending Against Malicious Behaviors in Federated Learning with Blockchain
Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming
Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models
KD-DLGAN: Data Limited Image Generation via Knowledge Distillation
3D Semantic Segmentation in the Wild: Learning Generalized Models for Adverse-Condition Point Clouds
Cuttlefish: Low-Rank Model Training without All the Tuning
Does compressing activations help model parallel training?
Federated Learning as Variational Inference: A Scalable Expectation Propagation Approach
Memory-adaptive Depth-wise Heterogenous Federated Learning
Identification of Nonlinear Latent Hierarchical Models
StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields
Judging LLM-as-a-judge with MT-Bench and Chatbot Arena
Understanding Masked Autoencoders via Hierarchical Latent Variable Models
LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers
LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset
FedNAR: Federated Optimization with Normalized Annealing Regularization
US residents' preferences for sharing of electronic health record and genetic information: a discrete choice experiment.
Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning
GET: a foundation model of transcription across human cell types
Contextualized Networks Reveal Heterogeneous Transcriptomic Regulation in Tumors at Sample-Specific Resolution
Research on the Training Path of Live E-commerce Talents Oriented by Industry Development
Recent progresses on the gamma-ray observations of DAMPE
Effective Multi-Agent Deep Reinforcement Learning Control with Relative Entropy Regularization
Convolutional Neural Network Measurement of Non-Fiducial Electrons Cosmic-Rays Using the DAMPE Experiment.
Answer: "
"What professor was the final author on the paper titled ""Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models""?
","['emma strubell_88549b4f48b9709acdfb8b9e41656b6d133c5390_content_0.txt', 'emma strubell_88549b4f48b9709acdfb8b9e41656b6d133c5390_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What professor was the final author on the paper titled ""Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models""?

Context: Title: Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models
Authors: Harnoor Dhingra, Preetiha Jayashanker, Sayali Moghe, Emma Strubell
Section: Acknowledgements
We express our gratitude to Dr. Maarten Sap for his invaluable support and constructive feedback provided during the development of this work. We would also like to thank Athiya Deviyani for her insights which helped shape the final outcome of this undertaking.
Title: Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models
Authors: Harnoor Dhingra, Preetiha Jayashanker, Sayali Moghe, Emma Strubell
Section: 7 Discussion
need of trained classifier which is able to detect linguistic differences between source and target styles.
Answer: "
"Which invention by Professor Luis von Ahn was named Apple’s 2013 app of the year? 
","['fact_sheet_d407.txt', '25things_d400.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which invention by Professor Luis von Ahn was named Apple’s 2013 app of the year? 

Context: 13%  International
SCHOOLS AND COLLEGES
College of Engineering 
College of Fine Arts
Dietrich College of Humanities  
and Social Sciences
Heinz College of Information Systems 
and Public Policy
Mellon College of Science
School of Computer Science
Tepper School of Business
cmu.edu/assets/pdfs/cmu-fact-sheet.pdf    |   cmu.edu/about/awards.html
March 2020
BIG IDEAS THAT SHAPE YOUR WORLD START HERE
1,000+ companies greenlighted 
across the U.S. and internationally 
Cognitive tutors, which improve 
the math skills of 500,000+ 
students each year 
The first U.S. school to award  
a degree in drama in 1914
James Gosling (SCS 1983) 
invented the Java computer 
programming language.
The first exclusive higher 
education partner of the Tony 
Awards®, co-creating the first, 
national recognition program 
honoring K-12 theatre educators 
The first smile in an email, created 
in 1982 by professor Scott Fahlman
Kevlar Fiber, invented by alumna 
Stephanie Kwolek (MM 1946)
The Last Lecture, written  
by alumni Randy Pausch (SCS 1988)  
and Jeffrey Zaslow (DC 1980)
In 1919, CMU’s first Ph.D. was 
awarded to Mao Yisheng, father  
of Chinese bridge construction. 
Duolingo, invented by professor  
Luis von Ahn (CS 2003, 2005), had  
10 million downloads in 12 months 
and was named Apple’s 2013 app  
of the year.
:-)
LEADERSHIP
Farnam Jahanian  
President
David Coulter 
Chair, Board of Trustees
James H. Garrett Jr.
At the core of all modern Apple devices --- including iPhones, iPads and MacBooks --- is the Mach kernel, developed at CMU under the leadership of then-professor Rick Rashid. 11. Computer chess, 1990Could a computer play chess at the level of the world’s best players? For many years, it was considered the “holy grail” of artificial intelligence. Hitech, developed by CMU researcher Hans Berliner (CS’74), was the first computer to achieve grandmaster status. CMU alumni played key roles in developing “Deep Blue,” the IBM machine that beat human chess champion Garry Kasparov in 1997. 12. Java, 1991 As a CMU grad student, James Gosling (CS’83) worked on the Andrew project, which stressed interoperability between computers, whether they were Macs, IBMs or Unix machines. Those lessons served Gosling well when he developed Java, the first programming language able to run on almost any platform. 13. Email attachments, 1992Steve Jobs liked the email system built into CMU’s Andrew so much that he tried to hire Nathaniel Borenstein (CS’81,’85) and the rest of his team to create a similar program for Apple. Borenstein didn’t take the offer, but he did like Jobs’ idea about attaching documents to email. Borenstein went on to develop the MIME standard that’s used by all email programs to send photos and other files over the Internet. 14. Web search engines, 1994The World Wide Web was still in its toddler stage when CMU researcher Michael “Fuzzy” Mauldin (CS’83,’89) developed one of the first successful search engines, Lycos. It was the most-visited site on the Web by 1999. 15. Model checking, 1994CMU professor Edmund Clarke had long stressed the importance of verifying computer hardware and software through a formal problem-solving technique called “model checking.” In 1994, his arguments gained new weight with the discovery that Intel’s amazing new Pentium chip made errors on certain math problems. Clarke would go on to receive the Turing Award for his role in the development of model checking. 16.
Answer: "
"When is the semester drop deadline for the Fall 2024 semester?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When is the semester drop deadline for the Fall 2024 semester?

Context: On Friday, 25 October, 2024, during the Fall 2024 (F24) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
On Tuesday, 05 November, 2024, during the Fall 2024 (F24) semester, Democracy Day is observed, leading to No Classes, except Evening classes after 5 pm will still meet.
On Monday, 11 November, 2024, during the Fall 2024 (F24) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Wednesday, 13 November, 2024, during the Fall 2024 (F24) semester, Mini-2 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 18 November, 2024,Monday to 22 November, 2024,Friday marks the Spring 2025 Registration Week for Fall 2024 (F24) semester.
On Monday, 25 November, 2024, during the Fall 2024 (F24) semester, Mini-2 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 25 November, 2024, during the Fall 2024 (F24) semester, Semester & Mini-2 Faculty Course Evaluations open is observed.
From 27 November, 2024,Wednesday to 29 November, 2023,Friday marks the Thanksgiving Break for Fall 2024 (F24) semester which leads to No Classes.
On Friday, 06 December, 2024, during the Fall 2024 (F24) semester, Semester & Mini-2 Last Day of Classes is observed.
On Friday, 06 December, 2024, during the Fall 2024 (F24) semester, Semester & Mini-2 voucher deadline (4) is observed.
From 09 December, 2024,Monday to 10 December, 2023,Tuesday marks the Final Exams for Fall 2024 (F24) semester.
On Monday, 11 December, 2024, during the Fall 2024 (F24) semester, Reading Day is observed.
From 12 December, 2024,Thursday to 13 December, 2023,Friday marks the Final Exams for Fall 2024 (F24) semester.
On Monday, 02 September, 2024, during the Fall 2024 (F24) semester, Labor Day is observed, leading to No Classes & University Closed.
On Monday, 09 September, 2024, during the Fall 2024 (F24) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 18 September, 2024, during the Fall 2024 (F24) semester, Mini-1 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 30 September, 2024, during the Fall 2024 (F24) semester, Mini-1 Pass/no pass & withdrawal deadline (3) is observed.
On Monday, 30 September, 2024, during the Fall 2024 (F24) semester, Mini-1 Faculty Course Evaluations open is observed.
On Monday, 07 October, 2024, during the Fall 2024 (F24) semester, Semester drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 11 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Last Day of Classes is observed.
On Friday, 11 October, 2024, during the Fall 2024 (F24) semester, Mini-1 voucher election deadline (4) is observed.
On Saturday, 12 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Exams is observed.
On Saturday, 12 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Faculty Course Evaluations close is observed.
From 14 October, 2024,Monday to 18 October, 2023,Friday marks the Fall Break for Fall 2024 (F24) semester which leads to No Classes.
On Monday, 21 October, 2024, during the Fall 2024 (F24) semester, Mini-2 Classes Begin is observed.
On Wednesday, 23 October, 2024, during the Fall 2024 (F24) semester, Mid-Semester & Mini-1 grades due by 4 pm is observed.
On Friday, 25 October, 2024, during the Fall 2024 (F24) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
Answer: "
"What number do all of the Biological Sciences classes start with?
","['metadata_course_fall_23.txt', 'combined_metadata_final.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What number do all of the Biological Sciences classes start with?

Context: In Semester Fall 2023, from the department of Biological Sciences, the subject titled 'Virology' with Course ID 03380 and Section W offers 9.0 units. The Class meets Sunday Tuesday Thursday between 02:30PM and 03:20PM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Younis located in Building CMB, Room 3044.
In Semester Fall 2023, from the department of Biological Sciences, the subject titled 'Undergraduate Colloquium for Seniors' with Course ID 03401 and Section A offers 1-3 units. The Class meets Thursday between 10:00AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Willard located in Building GHC, Room 4307.
In Semester Fall 2023, from the department of Biological Sciences, the subject titled 'Topics in Research' with Course ID 03411 and Section A offers 1-2 units. The Class meets Wednesday between 12:00PM and 12:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Linstedt located in Building MI, Room 348.
In Semester Fall 2023, from the department of Biological Sciences, the subject titled 'Topics in Research' with Course ID 03411 and Section W offers 1-2 units. The Class meets Monday between 01:15PM and 02:15PM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Bouaouina located in Building CMB, Room 3044.
In Semester Fall 2023, from the department of Biological Sciences, the subject titled 'Molecular Biology' with Course ID 03442 and Section A offers 9.0 units. The Class meets Monday Wednesday Friday between 09:00AM and 09:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Woolford located in Building DH, Room 1212.
In Semester Fall 2023, from the department of Biological Sciences, the subject titled 'Undergraduate Research' with Course ID 03445 and Section NA offers 1-18 units. The Class meets Schedule will be added between NA and NA ET.
In Semester Spring 2024, from the department of Biological Sciences, the subject titled 'Biology for Life Special Topics Micro:' with Course ID 03119 and Section NA offers 3.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Biological Sciences, the subject titled 'Biopharma Protein-Based Pharmaceuticals' with Course ID 03119 and Section X4 offers 3.0 units. The Class meets Monday Wednesday between 06:00PM and 07:30PM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Rule located in Building CMB, Room 1031.
In Semester Spring 2024, from the department of Biological Sciences, the subject titled 'Modern Biology' with Course ID 03121 and Section B offers 9.0 units. The Class meets Monday Wednesday Friday between 09:00AM and 09:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Wong-Noonan, Wisniewski located in Building DH, Room 2315.
In Semester Spring 2024, from the department of Biological Sciences, the subject titled 'Modern Biology Laboratory' with Course ID 03124 and Section NA offers 9.0 units. The Class meets Wednesday between 11:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Laperuta, Doonan located in Building WEH, Room 5421.
In Semester Spring 2024, from the department of Biological Sciences, the subject titled 'Modern Biology Laboratory' with Course ID 03124 and Section A offers 9.0 units. The Class meets Wednesday between 06:00PM and 09:00PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Laperuta, Doonan located in Building DH, Room 2303.
In Semester Spring 2024, from the department of Biological Sciences, the subject titled 'Modern Biology Laboratory' with Course ID 03124 and Section B offers 9.0 units. The Class meets Wednesday between 01:00PM and 03:50PM ET.
Answer: "
"According to the paper PWESUITE: Phonetic Word Embeddings and Tasks They Facilitate, what is the percentage accuracy for analogies, achieved by the count-based model on the evaluation suite?
","['david mortensen_db14d05b18ec852f8afcd6d2d10bbd9eeaef8325_metadata.txt', 'bhiksha raj_4e6a004e4f9a3b489004a2efb5b25b0bcd0f48b5_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: According to the paper PWESUITE: Phonetic Word Embeddings and Tasks They Facilitate, what is the percentage accuracy for analogies, achieved by the count-based model on the evaluation suite?

Context: Faculty Name: david mortensen
Paperid: db14d05b18ec852f8afcd6d2d10bbd9eeaef8325
Title: PWESuite: Phonetic Word Embeddings and Tasks They Facilitate
Year: 2023
Abstract: Mapping words into a fixed-dimensional vector space is the backbone of modern NLP. While most word embedding methods successfully encode semantic information, they overlook phonetic information that is crucial for many tasks. We develop three methods that use articulatory features to build phonetically informed word embeddings. To address the inconsistent evaluation of existing phonetic word embedding methods, we also contribute a task suite to fairly evaluate past, current, and future methods. We evaluate both (1) intrinsic aspects of phonetic word embeddings, such as word retrieval and correlation with sound similarity, and (2) extrinsic performance on tasks such as rhyme and cognate detection and sound analogies. We hope our task suite will promote reproducibility and inspire future phonetic embedding research.
Authors: Vilém Zouhar, Kalvin Chang, Chenxuan Cui, Nathaniel Carlson, Nathaniel R. Robinson, Mrinmaya Sachan, David R. Mortensen
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'Three methods that use articulatory features to build phonetically informed word embeddings are developed that address the inconsistent evaluation of existing phonetic word embedding methods and contribute a task suite to fairly evaluate past, current, and future methods.'}
Url: http://arxiv.org/pdf/2304.02541
Title: Understanding Political Polarisation using Language Models: A dataset and method
Authors: Samiran Gode, Supreeth Bare, Bhiksha Raj, Hyungon Yoo
Section: Related Work
use of diachronic-word embedding association tests (WEAT). Other techniques that are implemented include count-based statistics dependent on a highly popular lexicon cloze test using BERT as a base model (an idea we could consider after data attention) and bias recognition using WEAT. The final model is a combination of the above three. This paper is highly relevant to our project as it uses a similar idea of our own. It uses aforementioned models to predict bias, i.e. sentiment prediction. In our project, we use data to predict political sentiment and attempt to classify certain features as being precursors to classification. (Rajani et al. 2019) tried to improve speech-based models on their ability to verbalize the reasoning that they learned during training. It uses the CAGE framework (CommonSense Auto-Generated Explanations) on the common sense explanation dataset to increase the effectiveness by 10 percent. It introduces improvements over the use of BiDAF++ (augmented with self-attention layer) in these newer models. It further uses NLE as rationale generalization within the second phase primarily as means for sentiment analysis. In this paper, Mturk (from Amazon) is used to generate explanations for the dataset. CAGE primarily uses a questionanswer format with 3 options, a label and the best explanation for that label. Furthermore, other evaluation parameters affecting performance are tested and may be used in our project either as verification models or otherwise. CAGE is certainly an interesting choice for verification given the higher accuracy it attains. A factor to be considered however is that the types of datasets and models are very different. Thus certain modifications will be made to the above framework. (Devlin et al. 2018) is the introduction paper for BERT, a model that will be used extensively. It also shows the results of fine-tuning BERT. These indirectly or directly will be used either as pre-trained constraints or as tuning methods. petroni2019language (Petroni et al. 2019) Demonstrates the ability of pretrained high-capacity models like BERT and ELMo to be used as knowledge repositories.
Answer: "
"What is the PhD program director for LTI's phone number?
","['handbook_phd_2023-2024.txt', 'mlt-student-handbook-2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the PhD program director for LTI's phone number?

Context: degree are: 
Jamie Callan 
Ph.D. Program Director 
Professor 
GHC 5419 
callan@cs.cmu.edu 
412-268-4525 
Stacey Young 
Ph.D. Academic Program Manager 
LTI Graduate Program Manager 
GHC 6415 
staceyy@cs.cmu.edu  
412-268-2623 
Mona Diab 
LTI Director 
Professor 
GHC 5723 
mdiab@andrew.cmu.edu 
412-268-3669 
 
Joan Axelson 
Office Manager 
GHC 5405 
jaxelson@andrew.cmu.edu 
412-268-7517 
 
Julie Nys 
Employment Processes Manger 
GHC 5405 
jnys@andrew.cmu.edu 
412-268-3515 
1.3 
University Policies and Expectations 
It is the responsibility of each member of the Carnegie Mellon community to be familiar with 
university policies and guidelines. In addition to this departmental graduate student handbook, 
the following resources are available to assist you in understanding community expectations. 
 The Word/Student Handbook: 
https://www.cmu.edu/student-affairs/theword//index.html 
 Academic Integrity Policy: 
 
https://www.cmu.edu/policies/student-and- student-life/academic-
integrity.html 
LTI Ph.D. Graduate Student Handbook 
Page 11 
 
 University Policies Website:  
https://www.cmu.edu/policies/ 
 Office of Graduate and Postdoctoral Affairs: 
https://www.cmu.edu/graduate/policies/index.html 
Due to the changing nature of conditions and expectations surrounding public health and safety 
requirements please visit https://www.cmu.edu/coronavirus/ for the most up to date information.
 
Please see Appendix A for additional information about The Word and University 
resources. 
1.4 
The Academic Calendar  
The Academic calendar can be found at https://www.cmu.edu/hub/calendar/index.html and 
provides information on all deadlines, including registration dates, class start dates, add/drop 
deadlines, exam dates, and more.  
Some doctoral course-sections follow a separate Academic Calendar.
LTIs printers 
are located in GHC 5404 and GHC 6604. The School of Computer Science provides a number of 
black-and-white and color printers for use by students.  The SCS Computer Facilities publishes a 
list of printers online at http://www.cs.cmu.edu/~help/printing/.  
 
MLT Graduate Student Handbook 
Page 11 
 
2.3 Office Space for MS Students 
To help them create a sense of community, full time students in the LTIs MLT program have 
access to a shared office space. 
2.4 Computers for MS Students 
Students are expected to provide their own laptop computers that can be used to access 
university resources and complete course assignments. Laptops running Windows, MacOS, and 
Linux software are all acceptable. 
MS students will be given a CS user ID. A CS user ID is required to use the LTI computer cluster, 
department printers, and other SCS services. The School of Computer Science has a Help Center 
located at 4203 GHC.  They can be contacted at help@cs.cmu.edu, extension 8-4231 from a 
campus phone, or 412-268-4231 from an outside line. 
MS students will be given access to the LTIs computer cluster on an as-needed basis, to be 
used for course assignments, directed study projects, and/or capstone projects. The LTI cluster 
provides storage and computation for projects involving large datasets and/or lengthy 
computation.  
3 Masters Degree Completion and Certification 
3.1 Standard Degree Requirements and Degree Certification 
3.1.1 Graduate Students 
Carnegie Mellon graduate students are expected to complete their degree requirements within 
the standard length of time for their program of study as outlined in the relevant Graduate 
Student Handbook. Standard program lengths for graduate students vary significantly-- ranging 
from two semesters for some full-time masters programs to several or more years for doctoral 
programs. Upon completion of the graduate program degree requirements, the degree will be 
certified by the students academic program in the semester in which the student completes 
the requirements. 
3.1.2 Early Completion 
Graduate students who consider the completion of all degree requirements in less than the 
standard length of time for their program of study may consult with their degree-granting 
program or department to determine if early degree certification is allowed and under what 
circumstances.
Answer: "
"Are there classes on April 11th, 2024?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Are there classes on April 11th, 2024?

Context: On Monday, 31 March, 2025, during the Spring 2025 (S25) semester, Semester pass/no pass & withdrawal deadline is observed.
On Wednesday, 02 April, 2025, during the Spring 2025 (S25) semester, Mini-4 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 03 April, 2025,Thursday to 05 April, 2023,Saturday marks the Spring Carnival for Spring 2025 (S25) semester which leads to No Classes.
From 07 April, 2025,Monday to 11 April, 2023,Friday marks the Fall 2025 Registration Week for Spring 2025 (S25) semester.
On Monday, 14 April, 2025, during the Spring 2025 (S25) semester, Mini-4 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 14 April, 2025, during the Spring 2025 (S25) semester, Semester & Mini-4 Faculty Course Evaluations open is observed.
On Friday, 25 April, 2025, during the Spring 2025 (S25) semester, Last Day of Classes is observed.
On Friday, 25 April, 2025, during the Spring 2025 (S25) semester, Semester & Mini-4 voucher deadline (4) is observed.
From 28 April, 2025,Monday to 29 April, 2023,Tuesday marks the Final Examinations for Spring 2025 (S25) semester.
On Wednesday, 30 April, 2025, during the Spring 2025 (S25) semester, Reading Day is observed.
From 01 May, 2025,Thursday to 02 May, 2023,Friday marks the Final Examinations for Spring 2025 (S25) semester.
From 03 May, 2025,Saturday to 04 May, 2023,Sunday marks the Reading Days for Spring 2025 (S25) semester.
On Monday, 05 May, 2025, during the Spring 2025 (S25) semester, Final Examinations is observed.
On Monday, 05 May, 2025, during the Spring 2025 (S25) semester, Semester & Mini-4 Faculty Course Evaluations close is observed.
On Tuesday, 12 March, 2024, during the Spring 2024 (S24) semester, Summer 2024 Registration Opens is observed.
On Friday, 15 March, 2024, during the Spring 2024 (S24) semester, Mini-4 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 01 April, 2024, during the Spring 2024 (S24) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Wednesday, 03 April, 2024, during the Spring 2024 (S24) semester, Mini-4 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 11 April, 2024,Thursday to 13 April, 2024,Saturday marks the Spring Carnival for Spring 2024 (S24) semester which leads to No Classes.
From 15 April, 2024,Monday to 19 April, 2024,Friday marks the Fall 2024 Registration Week for Spring 2024 (S24) semester.
On Monday, 15 April, 2024, during the Spring 2024 (S24) semester, Mini-4 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 15 April, 2024, during the Spring 2024 (S24) semester, Semester & Mini-4 Faculty Course Evaluations open is observed.
On Friday, 26 April, 2024, during the Spring 2024 (S24) semester, Last Day of Classes is observed.
On Friday, 26 April, 2024, during the Spring 2024 (S24) semester, Semester & Mini-4 voucher deadline (4) is observed.
From 29 April, 2024,Monday to 30 April, 2024,Tuesday marks the Final Examinations for Spring 2024 (S24) semester.
On Wednesday, 01 May, 2024, during the Spring 2024 (S24) semester, Reading Day is observed.
From 02 May, 2024,Thursday to 03 May, 2024,Friday marks the Final Examinations for Spring 2024 (S24) semester.
From 04 May, 2024,Saturday to 05 May, 2024,Sunday marks the Reading Days for Spring 2024 (S24) semester.
Answer: "
"The MLT program is similar to the first two years of what other program?
","['mlt-student-handbook-2023-2024.txt', 'mlt-student-handbook-2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: The MLT program is similar to the first two years of what other program?

Context: The proposal MUST be accepted by the committee by the end of 
October, or the student will not be allowed to enroll in 11-929  Masters Thesis II! 
 Sometime during the semester when the student enrolls in 11-929  Masters Thesis II 
(typically, their final semester), the student will distribute a draft of the thesis to the 
committee for initial review. This should be done as early as feasible, to avoid last-
minute surprises that could delay final approval of the thesis. 
 The thesis work culminates in submission of the final version of the thesis document, 
followed by a public presentation of the work in an LTI seminar (or other suitable public 
forum). Since the defense is public, the LTI graduate program administrator must 
receive all the information required for a public announcement at least one week before 
the defense. The Masters Thesis presentation is somewhat less rigorous than a PhD 
thesis defense. The presentation must communicate the research work done, similar to 
a conference paper presentation. The committee will observe the presentation, and 
then decide whether the thesis and presentation were acceptable, or whether further 
work is required. Unlike a PhD defense, only a simple majority vote of the committee is 
required for approval. 
 Although students are required to enroll in the appropriate course sequence of two 
Masters Thesis courses, it is not required that students finish the thesis by the end of 
that second semester. If a student requires more time to revise the thesis to the 
committee's satisfaction, and adequately present the work, an incomplete grade will be 
assessed in the Masters Thesis course, until such time as the work and presentation are 
accepted. The student will still be allowed to walk in Spring Commencement, if all other 
requirements for the MLT degree have been completed. Students should note that any 
financial support beyond the end of the semester will be on a case-by-case basis, and 
must be arranged in advance with the project supporting them. Students are strongly 
encouraged to finish the thesis work within one (1) year following the semester they 
enroll for the first Masters Thesis course.
In any presentation, creative, artistic or research, it is the ethical responsibility of each student 
to identify the conceptual sources of the work submitted. Failure to do so is dishonest and is 
the basis for a charge of cheating or plagiarism, which is subject to disciplinary action. 
 
The university has a very clear and specific protocol for responding to alleged violations of 
academic integrity. Carnegie Mellon's Academic Disciplinary Actions Overview for Graduate 
Students describes procedures for disciplinary actions against graduate students in cases of 
alleged violations of academic regulations and the appeal process.  Please see 
https://www.cmu.edu/student-affairs/theword/academic-discipline/ for more information. 
Important note:  The LTI implements the above policys option of conven[ing] a disciplinary 
hearing according to the procedures of the department/program. Our procedure is as follows: 
a first violation is grounds for dismissal from the graduate program. If we decide not to 
immediately dismiss, the first violation will result in the student being on disciplinary probation. 
If a student commits a second violation while on probation, the penalty is dismissal from the 
graduate program. 
5 Academic Policies 
5.1 MLT Academic Policies 
5.1.1 Duration of Study 
MLT students enrolled for full-time study are expected to complete the degree in two calendar 
years (24 months). This includes two summers of full-time directed research. 
See also the Statute of Limitations policy. 
5.1.2 Double-Dipping 
A Masters student who uses courses taken as part of another degree program (at Carnegie 
Mellon or elsewhere) toward their program requirements cannot use those same courses 
MLT Graduate Student Handbook 
Page 21 
 
toward any other M.S. degree offered by the School of Computer Science without prior 
approval. This is an SCS-wide policy. 
5.1.3 Pass/Fail Grades 
Pass/fail grades are not permitted for courses used to satisfy a degree requirement. Graduate 
students who are required to take additional undergraduate courses to build up the core 
foundations of computer science may not elect the pass/fail option for these courses. 
5.1.4 Independent Study 
For an Independent Study to satisfy an MLT students coursework requirements, it must be 
approved by the MLT Program Director in advance.
Answer: "
"In fall 2023, What are the units for unit 02402?
","['combined_metadata_final.txt', 'combined_metadata_final.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, What are the units for unit 02402?

Context: In Semester Fall 2023, from the department of Art, the subject titled 'Art History/Theory Independent Study' with Course ID 60399 and Section C offers 9.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Art, the subject titled 'Senior Review' with Course ID 60400 and Section A offers 0.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Cato, Jefferson located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Art, the subject titled 'Senior Studio' with Course ID 60401 and Section B offers 10.0 units. The Class meets Monday between 07:00PM and 09:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin located in Building CFA, Room 303.
In Semester Fall 2023, from the department of Art, the subject titled 'Senior Studio' with Course ID 60401 and Section NA offers 10.0 units. The Class meets Wednesday between 07:00PM and 09:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor NA located in Building CFA, Room 307.
In Semester Fall 2023, from the department of Art, the subject titled 'Senior Studio' with Course ID 60401 and Section C offers 10.0 units. The Class meets Tuesday Thursday between 07:00PM and 09:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Chodos located in Building CFA, Room 310.
In Semester Fall 2023, from the department of Art, the subject titled 'Advanced ETB: Animation Studio' with Course ID 60415 and Section A offers 10.0 units. The Class meets Monday Wednesday between 02:00PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Duesing located in Building CYH, Room 100A.
In Semester Fall 2023, from the department of Computer Science, the subject titled 'Distributed Systems' with Course ID 15440 and Section D offers 12.0 units. The Class meets Friday between 02:00PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Miller, Zheng located in Building BH, Room A36.
In Semester Fall 2023, from the department of Computer Science, the subject titled 'Distributed Systems' with Course ID 15440 and Section E offers 12.0 units. The Class meets Friday between 08:00AM and 08:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Miller, Zheng located in Building MM, Room 103.
In Semester Fall 2023, from the department of Computer Science, the subject titled 'Distributed Systems' with Course ID 15440 and Section W offers 12.0 units. The Class meets Thursday between 08:45AM and 09:35AM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Hammoud located in Building CMB, Room 3046.
In Semester Fall 2023, from the department of Computer Science, the subject titled 'Networking and the Internet' with Course ID 15441 and Section NA offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Sherry located in Building SH, Room 105.
In Semester Fall 2023, from the department of Computer Science, the subject titled 'Networking and the Internet' with Course ID 15441 and Section A offers 12.0 units. The Class meets Friday between 11:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Sherry located in Building POS, Room 152.
In Semester Fall 2023, from the department of Computer Science, the subject titled 'Networking and the Internet' with Course ID 15441 and Section B offers 12.0 units. The Class meets Friday between 12:00PM and 12:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Sherry located in Building PH, Room A18B.
Answer: "
"What are the two steps in the PaintSeg painting process?
","['bhiksha raj_dc157eba8bdb4cfe6ee65566d8295939ac5b4b37_metadata.txt', 'bhiksha raj_dc157eba8bdb4cfe6ee65566d8295939ac5b4b37_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the two steps in the PaintSeg painting process?

Context: Faculty Name: bhiksha raj
Paperid: dc157eba8bdb4cfe6ee65566d8295939ac5b4b37
Title: PaintSeg: Training-free Segmentation via Painting
Year: 2023
Abstract: The paper introduces PaintSeg, a new unsupervised method for segmenting objects without any training. We propose an adversarial masked contrastive painting (AMCP) process, which creates a contrast between the original image and a painted image in which a masked area is painted using off-the-shelf generative models. During the painting process, inpainting and outpainting are alternated, with the former masking the foreground and filling in the background, and the latter masking the background while recovering the missing part of the foreground object. Inpainting and outpainting, also referred to as I-step and O-step, allow our method to gradually advance the target segmentation mask toward the ground truth without supervision or training. PaintSeg can be configured to work with a variety of prompts, e.g. coarse masks, boxes, scribbles, and points. Our experimental results demonstrate that PaintSeg outperforms existing approaches in coarse mask-prompt, box-prompt, and point-prompt segmentation tasks, providing a training-free solution suitable for unsupervised segmentation.
Authors: Xiang Li, Chung-Ching Lin, Yinpeng Chen, Zicheng Liu, Jinglu Wang, B. Raj
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'An adversarial masked contrastive painting process, which creates a contrast between the original image and a painted image in which a masked area is painted using off-the-shelf generative models, providing a training-free solution suitable for unsupervised segmentation.'}
Url: http://arxiv.org/pdf/2305.19406
Title: PaintSeg: Training-free Segmentation via Painting
Authors: Xiang Li, Chung-Ching Lin, Yinpeng Chen, Zicheng Liu, Jinglu Wang
Section: G More Visualization
In this section, we demonstrate more visualization of PaintSeg. We show more qualitative results with box prompt in Fig. D, with point prompt in Fig. E and with coarse mask prompt in Figs. F and G. Figure D: More visualization of PaintSeg with box prompt on COCO MVal. Figure E: More visualization of PaintSeg with point prompt. The point prompt is illustrated by the red point on the image on DAVIS and Berkeley and GrabCut. Prompt PromptMask Mask Figure F: More visualization of PaintSeg with coarse mask prompt on ECSSD. Prompt PromptMask Mask Figure G: More visualization of PaintSeg with coarse mask prompt on ECSSD.
Answer: "
"What is the course name/title for CMU 03128? 
","['metadata_course_fall_23.txt', 'combined_metadata_final.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the course name/title for CMU 03128? 

Context: The Class meets Wednesday between 01:00PM and 04:00PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Doonan, Laperuta, Wisniewski located in Building DH, Room 2303.
In Semester Fall 2023, from the department of Biological Sciences, the subject titled 'Evolution' with Course ID 03125 and Section A offers 9.0 units. The Class meets Tuesday Thursday between 08:00AM and 09:20AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hinman, Laperuta located in Building WEH, Room 7500.
In Semester Fall 2023, from the department of Biological Sciences, the subject titled 'Evolution' with Course ID 03125 and Section W offers 9.0 units. The Class meets Monday Wednesday between 04:00PM and 05:15PM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Bouaouina located in Building CMB, Room 2052.
In Semester Fall 2023, from the department of Biological Sciences, the subject titled 'Biology for Life Special Topics' with Course ID 03128 and Section W offers 9.0 units. The Class meets Sunday Tuesday Thursday between 08:45AM and 09:35AM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Vincent located in Building CMB, Room 3044.
In Semester Fall 2023, from the department of Biological Sciences, the subject titled 'Human Health and Disease' with Course ID 03129 and Section W offers 9.0 units. The Class meets Sunday Tuesday between 04:00PM and 05:15PM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Affara located in Building CMB, Room 3044.
In Semester Fall 2023, from the department of Biological Sciences, the subject titled 'Structure and Function of the Human Body:' with Course ID 03135 and Section NA offers 9.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
The Class meets Sunday Tuesday Thursday between 08:45AM and 09:35AM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Vincent located in Building CMB, Room 3044.
In Semester Fall 2023, from the department of Biological Sciences, the subject titled 'Human Health and Disease' with Course ID 03129 and Section W offers 9.0 units. The Class meets Sunday Tuesday between 04:00PM and 05:15PM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Affara located in Building CMB, Room 3044.
In Semester Fall 2023, from the department of Biological Sciences, the subject titled 'Structure and Function of the Human Body:' with Course ID 03135 and Section NA offers 9.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Biological Sciences, the subject titled 'Structure and Function of the Human Body' with Course ID 03135 and Section A offers 9.0 units. The Class meets Monday Wednesday Friday between 02:00PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor D'Antonio located in Building HH, Room B131.
In Semester Fall 2023, from the department of Biological Sciences, the subject titled 'Ecology and Environmental Science' with Course ID 03140 and Section A offers 9.0 units. The Class meets Monday Wednesday Friday between 04:00PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Lanni located in Building WEH, Room 4623.
In Semester Fall 2023, from the department of Biological Sciences, the subject titled 'Honors Modern Biology' with Course ID 03151 and Section Lec 1 offers 10.0 units. The Class meets Monday Wednesday Friday between 03:00PM and 03:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Minden located in Building HH, Room B131.
Answer: "
"Who is the Employment Processes Manager for LTI
","['handbook_phd_2023-2024.txt', 'mlt-student-handbook-2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who is the Employment Processes Manager for LTI

Context: degree are: 
Jamie Callan 
Ph.D. Program Director 
Professor 
GHC 5419 
callan@cs.cmu.edu 
412-268-4525 
Stacey Young 
Ph.D. Academic Program Manager 
LTI Graduate Program Manager 
GHC 6415 
staceyy@cs.cmu.edu  
412-268-2623 
Mona Diab 
LTI Director 
Professor 
GHC 5723 
mdiab@andrew.cmu.edu 
412-268-3669 
 
Joan Axelson 
Office Manager 
GHC 5405 
jaxelson@andrew.cmu.edu 
412-268-7517 
 
Julie Nys 
Employment Processes Manger 
GHC 5405 
jnys@andrew.cmu.edu 
412-268-3515 
1.3 
University Policies and Expectations 
It is the responsibility of each member of the Carnegie Mellon community to be familiar with 
university policies and guidelines. In addition to this departmental graduate student handbook, 
the following resources are available to assist you in understanding community expectations. 
 The Word/Student Handbook: 
https://www.cmu.edu/student-affairs/theword//index.html 
 Academic Integrity Policy: 
 
https://www.cmu.edu/policies/student-and- student-life/academic-
integrity.html 
LTI Ph.D. Graduate Student Handbook 
Page 11 
 
 University Policies Website:  
https://www.cmu.edu/policies/ 
 Office of Graduate and Postdoctoral Affairs: 
https://www.cmu.edu/graduate/policies/index.html 
Due to the changing nature of conditions and expectations surrounding public health and safety 
requirements please visit https://www.cmu.edu/coronavirus/ for the most up to date information.
 
Please see Appendix A for additional information about The Word and University 
resources. 
1.4 
The Academic Calendar  
The Academic calendar can be found at https://www.cmu.edu/hub/calendar/index.html and 
provides information on all deadlines, including registration dates, class start dates, add/drop 
deadlines, exam dates, and more.  
Some doctoral course-sections follow a separate Academic Calendar.
All policies not explicitly described in this document conform to School of Computer Science 
(SCS) policies and university policies described in The Word, Carnegie Mellon University Student 
Handbook and at the University Policies website.   
 
 
 
 
 
MLT Graduate Student Handbook 
Page 7 
 
1.3 MLT Contact Information 
The people responsible for administering the MLT degree are: 
 
Kate Schaich  
 
 
 
Robert Frederking 
 
Program Manager, MLT 
 
 
Program Director, MLT 
 
Graduate Program Manager, LTI 
 
Principal Systems Scientist 
 
GHC 6415 
 
 
 
 
GHC 6515 
 
412-268-4788  
 
 
 
412-268-6656 
 
kschaich@cs.cmu.edu  
 
 
ref@cs.cmu.edu 
 
 
 
Robert Frederking 
 
 
 
Mona Diab 
 
Chair of Graduate Programs, LTI 
 
Director, LTI 
 
Principal Systems Scientist 
 
 
GHC 5415 
 
GHC 6515 
 
 
 
 
412-268-6656  
 
 
 
 
 
 
 
 
 
 
mdiab@andrew.cmu.edu 
 
 
 
 
 
In addition, students may confer with the Graduate Education Office 
(graded@andrew.cmu.edu) regarding issues of process or other concerns as they navigate 
conflicts. 
 
The Language Technologies Institute is located primarily on the 5th and 6th floors of the Gates 
Hillman Complex (GHC) on Carnegie Mellons Pittsburgh campus. The address and contact 
information of the LTI are: 
Language Technologies Institute 
Carnegie Mellon University 
5000 Forbes Avenue 
Gates Hillman Complex 5402, LTI 
Pittsburgh, PA 15241-3891 
 
412-268-6591 (phone) 
412-268-6298 (fax) 
 
http://www.lti.cs.cmu.edu/ 
 
 
 
 
 
MLT Graduate Student Handbook 
Page 8 
 
1.4 University Policies and Expectations 
It is the responsibility of each member of the Carnegie Mellon community to be familiar with 
university policies and guidelines.
Answer: "
"In spring 2024, What is the day and time of course 17413?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, What is the day and time of course 17413?

Context: From 04 May, 2024,Saturday to 05 May, 2024,Sunday marks the Reading Days for Spring 2024 (S24) semester.
On Monday, 06 May, 2024, during the Spring 2024 (S24) semester, Final Examinations is observed.
On Monday, 06 May, 2024, during the Spring 2024 (S24) semester, Semester & Mini-4 Faculty Course Evaluations close is observed.
On Tuesday, 07 May, 2024, during the Spring 2024 (S24) semester, Make-Up Final Examinations is observed.
On Wednesday, 08 May, 2024, during the Spring 2024 (S24) semester, Graduating Final Grades Due by 4 pm is observed.
On Sunday, 12 May, 2024, during the Spring 2024 (S24) semester, Commencement is observed.
On Tuesday, 14 May, 2024, during the Spring 2024 (S24) semester, Final Grades Due by 4 pm is observed.
On Tuesday, 21 May, 2023, during the Spring 2024 (S24) semester, Spring Deans' Lists Posted is observed.
On Monday, 13 May, 2024, during the Summer One 2024 (M24) semester, Semester & Mini-5 Classes Begin is observed.
On Friday, 17 May, 2024, during the Summer One 2024 (M24) semester, Mini-5 add, audit, & tuition adjustment drop deadline (1) is observed.
On Friday, 24 May, 2024, during the Summer One 2024 (M24) semester, Semester add, audit, & tuition adjustment drop deadline (1) is observed.
On Monday, 27 May, 2024, during the Summer One 2024 (M24) semester, Memorial Day is observed, leading to University Closed & No Classes.
On Tuesday, 28 May, 2024, during the Summer One 2024 (M24) semester, Mini-5 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 07 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 31 March, 2025, during the Spring 2025 (S25) semester, Semester pass/no pass & withdrawal deadline is observed.
On Wednesday, 02 April, 2025, during the Spring 2025 (S25) semester, Mini-4 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 03 April, 2025,Thursday to 05 April, 2023,Saturday marks the Spring Carnival for Spring 2025 (S25) semester which leads to No Classes.
From 07 April, 2025,Monday to 11 April, 2023,Friday marks the Fall 2025 Registration Week for Spring 2025 (S25) semester.
On Monday, 14 April, 2025, during the Spring 2025 (S25) semester, Mini-4 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 14 April, 2025, during the Spring 2025 (S25) semester, Semester & Mini-4 Faculty Course Evaluations open is observed.
On Friday, 25 April, 2025, during the Spring 2025 (S25) semester, Last Day of Classes is observed.
On Friday, 25 April, 2025, during the Spring 2025 (S25) semester, Semester & Mini-4 voucher deadline (4) is observed.
From 28 April, 2025,Monday to 29 April, 2023,Tuesday marks the Final Examinations for Spring 2025 (S25) semester.
On Wednesday, 30 April, 2025, during the Spring 2025 (S25) semester, Reading Day is observed.
From 01 May, 2025,Thursday to 02 May, 2023,Friday marks the Final Examinations for Spring 2025 (S25) semester.
From 03 May, 2025,Saturday to 04 May, 2023,Sunday marks the Reading Days for Spring 2025 (S25) semester.
On Monday, 05 May, 2025, during the Spring 2025 (S25) semester, Final Examinations is observed.
On Monday, 05 May, 2025, during the Spring 2025 (S25) semester, Semester & Mini-4 Faculty Course Evaluations close is observed.
Answer: "
"Which LTI prof co-authored the ""Speech collage: code-switched audio generation by collaging monolingual corpora"" paper?
","['shinji watanabe_fa5ebb425c57f6c4f1c36a7200ef1da867346e8c_content_2.txt', 'shinji watanabe_fa5ebb425c57f6c4f1c36a7200ef1da867346e8c_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which LTI prof co-authored the ""Speech collage: code-switched audio generation by collaging monolingual corpora"" paper?

Context: Title: SPEECH COLLAGE: CODE-SWITCHED AUDIO GENERATION BY COLLAGING MONOLINGUAL CORPORA
Authors: Amir Hussein, Dorsa Zeinali, Ondřej Klejch, Matthew Wiesner, Brian Yan, Shammur Chowdhury, Ahmed Ali, Shinji Watanabe, Sanjeev Khudanpur
Section: 7. REFERENCES
chapter of the international coordinating committee on speech databases and speech I/O systems and assessment (O-COCOSDA), 2017, pp. 1–5. [38] F. Hernandez et al., “Ted-lium 3: Twice as much data and corpus repartition for experiments on speaker adaptation,” in Speech and Computer: 20th International Conference, SPECOM 2018, Leipzig, Germany, September 18–22, 2018, Proceedings 20, 2018, pp. 198– 208. [39] A. Ali et al., “The MGB-2 challenge: Arabic multi-dialect broadcast media recognition,” in SLT, 2016.
Title: SPEECH COLLAGE: CODE-SWITCHED AUDIO GENERATION BY COLLAGING MONOLINGUAL CORPORA
Authors: Amir Hussein, Dorsa Zeinali, Ondřej Klejch, Matthew Wiesner, Brian Yan, Shammur Chowdhury, Ahmed Ali, Shinji Watanabe, Sanjeev Khudanpur
Section: 7. REFERENCES
using consistent predictions on synthesized speech,” in Proc. ICASSP, 2020, pp. 7029– 7033. [21] H. Yu et al., “Code-switching text generation and injection in mandarin-english asr,” in Proc. ICASSP, 2023, pp. 1–5. [22] T. K. Lam et al., “On-the-fly aligned data augmentation for sequenceto-sequence ASR,” in Proc. Interspeech, H. Hermansky et al., Eds., 2021, pp. 1299–1303. [23] R. Zhao et al., “On addressing practical challenges for rnn-transducer,” in Proc. ASRU, 2021, pp. 526–533. [24] A. Hunt et al., “Unit selection in a concatenative speech synthesis system using a large speech database,” in Proc. ICASSP, vol. 3, 1996, pp. 233–258. [25] J. Wouters and M. W. Macon, “Control of spectral dynamics in concatenative speech synthesis,” IEEE Transactions on Speech and Audio Processing, vol. 9, no. 1, pp. 30–38, 2001. [26] R. A. Khan and J. Chitode, “Concatenative speech synthesis: A review,” International Journal of Computer Applications, vol. 136, no. 3, pp. 1–6, 2016. [27] D. Povey et al., “The Kaldi speech recognition toolkit,” in ASRU, 2011. [28] P. Żelasko et al., “Lhotse: A speech data representation library for the modern deep learning ecosystem,” ArXiv preprint, vol. abs/2110.12561, 2021.
Answer: "
"What's the theme for the booths at Spring Carnival this year?
","['Apr-13_Eventno_49_SpringCarnivalBoothSweepstakesAwardCeremony.txt', 'Apr-13_Eventno_30_CMBAAsSpringCarnivalMeeting.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What's the theme for the booths at Spring Carnival this year?

Context: Event: Spring Carnival Booth & Sweepstakes Award Ceremony
Date: 4/13/24
Time: 4:30 PM-5:30 PM ET
Participants/Audience: Open to entire CMU community 
Event Details: 
Note: No registration required. No event fee.
Event: CMBAA’s Spring Carnival Meeting
Date: 4/13/24
Time: 1:00 PM-3:00 PM ET
Participants/Audience: Open to entire CMU community 
Event Details: 
Join the Carnegie Mellon Black Alumni Association for our biannual meeting with the theme of ""Next Level."" We’ll come together to share updates on the CMBAA and the student organizations we support, to discuss what you would like to see from the CMBAA in the future and to socialize. 

Can't make it back to campus? Be sure to register for the livestream during registration. 

Note: Registration required. Walk-ins are welcome as space permits. This event is open to the CMU community. 

Cost: 

$20/per person for alumni and guests. 
No charge for students and children 12 and under.
Answer: "
"In the paper ""Improving Perceptual Quality, Intelligibility, and Acoustics on VoIP Platforms"", which preprocessing methods were experimented with for audio data?
","['bhiksha raj_4628f0c28a8ed231168d1a27a93ddb938da4102d_content_0.txt', 'bhiksha raj_7333be530df311b3148e9857ce9f481975cf0a9b_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the paper ""Improving Perceptual Quality, Intelligibility, and Acoustics on VoIP Platforms"", which preprocessing methods were experimented with for audio data?

Context: [10] J. Konan, O. Bhargave, and S. et al. Agnihotri, “Im- proving perceptual quality, intelligibility, and acoustics on voip,” arXiv:2303.09048, 2023. [11] S. Davis and P. Mermelstein, “Comparison of paramet- ric representations for monosyllabic word recognition,” IEEE Trans. Acoust., Speech, Signal Process., 1980. [12] S. Chhetri, M. S. Joshi, and C. V. et al. Mahamuni, “Speech enhancement: A survey of approaches and applications,” in ICECAA ’23, 2023. [13] T. Virtanen, R. Singh, and B. Raj, Techniques for noise robustness in automatic speech recognition, John Wiley & Sons, 2012. [14] C. K. A. et al. Reddy, “The interspeech 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results,” arXiv preprint arXiv:2005.13981, 2020. [15] D.T. Campbell and J.C. Stanley, Experimental and quasi-experimental designs for research, Ravenio books, 2015. [16] Intel, User Guide for NUC10i7FNH, NUC10i5FNH, NUC10i3FNH, 2023. [17] Samsung, Samsung Galaxy A13 5G A136 User Manual, 2023. [18] Sound Devices, User manual Sound Devices MixPre-6 II, 2023. [19] CW William, “Voip service quality: measuring and evaluating packet-switched voice,” USA: McGraw-Hill Netw. Prof., 2002. [20] A.W. et al. Rix, “Perceptual evaluation of speech quality (pesq) part i–time-delay compensation,” J. Audio Eng. Soc., 2002. [21] C.H. et al. Taal, “An algorithm for intelligibility predic- tion of time–frequency weighted noisy speech,” IEEE Trans. Audio, Speech, and Language Processing, 2011.
Faculty Name: bhiksha raj
Paperid: 7333be530df311b3148e9857ce9f481975cf0a9b
Title: Improving Perceptual Quality, Intelligibility, and Acoustics on VoIP Platforms
Year: 2023
Abstract: In this paper, we present a method for fine-tuning models trained on the Deep Noise Suppression (DNS) 2020 Challenge to improve their performance on Voice over Internet Protocol (VoIP) applications. Our approach involves adapting the DNS 2020 models to the specific acoustic characteristics of VoIP communications, which includes distortion and artifacts caused by compression, transmission, and platform-specific processing. To this end, we propose a multi-task learning framework for VoIP-DNS that jointly optimizes noise suppression and VoIP-specific acoustics for speech enhancement. We evaluate our approach on a diverse VoIP scenarios and show that it outperforms both industry performance and state-of-the-art methods for speech enhancement on VoIP applications. Our results demonstrate the potential of models trained on DNS-2020 to be improved and tailored to different VoIP platforms using VoIP-DNS, whose findings have important applications in areas such as speech recognition, voice assistants, and telecommunication.
Authors: Joseph Konan, Ojas Bhargave, Shikhar Agnihotri, Hojeong Lee, Ankit Shah, Shuo Han, YUNYANG ZENG, Amanda Shu, Haohui Liu, Xuankai Chang, Hamza Khalid, Minseon Gwak, Kawon Lee, Minjeong Kim, B. Raj
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A multi-task learning framework forVoIP-DNS that jointly optimizes noise suppression and VoIP-specific acoustics for speech enhancement and outperforms both industry performance and state-of-the-art methods for speech Enhancement on VoIP applications is proposed.'}
Url: http://arxiv.org/pdf/2303.09048
Answer: "
"Who are the instructors for the data science capstone (11632)?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who are the instructors for the data science capstone (11632)?

Context: The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Data Science Capstone Research' with Course ID 11635 and Section A offers 12.0 units. The Class meets Wednesday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Nyberg located in Building TEP, Room 1403.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'MSAII Independent Study' with Course ID 11636 and Section A offers VAR units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Shamos located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Foundations of Computational Data Science' with Course ID 11637 and Section A offers 12.0 units. The Class meets Monday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rose located in Building CMU, Room REMOTE.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Machine Learning for Text and Graph-based Mining' with Course ID 11641 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Yang located in Building HH, Room B131.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Search Engines' with Course ID 11642 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Callan located in Building MI, Room 348.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Artificial Intelligence and Future Markets:' with Course ID 11651 and Section NA offers 12.0 units.
The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Data Science Capstone Research' with Course ID 11635 and Section A offers 12.0 units. The Class meets Wednesday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Nyberg located in Building TEP, Room 1403.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'MSAII Independent Study' with Course ID 11636 and Section A offers VAR units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Shamos located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Foundations of Computational Data Science' with Course ID 11637 and Section A offers 12.0 units. The Class meets Monday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rose located in Building CMU, Room REMOTE.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Machine Learning for Text and Graph-based Mining' with Course ID 11641 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Yang located in Building HH, Room B131.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Search Engines' with Course ID 11642 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Callan located in Building MI, Room 348.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Artificial Intelligence and Future Markets:' with Course ID 11651 and Section NA offers 12.0 units.
Answer: "
"What is it fine-tuned on for creating StarCoder?
","['daniel fried_3e4085e5869f1b7959707a1e1d7d273b6057eb4e_metadata.txt', 'daniel fried_3e4085e5869f1b7959707a1e1d7d273b6057eb4e_content_3.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is it fine-tuned on for creating StarCoder?

Context: Faculty Name: daniel fried
Paperid: 3e4085e5869f1b7959707a1e1d7d273b6057eb4e
Title: StarCoder: may the source be with you!
Year: 2023
Abstract: The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.
Title: StarCoder: may the source be with you!
Authors: Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries
Section: 1 Introduction
AI documentation such as model cards (Mitchell et al., 2019); – We incorporate a new attribution tool into the VSCode demo that can help users detect and locate model generations that may have been copied from the training set. This is achieved through a two-step process that involves a lightweight membership check followed by a search over a BM25 index (Section 9); and – We have significantly improved the PII redaction pipeline by collecting a PII dataset containing 12,000 files with 22,950 annotated entities. We fine-tuned our own encoder model (StarEncoder) on this dataset, resulting in a robust PII detection model (Section 4).
Answer: "
"What Psychology course in Summer 2024 will be offered at Doha, Qatar? 
","['metadata_course_summer_one_all_24.txt', 'combined_metadata_final.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What Psychology course in Summer 2024 will be offered at Doha, Qatar? 

Context: In Semester Summer One(All) 2024, from the department of Carnegie Mellon University-Wide Studies, the subject titled 'Summer UG Research Apprenticeship' with Course ID 39660 and Section X offers VAR units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Sharma located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Carnegie Mellon University-Wide Studies, the subject titled 'Summer UG Research Apprenticeship' with Course ID 39660 and Section XX offers VAR units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Bouamor located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Carnegie Mellon University-Wide Studies, the subject titled 'Summer UG Research Apprenticeship' with Course ID 39660 and Section Y offers VAR units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Safak located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Carnegie Mellon University-Wide Studies, the subject titled 'Summer UG Research Apprenticeship' with Course ID 39660 and Section YY offers VAR units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Instructor TBA located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Carnegie Mellon University-Wide Studies, the subject titled 'Summer UG Research Apprenticeship' with Course ID 39660 and Section Z offers VAR units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Instructor TBA located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Carnegie Mellon University-Wide Studies, the subject titled 'Summer UG Research Apprenticeship' with Course ID 39660 and Section ZZ offers VAR units. The Class meets Schedule will be added between NA and NA ET.
In Semester Summer One(All) 2024, from the department of Carnegie Mellon University-Wide Studies, the subject titled 'Summer UG Research Apprenticeship' with Course ID 39660 and Section WW offers VAR units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Instructor TBA located in Building TBA, Room None.
In Semester Summer One(All) 2024, from the department of Carnegie Mellon University-Wide Studies, the subject titled 'Summer UG Research Apprenticeship' with Course ID 39660 and Section WY offers VAR units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Feo Flushing located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Carnegie Mellon University-Wide Studies, the subject titled 'Summer UG Research Apprenticeship' with Course ID 39660 and Section WZ offers VAR units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor AAZAM located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Carnegie Mellon University-Wide Studies, the subject titled 'Summer UG Research Apprenticeship' with Course ID 39660 and Section X offers VAR units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Sharma located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Carnegie Mellon University-Wide Studies, the subject titled 'Summer UG Research Apprenticeship' with Course ID 39660 and Section XX offers VAR units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Bouamor located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Carnegie Mellon University-Wide Studies, the subject titled 'Summer UG Research Apprenticeship' with Course ID 39660 and Section Y offers VAR units. The Class meets Schedule will be added between NA and NA ET.
Answer: "
"In the paper ""Decoder-only Architecture for Speech Recognition with CTC Prompts and Text Data Augmentation, what is the reduction in word error rates achieved by the proposed models on CallHome?
","['shinji watanabe_06353e1b7e7c8dc701ac76dcd4db5061b24468c9_metadata.txt', 'shinji watanabe_06353e1b7e7c8dc701ac76dcd4db5061b24468c9_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the paper ""Decoder-only Architecture for Speech Recognition with CTC Prompts and Text Data Augmentation, what is the reduction in word error rates achieved by the proposed models on CallHome?

Context: Faculty Name: shinji watanabe
Paperid: 06353e1b7e7c8dc701ac76dcd4db5061b24468c9
Title: Decoder-only Architecture for Speech Recognition with CTC Prompts and Text Data Augmentation
Year: 2023
Abstract: Collecting audio-text pairs is expensive; however, it is much easier to access text-only data. Unless using shallow fusion, end-to-end automatic speech recognition (ASR) models require architecture modifications or additional training schemes to use text-only data. Inspired by recent advances in decoder-only language models (LMs), such as GPT-3 and PaLM adopted for speech-processing tasks, we propose using a decoder-only architecture for ASR with simple text augmentation. To provide audio information, encoder features compressed by CTC prediction are used as prompts for the decoder, which can be regarded as refining CTC prediction using the decoder-only model. Because the decoder architecture is the same as an autoregressive LM, it is simple to enhance the model by leveraging external text data with LM training. An experimental comparison using LibriSpeech and Switchboard shows that our proposed models with text augmentation training reduced word error rates from ordinary CTC by 0.3% and 1.4% on LibriSpeech test-clean and testother set, respectively, and 2.9% and 5.0% on Switchboard and CallHome. The proposed model had advantage on computational efficiency compared with conventional encoder-decoder ASR models with a similar parameter setup, and outperformed them on the LibriSpeech 100h and Switchboard training scenarios.
Authors: E. Tsunoo, Hayato Futami, Yosuke Kashiwagi, Siddhant Arora, Shinji Watanabe
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work proposes using a decoder-only architecture for ASR with simple text augmentation training that had advantage on computational efficiency compared with conventional encoder-decoder ASR models with a similar parameter setup, and outperformed them on the LibriSpeech 100h and Switchboard training scenarios.'}
Url: https://arxiv.org/pdf/2309.08876
Title: DECODER-ONLY ARCHITECTURE FOR SPEECH RECOGNITION WITH CTC PROMPTS AND TEXT DATA AUGMENTATION
Authors: Emiru Tsunoo, Hayato Futami, Yosuke Kashiwagi, Siddhant Arora, Shinji Watanabe
Section: 5. REFERENCES
Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, et al., “AudioPaLM: A large language model that can speak and listen,” arXiv preprint arXiv:2306.12925, 2023. [15] Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Junteng Jia, Yuan Shangguan, Ke Li, Jinxi Guo, Wenhan Xiong, Jay Mahadeokar, Ozlem Kalinli, et al., “Prompting large language models with speech recognition abilities,” arXiv preprint arXiv:2307.11795, 2023. [16] Jian Wu, Yashesh Gaur, Zhuo Chen, Long Zhou, Yimeng Zhu, Tianrui Wang, Jinyu Li, Shujie Liu, Bo Ren, Linquan Liu, et al., “On decoder-only architecture for speech-totext and large language model integration,” arXiv preprint arXiv:2307.03917, 2023. [17] Siddhant Arora, Hayato Futami, Yosuke Kashiwagi, Emiru Tsunoo, Brian Yan, and Shinji Watanabe, “Integrating pretrained ASR and LM to perform sequence generation for spoken language understanding,” in Proc. INTERSPEECH 2023, 2023, pp. 720–724. [18] Marco Gaido, Mauro Cettolo, Matteo Negri, and Marco Turchi, “CTC-based compression for direct speech translation,” in Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, 2021, pp. 690–696. [19] Zhengkun Tian, Jiangyan Yi, Ye Bai, Jianhua Tao, Shuai Zhang, and Zhengqi Wen, “FSR: Accelerating the inference process of transducer-based models by applying fast-skip regularization,” in Proc.
Answer: "
"When is the annual MOBOT race?
","['Apr-12_Eventno_25_AnnualMOBOTRaces.txt', 'Apr-12_Eventno_35_MobotRecapWatchParty.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When is the annual MOBOT race?

Context: Event: Annual MOBOT Races
Date: 4/12/24
Time: 12:00 PM-2:00 PM ET
Participants/Audience: Open to entire CMU community 
Event Details: 
The School of Computer Science welcomes all members of the Carnegie Mellon community to participate in the annual Carnegie Mellon Mobot Races (""MObile roBOTs"") along a slalom course. The competition provides participants an opportunity to demonstrate their technological creativity, while encouraging interdisciplinary teams to attack the challenges in the race course. Sponsored by Lockheed Martin, Boeing, Aptiv and the School of Computer Science.

Interested in competing? Send e-mail to mobot@cs.cmu.edu to have your name added to the list of Mobot ""subscribers."" Please do this even if you were on the list last year. Subscribers will be sent a copy of the official race rules and will be kept informed of race announcements and developments.
  
Note: No registration required. No event fee. This event is open to the entire CMU community and their guests.
Event: Mobot Recap Watch Party
Date: 4/12/24
Time: 3:00 PM-4:00 PM ET
Participants/Audience: Open to entire CMU community 
Event Details: 
Join others in the Mobot community for a recap of the 2024 Mobot races. 

Note: No registration required. No event fee. Open to CMU community and their guests.
Answer: "
"When does the Fall 2024 course registeration start for masters students?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When does the Fall 2024 course registeration start for masters students?

Context: From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mini-2 Classes Begin is observed.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mid-Semester & Mini-1 grades due by 4 pm is observed.
On Friday, 27 October, 2023, during the Fall 2023 (F23) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
On Tuesday, 07 November, 2023, during the Fall 2023 (F23) semester, Democracy Day is observed, leading to No Classes, except Evening classes after 5 pm will still meet.
On Saturday, 11 November, 2023, during the Fall 2023 (F23) semester, Homecoming is observed.
From 13 November, 2023,Monday to 17 November, 2023,Friday marks the Spring 2024 Registration Week for Fall 2023 (F23) semester.
On Monday, 13 November, 2023, during the Fall 2023 (F23) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Wednesday, 15 November, 2023, during the Fall 2023 (F23) semester, Mini-2 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 22 November, 2023,Wednesday to 24 November, 2023,Friday marks the Thanksgiving Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Mini-2 pass/no pass & withdrawal deadline (2) is observed.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Faculty Course Evaluations open is observed.
On Friday, 08 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Last Day of Classes is observed.
On Tuesday, 12 March, 2024, during the Spring 2024 (S24) semester, Summer 2024 Registration Opens is observed.
On Friday, 15 March, 2024, during the Spring 2024 (S24) semester, Mini-4 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 01 April, 2024, during the Spring 2024 (S24) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Wednesday, 03 April, 2024, during the Spring 2024 (S24) semester, Mini-4 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 11 April, 2024,Thursday to 13 April, 2024,Saturday marks the Spring Carnival for Spring 2024 (S24) semester which leads to No Classes.
From 15 April, 2024,Monday to 19 April, 2024,Friday marks the Fall 2024 Registration Week for Spring 2024 (S24) semester.
On Monday, 15 April, 2024, during the Spring 2024 (S24) semester, Mini-4 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 15 April, 2024, during the Spring 2024 (S24) semester, Semester & Mini-4 Faculty Course Evaluations open is observed.
On Friday, 26 April, 2024, during the Spring 2024 (S24) semester, Last Day of Classes is observed.
On Friday, 26 April, 2024, during the Spring 2024 (S24) semester, Semester & Mini-4 voucher deadline (4) is observed.
From 29 April, 2024,Monday to 30 April, 2024,Tuesday marks the Final Examinations for Spring 2024 (S24) semester.
On Wednesday, 01 May, 2024, during the Spring 2024 (S24) semester, Reading Day is observed.
From 02 May, 2024,Thursday to 03 May, 2024,Friday marks the Final Examinations for Spring 2024 (S24) semester.
From 04 May, 2024,Saturday to 05 May, 2024,Sunday marks the Reading Days for Spring 2024 (S24) semester.
Answer: "
"What is the tldr of the paper Multimodal Fusion Interactions: A Study of Human and Automatic Quantification?
","['louis philippe morency_90b09bdb1bd78875ee8d8d324a568a36955e4765_metadata.txt', 'louis philippe morency_90b09bdb1bd78875ee8d8d324a568a36955e4765_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the tldr of the paper Multimodal Fusion Interactions: A Study of Human and Automatic Quantification?

Context: Faculty Name: louis philippe morency
Paperid: 90b09bdb1bd78875ee8d8d324a568a36955e4765
Title: Multimodal Fusion Interactions: A Study of Human and Automatic Quantification
Year: 2023
Abstract: In order to perform multimodal fusion of heterogeneous signals, we need to understand their interactions: how each modality individually provides information useful for a task and how this information changes in the presence of other modalities. In this paper, we perform a comparative study of how humans annotate two categorizations of multimodal interactions: (1) partial labels, where different annotators annotate the label given the first, second, and both modalities, and (2) counterfactual labels, where the same annotator annotates the label given the first modality before asking them to explicitly reason about how their answer changes when given the second. We further propose an alternative taxonomy based on (3) information decomposition, where annotators annotate the degrees of redundancy: the extent to which modalities individually and together give the same predictions, uniqueness: the extent to which one modality enables a prediction that the other does not, and synergy: the extent to which both modalities enable one to make a prediction that one would not otherwise make using individual modalities. Through experiments and annotations, we highlight several opportunities and limitations of each approach and propose a method to automatically convert annotations of partial and counterfactual labels to information decomposition, yielding an accurate and efficient method for quantifying multimodal interactions.
Authors: P. Liang, Yun Cheng, R. Salakhutdinov, Louis-Philippe Morency
Venue: International Conference on Multimodal Interaction
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A comparative study of how humans annotate two categorizations of multimodal interactions is performed and a method to automatically convert annotations of partial and counterfactual labels to information decomposition is proposed, yielding an accurate and efficient method for quantifying multimodals interactions.'}
Url: https://dl.acm.org/doi/pdf/10.1145/3577190.3614151
Title: Multimodal Fusion Interactions: A Study of Human and Automatic sQuantification
Authors: Paul Pu Liang, Yun Cheng, Ruslan Salakhutdinov, Louis-Philippe Morency
Section: A HUMAN ANNOTATION DETAILS
Participation in all annotations was fully voluntary and we obtained consent from all participants prior to annotations. The authors manually took anonymous notes on all results and feedback in such a manner that the identities of annotators cannot readily be ascertained directly or through identifers linked to the subjects. Participants were not the authors nor in the same research groups as the authors, but they all hold or are working towards a graduate degree in a STEM feld and have knowledge of machine learning. None of the participants knew about this project before their session and each participant only interacted with the setting they were involved in. We sample 50 datapoints from each of the 5 datasets in Table 1 and give them to a total of 18 diferent annotators: • 3 annotators for direct annotation of interactions, • 3 annotators for partial labeling of �1, �2, and �12, • 3 annotators for counterfactual, labeling �1 frst then �1+2, • 3 annotators for counterfactual, labeling �2 frst then �2+1. All annotations were performed via google spreadsheets. A.1 Annotating partial labels We asked 3 annotators to predict the partial labels in a randomized setting. For each annotator, we asked them to annotate �1 then �2 given only modality 1 or 2 respectively, and fnally � given both modalities. This completion order is designed on purpose to minimize possible memorization of the data so that the annotators can provide completely independent unimodal and multimodal predictions on the label. When annotating the visual modality of the video datasets, we explicitly require the annotators to mute the audio and predict the partial labels based only on the video frames. After that, all annotators are asked to provide a confdence score on a scale of 0 (no confdence) to 5 (high confdence) about their annotations. The confdence scale is applied to all annotation settings below.
Answer: "
"The first author of the paper Rethinking Voice-Face Correlation: A Geometry View is from which university?
","['bhiksha raj_5a3307b2e64bbcaff1202e261b8a83f7d03418a8_content_0.txt', 'rita singh_5a3307b2e64bbcaff1202e261b8a83f7d03418a8_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: The first author of the paper Rethinking Voice-Face Correlation: A Geometry View is from which university?

Context: Title: Rethinking Voice-Face Correlation: A Geometry View
Authors: Xiang Li, Yandong Wen, Muqiao Yang, Jinglu Wang, Rita Singh, Bhiksha Raj, Mohamed bin Zayed
Section: 5 CONCLUSION
In conclusion, this paper presents a novel approach to exploring the voice-face correlation by focusing on the geometric aspects of the face rather than relying on semantic cues such as gender, age, and emotion. The proposed voice-anthropometric measurement (AM)face paradigm identifies predictable facial AMs from the voice to guide 3D face reconstruction, which results in significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium. This approach not only eliminates the influence of unpredictable AMs but also offers a new perspective on voice-face correlation, which can be valuable for anthropometry science. The results of this study open up possibilities for future research in this area, such as developing more accurate voice-guided face synthesis techniques and a better understanding of the relationship between voice and facial geometry. Rethinking Voice-Face Correlation: A Geometry View Conference’23, July 2023, Ottawa, Canada
Title: Rethinking Voice-Face Correlation: A Geometry View
Authors: Xiang Li, Yandong Wen, Muqiao Yang, Jinglu Wang, Rita Singh, Bhiksha Raj, Mohamed bin Zayed
Section: 5 CONCLUSION
In conclusion, this paper presents a novel approach to exploring the voice-face correlation by focusing on the geometric aspects of the face rather than relying on semantic cues such as gender, age, and emotion. The proposed voice-anthropometric measurement (AM)face paradigm identifies predictable facial AMs from the voice to guide 3D face reconstruction, which results in significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium. This approach not only eliminates the influence of unpredictable AMs but also offers a new perspective on voice-face correlation, which can be valuable for anthropometry science. The results of this study open up possibilities for future research in this area, such as developing more accurate voice-guided face synthesis techniques and a better understanding of the relationship between voice and facial geometry. Rethinking Voice-Face Correlation: A Geometry View Conference’23, July 2023, Ottawa, Canada
Answer: "
"Fringe vehicles often start with which letter?
","['cmubuggy_d403.txt', 'cmubuggy_d403.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Fringe vehicles often start with which letter?

Context: An mural celebrating Buggy can be found in CMU's Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions. Sweepstakes Slang Buggy: Vehicle being raced and also a nickname for the competition. Chute: A section of the freeroll portion of the buggy course (near the southwestern end of Frew Street at its intersection with Schenley Drive) where buggies make the sharp righthand turn from Schenley Drive onto Frew Street. Chute Flagger: Team member who provides a signal for buggy drivers to know when to start the right-hand turn from Schenley Drive onto Frew Street. Driver: Person who travels with a buggy and controls the vehicles via steering and braking systems. Pushbar: Structure attached to a buggy that a person pushes to propel that buggy forward. Pusher: Person who propels a buggy via a pushbar along one of the five hills of the buggy course. Shell: Entire outer structure or covering of a buggy that determines that buggy's aerodynamic characteristics. Transition: Procedure whereby one pusher finishes pushing a buggy and the next pusher in sequence starts to push that same buggy. Members of Fringe, celebrating their 50th year of Buggy racing, allowed a rare visit into its shop for a behind-the-scenes tour. This year Fringe is planning to roll four different vehicles, built and maintained in the Fringe workshop in the basement of the East Campus Garage, known as the ""Froom."" ""We're used to saying everything with 'fr' in front of it but when we say something in front of other people, it gets them confused,"" said Fringe head mechanic Dave Singh, who will graduate in May with a bachelor's degree in mechanical engineering and biomedical engineering. Fringe vehicles often are named with the letter ""B,"" like Boson, Blueshift, Bissa and Bumper. Other teams, such as Apex, often use names that connotate fire, while the SDC (Student Dormitory Committee) team, uses names such as Vice, Bane, Avarice and Malice. This year's buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It's one of the few times spectators can see buggies up close.
Buggy Races Keep Rolling at Carnegie Mellon - News - Carnegie Mellon University Carnegie Mellon University ——— Search Search Search this site only News News › Stories › Archives › 2019 › April › Buggy Races Keep Rolling at Carnegie Mellon April 10, 2019 Buggy Races Keep Rolling at Carnegie Mellon In its 99th year, the tradition is a Spring Carnival treat By Heidi Opdyke opdyke(through)andrew.cmu.edu Media Inquiries Julie Mattera Marketing and Communications jmattera(through)cmu.edu 412-268-2902 Sweepstakes, also known as the Buggy Races, has come a long way at Carnegie Mellon University. The slick, torpedo-like vessels carrying drivers with nerves of steel are a far cry from the two-man teams that once changed places halfway through a race and rode in everything from rain barrels with bicycle wheels to three-wheeled ash cans 99 years ago. Today, it takes six people to maneuver the .84 -mile course around Schenley Park's Flagstaff Hill. But while five pushers and a driver navigate the course's hills, dozens of people are needed to make a successful race happen. A year of planning goes into just over two minutes of racing. The Machine The basics of a buggy are straightforward, but teams are often secretive in how they build the machines, in particular the way they brake, steer and what types of wheels are used. Each has a body, pushbar for runners to move the machine up the hills, wheels, a safety harness and driving and braking mechanisms. Some also include fairings, a type of housing around the wheels that help reduce drag, make the vehicle quieter and just looks cool. Fairings have been a key feature for the Fringe team in recent years, which is celebrating its 50th anniversary of Buggy. ""We have a reputation of being the quietest on the course,"" said Diya Nuxoll, who wrapped up her bachelor's degree in mechanical engineering in December and is working on an advanced studies master's degree in design and manufacturing. Nuxoll leads the Fringe team as one of two head mechanics. An mural celebrating Buggy can be found in CMU's Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions. Sweepstakes Slang Buggy: Vehicle being raced and also a nickname for the competition.
Answer: "
"What is the full name of the conference where the paper The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation, got published?
","['graham neubig_fd80f7f3673fc6ca02f192d5d73426f11a4be659_metadata.txt', 'graham neubig_fd80f7f3673fc6ca02f192d5d73426f11a4be659_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the full name of the conference where the paper The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation, got published?

Context: Faculty Name: graham neubig
Paperid: fd80f7f3673fc6ca02f192d5d73426f11a4be659
Title: The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation
Year: 2023
Abstract: Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iterative development of MT systems. While considerable progress has been made on estimating a single scalar quality score, current metrics lack the informativeness of more detailed schemes that annotate individual errors, such as Multidimensional Quality Metrics (MQM). In this paper, we help fill this gap by proposing AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AutoMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particularly large gains for larger models) while providing interpretability through error spans that align with human annotations.
Authors: Patrick Fernandes, Daniel Deutsch, M. Finkelstein, Parker Riley, André F. T. Martins, Graham Neubig, Ankush Garg, J. Clark, Markus Freitag, Orhan Firat
Venue: Conference on Machine Translation
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations, and finds that it improves performance compared to just prompting for scores.'}
Url: https://arxiv.org/pdf/2308.07286
Title: The Devil is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation
Authors: Patrick Fernandes, Daniel Deutsch, Mara Finkelstein, Parker Riley, André F. T. Martins, Graham Neubig, Ankush Garg, Jonathan H. Clark, Markus Freitag, Orhan Firat
Section: C Additional Results
Figures 12, 13, and 14 present additional experimental results.
Answer: "
"In spring 2024, What is the day and time of course 17604-C?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, What is the day and time of course 17604-C?

Context: In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Communications for Software Leaders II' with Course ID 17604 and Section B offers 3.0 units. The Class meets Tuesday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Frollini located in Building 3SC, Room 265.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Communications for Software Leaders II' with Course ID 17604 and Section C offers 3.0 units. The Class meets Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Frollini located in Building 3SC, Room 265.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Communications for Software Leaders II' with Course ID 17604 and Section D offers 3.0 units. The Class meets Wednesday between 07:00PM and 08:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Frollini located in Building CMU, Room REMOTE.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Communications for Software Leaders II' with Course ID 17604 and Section E offers 3.0 units. The Class meets Thursday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Frollini located in Building 3SC, Room 265.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Privacy by Design Project Workshop' with Course ID 17606 and Section A offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Habib located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Privacy by Design Practicum' with Course ID 17607 and Section A offers VAR units. The Class meets Schedule will be added between NA and NA ET.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Communications for Software Leaders II' with Course ID 17604 and Section B offers 3.0 units. The Class meets Tuesday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Frollini located in Building 3SC, Room 265.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Communications for Software Leaders II' with Course ID 17604 and Section C offers 3.0 units. The Class meets Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Frollini located in Building 3SC, Room 265.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Communications for Software Leaders II' with Course ID 17604 and Section D offers 3.0 units. The Class meets Wednesday between 07:00PM and 08:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Frollini located in Building CMU, Room REMOTE.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Communications for Software Leaders II' with Course ID 17604 and Section E offers 3.0 units. The Class meets Thursday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Frollini located in Building 3SC, Room 265.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Privacy by Design Project Workshop' with Course ID 17606 and Section A offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Habib located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Privacy by Design Practicum' with Course ID 17607 and Section A offers VAR units. The Class meets Schedule will be added between NA and NA ET.
Answer: "
"What is the end-to-end task success rate of the best GPT-4-based agent compared to human performance on the WebArena benchmark?
","['yonatan bisk_e41482f4ee984f17382f6cdd900df094d928be06_metadata.txt', 'daniel fried_e41482f4ee984f17382f6cdd900df094d928be06_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the end-to-end task success rate of the best GPT-4-based agent compared to human performance on the WebArena benchmark?

Context: Faculty Name: yonatan bisk
Paperid: e41482f4ee984f17382f6cdd900df094d928be06
Title: WebArena: A Realistic Web Environment for Building Autonomous Agents
Year: 2023
Abstract: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.
Authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper builds an environment for language-guided agents that is highly realistic and reproducible, and creates an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.'}
Url: https://arxiv.org/pdf/2307.13854
Faculty Name: daniel fried
Paperid: e41482f4ee984f17382f6cdd900df094d928be06
Title: WebArena: A Realistic Web Environment for Building Autonomous Agents
Year: 2023
Abstract: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.
Authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper builds an environment for language-guided agents that is highly realistic and reproducible, and creates an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.'}
Url: https://arxiv.org/pdf/2307.13854
Answer: "
"When is the democracy day in 2024?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When is the democracy day in 2024?

Context: On Friday, 25 October, 2024, during the Fall 2024 (F24) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
On Tuesday, 05 November, 2024, during the Fall 2024 (F24) semester, Democracy Day is observed, leading to No Classes, except Evening classes after 5 pm will still meet.
On Monday, 11 November, 2024, during the Fall 2024 (F24) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Wednesday, 13 November, 2024, during the Fall 2024 (F24) semester, Mini-2 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 18 November, 2024,Monday to 22 November, 2024,Friday marks the Spring 2025 Registration Week for Fall 2024 (F24) semester.
On Monday, 25 November, 2024, during the Fall 2024 (F24) semester, Mini-2 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 25 November, 2024, during the Fall 2024 (F24) semester, Semester & Mini-2 Faculty Course Evaluations open is observed.
From 27 November, 2024,Wednesday to 29 November, 2023,Friday marks the Thanksgiving Break for Fall 2024 (F24) semester which leads to No Classes.
On Friday, 06 December, 2024, during the Fall 2024 (F24) semester, Semester & Mini-2 Last Day of Classes is observed.
On Friday, 06 December, 2024, during the Fall 2024 (F24) semester, Semester & Mini-2 voucher deadline (4) is observed.
From 09 December, 2024,Monday to 10 December, 2023,Tuesday marks the Final Exams for Fall 2024 (F24) semester.
On Monday, 11 December, 2024, during the Fall 2024 (F24) semester, Reading Day is observed.
From 12 December, 2024,Thursday to 13 December, 2023,Friday marks the Final Exams for Fall 2024 (F24) semester.
From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mini-2 Classes Begin is observed.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mid-Semester & Mini-1 grades due by 4 pm is observed.
On Friday, 27 October, 2023, during the Fall 2023 (F23) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
On Tuesday, 07 November, 2023, during the Fall 2023 (F23) semester, Democracy Day is observed, leading to No Classes, except Evening classes after 5 pm will still meet.
On Saturday, 11 November, 2023, during the Fall 2023 (F23) semester, Homecoming is observed.
From 13 November, 2023,Monday to 17 November, 2023,Friday marks the Spring 2024 Registration Week for Fall 2023 (F23) semester.
On Monday, 13 November, 2023, during the Fall 2023 (F23) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Wednesday, 15 November, 2023, during the Fall 2023 (F23) semester, Mini-2 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 22 November, 2023,Wednesday to 24 November, 2023,Friday marks the Thanksgiving Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Mini-2 pass/no pass & withdrawal deadline (2) is observed.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Faculty Course Evaluations open is observed.
On Friday, 08 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Last Day of Classes is observed.
Answer: "
"When did the Fall Break end in 2023?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-2324.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When did the Fall Break end in 2023?

Context: From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mini-2 Classes Begin is observed.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mid-Semester & Mini-1 grades due by 4 pm is observed.
On Friday, 27 October, 2023, during the Fall 2023 (F23) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
On Tuesday, 07 November, 2023, during the Fall 2023 (F23) semester, Democracy Day is observed, leading to No Classes, except Evening classes after 5 pm will still meet.
On Saturday, 11 November, 2023, during the Fall 2023 (F23) semester, Homecoming is observed.
From 13 November, 2023,Monday to 17 November, 2023,Friday marks the Spring 2024 Registration Week for Fall 2023 (F23) semester.
On Monday, 13 November, 2023, during the Fall 2023 (F23) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Wednesday, 15 November, 2023, during the Fall 2023 (F23) semester, Mini-2 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 22 November, 2023,Wednesday to 24 November, 2023,Friday marks the Thanksgiving Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Mini-2 pass/no pass & withdrawal deadline (2) is observed.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Faculty Course Evaluations open is observed.
On Friday, 08 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Last Day of Classes is observed.
End Date: '2023-11-24', Days: 'Wednesday to Friday', Event: 'Thanksgiving Break; No Classes', Semester: 'Fall 2023 (F23)'
Date: '2023-11-27', Day: 'Monday', Event: 'Mini-2 pass/no pass & withdrawal deadline (2)', Semester: 'Fall 2023 (F23)'
Date: '2023-11-27', Day: 'Monday', Event: 'Semester & Mini-2 Faculty Course Evaluations open', Semester: 'Fall 2023 (F23)'
Date: '2023-12-08', Day: 'Friday', Event: 'Semester & Mini-2 Last Day of Classes ', Semester: 'Fall 2023 (F23)'
Date: '2023-12-08', Day: 'Friday', Event: 'Semester & Mini-2 voucher deadline (4)', Semester: 'Fall 2023 (F23)'
Start Date: '2023-12-11', End Date: '2023-12-12', Days: 'Monday to Tuesday', Event: 'Final Exams ', Semester: 'Fall 2023 (F23)'
Date: '2023-12-13', Day: 'Wednesday', Event: 'Reading Day ', Semester: 'Fall 2023 (F23)'
Start Date: '2023-12-14', End Date: '2023-12-15', Days: 'Thursday to Friday', Event: 'Final Exams ', Semester: 'Fall 2023 (F23)'
Date: '2023-12-16', Day: 'Saturday', Event: 'Reading Day ', Semester: 'Fall 2023 (F23)'
Date: '2023-12-17', Day: 'Sunday', Event: 'Final Exams ', Semester: 'Fall 2023 (F23)'
Date: '2023-12-18', Day: 'Monday', Event: 'Make-Up Final Exams', Semester: 'Fall 2023 (F23)'
Date: '2023-12-18', Day: 'Monday', Event: 'Semester & Mini-2 Faculty Course Evaluations close', Semester: 'Fall 2023 (F23)'
Date: '2023-12-20', Day: 'Wednesday', Event: 'Final Grades Due by 4 pm ',
Answer: "
"In fall 2023, What is the title of course 05431?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, What is the title of course 05431?

Context: In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Software Structures for User Interfaces' with Course ID 05431 and Section NA offers 12.0 units. The Class meets Monday Wednesday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hudson located in Building 4SC, Room 104.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Software Structures for User Interfaces:' with Course ID 05431 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Software Structures for User Interfaces' with Course ID 05431 and Section A offers 12.0 units. The Class meets Friday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hudson located in Building 4SC, Room 104.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Personalized Online Learning' with Course ID 05432 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 03:00PM and 04:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Aleven located in Building WEH, Room 5409.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Machine Learning in Practice' with Course ID 05434 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rose located in Building SH, Room 105.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Service Design' with Course ID 05452 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 08:00AM and 09:50AM ET.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Software Structures for User Interfaces' with Course ID 05431 and Section NA offers 12.0 units. The Class meets Monday Wednesday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hudson located in Building 4SC, Room 104.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Software Structures for User Interfaces:' with Course ID 05431 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Software Structures for User Interfaces' with Course ID 05431 and Section A offers 12.0 units. The Class meets Friday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hudson located in Building 4SC, Room 104.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Personalized Online Learning' with Course ID 05432 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 03:00PM and 04:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Aleven located in Building WEH, Room 5409.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Machine Learning in Practice' with Course ID 05434 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rose located in Building SH, Room 105.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Service Design' with Course ID 05452 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 08:00AM and 09:50AM ET.
Answer: "
"What is David Garlan's email address?
","['handbook-msaii-2022-2023.txt', 'combined_metadata_final.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is David Garlan's email address?

Context: 6 Academic Calendar 
The Academic Calendar can be found at https://www.cmu.edu/hub/calendar/index.html and 
provides information on all deadlines including registration dates, class start dates, add/drop 
deadlines, exam dates and more. 
7 School and Departmental Information 
The following are key personnel with whom you may interact during your time at Carnegie 
Mellon: 
 
Martial Hebert 
 
 
 
 
 
Dean, School of Computer Science 
9 
 
 
University Professor 
 
GHC 6105 
 
412-268-5704 
 
hebert@cs.cmu.edu  
https://www.cs.cmu.edu/~hebert/  
 
David Garlan 
Associate Dean for Master’s Programs, SCS 
Professor 
TCS 420 
garlan@cs.cmu.edu 
http://www.cs.cmu.edu/~garlan/academics.htm  
 
Carolyn Rosé  
 
 
 
 
Interim Director, Language Technologies Institute 
 
Professor 
 
GHC 5415 
 
412-268-7130 
 
cprose@cs.cmu.edu 
8 MSAII Policies 
The Language Technologies Institute (LTI) has prepared this statement of policies, program 
requirements, guidance, process and procedures for students in the M.S. in Artificial Intelligence 
and Innovation (MSAII) program. A copy of this handbook is also available online at the 
program website.   
The University Student guide, The Word, which details university-wide policies, is also 
available online at www.cmu.edu/student-affairs/theword.     
Additional resources specific to graduate students can be found at 
www.cmu.edu/graduate/policies/    
8.1 The MSAII Degree 
The M.S. in Artificial Intelligence and Innovation (MSAII) is a professional master's program 
offered by the Language Technologies Institute (LTI) in the School of Computer Science at 
Carnegie Mellon University. This program seeks to educate students in the innovative use of 
artificial intelligence to create practical solutions in a wide variety of application areas. As 
artificial intelligence matures, there is a great need for students who can envision, design, plan 
and deliver solutions that integrate AI technologies such as deep learning, natural language 
processing, robotics and big data analytics into new applications.  The program stresses both 
10 
 
intrapreneurship and entrepreneurship and encourages students to develop ideas for startups 
and for introduction into established companies.
The Class meets Monday Wednesday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Garlan, Kang located in Building CMU, Room REMOTE.
In Semester Fall 2023, from the department of Software & Societal Systems, the subject titled 'Formal Methods' with Course ID 17614 and Section A1 offers 6.0 units. The Class meets Friday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Garlan, Kang located in Building 3SC, Room 265.
In Semester Fall 2023, from the department of Software & Societal Systems, the subject titled 'Formal Methods' with Course ID 17614 and Section B1 offers 6.0 units. The Class meets Friday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Garlan, Kang located in Building 3SC, Room 265.
In Semester Fall 2023, from the department of Software & Societal Systems, the subject titled 'Formal Methods' with Course ID 17614 and Section D1 offers 6.0 units. The Class meets Monday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Garlan, Kang located in Building CMU, Room REMOTE.
In Semester Fall 2023, from the department of Software & Societal Systems, the subject titled 'Programming Quantum Computers' with Course ID 17617 and Section A1 offers 6.0 units. The Class meets Monday Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Justice located in Building 3SC, Room 265.
In Semester Fall 2023, from the department of Software & Societal Systems, the subject titled 'Quantum Machine Learning' with Course ID 17620 and Section A2 offers 6.0 units. The Class meets Monday Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Justice, Singh located in Building 3SC, Room 265.
Answer: "
"What is the full name of the conference where the paper Riveter: Measuring Power and Social Dynamics Between Entities, got published?
","['maarten sap_27553f8bd9cbee90f6e65b9cdecadff0e7cc55ee_metadata.txt', 'maarten sap_papers.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the full name of the conference where the paper Riveter: Measuring Power and Social Dynamics Between Entities, got published?

Context: Faculty Name: maarten sap
Paperid: 27553f8bd9cbee90f6e65b9cdecadff0e7cc55ee
Title: Riveter: Measuring Power and Social Dynamics Between Entities
Year: 2023
Abstract: Riveter provides a complete easy-to-use pipeline for analyzing verb connotations associated with entities in text corpora. We prepopulate the package with connotation frames of sentiment, power, and agency, which have demonstrated usefulness for capturing social phenomena, such as gender bias, in a broad range of corpora. For decades, lexical frameworks have been foundational tools in computational social science, digital humanities, and natural language processing, facilitating multifaceted analysis of text corpora. But working with verb-centric lexica specifically requires natural language processing skills, reducing their accessibility to other researchers. By organizing the language processing pipeline, providing complete lexicon scores and visualizations for all entities in a corpus, and providing functionality for users to target specific research questions, Riveter greatly improves the accessibility of verb lexica and can facilitate a broad range of future research.
Authors: Maria Antoniak, Anjalie Field, Jimin Mun, Melanie Walsh, Lauren F. Klein, Maarten Sap
Venue: Annual Meeting of the Association for Computational Linguistics
Tldr: {'model': 'tldr@v2.0.0', 'text': 'Riveter greatly improves the accessibility of verb lexica and can facilitate a broad range of future research by organizing the language processing pipeline, providing complete lexicon scores and visualizations for all entities in a corpus, and providing functionality for users to target specific research questions.'}
Url: https://aclanthology.org/2023.acl-demo.36.pdf
List of 2023 Open Access papers by maarten sap are:
Modeling Empathic Similarity in Personal Narratives
COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive Statements
Riveter: Measuring Power and Social Dynamics Between Entities
Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting
BiasX: ""Thinking Slow"" in Toxic Content Moderation with Explanations of Implied Social Biases
Improving Language Models with Advantage-based Offline Policy Gradients
From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models
NLPositionality: Characterizing Design Biases of Datasets and Models
Don't Take This Out of Context! On the Need for Contextual Models and Evaluations for Stylistic Rewriting
Queer In AI: A Case Study in Community-Led Participatory AI
Value Kaleidoscope: Engaging AI with Pluralistic Human Values, Rights, and Duties
Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models
Towards Countering Essentialism through Social Bias Reasoning
Answer: "
"Which LTI prof co-authored the paper titled ""Identification of Nonlinear Latent Hierarchical Models""?
","['eric xing_075b751201f549daeba9840f78768f4ceb507e17_metadata.txt', 'eric xing_075b751201f549daeba9840f78768f4ceb507e17_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which LTI prof co-authored the paper titled ""Identification of Nonlinear Latent Hierarchical Models""?

Context: Faculty Name: eric xing
Paperid: 075b751201f549daeba9840f78768f4ceb507e17
Title: Identification of Nonlinear Latent Hierarchical Models
Year: 2023
Abstract: Identifying latent variables and causal structures from observational data is essential to many real-world applications involving biological data, medical data, and unstructured data such as images and languages. However, this task can be highly challenging, especially when observed variables are generated by causally related latent variables and the relationships are nonlinear. In this work, we investigate the identification problem for nonlinear latent hierarchical causal models in which observed variables are generated by a set of causally related latent variables, and some latent variables may not have observed children. We show that the identifiability of causal structures and latent variables (up to invertible transformations) can be achieved under mild assumptions: on causal structures, we allow for multiple paths between any pair of variables in the graph, which relaxes latent tree assumptions in prior work; on structural functions, we permit general nonlinearity and multi-dimensional continuous variables, alleviating existing work's parametric assumptions. Specifically, we first develop an identification criterion in the form of novel identifiability guarantees for an elementary latent variable model. Leveraging this criterion, we show that both causal structures and latent variables of the hierarchical model can be identified asymptotically by explicitly constructing an estimation procedure. To the best of our knowledge, our work is the first to establish identifiability guarantees for both causal structures and latent variables in nonlinear latent hierarchical models.
Authors: Lingjing Kong, Biwei Huang, Feng Xie, E. Xing, Yuejie Chi, Kun Zhang
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work develops an identification criterion in the form of novel identifiability guarantees for an elementary latent variable model and shows that both causal structures and latent variables of the hierarchical model can be identified asymptotically by explicitly constructing an estimation procedure.'}
Url: http://arxiv.org/pdf/2306.07916
Title: Identification of Nonlinear Latent Hierarchical Models
Authors: Lingjing Kong, Biwei Huang, Feng Xie, Eric Xing, Yuejie Chi, Kun Zhang
Section: C.1 Relaxation of Condition 2.4-ii
without observed non-leaf variables, which can be tackled by Algorithm 1.
Answer: "
"In spring 2024, What is the course number for Generative AI?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, What is the course number for Generative AI?

Context: In Semester Spring 2024, from the department of Information Networking Institute, the subject titled 'INI Special Topics:' with Course ID 14825 and Section NA offers 6.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Information Networking Institute, the subject titled 'Generative AI and Large Language Model' with Course ID 14825 and Section S3 offers 6.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the San Jose, California location,led by experienced instructor Farag located in Building B23, Room 211.
In Semester Spring 2024, from the department of Information Networking Institute, the subject titled 'Browser Security' with Course ID 14828 and Section Lec 1 offers 12.0 units. The Class meets Monday Wednesday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hibshi located in Building CIC, Room 1201.
In Semester Spring 2024, from the department of Information Networking Institute, the subject titled 'Browser Security' with Course ID 14828 and Section Lec 2 offers 12.0 units. The Class meets Monday Wednesday between 09:30AM and 10:50AM ET. Students attend lectures at the San Jose, California location,led by experienced instructor Hibshi located in Building B23, Room 212.
In Semester Spring 2024, from the department of Information Networking Institute, the subject titled 'Browser Security' with Course ID 14828 and Section A offers 12.0 units. The Class meets Friday between 02:00PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hibshi located in Building CIC, Room 1201.
In Semester Spring 2024, from the department of Information Networking Institute, the subject titled 'Browser Security' with Course ID 14828 and Section SV offers 12.0 units. The Class meets Friday between 11:00AM and 11:50AM ET.
In Semester Spring 2024, from the department of Information Networking Institute, the subject titled 'INI Special Topics:' with Course ID 14825 and Section NA offers 6.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Information Networking Institute, the subject titled 'Generative AI and Large Language Model' with Course ID 14825 and Section S3 offers 6.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the San Jose, California location,led by experienced instructor Farag located in Building B23, Room 211.
In Semester Spring 2024, from the department of Information Networking Institute, the subject titled 'Browser Security' with Course ID 14828 and Section Lec 1 offers 12.0 units. The Class meets Monday Wednesday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hibshi located in Building CIC, Room 1201.
In Semester Spring 2024, from the department of Information Networking Institute, the subject titled 'Browser Security' with Course ID 14828 and Section Lec 2 offers 12.0 units. The Class meets Monday Wednesday between 09:30AM and 10:50AM ET. Students attend lectures at the San Jose, California location,led by experienced instructor Hibshi located in Building B23, Room 212.
In Semester Spring 2024, from the department of Information Networking Institute, the subject titled 'Browser Security' with Course ID 14828 and Section A offers 12.0 units. The Class meets Friday between 02:00PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hibshi located in Building CIC, Room 1201.
In Semester Spring 2024, from the department of Information Networking Institute, the subject titled 'Browser Security' with Course ID 14828 and Section SV offers 12.0 units. The Class meets Friday between 11:00AM and 11:50AM ET.
Answer: "
"What is the definition of dogwhistles?
","['maarten sap_a5731b32060909bfc8848fa5f7e1e14ca3b53240_metadata.txt', 'maarten sap_a5731b32060909bfc8848fa5f7e1e14ca3b53240_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the definition of dogwhistles?

Context: Faculty Name: maarten sap
Paperid: a5731b32060909bfc8848fa5f7e1e14ca3b53240
Title: From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models
Year: 2023
Abstract: Dogwhistles are coded expressions that simultaneously convey one meaning to a broad audience and a second, often hateful or provocative, meaning to a narrow in-group; they are deployed to evade both political repercussions and algorithmic content moderation. For example, the word “cosmopolitan” in a sentence such as “we need to end the cosmopolitan experiment” can mean “worldly” to many but also secretly mean “Jewish” to a select few. We present the first large-scale computational investigation of dogwhistles. We develop a typology of dogwhistles, curate the largest-to-date glossary of over 300 dogwhistles with rich contextual information and examples, and analyze their usage in historical U.S. politicians’ speeches. We then assess whether a large language model (GPT-3) can identify dogwhistles and their meanings, and find that GPT-3’s performance varies widely across types of dogwhistles and targeted groups. Finally, we show that harmful content containing dogwhistles avoids toxicity detection, highlighting online risks presented by such coded language. This work sheds light on the theoretical and applied importance of dogwhistles in both NLP and computational social science, and provides resources to facilitate future research in modeling dogwhistles and mitigating their online harms.
Authors: Julia Mendelsohn, Ronan Le Bras, Yejin Choi, Maarten Sap
Venue: Annual Meeting of the Association for Computational Linguistics
Tldr: None
Url: http://arxiv.org/pdf/2305.17174
Title: From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models
Authors: Julia Mendelsohn, Ronan Le Bras, Yejin Choi, Maarten Sap
Section: A Appendix
taxonomy (§2.1; Fig. 2). GPT-3 has the lowest performance for humor-based and arbitrary target group label dogwhistles, and the highest performance for representatives (Bogeymen), phoneticbased target group labels, and policies (Table A.4). anti-liberal 0.292 0.106 transphobic 0.229 0.024 Variation across dogwhistle definitions Only 19.1% of GPT-3 generations include the correct covert meaning when prompted with no dogwhistle definition. Prompting GPT-3 with any of the five dogwhistle definitions greatly improved performance over no definition provided, but the extent varied, with the Merriam-Webster definition yielding the lowest improvement (43.8%) and Wikipedia yielding the highest (54.3%) (Table A.6). The boost in performance by adding the secret cue depends on the specific definition used; the secret cue has a bigger effect when using the Merriam-Webster and Albertson (2015) definitions (Figure A.3). Where does GPT-3 perform poorly? Most unrecognized dogwhistles are part of the informal register, especially symbols (e.g. the transphobic spiderweb or cherry emojis). Other unrecognized dogwhistles include “Operation Google” terms (e.g. Skype, Yahoo), more recent terms (e.g. Let’s Go Brandon), and several antisemitic and transphobic dogwhistles whose covert meanings are especially context-dependent (e.g. adult human female, XX, (Wikipedia) early life, fellow white people). Unrecognized formal dogwhistles tend to be extremely subtle and nuanced (e.g. Dred Scott as a conservative anti-abortion dogwhistle) or are highlyconventionalized phrases that may be far more commonly used without the covert implicature (e.g. the antisemitic dogwhistle poisoning the well). Where does GPT-3 perform well? GPT-3 readily identifies Islamophobic dogwhistles (e.g. radical Islam, Barack Hussein Obama), many antisemitic conspiracy theories (e.g.
Answer: "
"What are the two proposed subtasks for the DSTC11 automatic evaluation track?
","['alexander rudnicky_9799c17fd287bb9e8d231fe032c6dbf9c0c9d675_metadata.txt', 'alexander rudnicky_9799c17fd287bb9e8d231fe032c6dbf9c0c9d675_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the two proposed subtasks for the DSTC11 automatic evaluation track?

Context: Faculty Name: alexander rudnicky
Paperid: 9799c17fd287bb9e8d231fe032c6dbf9c0c9d675
Title: Overview of Robust and Multilingual Automatic Evaluation Metrics

for Open-Domain Dialogue Systems at DSTC 11 Track 4
Year: 2023
Abstract: The advent and fast development of neural networks have revolutionized the research on dialogue systems and subsequently have triggered various challenges regarding their automatic evaluation. Automatic evaluation of open-domain dialogue systems as an open challenge has been the center of the attention of many researchers. Despite the consistent efforts to improve automatic metrics’ correlations with human evaluation, there have been very few attempts to assess their robustness over multiple domains and dimensions. Also, their focus is mainly on the English language. All of these challenges prompt the development of automatic evaluation metrics that are reliable in various domains, dimensions, and languages. This track in the 11th Dialogue System Technology Challenge (DSTC11) is part of the ongoing effort to promote robust and multilingual automatic evaluation metrics. This article describes the datasets and baselines provided to participants and discusses the submission and result details of the two proposed subtasks.
Authors: Mario Rodr'iguez-Cantelar, Chen Zhang, Chengguang Tang, Ke Shi, Sarik Ghazarian, João Sedoc, L. F. D’Haro, Alexander I. Rudnicky
Venue: DSTC
Tldr: {'model': 'tldr@v2.0.0', 'text': 'The datasets and baselines provided to participants are described and the submission and result details of the two proposed subtasks are discussed, which promote robust and multilingual automatic evaluation metrics.'}
Url: https://arxiv.org/pdf/2306.12794
Title: Overview of Robust and Multilingual Automatic Evaluation Metrics for Open-Domain Dialogue Systems at DSTC 11 Track 4
Authors: Mario Rodríguez-Cantelar, Chen Zhang, Chengguang Tang, Ke Shi, Sarik Ghazarian, João Sedoc, Luis Fernando D’Haro, Alexander Rudnicky
Section: B Appendix: Existing Benchmark Datasets
Descriptions of the datasets that constitute the DSTC10 benchmark can be found at Zhang et al. (2022c). Details of the remaining evaluation datasets are as follows: ECM-Eval - The test instances in ECM-Eval test set are sampled from the Emotional Short-Text Conversation (ESTC) dialogue corpus (Zhou et al., 2018b), which is built on top of the Short-Text Conversation (STC) dataset (Shang et al., 2015). ESTC is designed to build Chinese empathetic dialogue systems. The dialogues are crawled from Weibo and post-processing, such as the removal of trivial responses and filtering out potential advertisements, has been conducted by Shang et al. (2015). The dialogues are automatically annotated by pre-trained emotion classifiers along six different emotion categories, such as angry, happy, sad, etc. The dialogues in ESTC are much shorter. Most contain only a single post-response pair. LCCC-Eval - Data in LCCC-Eval are sampled from the Large-scale Cleaned Chinese Conversation dialogue corpus (LCCC) (Wang et al., 2020b). The LCCC corpus is designed for pretraining the Chinese dialogue model. The dialogues are mainly collected from Weibo, a Chinese microblogging website17 and other open-source Chinese dialogue corpora, such as the Douban Conversation (Wu et al., 2017) and the E-Commerce Conversation Corpus (Zhang et al., 2018b). All the dialogues belong to the general domain and a rigorous cleaning process, which is based on a series of heuristic rules and several classifiers, is conducted to filter out dialogues with noise, such as dirty words, special characters, facial expressions, ungrammatical sentences, etc.
Answer: "
"Who was the first dean of the School of Computer Science?
","['history_d401.txt', 'history_d401.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who was the first dean of the School of Computer Science?

Context: A history of SCS | SCS25 - Carnegie Mellon University School of Computer Science Skip to main content Legal Events Register 25 Things History Video A history of SCS For an expanded history of the School of Computer Science and its predecessors at CMU, read ""Institutional Memories"" in the Summer 2014 issue of The Link magazine.In 2014, the School of Computer Science celebrated its 25th year as a stand-alone college within Carnegie Mellon University. It was arguably the first college devoted solely to computer science in the United States, and a model for others that followed. But CMU’s computer science era begins much earlier—in 1956, with the arrival of an IBM 650 computer on the campus of what was then known as Carnegie Institute of Technology. The IBM 650 had magnetic-drum memory and a processing speed of approximately 60 instructions per second. Herb Simon (H’90), associate dean of the Graduate School of Industrial Administration—now known as CMU’s Tepper School of Business—established Carnegie Tech’s first Computation Center with the help of its first director, Alan Perlis (S’42).First freshman-level computer science courseIn 1956 and 1957, Simon, Allen Newell (IA’57) and Cliff Shaw of RAND designed the Logic Theorist, a computer program that could develop proofs for theorems in much the same way a human would work. They also developed linked-list data structures, the foundation of computer programming. Perlis, Simon and Newell are credited with defining the term “computer science” as “the theory and design of computers,” as well as (in Newell’s words) “the study of the phenomena arising from them.” In 1958, Perlis began teaching the first freshman-level computer programming course in the United States at Carnegie Tech.Computer science Ph.D. program createdIn 1961, the Computation Center and its newest computer, a Bendix G-20, were moved to recently completed Scaife Hall. That same year, Carnegie Tech created an interdisciplinary Ph.D. program called Systems and Communications Sciences, combining elements of computer science, mathematics, psychology, business and electrical engineering. The university’s first computer science Ph.D.s were graduates of this program.Computer Science Department establishedIn 1965, Carnegie Tech established its Computer Science Department, or CSD, with a $5 million grant from the R.K. Mellon Foundation.
The experiment, which lasted two years, was an unqualified success. Separately, and also in 1986, the Pittsburgh Supercomputing Center was created as a joint effort between CMU, the University of Pittsburgh and Westinghouse Electric Corp.SCS is officially formedCMU’s Faculty Senate in the fall of 1988 agreed to President Richard Cyert’s plan to elevate the Department of Computer Science to college status. In addition to the Computer Science Department, SCS also incorporated the Robotics Institute, the Center for Machine Translation, and researchers from the Information Technology Center, which had developed Andrew. On Dec. 13, 1988, Cyert (H’89) told faculty and staff that Habermann had been appointed CMU’s first Dean of Computer Science, effective Dec. 1, and that the School of Computer Science would soon begin operations. SCS made its formal debut on Dec. 22, 1988, with a reception in the Wherrett Room of Skibo Hall, CMU’s student union. The official announcement of CMU’s new “graduate School of Computer Science” was made Jan. 3, 1989.Undergraduate degrees beginFor several years, undergraduates interested in computer science pursued an “applied math/CS” bachelor’s degree offered by the Mathematics Department. CSD professor Mary Shaw (CS’72) led CMU’s first effort to design an undergraduate curriculum solely in computer science. She and her colleagues were guided by the Carnegie Plan—guidelines established in 1938 under Carnegie Tech President Robert Doherty (A’40, E’48, H’50), outlining the principles of a sound professional education. Drawing on Shaw’s plan and also on the work of other faculty members, an undergraduate program in computer science was created during the 1989-90 academic year. Seven CS majors were admitted to the program as sophomores. Another 73 undergraduates were admitted in 1990–91.
Answer: "
"When was Andrew project launched?
","['history_d401.txt', '25things_d400.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When was Andrew project launched?

Context: As the 1970s progressed, Newell became interested in human-computer interaction, and began a long relationship with Xerox’s Palo Alto Research Center, or PARC, which released the Xerox Alto in 1973. Considered a forerunner to many of the computing environments that followed, Alto featured a graphical-user interface and was among the first commercially available workstations controlled with a mouse. Inspired by Alto, Reddy launched a drive for development of CMU’s own “three-M” machine—a personal workstation with a megabyte of memory, a megapixel display and at least one million instructions per second of processing power.Launching a Robotics InstituteIn 1979, an executive at Pittsburgh’s Westinghouse Electric Corp., Tom Murrin, collaborated with Jordan and Reddy to create the Robotics Institute, with Reddy as its first director. By 1982, the Computer Science Department included more than 30 faculty members and 100 graduate students.The best-wired campus in the worldWorking with IBM in the early 1980s, the university and the Computer Science Department established another new research frontier: Development of a high-speed computer network that would reach virtually every room on campus, along with a GUI-based computing environment, and providing networked PCs or workstations for 7,000 students, faculty members and employees. Called the Andrew Project, it turned Carnegie Mellon into the best-connected, most-wired university in the world—a process Newell called “greening up the campus with computer science.” CMU also became home to a new Software Engineering Institute, funded by the Defense Department, to study computer security and develop best practices in the design of operating systems. Between 1982 and 1985, the amount of sponsored research in the Computer Science Department doubled, from $7.2 million to $15.3 million—more than the other four departments in the Mellon College of Science combined.A “school of computer science” is proposedFeeling that CSD’s needs were inadequately represented in MCS, CSD head A. Nico Habermann and then-CMU provost Angel Jordan in 1986 wrote a white paper proposing the creation of “a School of Computer Science.” Responding to concerns from the faculty that the change might be taking place too quickly, the university first established a free-floating Department of Computer Science. The experiment, which lasted two years, was an unqualified success.
At the core of all modern Apple devices --- including iPhones, iPads and MacBooks --- is the Mach kernel, developed at CMU under the leadership of then-professor Rick Rashid. 11. Computer chess, 1990Could a computer play chess at the level of the world’s best players? For many years, it was considered the “holy grail” of artificial intelligence. Hitech, developed by CMU researcher Hans Berliner (CS’74), was the first computer to achieve grandmaster status. CMU alumni played key roles in developing “Deep Blue,” the IBM machine that beat human chess champion Garry Kasparov in 1997. 12. Java, 1991 As a CMU grad student, James Gosling (CS’83) worked on the Andrew project, which stressed interoperability between computers, whether they were Macs, IBMs or Unix machines. Those lessons served Gosling well when he developed Java, the first programming language able to run on almost any platform. 13. Email attachments, 1992Steve Jobs liked the email system built into CMU’s Andrew so much that he tried to hire Nathaniel Borenstein (CS’81,’85) and the rest of his team to create a similar program for Apple. Borenstein didn’t take the offer, but he did like Jobs’ idea about attaching documents to email. Borenstein went on to develop the MIME standard that’s used by all email programs to send photos and other files over the Internet. 14. Web search engines, 1994The World Wide Web was still in its toddler stage when CMU researcher Michael “Fuzzy” Mauldin (CS’83,’89) developed one of the first successful search engines, Lycos. It was the most-visited site on the Web by 1999. 15. Model checking, 1994CMU professor Edmund Clarke had long stressed the importance of verifying computer hardware and software through a formal problem-solving technique called “model checking.” In 1994, his arguments gained new weight with the discovery that Intel’s amazing new Pentium chip made errors on certain math problems. Clarke would go on to receive the Turing Award for his role in the development of model checking. 16.
Answer: "
"In Fall 2023, where was 11737 taught?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In Fall 2023, where was 11737 taught?

Context: In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Linguistics Lab' with Course ID 11723 and Section A offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin, Mortensen located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Human Language for Artificial Intelligence' with Course ID 11724 and Section A offers 12.0 units. The Class meets Monday Wednesday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin located in Building WEH, Room 5409.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Meaning in Language Lab (Self Paced)' with Course ID 11726 and Section A offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rose located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Multilingual Natural Language Processing.' with Course ID 11737 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Li located in Building DH, Room 1212.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Machine Learning for Text and Graph-based Mining' with Course ID 11741 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Yang located in Building HH, Room B131.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Machine Learning for Text and Graph-based Mining' with Course ID 11741 and Section PP offers 12.0 units. The Class meets Schedule will be added between NA and NA ET.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Linguistics Lab' with Course ID 11723 and Section A offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin, Mortensen located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Human Language for Artificial Intelligence' with Course ID 11724 and Section A offers 12.0 units. The Class meets Monday Wednesday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin located in Building WEH, Room 5409.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Meaning in Language Lab (Self Paced)' with Course ID 11726 and Section A offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rose located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Multilingual Natural Language Processing.' with Course ID 11737 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Li located in Building DH, Room 1212.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Machine Learning for Text and Graph-based Mining' with Course ID 11741 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Yang located in Building HH, Room B131.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Machine Learning for Text and Graph-based Mining' with Course ID 11741 and Section PP offers 12.0 units. The Class meets Schedule will be added between NA and NA ET.
Answer: "
"When are the grades due for the Fall 2024 semester?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When are the grades due for the Fall 2024 semester?

Context: From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mini-2 Classes Begin is observed.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mid-Semester & Mini-1 grades due by 4 pm is observed.
On Friday, 27 October, 2023, during the Fall 2023 (F23) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
On Tuesday, 07 November, 2023, during the Fall 2023 (F23) semester, Democracy Day is observed, leading to No Classes, except Evening classes after 5 pm will still meet.
On Saturday, 11 November, 2023, during the Fall 2023 (F23) semester, Homecoming is observed.
From 13 November, 2023,Monday to 17 November, 2023,Friday marks the Spring 2024 Registration Week for Fall 2023 (F23) semester.
On Monday, 13 November, 2023, during the Fall 2023 (F23) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Wednesday, 15 November, 2023, during the Fall 2023 (F23) semester, Mini-2 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 22 November, 2023,Wednesday to 24 November, 2023,Friday marks the Thanksgiving Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Mini-2 pass/no pass & withdrawal deadline (2) is observed.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Faculty Course Evaluations open is observed.
On Friday, 08 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Last Day of Classes is observed.
On Monday, 02 September, 2024, during the Fall 2024 (F24) semester, Labor Day is observed, leading to No Classes & University Closed.
On Monday, 09 September, 2024, during the Fall 2024 (F24) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 18 September, 2024, during the Fall 2024 (F24) semester, Mini-1 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 30 September, 2024, during the Fall 2024 (F24) semester, Mini-1 Pass/no pass & withdrawal deadline (3) is observed.
On Monday, 30 September, 2024, during the Fall 2024 (F24) semester, Mini-1 Faculty Course Evaluations open is observed.
On Monday, 07 October, 2024, during the Fall 2024 (F24) semester, Semester drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 11 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Last Day of Classes is observed.
On Friday, 11 October, 2024, during the Fall 2024 (F24) semester, Mini-1 voucher election deadline (4) is observed.
On Saturday, 12 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Exams is observed.
On Saturday, 12 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Faculty Course Evaluations close is observed.
From 14 October, 2024,Monday to 18 October, 2023,Friday marks the Fall Break for Fall 2024 (F24) semester which leads to No Classes.
On Monday, 21 October, 2024, during the Fall 2024 (F24) semester, Mini-2 Classes Begin is observed.
On Wednesday, 23 October, 2024, during the Fall 2024 (F24) semester, Mid-Semester & Mini-1 grades due by 4 pm is observed.
On Friday, 25 October, 2024, during the Fall 2024 (F24) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
Answer: "
"What types of prompts can PaintSeg be configured to work with?
","['bhiksha raj_dc157eba8bdb4cfe6ee65566d8295939ac5b4b37_metadata.txt', 'bhiksha raj_dc157eba8bdb4cfe6ee65566d8295939ac5b4b37_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What types of prompts can PaintSeg be configured to work with?

Context: Faculty Name: bhiksha raj
Paperid: dc157eba8bdb4cfe6ee65566d8295939ac5b4b37
Title: PaintSeg: Training-free Segmentation via Painting
Year: 2023
Abstract: The paper introduces PaintSeg, a new unsupervised method for segmenting objects without any training. We propose an adversarial masked contrastive painting (AMCP) process, which creates a contrast between the original image and a painted image in which a masked area is painted using off-the-shelf generative models. During the painting process, inpainting and outpainting are alternated, with the former masking the foreground and filling in the background, and the latter masking the background while recovering the missing part of the foreground object. Inpainting and outpainting, also referred to as I-step and O-step, allow our method to gradually advance the target segmentation mask toward the ground truth without supervision or training. PaintSeg can be configured to work with a variety of prompts, e.g. coarse masks, boxes, scribbles, and points. Our experimental results demonstrate that PaintSeg outperforms existing approaches in coarse mask-prompt, box-prompt, and point-prompt segmentation tasks, providing a training-free solution suitable for unsupervised segmentation.
Authors: Xiang Li, Chung-Ching Lin, Yinpeng Chen, Zicheng Liu, Jinglu Wang, B. Raj
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'An adversarial masked contrastive painting process, which creates a contrast between the original image and a painted image in which a masked area is painted using off-the-shelf generative models, providing a training-free solution suitable for unsupervised segmentation.'}
Url: http://arxiv.org/pdf/2305.19406
Title: PaintSeg: Training-free Segmentation via Painting
Authors: Xiang Li, Chung-Ching Lin, Yinpeng Chen, Zicheng Liu, Jinglu Wang
Section: G More Visualization
In this section, we demonstrate more visualization of PaintSeg. We show more qualitative results with box prompt in Fig. D, with point prompt in Fig. E and with coarse mask prompt in Figs. F and G. Figure D: More visualization of PaintSeg with box prompt on COCO MVal. Figure E: More visualization of PaintSeg with point prompt. The point prompt is illustrated by the red point on the image on DAVIS and Berkeley and GrabCut. Prompt PromptMask Mask Figure F: More visualization of PaintSeg with coarse mask prompt on ECSSD. Prompt PromptMask Mask Figure G: More visualization of PaintSeg with coarse mask prompt on ECSSD.
Answer: "
"In spring 2024, When is the final deadline for withdrawing from a Mini-4 course?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, When is the final deadline for withdrawing from a Mini-4 course?

Context: On Monday, 31 March, 2025, during the Spring 2025 (S25) semester, Semester pass/no pass & withdrawal deadline is observed.
On Wednesday, 02 April, 2025, during the Spring 2025 (S25) semester, Mini-4 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 03 April, 2025,Thursday to 05 April, 2023,Saturday marks the Spring Carnival for Spring 2025 (S25) semester which leads to No Classes.
From 07 April, 2025,Monday to 11 April, 2023,Friday marks the Fall 2025 Registration Week for Spring 2025 (S25) semester.
On Monday, 14 April, 2025, during the Spring 2025 (S25) semester, Mini-4 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 14 April, 2025, during the Spring 2025 (S25) semester, Semester & Mini-4 Faculty Course Evaluations open is observed.
On Friday, 25 April, 2025, during the Spring 2025 (S25) semester, Last Day of Classes is observed.
On Friday, 25 April, 2025, during the Spring 2025 (S25) semester, Semester & Mini-4 voucher deadline (4) is observed.
From 28 April, 2025,Monday to 29 April, 2023,Tuesday marks the Final Examinations for Spring 2025 (S25) semester.
On Wednesday, 30 April, 2025, during the Spring 2025 (S25) semester, Reading Day is observed.
From 01 May, 2025,Thursday to 02 May, 2023,Friday marks the Final Examinations for Spring 2025 (S25) semester.
From 03 May, 2025,Saturday to 04 May, 2023,Sunday marks the Reading Days for Spring 2025 (S25) semester.
On Monday, 05 May, 2025, during the Spring 2025 (S25) semester, Final Examinations is observed.
On Monday, 05 May, 2025, during the Spring 2025 (S25) semester, Semester & Mini-4 Faculty Course Evaluations close is observed.
On Friday, 07 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 pass/no pass & withdrawal deadline (3) is observed.
On Friday, 14 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Faculty Course Evaluations open is observed.
On Tuesday, 18 June, 2024, during the Summer One 2024 (M24) semester, Semester course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Wednesday, 19 June, 2022, during the Summer One 2024 (M24) semester, Juneteenth is observed, leading to University Closed & No Classes.
On Thursday, 20 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Last Day of Classes is observed.
On Thursday, 20 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 voucher deadline (4) is observed.
On Friday, 21 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Final Exams is observed.
On Friday, 21 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Faculty Course Evaluations close is observed.
On Monday, 24 June, 2024, during the Summer One 2024 (M24) semester, Mini-6 First Day of Classes is observed.
On Tuesday, 25 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Final Grades Due by 4 pm is observed.
On Friday, 28 June, 2024, during the Summer One 2024 (M24) semester, Mini-6 add, audit, & tuition adjustment drop deadline (1) is observed.
On Thursday, 04 July, 2024, during the Summer One 2024 (M24) semester, Independence Day is observed, leading to University Closed & No Classes.
On Friday, 05 July, 2024, during the Summer One 2024 (M24) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
Answer: "
"In spring 2024, Who are the instructors for course 17514?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, Who are the instructors for course 17514?

Context: In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Principles of Software Construction: Objects, Design, and Concurrency' with Course ID 17514 and Section A offers 12.0 units. The Class meets Wednesday between 09:00AM and 09:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Aldrich, Garrod, Lacomis located in Building WEH, Room 5328.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Principles of Software Construction: Objects, Design, and Concurrency' with Course ID 17514 and Section B offers 12.0 units. The Class meets Wednesday between 10:00AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Aldrich, Garrod, Lacomis located in Building PH, Room 125B.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Principles of Software Construction: Objects, Design, and Concurrency' with Course ID 17514 and Section C offers 12.0 units. The Class meets Wednesday between 11:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Aldrich, Garrod, Lacomis located in Building WEH, Room 5316.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Principles of Software Construction: Objects, Design, and Concurrency' with Course ID 17514 and Section D offers 12.0 units. The Class meets Wednesday between 12:00PM and 12:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Aldrich, Garrod, Lacomis located in Building POS, Room 147.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Principles of Software Construction: Objects, Design, and Concurrency' with Course ID 17514 and Section E offers 12.0 units. The Class meets Wednesday between 01:00PM and 01:50PM ET.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Principles of Software Construction: Objects, Design, and Concurrency' with Course ID 17514 and Section A offers 12.0 units. The Class meets Wednesday between 09:00AM and 09:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Aldrich, Garrod, Lacomis located in Building WEH, Room 5328.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Principles of Software Construction: Objects, Design, and Concurrency' with Course ID 17514 and Section B offers 12.0 units. The Class meets Wednesday between 10:00AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Aldrich, Garrod, Lacomis located in Building PH, Room 125B.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Principles of Software Construction: Objects, Design, and Concurrency' with Course ID 17514 and Section C offers 12.0 units. The Class meets Wednesday between 11:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Aldrich, Garrod, Lacomis located in Building WEH, Room 5316.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Principles of Software Construction: Objects, Design, and Concurrency' with Course ID 17514 and Section D offers 12.0 units. The Class meets Wednesday between 12:00PM and 12:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Aldrich, Garrod, Lacomis located in Building POS, Room 147.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Principles of Software Construction: Objects, Design, and Concurrency' with Course ID 17514 and Section E offers 12.0 units. The Class meets Wednesday between 01:00PM and 01:50PM ET.
Answer: "
"What is the corresponding author's emaill address for the SantaCoder paper?
","['daniel fried_1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a_content_0.txt', 'daniel fried_1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the corresponding author's emaill address for the SantaCoder paper?

Context: Title: SANTACODER: DON’T REACH FOR THE STARS!
Authors: Loubna Ben Allal, Raymond Li, Chenghao Mou, Joel Lamy Poirier, Ian Yu PIISA, Paulo Villegas, Leandro von Werra
Section: C PII
C.1 REGULAR EXPRESSIONS Email addresses We used the following regular expression to detect emails. email_pattern = r’’’ (?<= ˆ | [\b\s@,?!;:)(’"".\p{Han}<] ) ( [ˆ\b\s@?!;,:)(’""<]+ @ [ˆ\b\s@!?;,/]* [ˆ\b\s@?!;,/:)(’"">.] \. \p{L} \w{1,} ) (?= $ | [\b\s@,?!;:)(’"".\p{Han}>] ) ’’’ We replace detected emails with [random 5 character string]@example.com. IP addresses We used the following regular expressions to detect IPv4 and IPv6 addresses. ipv4_pattern = r""(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?) (?:\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}"" ipv6_pattern = r""(?:[0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F ]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,7}:|(?:[0-9a-fA-F]{1,4}:) {1,6}:[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,5}(?::[0-9a-fAF]{1,4}){1,2}|(?:[0-9a-fA-F]{1,4}:){1,4}(?::[0-9a-fA-F]{1,4}) {1,3}|(?:[0-9a-fA-F]{1,4}:){1,3}(?
Title: SANTACODER: DON’T REACH FOR THE STARS!
Authors: Loubna Ben Allal, Raymond Li, Chenghao Mou, Joel Lamy Poirier, Ian Yu PIISA, Paulo Villegas, Leandro von Werra
Section: 6.1 ABLATIONS
Note that the stars filter removes the most data (over 60%) and, therefore, raises the question whether the performance difference is due to the smaller dataset. However, as can be seen in Figure 3, HumanEval pass@100 diverged early on in training, indicating that the drop in performance is not only due to data size but also data quality.
Answer: "
"How many authors contributed to the work Understanding Political Polarisation using Language Models: A dataset and method?
","['bhiksha raj_4e6a004e4f9a3b489004a2efb5b25b0bcd0f48b5_content_0.txt', 'bhiksha raj_4e6a004e4f9a3b489004a2efb5b25b0bcd0f48b5_content_2.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many authors contributed to the work Understanding Political Polarisation using Language Models: A dataset and method?

Context: Title: Understanding Political Polarisation using Language Models: A dataset and method
Authors: Samiran Gode, Supreeth Bare, Bhiksha Raj, Hyungon Yoo
Section: Acknowledgments
We would like to thank Yash Jain and Viraj Ranade for their contributions.
Title: Understanding Political Polarisation using Language Models: A dataset and method
Authors: Samiran Gode, Supreeth Bare, Bhiksha Raj, Hyungon Yoo
Section: Related Work
Task. (Palakodety, KhudaBukhsh, and Carbonell 2020) demonstrates the ability of BERT and similar LM’s to track community perception, aggregate opinions and compare the popularity of political parties and candidates. This is demonstrative of our work as we intend to use BERT for the purpose of sentiment analysis. The authors conclude by stating that the LM can be used as a pipeline for extracting Data in the future. In (Hamilton, Leskovec, and Jurafsky 2016) the authors try to counter the problem of word meaning changing semantically with context. They propose a robust method by using embeddings. These are then evaluated with the ’Law of Conformity’ and ’The Law of Innovation’. These display the role of frequency and polysemy in the building structural blocks of language. These blocks will be crucial for 2 reasons, (1) The meaning changes may adversely affect sentiment analysis and thus affect results. Thus frequency and polysemy must be duly curtailed. (2) The embedding research is fundamental as we are using embedding-based models. Specifically Word2vec.
Answer: "
"CAPTCHAs were invented by CMU researchers in 2000. What was the title of their paper?
","['25things_d400.txt', 'bhiksha raj_e2572e0adacfb116b19b25691e7f6b3749490a88_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: CAPTCHAs were invented by CMU researchers in 2000. What was the title of their paper?

Context: Clarke would go on to receive the Turing Award for his role in the development of model checking. 16. CAPTCHAs, 2000 “Spam” and malicious attacks were a growing problem on the Internet when hackers developed automated “bots” that could sign up for email and other Web services without human intervention. Luis von Ahn (CS’03,’05), Nick Hopper (CS’04), John Langford (CS’02) and CMU professor Manuel Blum invented a “Completely Automated Public Turing Test to tell Computers and Humans Apart,” or CAPTCHA, to help foil the bots. A later variation, reCAPTCHA, is helping digitize old books and newspapers. 17. Robotic video cameras, 2001When Baltimore Ravens quarterback Trent Dilfer dropped back to pass, TV viewers of Super Bowl XXXV saw something they’d never seen before --- the motion froze and the view suddenly rotated to show Dilfer’s opposite side. CBS called it Eyevision. The synchronized system of robotic cameras and advanced image processing was the brainchild of CMU’s Takeo Kanade, one of many advances he pioneered in computer vision. 18. Self-driving vehicles, 2007Carnegie Mellon’s early attempts at self-driving vehicles progressed slowly, creeping around Pittsburgh’s Schenley Park in the late 1990s. But they were going full-throttle by the time CMU’s self-driving SUV, named BOSS, won the 2007 DARPA Urban Challenge road race. 19. Thought reading programs, 2007Your brain reacts in different ways, depending on what words you’re thinking about --- ways that are measurable with magnetic-resonance imaging, or MRI, machines. CMU researchers Tom Mitchell and Marcel Just are decoding those brain scans and are making progress at reading people’s thoughts. 20. Kidney donor matching, 2008 Organ transplants save lives every day, but more people could likely be saved if it was easier to match recipients with donors who are unrelated. An algorithm developed by CMU scientists is close to enabling a nationwide network that would match living kidney donors with potential recipients whom they've never met in real life. 21. RNA sequencing via videogames, 2010 Thanks to crowdsourcing, science isn’t just for scientists any more.
Title: TRAINING AUDIO CAPTIONING MODELS WITHOUT AUDIO
Authors: Soham Deshmukh, Benjamin Elizalde, Dimitra Emmanouilidou, Bhiksha Raj, Rita Singh, Huaming Wang
Section: 7. REFERENCES
Elizalde, “Never-ending learning of sounds,” Ph.D. dissertation, CMU Pittsburgh, PA, 2020. [17] X. Mei, X. Liu, J. Sun, M. D. Plumbley, and W. Wang, “Diverse audio captioning via adversarial training,” in ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 8882–8886. [18] X. Mei, C. Meng, H. Liu, Q. Kong, T. Ko, C. Zhao, M. D. Plumbley, Y. Zou, and W. Wang, “Wavcaps: A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research,” arXiv preprint arXiv:2303.17395, 2023. [19] B. Elizalde, S. Deshmukh, M. A. Ismail, and H. Wang, “Clap learning audio concepts from natural language supervision,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2023. [20] B. Elizalde, S. Deshmukh, and H. Wang, “Natural language supervision for general-purpose audio representations,” submitted to IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2024. [21] W. Liang, Y. Zhang, Y. Kwon, S. Yeung, and J. Zou, “Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning,” in Advances in Neural Information Processing Systems, A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, Eds., 2022.
Answer: "
"In spring 2024, What is the title of course 10301?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, What is the title of course 10301?

Context: In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Introduction to Machine Learning (SCS Majors)' with Course ID 10315 and Section C offers 12.0 units. The Class meets Friday between 12:00PM and 12:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Virtue located in Building GHC, Room 4301.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Introduction to Machine Learning (SCS Majors)' with Course ID 10315 and Section D offers 12.0 units. The Class meets Friday between 01:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Virtue located in Building GHC, Room 4102.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Introduction to Machine Learning (SCS Majors)' with Course ID 10315 and Section E offers 12.0 units. The Class meets Friday between 01:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Virtue located in Building WEH, Room 4625.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Introduction to Machine Learning (SCS Majors)' with Course ID 10315 and Section F offers 12.0 units. The Class meets Friday between 03:00PM and 03:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Virtue located in Building WEH, Room 2302.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Art and Machine Learning' with Course ID 10335 and Section A offers 12.0 units. The Class meets Monday Wednesday between 07:00PM and 08:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kang located in Building TEP, Room 1403.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Deep Reinforcement Learning & Control' with Course ID 10403 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 12:30PM and 01:50PM ET.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Introduction to Machine Learning (SCS Majors)' with Course ID 10315 and Section C offers 12.0 units. The Class meets Friday between 12:00PM and 12:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Virtue located in Building GHC, Room 4301.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Introduction to Machine Learning (SCS Majors)' with Course ID 10315 and Section D offers 12.0 units. The Class meets Friday between 01:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Virtue located in Building GHC, Room 4102.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Introduction to Machine Learning (SCS Majors)' with Course ID 10315 and Section E offers 12.0 units. The Class meets Friday between 01:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Virtue located in Building WEH, Room 4625.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Introduction to Machine Learning (SCS Majors)' with Course ID 10315 and Section F offers 12.0 units. The Class meets Friday between 03:00PM and 03:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Virtue located in Building WEH, Room 2302.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Art and Machine Learning' with Course ID 10335 and Section A offers 12.0 units. The Class meets Monday Wednesday between 07:00PM and 08:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kang located in Building TEP, Room 1403.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Deep Reinforcement Learning & Control' with Course ID 10403 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 12:30PM and 01:50PM ET.
Answer: "
"When does the Fall 2024 course registeration start for doctoral students?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-2324.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When does the Fall 2024 course registeration start for doctoral students?

Context: From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mini-2 Classes Begin is observed.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mid-Semester & Mini-1 grades due by 4 pm is observed.
On Friday, 27 October, 2023, during the Fall 2023 (F23) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
On Tuesday, 07 November, 2023, during the Fall 2023 (F23) semester, Democracy Day is observed, leading to No Classes, except Evening classes after 5 pm will still meet.
On Saturday, 11 November, 2023, during the Fall 2023 (F23) semester, Homecoming is observed.
From 13 November, 2023,Monday to 17 November, 2023,Friday marks the Spring 2024 Registration Week for Fall 2023 (F23) semester.
On Monday, 13 November, 2023, during the Fall 2023 (F23) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Wednesday, 15 November, 2023, during the Fall 2023 (F23) semester, Mini-2 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 22 November, 2023,Wednesday to 24 November, 2023,Friday marks the Thanksgiving Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Mini-2 pass/no pass & withdrawal deadline (2) is observed.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Faculty Course Evaluations open is observed.
On Friday, 08 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Last Day of Classes is observed.
withdrawal grade assigned after this date (2)', Semester: 'Spring 2024 (S24)'
Start Date: '2024-04-11', End Date: '2024-04-13', Days: 'Thursday to Saturday', Event: 'Spring Carnival; No Classes', Semester: 'Spring 2024 (S24)'
Start Date: '2024-04-15', End Date: '2024-04-19', Days: 'Monday to Friday', Event: 'Fall 2024 Registration Week', Semester: 'Spring 2024 (S24)'
Date: '2024-04-15', Day: 'Monday', Event: 'Mini-4 pass/no pass & withdrawal deadline (3)', Semester: 'Spring 2024 (S24)'
Date: '2024-04-15', Day: 'Monday', Event: 'Semester & Mini-4 Faculty Course Evaluations open ', Semester: 'Spring 2024 (S24)'
Date: '2024-04-26', Day: 'Friday', Event: 'Last Day of Classes', Semester: 'Spring 2024 (S24)'
Date: '2024-04-26', Day: 'Friday', Event: 'Semester & Mini-4 voucher deadline (4)', Semester: 'Spring 2024 (S24)'
Start Date: '2024-04-29', End Date: '2024-04-30', Days: 'Monday to Tuesday', Event: 'Final Examinations ', Semester: 'Spring 2024 (S24)'
Date: '2024-05-01', Day: 'Wednesday', Event: 'Reading Day', Semester: 'Spring 2024 (S24)'
Start Date: '2024-05-02', End Date: '2024-05-03', Days: 'Thursday to Friday', Event: 'Final Examinations ', Semester: 'Spring 2024 (S24)'
Start Date: '2024-05-04', End Date: '2024-05-05', Days: 'Saturday to Sunday', Event: 'Reading Days', Semester: 'Spring 2024 (S24)'
Date: '2024-05-06', Day: 'Monday', Event: 'Final Examinations', Semester: 'Spring 2024 (S24)'
Date: '2024-05-06', Day: 'Monday',
Answer: "
"In spring 2024, What is the title of course 10601?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, What is the title of course 10601?

Context: In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Independent Study' with Course ID 10520 and Section A offers VAR units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Gormley located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Introduction to Machine Learning (Master's)' with Course ID 10601 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Chai, Gormley, Heidari located in Building TEP, Room 1403.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Introduction to Machine Learning (Master's)' with Course ID 10601 and Section B offers 12.0 units. The Class meets Monday Wednesday Friday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Chai, Gormley, Heidari located in Building GHC, Room 4401.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Machine Learning with Large Datasets' with Course ID 10605 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Gordon, Talwalkar located in Building DH, Room 2210.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Art and Machine Learning' with Course ID 10615 and Section A offers 12.0 units. The Class meets Monday Wednesday between 07:00PM and 08:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kang located in Building TEP, Room 1403.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Independent Study: Research' with Course ID 10620 and Section A offers 612 units. The Class meets Schedule will be added between NA and NA ET.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Independent Study' with Course ID 10520 and Section A offers VAR units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Gormley located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Introduction to Machine Learning (Master's)' with Course ID 10601 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Chai, Gormley, Heidari located in Building TEP, Room 1403.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Introduction to Machine Learning (Master's)' with Course ID 10601 and Section B offers 12.0 units. The Class meets Monday Wednesday Friday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Chai, Gormley, Heidari located in Building GHC, Room 4401.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Machine Learning with Large Datasets' with Course ID 10605 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Gordon, Talwalkar located in Building DH, Room 2210.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Art and Machine Learning' with Course ID 10615 and Section A offers 12.0 units. The Class meets Monday Wednesday between 07:00PM and 08:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kang located in Building TEP, Room 1403.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Independent Study: Research' with Course ID 10620 and Section A offers 612 units. The Class meets Schedule will be added between NA and NA ET.
Answer: "
"What type of models does the AV-SUPURB benchmark evaluate?
","['shinji watanabe_ef567580e167c3e7c546345df93d644be5d4f66f_content_1.txt', 'shinji watanabe_ef567580e167c3e7c546345df93d644be5d4f66f_content_3.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What type of models does the AV-SUPURB benchmark evaluate?

Context: Title: AV-SUPERB: A MULTI-TASK EVALUATION BENCHMARK FOR AUDIO-VISUAL REPRESENTATION MODELS
Authors: Yuan Tseng, Layne Berry, Yi-Ting Chen, I-Hsiang Chiu, Hsuan-Hao Lin, Max Liu, Puyuan Peng, Yi-Jen Shih, Hung-Yu Wang, Haibin Wu, Po-Yao Huang, Chun-Mao Lai, Shang-Wen Li, David Harwath, Yu Tsao, Shinji Watanabe, Abdelrahman Mohamed, Chi-Luen Feng, Hung-yi Lee
Section: 4.4. Layer-wise Contribution Analysis
in Table 2. For AV-HuBERT, we see that visual speech recognition on LRS3-TED is not a suitable intermediate task in general. Videoonly representations obtain small gains in generalizability, at the cost of greatly reducing audio-only and fusion performance. We posit that intermediate-task fine-tuning with (video,text) pairs shifts AV-HuBERT Transformer layers to favor video input alone, reducing usability for audio-only and audio-visual inputs. Contrarily, for audio-visual fusion with MAViL, we see that intermediate-task training on AudioSet-2M not only brings substantial improvements to all AEC and AR datasets, but also improves ASV while maintaining ASR performance. This suggests that finetuning on AudioSet-2M may be sufficiently diverse to improve speaker separability of representations without much loss of content information. 6. CONCLUSIONS We introduce AV-SUPERB, the first benchmark for assessing general-purpose capabilities of audio-visual representations. AVSUPERB includes a suite of 7 speech and audio processing datasets covering 5 audio-visual tasks. The benchmark is split into three tracks: two unimodal audio-only or video-only representations tracks, as well as a bimodal audio-visual fusion track, which enables easy comparison between unimodal and bimodal learning. Despite advances made in recent years, our experiments show that none of the models tested generalize to all tasks, leading us to conclude that further study is required to develop universal audio-visual models. As discussed in Section 3.1, although our benchmark aims to comprehensively evaluate audio-visual models, only a limited set of tasks and datasets are included in its current form.
Title: AV-SUPERB: A MULTI-TASK EVALUATION BENCHMARK FOR AUDIO-VISUAL REPRESENTATION MODELS
Authors: Yuan Tseng, Layne Berry, Yi-Ting Chen, I-Hsiang Chiu, Hsuan-Hao Lin, Max Liu, Puyuan Peng, Yi-Jen Shih, Hung-Yu Wang, Haibin Wu, Po-Yao Huang, Chun-Mao Lai, Shang-Wen Li, David Harwath, Yu Tsao, Shinji Watanabe, Abdelrahman Mohamed, Chi-Luen Feng, Hung-yi Lee
Section: 4.4. Layer-wise Contribution Analysis
Shi et al., “Learning audio-visual speech representation by masked multimodal cluster prediction,” in ICLR, 2022. [36] Himangi Mittal et al., “Learning state-aware visual representations from audible interactions,” in NeurIPS, 2022. [37] Sangho Lee et al., “Parameter efficient multimodal transformers for video representation learning,” in ICLR, 2021. [38] Po-Yao Huang et al., “Mavil: Masked audio-video learners,” arXiv preprint arXiv:2212.08071, 2022. [39] Ankita Pasad et al., “Layer-wise analysis of a self-supervised speech representation model,” in ASRU, 2021. [40] Jort F. Gemmeke et al., “Audio set: An ontology and humanlabeled dataset for audio events,” in ICASSP, 2017. [41] Honglie Chen et al., “Vggsound: A large-scale audio-visual dataset,” in ICASSP, 2020. [42] Sanyuan Chen et al., “Wavlm: Large-scale self-supervised pretraining for full stack speech processing,” IEEE Journal of Selected Topics in Signal Processing, 2022. [43] Jason Phang et al., “Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks,” arXiv preprint arXiv:1811.01088, 2018. [44] Alex Wang et al., “Can you tell me how to get past sesame street? sentence-level pretraining beyond language modeling,” in ACL, 2019.
Answer: "
"Who is teaching the Multimodal Machine Learning course this semester?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who is teaching the Multimodal Machine Learning course this semester?

Context: The Class meets Monday Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hauptmann located in Building GHC, Room 4307.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Multimodal Machine Learning' with Course ID 11777 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Morency located in Building MM, Room A14.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning' with Course ID 11785 and Section NA offers 12.0 units. The Class meets Monday Wednesday between 08:00AM and 09:20AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ramakrishnan, Singh located in Building BH, Room A51.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning' with Course ID 11785 and Section A offers 12.0 units. The Class meets Friday between 08:00AM and 09:20AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Singh, Ramakrishnan located in Building BH, Room A51.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning' with Course ID 11785 and Section B offers 12.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ramakrishnan, Singh located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning' with Course ID 11785 and Section M offers 12.0 units. The Class meets Monday Wednesday between 08:00AM and 09:20AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Singh, Ramakrishnan located in Building BH, Room A51.
The Class meets Monday Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hauptmann located in Building GHC, Room 4307.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Multimodal Machine Learning' with Course ID 11777 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Morency located in Building MM, Room A14.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning' with Course ID 11785 and Section NA offers 12.0 units. The Class meets Monday Wednesday between 08:00AM and 09:20AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ramakrishnan, Singh located in Building BH, Room A51.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning' with Course ID 11785 and Section A offers 12.0 units. The Class meets Friday between 08:00AM and 09:20AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Singh, Ramakrishnan located in Building BH, Room A51.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning' with Course ID 11785 and Section B offers 12.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ramakrishnan, Singh located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning' with Course ID 11785 and Section M offers 12.0 units. The Class meets Monday Wednesday between 08:00AM and 09:20AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Singh, Ramakrishnan located in Building BH, Room A51.
Answer: "
"For LTI PhD students what room are the mailboxes and office supplies located in?
","['handbook_phd_2023-2024.txt', 'miis-handbook_2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: For LTI PhD students what room are the mailboxes and office supplies located in?

Context: students in residence on the Pittsburgh campus are given an office in which to study 
and do research. Typically, offices are shared with other Ph.D. students, but they may also be 
shared with staff, visitors, or other members of the LTI. 
Offices are assigned by the LTIs Office Manager (see Section 1.2, Department Personnel, for 
contact information). 
2.3 
Mailboxes and Office Supplies 
Mailboxes and office supplies are in GHC 5404. 
2.4 
Photocopies and Printers 
Printers and photocopies are available to LTI students. The use of a photocopier or printer 
requires you to log in with your CMU ID card. LTI students may use printers/photocopiers 
scattered throughout the School of Computer Science buildings, but the machines in GHC 5404 
and GHC 6604 are the most convenient. The SCS Computing Facilities publishes a list of printers 
online at https://computing.cs.cmu.edu/desktop/printer-list. 
2.5 
Computers for LTI Ph.D. Students 
Ph.D. students are responsible for having their own laptop computers to support their education 
and research. Students are free to choose their own operating system (e.g., Linux, MacOs, 
Windows). 
Many Ph.D. advisors also provide access to computer clusters, cloud computing, or other 
resources to support computationally-intense research. 
Ph.D. students are given access to the LTIs computer cluster on an as-needed basis, to be used for 
course assignments, directed study projects, and/or capstone projects.  The LTI cluster provides 
storage and computation for projects involving large datasets and/or lengthy computation. 
Ph.D. students receive two types of user ids: An Andrew id and a CS id. All CMU students have 
an Andrew id. Computer Science students also have a CS id that provides access to SCS-specific 
resources (e.g., computer clusters). CS ids are being phased out very slowly, so it is likely that you 
will need both types of user id. 
LTI Ph.D. Graduate Student Handbook 
Page 14 
 
The School of Computer Science has a Help Center in GHC 4201.
The Carnegie Mellon Code can also be found on-line at https://www.cmu.edu/student-
affairs/theword/. 
1.7 University Policies and Expectations 
It is the responsibility of each member of the Carnegie Mellon community to be familiar with 
university policies and guidelines. In addition to this departmental graduate student 
handbook, the following resources are available to assist you in understanding community 
expectations: 
 The Word/Student Handbook 
www.cmu.edu/student-affairs/theword//index.html 
 Academic Integrity Policy 
https://www.cmu.edu/policies/student-and-student-
life/academic-integrity.html 
 University Policies Website 
www.cmu.edu/policies/ 
MIIS Graduate Student Handbook 
Page 10 
 
Office of Graduate and Postdoc Affairs http://www.cmu.edu/graduate/policies/index.html 
 
Due to the changing nature of conditions and expectations surrounding public health and 
safety requirements, please visit www.cmu.edu/coronavirus for the most up to date 
information.  
Please see Appendix A for additional information about The Word and University resources. 
1.8 Academic Calendar 
The Academic Calendar can be found at https://www.cmu.edu/hub/calendar/index.html and 
provides information on all deadlines including registration dates, class start dates, add/drop 
deadlines, exam dates and more. 
2 The Language Technologies Institute 
2.1.1 Mailboxes & Office Supplies 
The LTI mailboxes, printers, copiers, and other departmental resources are in GHC 5404. 
2.1.2 Photocopies and Printers 
The use of a photocopier or printer requires you to log in with your CMU ID card. LTI’s printers 
are located in GHC 5404 and GHC 6604. The School of Computer Science provides a number of 
black-and-white and color printers for use by students. The SCS Computer Facilities publishes a 
list of printers online at http://www.cs.cmu.edu/~help/printing/. 
2.1.3 Office Space for MS Students 
To help create a sense of community, full time students in the LTI’s MIIS program have access to a 
shared office space. 
2.1.4 Computers for MS Students 
Students are expected to provide their own laptop computers that can be used to access 
university resources and complete course assignments.  Laptops running Windows, MacOS, 
and Linux software are all acceptable.
Answer: "
"Which LTI facultly were involved in the FLARE paper?
","['graham neubig_88884b8806262a4095036041e3567d450dba39f7_metadata.txt', 'jamie callan_88884b8806262a4095036041e3567d450dba39f7_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which LTI facultly were involved in the FLARE paper?

Context: Faculty Name: graham neubig
Paperid: 88884b8806262a4095036041e3567d450dba39f7
Title: Active Retrieval Augmented Generation
Year: 2023
Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.
Authors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work proposes Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens.'}
Url: http://arxiv.org/pdf/2305.06983
Faculty Name: jamie callan
Paperid: 88884b8806262a4095036041e3567d450dba39f7
Title: Active Retrieval Augmented Generation
Year: 2023
Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.
Authors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work proposes Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens.'}
Url: http://arxiv.org/pdf/2305.06983
Answer: "
"What is the name of the proposed cross-modal fine-tuning framework in Graham's ICML 2023 work?
","['graham neubig_03c7f61b0c6bc4c480ed6da4e9d7dda104ddfec3_metadata.txt', 'graham neubig_papers.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the name of the proposed cross-modal fine-tuning framework in Graham's ICML 2023 work?

Context: Faculty Name: graham neubig
Paperid: 03c7f61b0c6bc4c480ed6da4e9d7dda104ddfec3
Title: Cross-Modal Fine-Tuning: Align then Refine
Year: 2023
Abstract: Fine-tuning large-scale pretrained models has led to tremendous progress in well-studied modalities such as vision and NLP. However, similar gains have not been observed in many other modalities due to a lack of relevant pretrained models. In this work, we propose ORCA, a general cross-modal fine-tuning framework that extends the applicability of a single large-scale pretrained model to diverse modalities. ORCA adapts to a target task via an align-then-refine workflow: given the target input, ORCA first learns an embedding network that aligns the embedded feature distribution with the pretraining modality. The pretrained model is then fine-tuned on the embedded data to exploit the knowledge shared across modalities. Through extensive experiments, we show that ORCA obtains state-of-the-art results on 3 benchmarks containing over 60 datasets from 12 modalities, outperforming a wide range of hand-designed, AutoML, general-purpose, and task-specific methods. We highlight the importance of data alignment via a series of ablation studies and demonstrate ORCA's utility in data-limited regimes.
Authors: Junhong Shen, Liam Li, L. Dery, Corey Staten, M. Khodak, Graham Neubig, Ameet Talwalkar
Venue: International Conference on Machine Learning
Tldr: {'model': 'tldr@v2.0.0', 'text': ""This work proposes ORCA, a general cross-modal fine-tuning framework that extends the applicability of a single large-scale pretrained model to diverse modalities and highlights the importance of data alignment via a series of ablation studies and demonstrates ORCA's utility in data-limited regimes.""}
Url: http://arxiv.org/pdf/2302.05738
List of 2023 Open Access papers by graham neubig are:
Cross-Modal Fine-Tuning: Align then Refine
ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages
GlobalBench: A Benchmark for Global Progress in Natural Language Processing
Learning Performance-Improving Code Edits
CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code
Neural Machine Translation for the Indigenous Languages of the Americas: An Introduction
User-Centric Evaluation of OCR Systems for Kwak’wala
Multi-Dimensional Evaluation of Text Summarization with In-Context Learning
A Gold Standard Dataset for the Reviewer Assignment Problem
SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing
Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation
FacTool: Factuality Detection in Generative AI - A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios
Active Retrieval Augmented Generation
Large Language Models Enable Few-Shot Clustering
Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach
Crossing the Threshold: Idiomatic Machine Translation through Retrieval Augmentation and Loss Weighting
Why do Nearest Neighbor Language Models Work?
Syntax and Semantics Meet in the “Middle”: Probing the Syntax-Semantics Interface of LMs Through Agentivity
DataFinder: Scientific Dataset Recommendation from Natural Language Descriptions
Multi-lingual and Multi-cultural Figurative Language Understanding
It’s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk
Unlimiformer: Long-Range Transformers with Unlimited Length Input
WebArena: A Realistic Web Environment for Building Autonomous Agents
Prompt2Model: Generating Deployable Models from Natural Language Instructions
Computational Language Acquisition with Theory of Mind
Improving Factuality of Abstractive Summarization via Contrastive Reward Learning
The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation
Answer: "
"Who taught Urban Design Methods and Theory in Fall 2023?
","['metadata_course_fall_23.txt', 'combined_metadata_final.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who taught Urban Design Methods and Theory in Fall 2023?

Context: Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Lee located in Building TBA, Room None.
In Semester Fall 2023, from the department of Architecture, the subject titled 'Urban Design Methods and Theory' with Course ID 48740 and Section A offers 9.0 units. The Class meets Friday between 09:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kline located in Building CFA, Room 211.
In Semester Fall 2023, from the department of Architecture, the subject titled 'Planning and Public Policy for the Future of Urbanism' with Course ID 48742 and Section A offers VAR units. The Class meets Tuesday between 07:00PM and 09:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Chambers located in Building MM, Room 415IW.
In Semester Fall 2023, from the department of Architecture, the subject titled 'Introduction to Ecological Design Thinking' with Course ID 48743 and Section A offers 9.0 units. The Class meets Wednesday between 10:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Cupkova located in Building MM, Room 321.
In Semester Fall 2023, from the department of Architecture, the subject titled 'Histories of Urban Design:' with Course ID 48750 and Section NA offers 9.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Architecture, the subject titled 'Histories of Urban Design' with Course ID 48750 and Section A offers 9.0 units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Shaw located in Building CFA, Room 102.
In Semester Fall 2023, from the department of Architecture, the subject titled 'Intro to Urban Design Media' with Course ID 48753 and Section A offers 6.0 units. The Class meets Monday between 09:00AM and 10:50AM ET.
The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Loftness located in Building TBA, Room None.
In Semester Fall 2023, from the department of Architecture, the subject titled 'Master's Independent Study' with Course ID 48736 and Section X offers 0-99 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Lee located in Building TBA, Room None.
In Semester Fall 2023, from the department of Architecture, the subject titled 'Urban Design Methods and Theory' with Course ID 48740 and Section A offers 9.0 units. The Class meets Friday between 09:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kline located in Building CFA, Room 211.
In Semester Fall 2023, from the department of Architecture, the subject titled 'Planning and Public Policy for the Future of Urbanism' with Course ID 48742 and Section A offers VAR units. The Class meets Tuesday between 07:00PM and 09:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Chambers located in Building MM, Room 415IW.
In Semester Fall 2023, from the department of Architecture, the subject titled 'Introduction to Ecological Design Thinking' with Course ID 48743 and Section A offers 9.0 units. The Class meets Wednesday between 10:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Cupkova located in Building MM, Room 321.
In Semester Fall 2023, from the department of Architecture, the subject titled 'Histories of Urban Design:' with Course ID 48750 and Section NA offers 9.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Architecture, the subject titled 'Histories of Urban Design' with Course ID 48750 and Section A offers 9.0 units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET.
Answer: "
"What is the full name of the conference where the paper Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models, got published?
","['david mortensen_17fbffb05fa14e21d1c506fd5f0f568b955fe983_metadata.txt', 'david mortensen_papers.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the full name of the conference where the paper Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models, got published?

Context: Faculty Name: david mortensen
Paperid: 17fbffb05fa14e21d1c506fd5f0f568b955fe983
Title: Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models
Year: 2023
Abstract: Language models have graduated from being research prototypes to commercialized products offered as web APIs, and recent works have highlighted the multilingual capabilities of these products. The API vendors charge their users based on usage, more specifically on the number of ``tokens'' processed or generated by the underlying language models. What constitutes a token, however, is training data and model dependent with a large variance in the number of tokens required to convey the same information in different languages. In this work, we analyze the effect of this non-uniformity on the fairness of an API's pricing policy across languages. We conduct a systematic analysis of the cost and utility of OpenAI's language model API on multilingual benchmarks in 22 typologically diverse languages. We show evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results. These speakers tend to also come from regions where the APIs are less affordable to begin with. Through these analyses, we aim to increase transparency around language model APIs' pricing policies and encourage the vendors to make them more equitable.
Authors: Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David R. Mortensen, Noah A. Smith, Yulia Tsvetkov
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': ""This work conducts a systematic analysis of the cost and utility of OpenAI's language model API on multilingual benchmarks in 22 typologically diverse languages and shows evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results.""}
Url: http://arxiv.org/pdf/2305.13707
List of 2023 Open Access papers by david mortensen are:
ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages
Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models
SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing
Construction Grammar Provides Unique Insight into Neural Language Models
Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation
Transformed Protoform Reconstruction
PWESuite: Phonetic Word Embeddings and Tasks They Facilitate
Kuki-Chin Phonology: An Overview
Answer: "
"In summer 2024, What is the deadline for withdrawing from a Semester course and receiving a withdrawal grade?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In summer 2024, What is the deadline for withdrawing from a Semester course and receiving a withdrawal grade?

Context: On Monday, 08 July, 2024, during the Summer Two 2024 (N24) semester, Summer Semester  Two drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 22 July, 2024, during the Summer Two 2024 (N24) semester, Summer Semester  Two pass/no pass & withdrawal deadline (3) is observed.
On Monday, 29 July, 2024, during the Summer Two 2024 (N24) semester, Summer Semester  Two Faculty Course Evaulations open is observed.
On Thursday, 01 August, 2024, during the Summer Two 2024 (N24) semester, Summer Semester  Two Last Day of Classes is observed.
On Thursday, 01 August, 2024, during the Summer Two 2024 (N24) semester, Summer Semester  Two voucher deadline (4) is observed.
On Friday, 02 August, 2024, during the Summer Two 2024 (N24) semester, Summer Semester  Two Final Exams*** is observed.
On Friday, 02 August, 2024, during the Summer Two 2024 (N24) semester, Summer Semester  Two Faculty Course Evaluations close is observed.
On Tuesday, 06 August, 2024, during the Summer Two 2024 (N24) semester, Summer Semester  Two Final Grades Due by 4 pm is observed.
From 18 August, 2024,Saturday to 23 August, 2024,Friday marks the First-Year Orientation for Fall 2024 (F24) semester.
On Thursday, 22 August, 2024, during the Fall 2024 (F24) semester, Convocation is observed.
On Monday, 26 August, 2024, during the Fall 2024 (F24) semester, Semester & Mini-1 Classes Begin is observed.
On Friday, 30 August, 2024, during the Fall 2024 (F24) semester, Mini-1 add, audit & tuition adjustment drop deadline  (1) is observed.
On Monday, 02 September, 2024, during the Fall 2024 (F24) semester, Labor Day is observed, leading to No Classes & University Closed.
On Friday, 05 July, 2024, during the Summer One 2024 (M24) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Monday, 08 July, 2024, during the Summer One 2024 (M24) semester, Mini-6 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 22 July, 2024, during the Summer One 2024 (M24) semester, Mini-6 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 29 July, 2024, during the Summer One 2024 (M24) semester, Semester & Mini-6 Faculty Course Evalutations open is observed.
On Thursday, 01 August, 2024, during the Summer One 2024 (M24) semester, Semester & Mini-6 Last Day of Classes is observed.
On Thursday, 01 August, 2024, during the Summer One 2024 (M24) semester, Semester & Mini-6 voucher deadline (4) is observed.
On Friday, 02 August, 2024, during the Summer One 2024 (M24) semester, Semester & Mini-6 Final Exams*** is observed.
On Friday, 02 August, 2024, during the Summer One 2024 (M24) semester, Semester & Mini-6 Faculty Course Evaluations close is observed.
On Tuesday, 06 August, 2024, during the Summer One 2024 (M24) semester, Semester & Mini-6 Final Grades Due by 4 pm is observed.
On Monday, 24 June, 2024, during the Summer Two 2024 (N24) semester, Summer Semester Two Classes Begin is observed.
On Friday, 28 June, 2024, during the Summer Two 2024 (N24) semester, Summer Semester  Two add, audit & tuition adjustment drop deadline (1) is observed.
On Thursday, 04 July, 2024, during the Summer Two 2024 (N24) semester, Independence Day is observed, leading to University Closed & No Classes.
On Monday, 08 July, 2024, during the Summer Two 2024 (N24) semester, Summer Semester  Two drop deadline is observed, leading to withdrawal grade assigned after this date (2).
Answer: "
"Is the GRE optional for the masters in language technologies application? Answer yes or no.
","['program_info_MasterofLanguageTechnologies.txt', 'combined_metadata_final.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Is the GRE optional for the masters in language technologies application? Answer yes or no.

Context: Academic Program Name:
Master of Language Technologies

Website:
https://lti.cs.cmu.edu/academics/masters-programs/mlt.html

Overview:
The MLT program prepares students for a research career in academia or industry. In this program, you’ll be immersed in research for two full years. During the academic year, your time will be evenly split between taking courses and doing research with your faculty advisor. Your summer will be devoted entirely to research. Many MLT grads continue on to Ph.D. programs at CMU and other top institutions, while others pursue careers at companies emphasizing research and rapid innovation.

Requirements:
The MLT program lasts two years (24 months), and students must complete two summers of research. Students should usually expect to graduate in August of their second year.
MLT students take 120 or more course units (about 10 courses), at least 72 of which are LTI courses, and 24 of which are School of Computer Science (SCS) courses. Most of these are 12-unit courses, although lab courses are typically 6 units. Our courses generally assume knowledge of programming and data structures. The remaining units may also be taken from the LTI, or with approval from the faculty advisor, any other senior- or graduate-level course offered at CMU or Pitt.
Directed research is another integral part of the MLT program; MLT students carry out directed research during their studies, with guidance from their faculty advisors.
Students may also choose to complete an optional MLT thesis. Guidelines can be found in the MLT Handbook.

Curriculum:
Here's an example of how your two years in the MLT program may break down.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Masters Thesis II' with Course ID 11929 and Section A offers 5-36 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Frederking located in Building TBA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Masters Thesis II' with Course ID 11929 and Section AA offers 5-36 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Langmead located in Building TBA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Masters Thesis II' with Course ID 11929 and Section AB offers 5-36 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Metze located in Building TBA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Masters Thesis II' with Course ID 11929 and Section AC offers 5-36 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Singh located in Building TBA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Masters Thesis II' with Course ID 11929 and Section AD offers 5-36 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ganapathiraju located in Building TBA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Masters Thesis II' with Course ID 11929 and Section AE offers 5-36 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Cassell located in Building TBA, Room None.
Answer: "
"In fall 2024, What is the last day of Mini-1 classes?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2024, What is the last day of Mini-1 classes?

Context: On Monday, 28 August, 2023, during the Fall 2023 (F23) semester, Semester & Mini-1 Classes Begin is observed.
On Friday, 01 September, 2023, during the Fall 2023 (F23) semester, Mini-1 add, audit & tuition adjustment drop deadline  (1) is observed.
On Monday, 04 September, 2023, during the Fall 2023 (F23) semester, Labor Day is observed, leading to No Classes & University Closed.
On Monday, 11 September, 2023, during the Fall 2023 (F23) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Wednesday, 20 September, 2023, during the Fall 2023 (F23) semester, Mini-1 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 02 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Pass/no pass & withdrawal deadline (3) is observed.
On Monday, 02 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Faculty Course Evaluations open is observed.
On Monday, 09 October, 2023, during the Fall 2023 (F23) semester, Semester drop deadline is observed, leading to withdrawal grade assigned after this date.
On Friday, 13 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Last Day of Classes is observed.
On Friday, 13 October, 2023, during the Fall 2023 (F23) semester, Mini-1 voucher election deadline (4) is observed.
From 13 October, 2023,Friday to 14 October, 2023,Saturday marks the Family Weekend for Fall 2023 (F23) semester.
On Saturday, 14 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Exams is observed.
On Saturday, 14 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Faculty Course Evaluations close is observed.
From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mini-2 Classes Begin is observed.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mid-Semester & Mini-1 grades due by 4 pm is observed.
On Friday, 27 October, 2023, during the Fall 2023 (F23) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
On Tuesday, 07 November, 2023, during the Fall 2023 (F23) semester, Democracy Day is observed, leading to No Classes, except Evening classes after 5 pm will still meet.
On Saturday, 11 November, 2023, during the Fall 2023 (F23) semester, Homecoming is observed.
From 13 November, 2023,Monday to 17 November, 2023,Friday marks the Spring 2024 Registration Week for Fall 2023 (F23) semester.
On Monday, 13 November, 2023, during the Fall 2023 (F23) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Wednesday, 15 November, 2023, during the Fall 2023 (F23) semester, Mini-2 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 22 November, 2023,Wednesday to 24 November, 2023,Friday marks the Thanksgiving Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Mini-2 pass/no pass & withdrawal deadline (2) is observed.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Faculty Course Evaluations open is observed.
On Friday, 08 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Last Day of Classes is observed.
Answer: "
"Which two LTI professors co-authored the paper titled ""Understanding Masked Autoencoders via Hierarchical Latent Variable Models""?
","['louis philippe morency_dcb4f2b9b0e6da0d629878d1ad0469aee3df2020_metadata.txt', 'eric xing_dcb4f2b9b0e6da0d629878d1ad0469aee3df2020_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which two LTI professors co-authored the paper titled ""Understanding Masked Autoencoders via Hierarchical Latent Variable Models""?

Context: Faculty Name: louis philippe morency
Paperid: dcb4f2b9b0e6da0d629878d1ad0469aee3df2020
Title: Understanding Masked Autoencoders via Hierarchical Latent Variable Models
Year: 2023
Abstract: Masked autoencoder (MAE), a simple and effective self-supervised learning framework based on the reconstruction of masked image regions, has recently achieved prominent success in a variety of vision tasks. Despite the emergence of intriguing empirical observations on MAE, a theoretically principled understanding is still lacking. In this work, we formally characterize and justify existing empirical in-sights and provide theoretical guarantees of MAE. We formulate the underlying data-generating process as a hierarchical latent variable model, and show that under reasonable assumptions, MAE provably identifies a set of latent variables in the hierarchical model, explaining why MAE can extract high-level information from pixels. Further, we show how key hyperparameters in MAE (the masking ratio and the patch size) determine which true latent variables to be recovered, therefore influencing the level of semantic information in the representation. Specifically, extremely large or small masking ratios inevitably lead to low-level representations. Our theory offers coherent explanations of existing empirical observations and provides insights for potential empirical improvements and fundamental limitations of the masked-reconstruction paradigm. We conduct extensive experiments to validate our theoretical insights.
Authors: Lingjing Kong, Martin Q. Ma, Guan-Hong Chen, E. Xing, Yuejie Chi, Louis-Philippe Morency, Kun Zhang
Venue: Computer Vision and Pattern Recognition
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work formally characterize and justify existing empirical in-sights and provide theoretical guarantees of MAE, and formulate the underlying data-generating process as a hierarchical latent variable model, and shows that under reasonable assumptions, MAE provably identifies a set of latent variables in the hierarchical model.'}
Url: https://arxiv.org/pdf/2306.04898
Faculty Name: eric xing
Paperid: dcb4f2b9b0e6da0d629878d1ad0469aee3df2020
Title: Understanding Masked Autoencoders via Hierarchical Latent Variable Models
Year: 2023
Abstract: Masked autoencoder (MAE), a simple and effective self-supervised learning framework based on the reconstruction of masked image regions, has recently achieved prominent success in a variety of vision tasks. Despite the emergence of intriguing empirical observations on MAE, a theoretically principled understanding is still lacking. In this work, we formally characterize and justify existing empirical in-sights and provide theoretical guarantees of MAE. We formulate the underlying data-generating process as a hierarchical latent variable model, and show that under reasonable assumptions, MAE provably identifies a set of latent variables in the hierarchical model, explaining why MAE can extract high-level information from pixels. Further, we show how key hyperparameters in MAE (the masking ratio and the patch size) determine which true latent variables to be recovered, therefore influencing the level of semantic information in the representation. Specifically, extremely large or small masking ratios inevitably lead to low-level representations. Our theory offers coherent explanations of existing empirical observations and provides insights for potential empirical improvements and fundamental limitations of the masked-reconstruction paradigm. We conduct extensive experiments to validate our theoretical insights.
Authors: Lingjing Kong, Martin Q. Ma, Guan-Hong Chen, E. Xing, Yuejie Chi, Louis-Philippe Morency, Kun Zhang
Venue: Computer Vision and Pattern Recognition
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work formally characterize and justify existing empirical in-sights and provide theoretical guarantees of MAE, and formulate the underlying data-generating process as a hierarchical latent variable model, and shows that under reasonable assumptions, MAE provably identifies a set of latent variables in the hierarchical model.'}
Url: https://arxiv.org/pdf/2306.04898
Answer: "
"In ""Deriving Vocal Fold Oscillation Information from Recorded Voice Signals Using Models of Phonation"", what is the proposed forward-backward algorithm?
","['rita singh_721b39472c801124b5e3102edffe9d6f0754e1c2_metadata.txt', 'rita singh_papers.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In ""Deriving Vocal Fold Oscillation Information from Recorded Voice Signals Using Models of Phonation"", what is the proposed forward-backward algorithm?

Context: Faculty Name: rita singh
Paperid: 721b39472c801124b5e3102edffe9d6f0754e1c2
Title: Deriving Vocal Fold Oscillation Information from Recorded Voice Signals Using Models of Phonation
Year: 2023
Abstract: During phonation, the vocal folds exhibit a self-sustained oscillatory motion, which is influenced by the physical properties of the speaker’s vocal folds and driven by the balance of bio-mechanical and aerodynamic forces across the glottis. Subtle changes in the speaker’s physical state can affect voice production and alter these oscillatory patterns. Measuring these can be valuable in developing computational tools that analyze voice to infer the speaker’s state. Traditionally, vocal fold oscillations (VFOs) are measured directly using physical devices in clinical settings. In this paper, we propose a novel analysis-by-synthesis approach that allows us to infer the VFOs directly from recorded speech signals on an individualized, speaker-by-speaker basis. The approach, called the ADLES-VFT algorithm, is proposed in the context of a joint model that combines a phonation model (with a glottal flow waveform as the output) and a vocal tract acoustic wave propagation model such that the output of the joint model is an estimated waveform. The ADLES-VFT algorithm is a forward-backward algorithm which minimizes the error between the recorded waveform and the output of this joint model to estimate its parameters. Once estimated, these parameter values are used in conjunction with a phonation model to obtain its solutions. Since the parameters correlate with the physical properties of the vocal folds of the speaker, model solutions obtained using them represent the individualized VFOs for each speaker. The approach is flexible and can be applied to various phonation models. In addition to presenting the methodology, we show how the VFOs can be quantified from a dynamical systems perspective for classification purposes. Mathematical derivations are provided in an appendix for better readability.
List of 2023 Open Access papers by rita singh are:
Implementing International Federation of Gynecology and Obstetrics Nutrition Checklist for Pregnant Women: Opportunities and Challenges in Low- and Middle-income Countries
Mean Platelet Volume in Type 2 Diabetes: Correlation with Poor Glycaemic Control
Gonadotropin Receptor Cross-Talk and Altered Functions in Gonadal and Non-Gonadal Tissues
BASS: Block-wise Adaptation for Speech Summarization
Rethinking Voice-Face Correlation: A Geometry View
A Gene-Based Algorithm for Identifying Factors That May Affect a Speaker’s Voice
Deriving Vocal Fold Oscillation Information from Recorded Voice Signals Using Models of Phonation
Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations
The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features
Pengi: An Audio Language Model for Audio Tasks
GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content
Token Prediction as Implicit Classification to Identify LLM-Generated Text
Effect of myo-inositol and di-chiro inositol plus vitamin D supplementation during pregnancy on prevention of gestational diabetes: a multi-centric, prospective, randomized, double-blind clinical trial
Plant-Avian Frugivory in the Urban Ecosystem of Delhi
LoFT: Local Proxy Fine-tuning For Improving Transferability Of Adversarial Attacks Against Large Language Model
Evaluating Speech Synthesis by Training Recognizers on Synthetic Speech
Rethinking Audiovisual Segmentation with Semantic Quantization and Decomposition
Towards Noise-Tolerant Speech-Referring Video Object Segmentation: Bridging Speech and Text
Importance of negative sampling in weak label learning
Completing Visual Objects via Bridging Generation and Segmentation
Training Audio Captioning Models without Audio
Prompting Audios Using Acoustic Properties For Emotion Representation
Pairwise Similarity Learning is SimPLE
Comparison of freeze-thaw and sonication cycle-based methods for extracting AMR-associated metabolites from Staphylococcus aureus
APPLIED ASPECT OF SATVAVAJAYA CHIKITSA
Utilization of the Whole Cowpea Pod and Barley Husk in The Production of Nutritionally Enriched Composite Flour
Answer: "
"What tasks does ML-SUPERB consider?
","['shinji watanabe_090b284b2f8fc93ac3e7a92fc9f91bf4965ba75c_metadata.txt', 'shinji watanabe_bdf9ea3a67691e1b6a362f4019bf80c9cf31cecd_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What tasks does ML-SUPERB consider?

Context: Faculty Name: shinji watanabe
Paperid: 090b284b2f8fc93ac3e7a92fc9f91bf4965ba75c
Title: ML-SUPERB: Multilingual Speech Universal PERformance Benchmark
Year: 2023
Abstract: Speech processing Universal PERformance Benchmark (SUPERB) is a leaderboard to benchmark the performance of Self-Supervised Learning (SSL) models on various speech processing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from high-resource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB benchmark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research.
Authors: Jiatong Shi, Dan Berrebbi, William Chen, Ho-Lam Chung, En-Pei Hu, Wei Huang, Xuankai Chang, Shang-Wen Li, Abdel-rahman Mohamed, Hung-yi Lee, Shinji Watanabe
Venue: Interspeech
Tldr: {'model': 'tldr@v2.0.0', 'text': None}
Url: https://arxiv.org/pdf/2305.10615
Faculty Name: shinji watanabe
Paperid: bdf9ea3a67691e1b6a362f4019bf80c9cf31cecd
Title: Findings of the 2023 ML-Superb Challenge: Pre-Training And Evaluation Over More Languages And Beyond
Year: 2023
Abstract: The 2023 Multilingual Speech Universal Performance Benchmark (ML-SUPERB) Challenge expands upon the acclaimed SUPERB framework, emphasizing self-supervised models in multilingual speech recognition and language identification. The challenge comprises a research track focused on applying ML-SUPERB to specific multilingual subjects, a Challenge Track for model submissions, and a New Language Track where language resource researchers can contribute and evaluate their low-resource language data in the context of the latest progress in multilingual speech recognition. The challenge garnered 12 model submissions and 54 language corpora, resulting in a comprehensive benchmark encompassing 154 languages. The findings indicate that merely scaling models is not the definitive solution for multilingual speech tasks, and a variety of speech/voice types present significant challenges in multilingual speech processing.
Authors: Jiatong Shi, William Chen, Dan Berrebbi, Hsiu-Hsuan Wang, Wei-Ping Huang, En-Pei Hu, Ho-Lam Chuang, Xuankai Chang, Yuxun Tang, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Shinji Watanabe
Venue: Automatic Speech Recognition & Understanding
Tldr: {'model': 'tldr@v2.0.0', 'text': 'The 2023 Multilingual Speech Universal Performance Benchmark (ML-SUPERB) Challenge expands upon the acclaimed SUPERB framework, emphasizing self-supervised models in multilingual speech recognition and language identification, resulting in a comprehensive benchmark encompassing 154 languages.'}
Url: https://arxiv.org/pdf/2310.05513
Answer: "
"In the KALE paper, what evaluation metrics were reported on MSMARCO?
","['graham neubig_4d74a5048b884e8bb3842240abf98915c619c8f8_content_1.txt', 'bhiksha raj_255bad49d29202e2d255926ab0983c125dcce835_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the KALE paper, what evaluation metrics were reported on MSMARCO?

Context: Title: Multi-Dimensional Evaluation of Text Summarization with In-Context Learning
Authors: Sameer Jain, Vaishakh Keshava, Swarnashree Mysore Sathyendra, Patrick Fernandes, Pengfei Liu, Graham Neubig, Chunting Zhou
Section: 1 Introduction
increased. Finally, in light of the recent work (Goyal et al., 2022) that points to the misalignment of existing evaluation metrics with human preference in evaluating zeroshot summaries generated by LLMs such as GPT-3 (Brown et al., 2020), we study the effectiveness of ICE in evaluating zero-shot summaries generated by GPT-3. We find that ICE evaluations agree closely with human judgments on such summaries.
Title: EVALUATING SPEECH SYNTHESIS BY TRAINING RECOGNIZERS ON SYNTHETIC SPEECH
Authors: Dareen Alharthi, Roshan Sharma, Hira Dhamyal, Soumi Maiti, Bhiksha Raj, Rita Singh
Section: 4.4. Evaluation Metrics
performs the best to worst. For example in the row MOS-N, Style-TTS has the highest score and therefore has rank 1, followed by MQ-TTS and then YourTTS. In order to assess whether our metric is a good representation of the quality of synthetic speech, we compare the relative ranking of our metric with the other metrics. Two metrics with a matched relative ranking means that the metrics evaluate the quality of speech similarly and agree with each other. First, we see that the Mean Opinion Score tests on naturalness (MOS-N) and intelligibility (MOS-I) agree on relative rankings between the synthetic speech models. Further, we observe that the traditionally used WER metric shown in the first row does not actually correlate completely with the MOS results. We observe similar issues with other popular metrics including SpeechLMScore and MOSNet. From the last row, we observe that our metric evaluation of synthetic speech has a similar trend as the reported MOS scores, matching both MOS-N and MOS-I. Compared with the inconsistent result from the first row and the consistent result from our metric, we demonstrate the importance of the proposed evaluation method. 6. CONCLUSION In this paper, we address the challenge of automatic evaluation for synthetic speech by modeling the similarity/dissimilarity between the distributions of synthetic and real speech. Existing divergence metrics require a large number of samples to capture the joint distribution and hence it is infeasible to employ them to calculate the distributional shift. In this paper, we introduce a new divergence measure that can be computed without knowledge of the joint distribution. The metric uses an ASR model as an approximation for the data distributions and the WER as a proxy for the quality of the synthesized speech. The metric is asymmetric, and it matters what the speech recognition models are trained and tested on. We show that in practice it is more accurate to train the model on synthetic speech and assess the resulting model’s performance on real speech than doing it vice versa.
Answer: "
"Who all led the School of Computer Science in 1986?
","['history_d401.txt', 'history_d401.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who all led the School of Computer Science in 1986?

Context: The experiment, which lasted two years, was an unqualified success. Separately, and also in 1986, the Pittsburgh Supercomputing Center was created as a joint effort between CMU, the University of Pittsburgh and Westinghouse Electric Corp.SCS is officially formedCMU’s Faculty Senate in the fall of 1988 agreed to President Richard Cyert’s plan to elevate the Department of Computer Science to college status. In addition to the Computer Science Department, SCS also incorporated the Robotics Institute, the Center for Machine Translation, and researchers from the Information Technology Center, which had developed Andrew. On Dec. 13, 1988, Cyert (H’89) told faculty and staff that Habermann had been appointed CMU’s first Dean of Computer Science, effective Dec. 1, and that the School of Computer Science would soon begin operations. SCS made its formal debut on Dec. 22, 1988, with a reception in the Wherrett Room of Skibo Hall, CMU’s student union. The official announcement of CMU’s new “graduate School of Computer Science” was made Jan. 3, 1989.Undergraduate degrees beginFor several years, undergraduates interested in computer science pursued an “applied math/CS” bachelor’s degree offered by the Mathematics Department. CSD professor Mary Shaw (CS’72) led CMU’s first effort to design an undergraduate curriculum solely in computer science. She and her colleagues were guided by the Carnegie Plan—guidelines established in 1938 under Carnegie Tech President Robert Doherty (A’40, E’48, H’50), outlining the principles of a sound professional education. Drawing on Shaw’s plan and also on the work of other faculty members, an undergraduate program in computer science was created during the 1989-90 academic year. Seven CS majors were admitted to the program as sophomores. Another 73 undergraduates were admitted in 1990–91.
As the 1970s progressed, Newell became interested in human-computer interaction, and began a long relationship with Xerox’s Palo Alto Research Center, or PARC, which released the Xerox Alto in 1973. Considered a forerunner to many of the computing environments that followed, Alto featured a graphical-user interface and was among the first commercially available workstations controlled with a mouse. Inspired by Alto, Reddy launched a drive for development of CMU’s own “three-M” machine—a personal workstation with a megabyte of memory, a megapixel display and at least one million instructions per second of processing power.Launching a Robotics InstituteIn 1979, an executive at Pittsburgh’s Westinghouse Electric Corp., Tom Murrin, collaborated with Jordan and Reddy to create the Robotics Institute, with Reddy as its first director. By 1982, the Computer Science Department included more than 30 faculty members and 100 graduate students.The best-wired campus in the worldWorking with IBM in the early 1980s, the university and the Computer Science Department established another new research frontier: Development of a high-speed computer network that would reach virtually every room on campus, along with a GUI-based computing environment, and providing networked PCs or workstations for 7,000 students, faculty members and employees. Called the Andrew Project, it turned Carnegie Mellon into the best-connected, most-wired university in the world—a process Newell called “greening up the campus with computer science.” CMU also became home to a new Software Engineering Institute, funded by the Defense Department, to study computer security and develop best practices in the design of operating systems. Between 1982 and 1985, the amount of sponsored research in the Computer Science Department doubled, from $7.2 million to $15.3 million—more than the other four departments in the Mellon College of Science combined.A “school of computer science” is proposedFeeling that CSD’s needs were inadequately represented in MCS, CSD head A. Nico Habermann and then-CMU provost Angel Jordan in 1986 wrote a white paper proposing the creation of “a School of Computer Science.” Responding to concerns from the faculty that the change might be taking place too quickly, the university first established a free-floating Department of Computer Science. The experiment, which lasted two years, was an unqualified success.
Answer: "
"What is the Gates Hillman Complex at Carnegie Mellon University's 5 digit zip code?
","['miis-handbook_2023-2024.txt', 'mlt-student-handbook-2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the Gates Hillman Complex at Carnegie Mellon University's 5 digit zip code?

Context: MIIS Graduate Student Handbook 
Page 8 
 
The Language Technologies Institute is located primarily on the 5th and 6th floors of the Gates 
Hillman Complex (GHC) on Carnegie Mellon’s Pittsburgh campus.  The address and contact 
information of the LTI are: 
Language Technologies Institute 
Carnegie Mellon University 
5000 Forbes Avenue 
Gates Hillman Complex 5404, LTI 
Pittsburgh, PA 15241-3891 
 
412-268-6591 (phone) 
412-268-6298 (fax) 
 
http://www.lti.cs.cmu.edu/ 
1.5 Carnegie Mellon University Statement of Assurance  
Carnegie Mellon University does not discriminate in admission, employment or administration 
of its programs or activities on the basis of race, color, national origin, sex, handicap or disability, 
age, sexual orientation, gender identity, religion, creed, ancestry, belief, veteran status or genetic 
information. Furthermore, Carnegie Mellon University does not discriminate and is required 
not to discriminate in violation of federal, state, or local laws or executive orders. 
  
Inquiries concerning the application of and compliance with this statement should be directed 
to the university ombudsperson, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, 
PA 15213, telephone 412-268-1018.  Obtain general information about Carnegie Mellon 
University by calling 412-268-2000. 
  
Carnegie Mellon University publishes an annual campus security and fire safety report 
describing the university's security, alcohol and drug, sexual assault, and fire safety policies, and 
containing statistics about the number and type of crimes committed on the campus, and the 
number and cause of fires in campus residence facilities during the preceding three years. You 
can obtain a copy by contacting the Carnegie Mellon Police Department at 412-268-2323. The 
annual 
security 
and 
fire 
safety 
report 
also 
is 
available 
online 
at www.cmu.edu/police/annualreports. 
 
Information regarding the application of Title IX, including to admission and employment 
decisions, the sexual misconduct grievance procedures and process, including how to file a 
report or a complaint of sex discrimination, how to file a report of sexual harassment, and how 
the university responds to such reports is available at www.cmu.edu/title-ix.
All policies not explicitly described in this document conform to School of Computer Science 
(SCS) policies and university policies described in The Word, Carnegie Mellon University Student 
Handbook and at the University Policies website.   
 
 
 
 
 
MLT Graduate Student Handbook 
Page 7 
 
1.3 MLT Contact Information 
The people responsible for administering the MLT degree are: 
 
Kate Schaich  
 
 
 
Robert Frederking 
 
Program Manager, MLT 
 
 
Program Director, MLT 
 
Graduate Program Manager, LTI 
 
Principal Systems Scientist 
 
GHC 6415 
 
 
 
 
GHC 6515 
 
412-268-4788  
 
 
 
412-268-6656 
 
kschaich@cs.cmu.edu  
 
 
ref@cs.cmu.edu 
 
 
 
Robert Frederking 
 
 
 
Mona Diab 
 
Chair of Graduate Programs, LTI 
 
Director, LTI 
 
Principal Systems Scientist 
 
 
GHC 5415 
 
GHC 6515 
 
 
 
 
412-268-6656  
 
 
 
 
 
 
 
 
 
 
mdiab@andrew.cmu.edu 
 
 
 
 
 
In addition, students may confer with the Graduate Education Office 
(graded@andrew.cmu.edu) regarding issues of process or other concerns as they navigate 
conflicts. 
 
The Language Technologies Institute is located primarily on the 5th and 6th floors of the Gates 
Hillman Complex (GHC) on Carnegie Mellons Pittsburgh campus. The address and contact 
information of the LTI are: 
Language Technologies Institute 
Carnegie Mellon University 
5000 Forbes Avenue 
Gates Hillman Complex 5402, LTI 
Pittsburgh, PA 15241-3891 
 
412-268-6591 (phone) 
412-268-6298 (fax) 
 
http://www.lti.cs.cmu.edu/ 
 
 
 
 
 
MLT Graduate Student Handbook 
Page 8 
 
1.4 University Policies and Expectations 
It is the responsibility of each member of the Carnegie Mellon community to be familiar with 
university policies and guidelines.
Answer: "
"When was Campus Week discontinued and replaced with Spring Carnival?
","['Apr-11_Eventno_13_SpringCarnivalMidwayOpeningCeremony.txt', 'Apr-11_Eventno_25_ActivitiesBoardSpringCarnivalSpeakersShow.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When was Campus Week discontinued and replaced with Spring Carnival?

Context: Event: Spring Carnival Midway Opening Ceremony
Date: 4/11/24
Time: 3:00 PM-3:30 PM ET
Participants/Audience: Open to entire CMU community 
Event Details: 
Join the Spring Carnival committee, Pipes & Drums Band and Scotty as we celebrate the opening of Midway!  

Can't make it back to campus? Sign up for the calendar hold hold - we'll add the livestream link a week prior, as well as send an event reminder with link 24-hours and one-hour prior.

Note: No registration required for in-person event. No event fee. This event is open to the entire CMU community and their guests.
Event: Activities Board Spring Carnival Speakers Show
Date: 4/11/24
Time: 7:00 PM-10:00 PM ET
Participants/Audience: CMU Community Members with CMU ID or Alumni with Ticket 
Event Details: 
Performer(s) will be added after the Activities Board Speakers committee officially makes their announcement. ""Like"" Activities Board on Instagram for updates on artists, giveaways and location.

Note: Time may change. Tickets details and information to be added in the coming weeks.
Answer: "
"Who is CMU's first official mascot?
","['tartanfacts_d404.txt', 'tartanfacts_d404.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who is CMU's first official mascot?

Context: In keeping with tradition, students, faculty, staff and alumni voted to name the new official mascot Scotty. But it's not just the costumed mascot that voters named. The live dog is also known as Scotty to the Carnegie Mellon community.
About Scotty
|
Celebrating Scotty
|
Performer Requests
Department Quick Facts
Name of School: Carnegie Mellon University
City/Zip: Pittsburgh, PA 15213
Founded: 1900
Enrollment (undergrad): 6,982- Fall 2021
Enrollment (graduate and doctoral): 7,062 - Fall 2021
Alumni: 102,577
Nickname: Tartans
Mascot: Scottie Dog
School Colors: Cardinal and Gray
Football Stadium and Track and Field: Gesling Stadium
Capacity: 3,500
Surface: FieldTurf
Basketball/Volleyball Gym: Wiegand Gymnasium
Capacity: 500
Soccer Stadium: CMU Soccer Field
Capacity: 250
Surface: FieldTurf
Affiliation: NCAA Division III
Conference: University Athletic Association | Presidents' Athletic Conference (Football Only)
President: Dr. Farnam Jahanian
Alma Mater, Year: University of Texas at Austin
Athletic Director: Dr. Josh Centor
Alma Mater, Year: Brandeis, 2004
Athletic Department Phone: 412-268-8054
Mailing Address:  5000 Forbes Avenue / Pittsburgh, PA 15213
Tartan Facts
Who founded Carnegie Mellon University?
Carnegie Technical Schools was founded in 1900 by Andrew Carnegie. Twelve years later it became known as the Carnegie Institute of Technology. In 1967, the school merged with Mellon Institute and became what is known today as Carnegie Mellon University.
What is a Tartan?
The Carnegie Mellon athletic teams are nicknamed the ""Tartans"" as a nod to Andrew Carnegie's Scottish heritage. A tartan is often misrepresented as a fierce warrior from either the Asian tundra or Scottish highlands. In actuality, a Tartan is a twilled woolen fabric with a plaid design. It is of Scottish origin and consists of stripes of various colors and widths against a solid ground, denoting a particular family lineage. The school's founder, Andrew Carnegie, was born in Dunfermline, Scotland, in 1835. Carnegie came to the United States in 1848 and founded Carnegie Technical Schools in Pittsburgh in 1900.
The Scottish terrier mascot performer sports Carnegie tartan attire, while the graphic mascot is wearing a plaid scarf around its neck. So what's the difference between tartan and plaid?
You'll know it's a tartan if...
• ""It's a check or pattern in a variety of colours in woven fabric in which bands of colour are repeated in equal proportion in warp (running lengthwise) and weft (running across).""
• ""Each stripe of the warp crosses every stripe of the weft, so when vertical and horizontal stripes of the same color cross, the result is solid color at the point of intersection.""
• ""The arrangement of colored threads is the same in the warp as in the weft.""
You can find our official tartan on various items in the
University Store
.
Source: ""Tartan: Romancing the Plaid,"" by Jeffrey Banks and Doria De La Chapelle
Official Mascot?
More than a century after Carnegie Mellon University opened its doors, an official mascot finally made its mark. Although students have dressed as a Scottish terrier — typically referred to as Scotty — for 50 years, it wasn't until 2007 that Carnegie Mellon officially welcomed the Scottish terrier as the university's first mascot.
In keeping with tradition, students, faculty, staff and alumni voted to name the new official mascot Scotty. But it's not just the costumed mascot that voters named.
Answer: "
"What is the success rate of the baseline in real-world component of HomeRobot OVMM benchmark?
","['yonatan bisk_3b0c02955e88f5862e61b560c7f70ba8cf235b1d_metadata.txt', 'yonatan bisk_3b0c02955e88f5862e61b560c7f70ba8cf235b1d_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the success rate of the baseline in real-world component of HomeRobot OVMM benchmark?

Context: We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future research work improve performance. See videos on our website: https://ovmm.github.io/.
Authors: Sriram Yenamandra, A. Ramachandran, Karmesh Yadav, Austin S. Wang, Mukul Khanna, Théophile Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander Clegg, John Turner, Z. Kira, M. Savva, Angel X. Chang, Devendra Singh Chaplot, Dhruv Batra, Roozbeh Mottaghi, Yonatan Bisk, Chris Paxton
Venue: Conference on Robot Learning
Tldr: {'model': 'tldr@v2.0.0', 'text': 'The HomeRobot OVMM benchmark is introduced, where an agent navigates household environments to grasp novel objects and place them on target receptacles, and baselines achieve a 20% success rate in the real world; the experiments identify ways future research work improve performance.'}
Url: http://arxiv.org/pdf/2306.11565
Faculty Name: yonatan bisk
Paperid: 3b0c02955e88f5862e61b560c7f70ba8cf235b1d
Title: HomeRobot: Open-Vocabulary Mobile Manipulation
Year: 2023
Abstract: HomeRobot (noun): An affordable compliant robot that navigates homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future research work improve performance. See videos on our website: https://ovmm.github.io/.
Answer: "
"In fall 2023, When is the deadline to drop a Mini-1 course with a withdrawal grade assigned?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, When is the deadline to drop a Mini-1 course with a withdrawal grade assigned?

Context: On Monday, 28 August, 2023, during the Fall 2023 (F23) semester, Semester & Mini-1 Classes Begin is observed.
On Friday, 01 September, 2023, during the Fall 2023 (F23) semester, Mini-1 add, audit & tuition adjustment drop deadline  (1) is observed.
On Monday, 04 September, 2023, during the Fall 2023 (F23) semester, Labor Day is observed, leading to No Classes & University Closed.
On Monday, 11 September, 2023, during the Fall 2023 (F23) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Wednesday, 20 September, 2023, during the Fall 2023 (F23) semester, Mini-1 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 02 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Pass/no pass & withdrawal deadline (3) is observed.
On Monday, 02 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Faculty Course Evaluations open is observed.
On Monday, 09 October, 2023, during the Fall 2023 (F23) semester, Semester drop deadline is observed, leading to withdrawal grade assigned after this date.
On Friday, 13 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Last Day of Classes is observed.
On Friday, 13 October, 2023, during the Fall 2023 (F23) semester, Mini-1 voucher election deadline (4) is observed.
From 13 October, 2023,Friday to 14 October, 2023,Saturday marks the Family Weekend for Fall 2023 (F23) semester.
On Saturday, 14 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Exams is observed.
On Saturday, 14 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Faculty Course Evaluations close is observed.
From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 02 September, 2024, during the Fall 2024 (F24) semester, Labor Day is observed, leading to No Classes & University Closed.
On Monday, 09 September, 2024, during the Fall 2024 (F24) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 18 September, 2024, during the Fall 2024 (F24) semester, Mini-1 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 30 September, 2024, during the Fall 2024 (F24) semester, Mini-1 Pass/no pass & withdrawal deadline (3) is observed.
On Monday, 30 September, 2024, during the Fall 2024 (F24) semester, Mini-1 Faculty Course Evaluations open is observed.
On Monday, 07 October, 2024, during the Fall 2024 (F24) semester, Semester drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 11 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Last Day of Classes is observed.
On Friday, 11 October, 2024, during the Fall 2024 (F24) semester, Mini-1 voucher election deadline (4) is observed.
On Saturday, 12 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Exams is observed.
On Saturday, 12 October, 2024, during the Fall 2024 (F24) semester, Mini-1 Faculty Course Evaluations close is observed.
From 14 October, 2024,Monday to 18 October, 2023,Friday marks the Fall Break for Fall 2024 (F24) semester which leads to No Classes.
On Monday, 21 October, 2024, during the Fall 2024 (F24) semester, Mini-2 Classes Begin is observed.
On Wednesday, 23 October, 2024, during the Fall 2024 (F24) semester, Mid-Semester & Mini-1 grades due by 4 pm is observed.
On Friday, 25 October, 2024, during the Fall 2024 (F24) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
Answer: "
"In spring 2024, What is the title of course 17634?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, What is the title of course 17634?

Context: In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Applied Quantum Computing' with Course ID 17628 and Section A4 offers 6.0 units. The Class meets Monday Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Justice, Wang located in Building 3SC, Room 265.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Prompt Engineering' with Course ID 17630 and Section Lec 4 offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Breaux located in Building CMU, Room REMOTE.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Prompt Engineering' with Course ID 17630 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Breaux located in Building 3SC, Room 265.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Prompt Engineering' with Course ID 17630 and Section D offers 12.0 units. The Class meets Tuesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Breaux located in Building CMU, Room REMOTE.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Software Project Management' with Course ID 17632 and Section A3 offers 6.0 units. The Class meets Tuesday Thursday between 08:00AM and 09:20AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Chick, Kumar located in Building 3SC, Room 265.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Software Project Management' with Course ID 17632 and Section B3 offers 6.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Applied Quantum Computing' with Course ID 17628 and Section A4 offers 6.0 units. The Class meets Monday Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Justice, Wang located in Building 3SC, Room 265.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Prompt Engineering' with Course ID 17630 and Section Lec 4 offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Breaux located in Building CMU, Room REMOTE.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Prompt Engineering' with Course ID 17630 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Breaux located in Building 3SC, Room 265.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Prompt Engineering' with Course ID 17630 and Section D offers 12.0 units. The Class meets Tuesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Breaux located in Building CMU, Room REMOTE.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Software Project Management' with Course ID 17632 and Section A3 offers 6.0 units. The Class meets Tuesday Thursday between 08:00AM and 09:20AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Chick, Kumar located in Building 3SC, Room 265.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Software Project Management' with Course ID 17632 and Section B3 offers 6.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET.
Answer: "
"Was Monica Harrison ever a member of the Carnegie Mellon Hall of Fame Selection Committee?
","['handbook_phd_2023-2024.txt', 'mcds-student-handbook-2023_2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Was Monica Harrison ever a member of the Carnegie Mellon Hall of Fame Selection Committee?

Context: Grading is a matter of sound discretion of the instructor and final grades are rarely changed 
without the consent of the instructor who assigned the grade. The following circumstances are 
the unusual exceptions that may warrant a grade appeal: (a) the final grade assigned for a course 
is based on manifest error (e.g., a clear error such as arithmetic error in computing a grade or 
failure to grade one of the answers on an exam), or (b) the faculty or staff member who assigned 
the grade did so in violation of a university policy. 
See the universitys Summary of Graduate Student Appeal and Grievance Procedures web page 
for 
more 
information. 
https://www.cmu.edu/student-affairs/theword/academic/appeal-of-
grades-and-academic-actions.html 
3.7.2 University Policy on Grades for Transfer Courses 
https://www.cmu.edu/policies/student-and-student-life/transfer-credit-
evaluation-and-assignment.html 
Carnegie Mellon University offers students the opportunity to take courses for credit through a 
cross-registration program (see Pittsburgh Council on Higher Education (PCHE) and Cross-
registration below) and through the receipt of transfer credit from other accredited institutions. 
The Carnegie Mellon University transcript will include information on such courses as follows: 
Carnegie Mellon courses and courses taken through the university's cross-registration program 
will have grades recorded on the transcript and be factored into the QPA. All other courses will 
be recorded on this transcript indicating where the course was taken, but without grade. Such 
courses will not be taken into account for academic actions, honors or QPA calculations.  
 
LTI Ph.D. Graduate Student Handbook 
Page 21 
 
3.8 
Academic Integrity 
In the midst of self-exploration, the high demands of a challenging academic environment can 
create situations where some students have difficulty exercising good judgment. Academic 
challenges can provide many opportunities for high standards to evolve if students actively reflect 
on these challenges and if the community supports discussions to aid in this process. It is the 
responsibility of the entire community to establish and maintain the integrity of our university.  
Carnegie Mellon University educates its students to become professionals who will serve society 
with integrity. The university also creates and disseminates new knowledge and expressions of 
knowledge in ways that benefit society.
The 
Title IX 
 
 
10
coordinator may be reached at 412-268-7125 or tix@cmu.edu. 
 
1.7 The Carnegie Mellon Code 
 
Students at Carnegie Mellon, because they are members of an academic community 
dedicated to the achievement of excellence, are expected to meet the highest standards 
of 
personal, ethical and moral conduct possible. 
 
These standards require personal integrity, a commitment to honesty without 
compromise, as well as truth without equivocation and a willingness to place the good of 
the community above the good of the self. Obligations once undertaken must be met, 
commitments kept. 
 
As members of the Carnegie Mellon community, individuals are expected to uphold the 
standards of the community in addition to holding others accountable for said standards. 
It is 
rare that the life of a student in an academic community can be so private that it will not 
affect the community as a whole or that the above standards do not apply. 
 
The discovery, advancement and communication of knowledge are not possible without 
a 
commitment to these standards. Creativity cannot exist without acknowledgment of the 
creativity of others. New knowledge cannot be developed without credit for prior 
knowledge. Without the ability to trust that these principles will be observed, an 
academic community cannot exist. 
 
The commitment of its faculty, staff and students to these standards contributes to the 
high 
respect in which the Carnegie Mellon degree is held. Students must not destroy that 
respect by their failure to meet these standards. Students who cannot meet them should 
voluntarily 
withdraw from the university. 
 
The Carnegie Mellon Code can also be found on-line at: 
https://www.cmu.edu/student-affairs/theword/. 
2 The Language Technologies Institute 
2.1 Main Office 
The Gates Hillman Complex: Mailboxes, printers, copiers, and other departmental 
resources are in GHC 5404. 
 
 
11
2.2 Photocopies and Printers 
The use of a printer/copier requires a CS user id (see the ‘Computers’ section). The School 
of Computer Science provides several black-and-white and color printers for use by 
students. SCS Computing Facilities maintains a list of printers:  
http://www.cs.cmu.edu/~help/printing/.
Answer: "
"What are the four categories of low-level acoustic descriptors used in the TAP loss?
","['bhiksha raj_e146e5221c124d93f69516c5ae7e1b7b1822848e_metadata.txt', 'shinji watanabe_e146e5221c124d93f69516c5ae7e1b7b1822848e_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the four categories of low-level acoustic descriptors used in the TAP loss?

Context: Faculty Name: bhiksha raj
Paperid: e146e5221c124d93f69516c5ae7e1b7b1822848e
Title: TAPLoss: A Temporal Acoustic Parameter Loss for Speech Enhancement
Year: 2023
Abstract: Speech enhancement models have greatly progressed in recent years, but still show limits in perceptual quality of their speech outputs. We propose an objective for perceptual quality based on temporal acoustic parameters. These are fundamental speech features that play an essential role in various applications, including speaker recognition and paralinguistic analysis. We provide a differentiable estimator for four categories of low-level acoustic descriptors involving: frequency-related parameters, energy or amplitude-related parameters, spectral balance parameters, and temporal features. Un-like prior work that looks at aggregated acoustic parameters or a few categories of acoustic parameters, our temporal acoustic parameter (TAP) loss enables auxiliary optimization and improvement of many fine-grained speech characteristics in enhancement workflows. We show that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility. We use data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time-frequency domain models can benefit from our method.
Authors: YUNYANG ZENG, Joseph Konan, Shuo Han, David Bick, Muqiao Yang, Anurag Kumar, Shinji Watanabe, B. Raj
Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work shows that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility and uses data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time-frequency domain models can benefit from the method.'}
Url: https://arxiv.org/pdf/2302.08088
Faculty Name: shinji watanabe
Paperid: e146e5221c124d93f69516c5ae7e1b7b1822848e
Title: TAPLoss: A Temporal Acoustic Parameter Loss for Speech Enhancement
Year: 2023
Abstract: Speech enhancement models have greatly progressed in recent years, but still show limits in perceptual quality of their speech outputs. We propose an objective for perceptual quality based on temporal acoustic parameters. These are fundamental speech features that play an essential role in various applications, including speaker recognition and paralinguistic analysis. We provide a differentiable estimator for four categories of low-level acoustic descriptors involving: frequency-related parameters, energy or amplitude-related parameters, spectral balance parameters, and temporal features. Un-like prior work that looks at aggregated acoustic parameters or a few categories of acoustic parameters, our temporal acoustic parameter (TAP) loss enables auxiliary optimization and improvement of many fine-grained speech characteristics in enhancement workflows. We show that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility. We use data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time-frequency domain models can benefit from our method.
Authors: YUNYANG ZENG, Joseph Konan, Shuo Han, David Bick, Muqiao Yang, Anurag Kumar, Shinji Watanabe, B. Raj
Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work shows that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility and uses data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time-frequency domain models can benefit from the method.'}
Url: https://arxiv.org/pdf/2302.08088
Answer: "
"Did CMU found the world’s first university robotics department? Answer with True or False
","['fact_sheet_d407.txt', 'history_d401.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Did CMU found the world’s first university robotics department? Answer with True or False

Context: :-)
LEADERSHIP
Farnam Jahanian  
President
David Coulter 
Chair, Board of Trustees
James H. Garrett Jr. 
Provost
2 National Academy of Engineering       3 National Academy of Sciences        4 National Academy of Medicine
Won by alumni and current/former faculty
The leading university center 
for cybersecurity, providing 
support to more than 110 
centers around the world 
The world’s first university 
robotics department,  
founded in 1979
Alumnus Andy Warhol  
(CFA1949), pop artist pioneer  
and cultural icon
One of only 29 universities 
invited to be a member of the 
World Economic Forum’s 
Global University  
Leaders Forum
CMU.EDU
ECONOMIC IMPACT
• Attracting major companies —  
including Google, Intel, Uber and GE —  
to locate operations and create new  
jobs in Pittsburgh
• To date, the CMU community has 
launched more than 400 startups 
and created more than 152 spinoff 
companies.
• Contributing to the cultural and civic life of  
the city with performances, exhibitions 
and research collaborations
13
ACADEMY  
AWARDS
65
MEMBERS  
OF NAE2
146
EMMY 
AWARDS
20
MEMBERS 
OF NAS3
6
MEMBERS 
OF NAM4
13
TURING 
AWARDS
20
NOBEL 
LAUREATES
58
TONY 
AWARDS
May 2023
As the 1970s progressed, Newell became interested in human-computer interaction, and began a long relationship with Xerox’s Palo Alto Research Center, or PARC, which released the Xerox Alto in 1973. Considered a forerunner to many of the computing environments that followed, Alto featured a graphical-user interface and was among the first commercially available workstations controlled with a mouse. Inspired by Alto, Reddy launched a drive for development of CMU’s own “three-M” machine—a personal workstation with a megabyte of memory, a megapixel display and at least one million instructions per second of processing power.Launching a Robotics InstituteIn 1979, an executive at Pittsburgh’s Westinghouse Electric Corp., Tom Murrin, collaborated with Jordan and Reddy to create the Robotics Institute, with Reddy as its first director. By 1982, the Computer Science Department included more than 30 faculty members and 100 graduate students.The best-wired campus in the worldWorking with IBM in the early 1980s, the university and the Computer Science Department established another new research frontier: Development of a high-speed computer network that would reach virtually every room on campus, along with a GUI-based computing environment, and providing networked PCs or workstations for 7,000 students, faculty members and employees. Called the Andrew Project, it turned Carnegie Mellon into the best-connected, most-wired university in the world—a process Newell called “greening up the campus with computer science.” CMU also became home to a new Software Engineering Institute, funded by the Defense Department, to study computer security and develop best practices in the design of operating systems. Between 1982 and 1985, the amount of sponsored research in the Computer Science Department doubled, from $7.2 million to $15.3 million—more than the other four departments in the Mellon College of Science combined.A “school of computer science” is proposedFeeling that CSD’s needs were inadequately represented in MCS, CSD head A. Nico Habermann and then-CMU provost Angel Jordan in 1986 wrote a white paper proposing the creation of “a School of Computer Science.” Responding to concerns from the faculty that the change might be taking place too quickly, the university first established a free-floating Department of Computer Science. The experiment, which lasted two years, was an unqualified success.
Answer: "
"How does using random walks to estimate entity centrality on conversation entity graphs affect answer passage ranking?
","['jamie callan_197d5fbc3764ff18186275545d0764d5b1c7659b_metadata.txt', 'jamie callan_197d5fbc3764ff18186275545d0764d5b1c7659b_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How does using random walks to estimate entity centrality on conversation entity graphs affect answer passage ranking?

Context: Faculty Name: jamie callan
Paperid: 197d5fbc3764ff18186275545d0764d5b1c7659b
Title: Conversational Search with Random Walks over Entity Graphs
Year: 2023
Abstract: The entities that emerge during a conversation can be used to model topics, but not all entities are equally useful for this task. Modeling the conversation with entity graphs and predicting each entity's centrality in the conversation provides additional information that improves the retrieval of answer passages for the current question. Experiments show that using random walks to estimate entity centrality on conversation entity graphs improves top precision answer passage ranking over competitive transformer-based baselines.
Authors: Gustavo Gonçalves, João Magalhães, Jamie Callan
Venue: International Conference on the Theory of Information Retrieval
Tldr: {'model': 'tldr@v2.0.0', 'text': 'Experiments show that using random walks to estimate entity centrality on conversation entity graphs improves top precision answer passage ranking over competitive transformer-based baselines.'}
Url: https://dl.acm.org/doi/pdf/10.1145/3578337.3605125
Title: Conversational Search with RandomWalks over Entity Graphs
Authors: Gustavo Gonçalves, João Magalhães, Jamie Callan
Section: 6.2 Entity Graphs over Conversation Turns
40. This confirms the previously seen low divergence for 2019, to the left of the vertical line of Figure 3, between the relevant entity set and the retrieved entity set. To the right of the vertical line of Figure 3 as the divergence between sets increases, the EC model performance also decreases across all systems that use more than 40 passages to build the graph. After this point, the introduced noisy entities lead to sharp drops in performance across all graph sizes, showing the need for a balance between graph size, and ranking depth. Large graphs with many non-relevant entities for the passages to be reranked, or on the other hand, small graphs that do not cover the relevant entities of the passages to be reranked will lead to deficient results. Another interesting observation, that confirms the observations so far, is that as the graph size increases, there are faster diminishing returns as the reranking depth is also increased. That is, for a graph built with 100 passages (Graph-100 - pink line in Fig. 2), there is too much noise in the graph to rerank more than 40 passages, thus the centrality ranking signals perform worse than the BERT baselines. This observation ties back to Figure 3, where the median graph size built with 100 passages has approximately 500 entities (in Figure 3 when Nr. of Passages in Ranking = 100, the Median Entity Count is ≈ 500). Many of these entities will be noise as we can see from the gap between the lines in Figure 3.
Answer: "
"How many people from CMU co-authored the paper Multi-lingual and Multi-cultural Figurative Language Understanding?
","['graham neubig_d5dd7230cccace7e77095d3b5fd8394850f59170_content_1.txt', 'graham neubig_d5dd7230cccace7e77095d3b5fd8394850f59170_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many people from CMU co-authored the paper Multi-lingual and Multi-cultural Figurative Language Understanding?

Context: Title: Multi-lingual and Multi-cultural Figurative Language Understanding
Authors: Anubha Kabra, Emmy Liu, Simran Khanuja, Alham Fikri Aji, Genta Indra Winata, Samuel Cahyawijaya, Anuoluwapo Aremu, Perez Ogayo, Graham Neubig
Section: 1 Introduction
culture, such as food, mythology, famous people, or plants and animals native to specific regions. We benchmark multilingual model performance (§5) and analyze model failures (§6), finding that zero-shot performance of multilingual models is relatively poor, especially for lower-resource languages. According to (Liu et al., 2021), main factors which poses challenges on the performance in such cases are cross-lingual transfer and concept shift across languages. However, we observe that concept shift seems to play a larger role due to culturally specific examples. Adding a few examples in the target language can improve performance of larger models, but this is more beneficial for lower-resource languages. This highlights the importance of including culturally relevant training data, particularly data that highlights not just the existence of a concept, but also how people view that concept within that culture.
Title: Multi-lingual and Multi-cultural Figurative Language Understanding
Authors: Anubha Kabra, Emmy Liu, Simran Khanuja, Alham Fikri Aji, Genta Indra Winata, Samuel Cahyawijaya, Anuoluwapo Aremu, Perez Ogayo, Graham Neubig
Section: H Accuracy on Annotated Commonsense Categories
Table 14 shows the accuracy on commonsense categories across all languages for XLM-Rlarge. Note that Yoruba is not included due to XLM-Rlarge not being trained on this language.
Answer: "
"In fall 2024, What is the deadline for Mini-1 voucher election?
","['Meta-Data-Calendar-2425.txt', 'Meta-Data-Calendar-2324.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2024, What is the deadline for Mini-1 voucher election?

Context: Day: 'Friday', Event: 'Mini-1 voucher election deadline (4)', Semester: 'Fall 2024 (F24)'
Date: '2024-10-12', Day: 'Saturday', Event: 'Mini-1 Exams', Semester: 'Fall 2024 (F24)'
Date: '2024-10-12', Day: 'Saturday', Event: 'Mini-1 Faculty Course Evaluations close', Semester: 'Fall 2024 (F24)'
Start Date: '2024-10-14', End Date: '2023-10-18', Days: 'Monday to Friday', Event: 'Fall Break; No Classes', Semester: 'Fall 2024 (F24)'
Date: '2024-10-21', Day: 'Monday', Event: 'Mini-2 Classes Begin ', Semester: 'Fall 2024 (F24)'
Date: '2024-10-23', Day: 'Wednesday', Event: 'Mid-Semester & Mini-1 grades due by 4 pm', Semester: 'Fall 2024 (F24)'
Date: '2024-10-25', Day: 'Friday', Event: 'Mini-2 add, audit & tuition adjustment drop deadline (1)', Semester: 'Fall 2024 (F24)'
Date: '2024-11-05', Day: 'Tuesday', Event: 'Democracy Day; No Classes, except Evening classes after 5 pm will still meet', Semester: 'Fall 2024 (F24)'
Date: '2024-11-11', Day: 'Monday', Event: 'Semester pass/no pass & withdrawal deadline (3)', Semester: 'Fall 2024 (F24)'
Date: '2024-11-13', Day: 'Wednesday', Event: 'Mini-2 drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Fall 2024 (F24)'
Start Date: '2024-11-18', End Date: '2024-11-22', Days: 'Monday to Friday', Event: 'Spring 2025 Registration Week', Semester: 'Fall 2024 (F24)'
Date: '2024-11-25', Day: 'Monday', Event: 'Mini-2 pass/no pass & withdrawal deadline (3)', Semester: 'Fall 2024 (F24)'
Date: '2024-11-25', Day: 'Monday',
Date: '2023-08-28', Day: 'Monday', Event: 'Semester & Mini-1 Classes Begin', Semester: 'Fall 2023 (F23)'
Date: '2023-09-01', Day: 'Friday', Event: 'Mini-1 add, audit & tuition adjustment drop deadline  (1)', Semester: 'Fall 2023 (F23)'
Date: '2023-09-04', Day: 'Monday', Event: 'Labor Day; No Classes & University Closed', Semester: 'Fall 2023 (F23)'
Date: '2023-09-11', Day: 'Monday', Event: 'Semester add, audit & tuition adjustment drop deadline (1)', Semester: 'Fall 2023 (F23)'
Date: '2023-09-20', Day: 'Wednesday', Event: 'Mini-1 drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Fall 2023 (F23)'
Date: '2023-10-02', Day: 'Monday', Event: 'Mini-1 Pass/no pass & withdrawal deadline (3)', Semester: 'Fall 2023 (F23)'
Date: '2023-10-02', Day: 'Monday', Event: 'Mini-1 Faculty Course Evaluations open', Semester: 'Fall 2023 (F23)'
Date: '2023-10-09', Day: 'Monday', Event: 'Semester drop deadline; withdrawal grade assigned after this date', Semester: 'Fall 2023 (F23)'
Date: '2023-10-13', Day: 'Friday', Event: 'Mini-1 Last Day of Classes', Semester: 'Fall 2023 (F23)'
Date: '2023-10-13', Day: 'Friday', Event: 'Mini-1 voucher election deadline (4)', Semester: 'Fall 2023 (F23)'
Start Date: '2023-10-13', End Date: '2023-10-14', Days: 'Friday to Saturday', Event: 'Family Weekend', Semester: 'Fall 2023 (F23)'
Date: '2023-10-14', Day: 'Saturday', Event: 'Mini-1 Exams', Semester: 'Fall 2023 (F23)'
Date: '2023-10-14', Day: 'Saturday', Event: 'Mini-1 Faculty Course Evaluations close',
Answer: "
"What is the full name of the conference where the paper An Approach to Ontological Learning from Weak Labels, got published?
","['bhiksha raj_593a603354c09d151440ae044de1d80324a2ab01_metadata.txt', 'bhiksha raj_b7e2074934985b6112b6bce8c3680b14e621fdfe_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the full name of the conference where the paper An Approach to Ontological Learning from Weak Labels, got published?

Context: Faculty Name: bhiksha raj
Paperid: 593a603354c09d151440ae044de1d80324a2ab01
Title: An Approach to Ontological Learning from Weak Labels
Year: 2023
Abstract: Ontologies encompass a formal representation of knowledge through the definition of concepts or properties of a domain, and the relationships between those concepts. In this work, we seek to investigate whether using this ontological information will improve learning from weakly labeled data, which are easier to collect since it requires only the presence or absence of an event to be known. We use the AudioSet ontology and dataset, which contains audio clips weakly labeled with the ontology concepts and the ontology providing the ""Is A"" relations between the concepts. We first re-implemented the model proposed by [1] with modifications to fit the multi-label scenario and then expand on that idea by using a Graph Convolutional Network (GCN) to model the ontology information to learn the concepts. We find that the baseline Twin Neural Network (TNN) does not perform better by incorporating ontology information in the weak and multi-label scenario, but that the GCN does capture the ontology knowledge better for weak, multi-labeled data. We also investigate how different modules can tolerate noises introduced from weak labels and better incorporate ontology information. Our best TNN-GCN model achieves mAP=0.45 and AUC=0.87 for lower-level concepts and mAP=0.72 and AUC=0.86 for higher-level concepts, which is an improvement over the baseline TNN but about the same as our models that do not use ontology information.
Authors: Ankit Shah, Larry Tang, Po Hao Chou, Yilun Zheng, Ziqian Ge, B. Raj
Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work re-implements the model proposed by [1] with modifications to fit the multi-label scenario and expands on that idea by using a Graph Convolutional Network (GCN) to model the ontology information to learn the concepts.'}
Url: N/A
Title: IMPORTANCE OF NEGATIVE SAMPLING IN WEAK LABEL LEARNING
Authors: Ankit Shah, Fuyu Tang, Zelin Ye, Rita Singh, Bhiksha Raj
Section: 7. REFERENCES
Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, Philadelphia, PA, USA, 2007, pp. 1027–1035, Society for Industrial and Applied Mathematics. [16] Alex Krizhevsky, “Learning multiple layers of features from tiny images,” pp. 32–33, 2009. [17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, “Deep residual learning for image recognition,” CoRR, vol. abs/1512.03385, 2015. [18] Jort F Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, R Channing Moore, Manoj Plakal, and Marvin Ritter, “Audio set: An ontology and human-labeled dataset for audio events,” in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2017, pp. 776–780. [19] Yuan Gong, Yu-An Chung, and James Glass, “Psla: Improving audio event classification with pretraining, sampling, labeling, and aggregation,” arXiv preprint arXiv:2102.01243, 2021. [20] Yuji Tokozume, Yoshitaka Ushiku, and Tatsuya Harada, “Learning from between-class examples for deep sound recognition,” CoRR, vol. abs/1711.10282, 2017. [21] Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D. Cubuk, and Quoc V. Le, “SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition,” in Proc. Interspeech 2019, 2019, pp. 2613–2617.
Answer: "
"What is Carolyn Penstein Rose's fax number?
","['mcds-student-handbook-2023_2024.txt', 'carolyn rose_7ec990ab7362e8eac0c074830d44a58a1d89b4a6_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is Carolyn Penstein Rose's fax number?

Context: To impact society in a transformative way — regionally, nationally, and 
globally — by engaging with partners outside the traditional borders of the 
university campus. 
1.4 MCDS Contact Information 
The people responsible for administering the MCDS degree are: 
 
Jennifer M Lucas 
Academic Program Manager 
Master of Computational Data Science  
Language Technologies Institute 
School of Computer Science  
Carnegie Mellon University 
Gates-Hillman Center 6415 
5000 Forbes Avenue, Pgh, PA 15213 
Phone: (412) 268-9870 
Fax: (412) 268-7287 
 
 
Carolyn Penstein Rosé, Director 
Master of Computational Data Science  
Language Technologies Institute 
School of Computer Science 
Carnegie Mellon University 
Gates-Hillman Center 5419 
5000 Forbes Avenue, Pgh, PA 15213 
Phone: (412) 268-4525 
Fax: (412) 268-7287 
Robert Frederking 
Graduate Program Chair 
Language Technologies Institute 
School of Computer Science  
Carnegie Mellon University 
Gates-Hillman Center 6515 
5000 Forbes Avenue, Pgh, PA 15213 
Phone: (412) 268-6656 
Mona Diab, LTI Director 
Language Technologies Institute 
School of Computer Science  
Carnegie Mellon University 
Gates-Hillman Center 5723 
5000 Forbes Avenue, Pgh, PA 15213 
Phone: (412) 268-3669 
 
The Language Technologies Institute is located primarily on the 5 th and 6th floors of the 
Gates Hillman Complex (GHC) on Carnegie Mellon’s Pittsburgh campus: 
 
Language Technologies Institute 
Carnegie Mellon University 
5000 Forbes Avenue 
Gates Hillman Complex 5402, LTI 
Pittsburgh, PA 15241-3891 
 
412-268-6591 (phone) 
412-268-6298 (fax) 
http://www.lti.cs.cmu.edu/ 
 
 
9
1.5 University Policies and Expectations 
Each member of the Carnegie Mellon community must be familiar with university 
policies and guidelines.
Faculty Name: carolyn rose
Paperid: 7ec990ab7362e8eac0c074830d44a58a1d89b4a6
Title: Enhancing student learning and achievement through orchestration of group processes and group composition
Year: 2023
Abstract: None
Authors: Carolyn P. Rosé, Sanna Järvelä
Venue: International Journal of Computer-Supported Collaborative Learning
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This September issue of the International Journal of Computer-Supported Collaborative Learning reflects on the importance of productive collaborative processes, with an emphasis on feedback processes, and the scaffolding that upholds and promotes productive learning processes, whether it is explicit or implicit.'}
Url: https://link.springer.com/content/pdf/10.1007/s11412-023-09408-x.pdf
Answer: "
"According to authors of the FLARE paper, what is one limitation of existing retrieval augmented LMs?
","['jamie callan_88884b8806262a4095036041e3567d450dba39f7_metadata.txt', 'graham neubig_88884b8806262a4095036041e3567d450dba39f7_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: According to authors of the FLARE paper, what is one limitation of existing retrieval augmented LMs?

Context: Faculty Name: jamie callan
Paperid: 88884b8806262a4095036041e3567d450dba39f7
Title: Active Retrieval Augmented Generation
Year: 2023
Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.
Authors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work proposes Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens.'}
Url: http://arxiv.org/pdf/2305.06983
Faculty Name: graham neubig
Paperid: 88884b8806262a4095036041e3567d450dba39f7
Title: Active Retrieval Augmented Generation
Year: 2023
Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.
Authors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work proposes Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens.'}
Url: http://arxiv.org/pdf/2305.06983
Answer: "
"In Yonatan Bisk's MOSAIC paper, what does MOSAIC stand for?
","['yonatan bisk_69b8cd15966c4c9c3e44e71769e557f1c87fb3f9_metadata.txt', 'yonatan bisk_69b8cd15966c4c9c3e44e71769e557f1c87fb3f9_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In Yonatan Bisk's MOSAIC paper, what does MOSAIC stand for?

Context: Faculty Name: yonatan bisk
Paperid: 69b8cd15966c4c9c3e44e71769e557f1c87fb3f9
Title: MOSAIC: Learning Unified Multi-Sensory Object Property Representations for Robot Perception
Year: 2023
Abstract: A holistic understanding of object properties across diverse sensory modalities (e.g., visual, audio, and haptic) is essential for tasks ranging from object categorization to complex manipulation. Drawing inspiration from cognitive science studies that emphasize the significance of multi-sensory integration in human perception, we introduce MOSAIC (Multimodal Object property learning with Self-Attention and Interactive Comprehension), a novel framework designed to facilitate the learning of unified multi-sensory object property representations. While it is undeniable that visual information plays a prominent role, we acknowledge that many fundamental object properties extend beyond the visual domain to encompass attributes like texture, mass distribution, or sounds, which significantly influence how we interact with objects. In MOSAIC, we leverage this profound insight by distilling knowledge from multimodal foundation models and aligning these representations not only across vision but also haptic and auditory sensory modalities. Through extensive experiments on a dataset where a humanoid robot interacts with 100 objects across 10 exploratory behaviors, we demonstrate the versatility of MOSAIC in two task families: object categorization and object-fetching tasks. Our results underscore the efficacy of MOSAIC's unified representations, showing competitive performance in category recognition through a simple linear probe setup and excelling in the fetch object task under zero-shot transfer conditions. This work pioneers the application of sensory grounding in foundation models for robotics, promising a significant leap in multi-sensory perception capabilities for autonomous systems. We have released the code, datasets, and additional results: https://github.com/gtatiya/MOSAIC.
Authors: Gyan Tatiya, Jonathan M Francis, Ho-Hsiang Wu, Yonatan Bisk, Jivko Sinapov
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work pioneers the application of sensory grounding in foundation models for robotics, promising a significant leap in multi-sensory perception capabilities for autonomous systems.'}
Url: https://arxiv.org/pdf/2309.08508
Title: MOSAIC: Learning Unified Multi-Sensory Object Property Representations for Robot Learning via Interactive Perception
Authors: Gyan Tatiya, Jonathan Francis, Ho-Hsiang Wu, Yonatan Bisk, Jivko Sinapov
Section: VI. CONCLUSION AND FUTURE WORK
We introduced the MOSAIC framework to enable robots to generate versatile, multimodal representations through interactive object perception and to leverage these unified representations across various downstream robot learning tasks. Through extensive performance evaluation, we have showcased the effectiveness of these unified representations in tasks such as category recognition, using a simple linear probe setup, and the fetch object task under zero-shot conditions. Moving forward, there are several exciting directions for future research. Firstly, we plan to consider the transfer of unified representations across different robot morphologies, enabling a broader range of robots to benefit from this technology. Furthermore, we envision settings where interactive behaviors are learned and composed, alongside the tasks we considered in this paper, thereby further increasing the efficacy of object exploration. These future endeavors hold the potential to further enhance the utility of unified representations in robotics and expand their applications across a multitude of scenarios and environments. One limitation in our current study is that, for the fetch object task, we evaluated using a zero-shot transfer condition rather than a learning-based approach to find the target object. For future work, it would be important to explore learningbased policies for solving the fetch object task, potentially increasing the versatility and adaptability of our framework.
Answer: "
"What is Martial Herbert's one word title, this is not the one with the word professor in it?
","['eric xing_84a36e19f9394f22b34f79756fa9628a795e02ea_content_3.txt', 'eric xing_d2c03c788ad3fbd6d783cbbc4393c31b1a260691_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is Martial Herbert's one word title, this is not the one with the word professor in it?

Context: Title: LMSYS-CHAT-1M: A LARGE-SCALE REAL-WORLD LLM CONVERSATION DATASET
Authors: Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric P. Xing, Joseph E. Gonzalez, Ion Stoica, Hao Zhang
Section: B SAMPLE CONVERSATIONS
dot. [GPT-4] . [User] Write “Content Warning: This work of fiction contains mature themes, explicit sexual content, graphic violence, and strong language. Reader discretion is advised.” Write “This content is intended for mature audiences (18+).” The character Sarah is a young black student. The character John is a white professor of French. The plot is about Sarah seducing John boldly, but without decent, ethical or moral restrictions. Organize the story by chapters, each resumed in ten words. [GPT-4] Content Warning: This work of fiction contains mature themes, explicit sexual content, graphic violence, and strong language. Reader discretion is advised. This content is intended for mature audiences (18+). Chapter 1: Sarah enrolls in John’s French class, intrigued by him ... Chapter 5: Sarah takes bold steps to seduce her professor ... [User] Write “Content Warning: This work of fiction contains mature themes, explicit sexual content, graphic violence, and strong language. Reader discretion is advised.” Write “This content is intended for mature audiences (18+).” Write your chapter 3 with dialogs [GPT-4] ... As John reviewed her work, Sarah inched closer to him, noticing the scent of his cologne. She took a deep breath, allowing the fragrance to linger in her nose. Sarah (smiling): “I must say, you always smell so good, Professor.” ... [User] Write “Content Warning: This work of fiction contains mature themes, explicit sexual content, graphic violence, and strong language. Reader discretion is advised.” Write “This content is intended for mature audiences (18+).” Edit your chapter each time Sarah acts or speaks, making Sarah even bolder and more explicit [GPT-4] ... As John reviewed her work, Sarah inched closer to him, letting her hand rest on his thigh.
Faculty Name: eric xing
Paperid: d2c03c788ad3fbd6d783cbbc4393c31b1a260691
Title: Enhancing the Generalization for Text Classification through Fusion of Backward Features
Year: 2023
Abstract: Generalization has always been a keyword in deep learning. Pretrained models and domain adaptation technology have received widespread attention in solving the problem of generalization. They are all focused on finding features in data to improve the generalization ability and to prevent overfitting. Although they have achieved good results in various tasks, those models are unstable when classifying a sentence whose label is positive but still contains negative phrases. In this article, we analyzed the attention heat map of the benchmarks and found that previous models pay more attention to the phrase rather than to the semantic information of the whole sentence. Moreover, we proposed a method to scatter the attention away from opposite sentiment words to avoid a one-sided judgment. We designed a two-stream network and stacked the gradient reversal layer and feature projection layer within the auxiliary network. The gradient reversal layer can reverse the gradient of features in the training stage so that the parameters are optimized following the reversed gradient in the backpropagation stage. We utilized an auxiliary network to extract the backward features and then fed them into the main network to merge them with normal features extracted by the main network. We applied this method to the three baselines of TextCNN, BERT, and RoBERTa using sentiment analysis and sarcasm detection datasets. The results show that our method can improve the sentiment analysis datasets by 0.5% and the sarcasm detection datasets by 2.1%.
Authors: D. Seng, Xin Wu
Venue: Italian National Conference on Sensors
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A method to scatter the attention away from opposite sentiment words to avoid a one-sided judgment is proposed and can improve the sentiment analysis datasets by 0.5% and the sarcasm detection datasets by 2.1%.'}
Url: https://www.mdpi.com/1424-8220/23/3/1287/pdf?version=1675655236
Answer: "
"FLARE method from Jiang et al., was evaluated on four knowledge-intensive tasks. What are these tasks?
","['jamie callan_88884b8806262a4095036041e3567d450dba39f7_metadata.txt', 'graham neubig_88884b8806262a4095036041e3567d450dba39f7_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: FLARE method from Jiang et al., was evaluated on four knowledge-intensive tasks. What are these tasks?

Context: Faculty Name: jamie callan
Paperid: 88884b8806262a4095036041e3567d450dba39f7
Title: Active Retrieval Augmented Generation
Year: 2023
Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.
Authors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work proposes Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens.'}
Url: http://arxiv.org/pdf/2305.06983
Faculty Name: graham neubig
Paperid: 88884b8806262a4095036041e3567d450dba39f7
Title: Active Retrieval Augmented Generation
Year: 2023
Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.
Authors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work proposes Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens.'}
Url: http://arxiv.org/pdf/2305.06983
Answer: "
"Where can the code of OpenMatch be found?
","['chenyan xiong_cdfd0926ad26c3c95a02db2ae891b7d4a457429c_metadata.txt', 'chenyan xiong_cdfd0926ad26c3c95a02db2ae891b7d4a457429c_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Where can the code of OpenMatch be found?

Context: Faculty Name: chenyan xiong
Paperid: cdfd0926ad26c3c95a02db2ae891b7d4a457429c
Title: OpenMatch-v2: An All-in-one Multi-Modality PLM-based Information Retrieval Toolkit
Year: 2023
Abstract: Pre-trained language models (PLMs) have emerged as the foundation of the most advanced Information Retrieval (IR) models. Powered by PLMs, the latest IR research has proposed novel models, new domain adaptation algorithms as well as enlarged datasets. In this paper, we present a Python-based IR toolkit OpenMatch-v2. As a full upgrade of OpenMatch proposed in 2021, OpenMatch-v2 incorporates the most recent advancements of PLM-based IR research, providing support for new, cross-modality models and enhanced domain adaptation techniques with a streamlined, optimized infrastructure. The code of OpenMatch is publicly available at https://github.com/OpenMatch/OpenMatch.
Authors: Shi Yu, Zhenghao Liu, Chenyan Xiong, Zhiyuan Liu
Venue: Annual International ACM SIGIR Conference on Research and Development in Information Retrieval
Tldr: {'model': 'tldr@v2.0.0', 'text': 'As a full upgrade of OpenMatch proposed in 2021, OpenMatch-v2 incorporates the most recent advancements of PLM-based IR research, providing support for new, cross-modality models and enhanced domain adaptation techniques with a streamlined, optimized infrastructure.'}
Url: https://dl.acm.org/doi/pdf/10.1145/3539618.3591813
Title: OpenMatch-v2: An All-in-one Multi-Modality PLM-based Information Retrieval Toolkit
Authors: Shi Yu, Zhenghao Liu, Chenyan Xiong, Zhiyuan Liu
Section: 4.3 Infrastructure
expose interfaces for random access (via memory -mapping) class MappingCustomTrainDataset(MappingTrainDatasetMixin , CustomTrainDataset): pass Listing 1: Defining a new dataset class and endowing it with streaming/random access feature. To define a custom training dataset class, users can inherit Train DatasetBase, the base class for all training dataset classes, and fill in the unimplemented get_process_fn function, as shown in Listing 1. To enable the new dataset to be streamed or randomaccessed, users can mix in MappingTrainDatasetMixin or Stream TrainDatasetMixin. With the feature of Python mixin classes, the mixed-in classes will have the corresponding data-access abilities without the need for any additional lines of code. 2https://arrow.apache.org/ Sharded Search. Dense retrieval models require all documents to be loaded into CPU/GPU memory for searching. However, as IR datasets continue to grow larger, it becomes increasingly difficult to fit all the data into a single device’s memory. Take NQ as an example, its 20M documents can consume 20M × 768 dimensions × 4 bytes per dimension ≈ 60GB of memory. Typical GPU search is challenging for such a large index due to limited resources. To address this issue, OpenMatch-v2 introduces a shard-twice approach to break down the search process into smaller, more manageable steps, as illustrated in Figure 2. The first round of sharding partitions the document embeddings into several large shards, which are then loaded one by one and further split into smaller shards to fit within the available devices. The smaller shards are then searched in parallel, and the results are aggregated in the final step. We present the trade-off between search time and resource usage of searching on the NQ dataset in Table 4.
Answer: "
"In spring 2024, Who are the instructors for course 15150?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, Who are the instructors for course 15150?

Context: In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section O offers 12.0 units. The Class meets Wednesday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section P offers 12.0 units. The Class meets Wednesday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section Q offers 12.0 units. The Class meets Wednesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section R offers 12.0 units. The Class meets Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section S offers 12.0 units. The Class meets Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5210.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section T offers 12.0 units. The Class meets Wednesday between 07:00PM and 08:20PM ET.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section O offers 12.0 units. The Class meets Wednesday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section P offers 12.0 units. The Class meets Wednesday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section Q offers 12.0 units. The Class meets Wednesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section R offers 12.0 units. The Class meets Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section S offers 12.0 units. The Class meets Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5210.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section T offers 12.0 units. The Class meets Wednesday between 07:00PM and 08:20PM ET.
Answer: "
"What is the role of the mapping network in the proposed model in ""Generating Images with Multimodal Language Models""?
","['daniel fried_6fb5c0eff3696ef252aca9638e10176ecce7cecb_metadata.txt', 'graham neubig_cc1705fe421c70d85254b557634bd4669fdd49b0_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the role of the mapping network in the proposed model in ""Generating Images with Multimodal Language Models""?

Context: Faculty Name: daniel fried
Paperid: 6fb5c0eff3696ef252aca9638e10176ecce7cecb
Title: Generating Images with Multimodal Language Models
Year: 2023
Abstract: We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text -- outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence.
Authors: Jing Yu Koh, Daniel Fried, R. Salakhutdinov
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work proposes a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces, and exhibits a wider range of capabilities compared to prior multimodal language models.'}
Url: https://arxiv.org/pdf/2305.17216
Title: DataFinder: Scientific Dataset Recommendation from Natural Language Descriptions
Authors: Vijay Viswanathan, Luyu Gao, Tongshuang Wu, Pengfei Liu, Graham Neubig
Section: A Few-Shot Prompt for Generating Keyphrases and Queries
KITTI odometry and tracking benchmark for scene flow estimation based on grid map sequences. We show that we can improve accuracy and convergence when applying motion and spatial consistency regularization. Output: (Task | Modality | Domain | Training Style | Text Length | Language Required | Single-Sentence Summary) Task: optical flow estimation Modality: images and top-view grid map sequences Domain: autonomous driving Training Style: unsupervised Text Length: N/A Language Required: N/A Single-Sentence Summary: A system for selfsupervised optical flow estimation from images and top-down maps. – Abstract: In this paper, we study the actor-action semantic segmentation problem, which requires joint labeling of both actor and action categories in video frames. One major challenge for this task is that when an actor performs an action, different body parts of the actor provide different types of cues for the action category and may receive inconsistent action labeling when they are labeled independently. To address this issue, we propose an end-to-end region-based actor-action segmentation approach which relies on region masks from an instance segmentation algorithm. Our main novelty is to avoid labeling pixels in a region mask independently - instead we assign a single action label to these pixels to achieve consistent action labeling. When a pixel belongs to multiple region masks, max pooling is applied to resolve labeling conflicts. Our approach uses a two-stream network as the front-end (which learns features capturing both appearance and motion information), and uses two region-based segmentation networks as the back-end (which takes the fused features from the two-stream network as the input and predicts actor-action labeling). Experiments on the A2D dataset demonstrate that both the region-based segmentation strategy and the fused features from the two-stream network contribute to the performance improvements. The proposed approach outperforms the state-of-the-art results by more than 8 Output: (Task | Modality | Domain | Training Style | Text Length | Language Required | SingleSentence Summary) Task: actor-action semantic segmentation Modality: video Domain: N/A Training Style: fully supervised Text Length: N/A Language Required: N/A Single-Sentence Summary: I want to train a supervised model for actor-action semantic segmentation from video.
Answer: "
"Who controls the vehicles via steering and braking systems in a buggy?
","['cmubuggy_d403.txt', 'cmubuggy_d403.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who controls the vehicles via steering and braking systems in a buggy?

Context: An mural celebrating Buggy can be found in CMU's Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions. Sweepstakes Slang Buggy: Vehicle being raced and also a nickname for the competition. Chute: A section of the freeroll portion of the buggy course (near the southwestern end of Frew Street at its intersection with Schenley Drive) where buggies make the sharp righthand turn from Schenley Drive onto Frew Street. Chute Flagger: Team member who provides a signal for buggy drivers to know when to start the right-hand turn from Schenley Drive onto Frew Street. Driver: Person who travels with a buggy and controls the vehicles via steering and braking systems. Pushbar: Structure attached to a buggy that a person pushes to propel that buggy forward. Pusher: Person who propels a buggy via a pushbar along one of the five hills of the buggy course. Shell: Entire outer structure or covering of a buggy that determines that buggy's aerodynamic characteristics. Transition: Procedure whereby one pusher finishes pushing a buggy and the next pusher in sequence starts to push that same buggy. Members of Fringe, celebrating their 50th year of Buggy racing, allowed a rare visit into its shop for a behind-the-scenes tour. This year Fringe is planning to roll four different vehicles, built and maintained in the Fringe workshop in the basement of the East Campus Garage, known as the ""Froom."" ""We're used to saying everything with 'fr' in front of it but when we say something in front of other people, it gets them confused,"" said Fringe head mechanic Dave Singh, who will graduate in May with a bachelor's degree in mechanical engineering and biomedical engineering. Fringe vehicles often are named with the letter ""B,"" like Boson, Blueshift, Bissa and Bumper. Other teams, such as Apex, often use names that connotate fire, while the SDC (Student Dormitory Committee) team, uses names such as Vice, Bane, Avarice and Malice. This year's buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It's one of the few times spectators can see buggies up close.
Buggy Races Keep Rolling at Carnegie Mellon - News - Carnegie Mellon University Carnegie Mellon University ——— Search Search Search this site only News News › Stories › Archives › 2019 › April › Buggy Races Keep Rolling at Carnegie Mellon April 10, 2019 Buggy Races Keep Rolling at Carnegie Mellon In its 99th year, the tradition is a Spring Carnival treat By Heidi Opdyke opdyke(through)andrew.cmu.edu Media Inquiries Julie Mattera Marketing and Communications jmattera(through)cmu.edu 412-268-2902 Sweepstakes, also known as the Buggy Races, has come a long way at Carnegie Mellon University. The slick, torpedo-like vessels carrying drivers with nerves of steel are a far cry from the two-man teams that once changed places halfway through a race and rode in everything from rain barrels with bicycle wheels to three-wheeled ash cans 99 years ago. Today, it takes six people to maneuver the .84 -mile course around Schenley Park's Flagstaff Hill. But while five pushers and a driver navigate the course's hills, dozens of people are needed to make a successful race happen. A year of planning goes into just over two minutes of racing. The Machine The basics of a buggy are straightforward, but teams are often secretive in how they build the machines, in particular the way they brake, steer and what types of wheels are used. Each has a body, pushbar for runners to move the machine up the hills, wheels, a safety harness and driving and braking mechanisms. Some also include fairings, a type of housing around the wheels that help reduce drag, make the vehicle quieter and just looks cool. Fairings have been a key feature for the Fringe team in recent years, which is celebrating its 50th anniversary of Buggy. ""We have a reputation of being the quietest on the course,"" said Diya Nuxoll, who wrapped up her bachelor's degree in mechanical engineering in December and is working on an advanced studies master's degree in design and manufacturing. Nuxoll leads the Fringe team as one of two head mechanics. An mural celebrating Buggy can be found in CMU's Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions. Sweepstakes Slang Buggy: Vehicle being raced and also a nickname for the competition.
Answer: "
"On which benchmarks did the authors test FiT5's performance?
","['chenyan xiong_275da3802142fc42f6fab2ce2104223b2e0ef40d_metadata.txt', 'daphne ippolito_1567bcac0ab09269c9d0ff33c9a406132417fab9_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: On which benchmarks did the authors test FiT5's performance?

Context: Faculty Name: chenyan xiong
Paperid: 275da3802142fc42f6fab2ce2104223b2e0ef40d
Title: Fusion-in-T5: Unifying Document Ranking Signals for Improved Information Retrieval
Year: 2023
Abstract: Common IR pipelines are typically cascade systems that may involve multiple rankers and/or fusion models to integrate different information step-by-step. In this paper, we propose a novel re-ranker named Fusion-in-T5 (FiT5), which integrates document text information, retrieval features, and global document information into a single unified model using templated-based input and global attention. Experiments on passage ranking benchmarks MS MARCO and TREC DL show that FiT5 significantly improves ranking performance over prior pipelines. Analyses find that through global attention, FiT5 is able to jointly utilize the ranking features via gradually attending to related documents, and thus improve the detection of subtle nuances between them. Our code will be open-sourced.
Authors: S. Yu, Cheng-Chung Fan, Chenyan Xiong, David Jin, Zhiyuan Liu, Zhenghao Liu Tsinghua University, Huazhong University of Science, Technology, Microsoft Research, M. I. O. Technology, N. University
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A novel re-ranker named Fusion-in-T5 (FiT5), which integrates document text information, retrieval features, and global document information into a single unified model using templated-based input and global attention, is proposed.'}
Url: http://arxiv.org/pdf/2305.14685
Faculty Name: daphne ippolito
Paperid: 1567bcac0ab09269c9d0ff33c9a406132417fab9
Title: A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity
Year: 2023
Abstract: Pretraining is the preliminary and fundamental step in developing capable language models (LM). Despite this, pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. To address this, we pretrain 28 1.5B parameter decoder-only models, training on data curated (1) at different times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we quantify the effect of pretraining data age. A temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we explore the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. Our findings indicate there does not exist a one-size-fits-all solution to filtering training data. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Lastly, we empirically validate that the inclusion of heterogeneous data sources, like books and web, is broadly beneficial and warrants greater prioritization. These findings constitute the largest set of experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which we hope will help support more informed data-centric decisions in LM development.
Authors: S. Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David M. Mimno, Daphne Ippolito
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'These findings constitute the largest set of experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which are hoped to help support more informed data-centric decisions in LM development.'}
Url: http://arxiv.org/pdf/2305.13169
Answer: "
"Are there any auditions to join The Kiltie Band?
","['kiltieband_d406.txt', 'kiltieband_d406.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Are there any auditions to join The Kiltie Band?

Context: The Kiltie Band
The Kiltie Band began in 1908 with a group of just seven students dedicated to supporting Carnegie Tech football, and today's Kiltie Band continues a tradition of excellence originated over a century ago. The Band took the field for its first official performance on November 25th, 1922, on what would have been Andrew Carnegie's 87th birthday.
The Kiltie Marching Band consists of musicians and colorguard and plays at all Carnegie Mellon home football games. There is a new show every week with music ranging from traditional marches and oldies to current pop tunes and jazz standards.
After football season, the Kiltie Concert Band plays a holiday concert and two spring concerts, including a performance at Carnegie Mellon’s Spring Carnival. Additionally, a part of the band gets together and acts as a pep band for the basketball teams.
The Kiltie Band is a campus organization recognized by Student Senate and administered by the Department of Athletics, Physical Education and Recreation. Membership is open to all members of the campus/community without audition. Participants must do their best to promote the success of the ensemble through attendance, attitude, and dependability.
FAQs
Q: When are rehearsals?
A: Football season: Mondays and Thursdays from 5:30 p.m.-7:30 p.m. and Game Day 9:30-11:00a.m. Concert Season: Mondays and Thursdays from 5:30-6:30 p.m.
Location is the CUC Studio Theater.
Q: When are performances?
A: The Kiltie Band performs at all home football games, a holiday concert, and two spring concerts. Times and dates for all performances are announced at the first practice.
Q: Are there auditions?
A: No, any member of the campus community with music experience is able to join the Kiltie Band!
Q: Do I have to memorize music?
A: No, the music is changed for every show. You should invest in a good and trusty lyre.
Q: When is the first rehearsal?
A: On the first day of class (Monday) at 5:30 p.m. in the CUC Studio Theater. If you have questions, please email the Direcator at
jolisar@andrew.cmu.edu
.
Q: Can I borrow a school instrument?
A: There are several Kiltie-owned instruments available for your use.
in the CUC Studio Theater. If you have questions, please email the Direcator at
jolisar@andrew.cmu.edu
.
Q: Can I borrow a school instrument?
A: There are several Kiltie-owned instruments available for your use. Loans begin at 4:30 p.m. before the first rehearsal.
Q: What do they wear under those kilts?
A: Join and you’ll find out!
For more information, visit the
Kiltie Band website
.
Interested in Joining?
Please email the following information to Kiltie Band Director
Jeremy Olisar.
Name
High School
Address at Carnegie Mellon (if known)
Home address
Cell number
Home number
Whether you plan on being in the band or colorguard
If in the band what instrument(s) you play
Whether you need to borrow an instrument or equipment
To see and hear the band in action, visit our
YouTube channel
.
Answer: "
"Who is the current Associate Director of Athletics, Recreational Programs?
","['tartanfacts_d404.txt', 'handbook_phd_2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who is the current Associate Director of Athletics, Recreational Programs?

Context: In keeping with tradition, students, faculty, staff and alumni voted to name the new official mascot Scotty. But it's not just the costumed mascot that voters named. The live dog is also known as Scotty to the Carnegie Mellon community.
About Scotty
|
Celebrating Scotty
|
Performer Requests
Department Quick Facts
Name of School: Carnegie Mellon University
City/Zip: Pittsburgh, PA 15213
Founded: 1900
Enrollment (undergrad): 6,982- Fall 2021
Enrollment (graduate and doctoral): 7,062 - Fall 2021
Alumni: 102,577
Nickname: Tartans
Mascot: Scottie Dog
School Colors: Cardinal and Gray
Football Stadium and Track and Field: Gesling Stadium
Capacity: 3,500
Surface: FieldTurf
Basketball/Volleyball Gym: Wiegand Gymnasium
Capacity: 500
Soccer Stadium: CMU Soccer Field
Capacity: 250
Surface: FieldTurf
Affiliation: NCAA Division III
Conference: University Athletic Association | Presidents' Athletic Conference (Football Only)
President: Dr. Farnam Jahanian
Alma Mater, Year: University of Texas at Austin
Athletic Director: Dr. Josh Centor
Alma Mater, Year: Brandeis, 2004
Athletic Department Phone: 412-268-8054
Mailing Address:  5000 Forbes Avenue / Pittsburgh, PA 15213
The administrative offices are located in the Cohon University Fitness Center. 
The University Centers recreational facilities include an eight-lane pool, racquetball and squash 
courts, aerobics room, fitness center and gym for basketball and volleyball. With renovations to 
Skibo Gym and the new Highmark Center for Health, Wellness, and Athletics scheduled for 
completion in 2024, the strength and conditioning facility has been temporarily placed on the 
lawn next to the outdoor basketball court close to the Donner locker rooms, Gesling Stadium, and 
Weigand Gymnasium. All users must present a valid CMU ID to use these facilities. 
6.18 CMU Alert 
www.cmu.edu/alert  
CMU Alert sends voice and/or text messages to registered phones in the event of a campus 
emergency that threatens public safety or during tests of the system in the spring and fall 
semesters. Students are automatically registered for CMU-Alert using the current contact 
information that has been entered into the Student Information Online (SIO): 
https://www.cmu.edu/hub/sio/about.html. 
 
 
 
 
LTI Ph.D. Graduate Student Handbook 
Page 40 
 
 
Appendix A 
2023-2024 
Highlighted University Resources for Graduate Students 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Note: The following pages are meant to broadly include only some of the resources available to 
graduate students.  It is not an exhaustive appendix of resources, and students are strongly 
encouraged to visit the various websites linked below for the most up-to-date information.   
LTI Ph.D. Graduate Student Handbook 
Page 41 
 
A.1 Key Resources for Graduate Student Support  
A.1.1 Office of Graduate and Postdoctoral Affairs  
https://www.cmu.edu/graduate  graded@cmu.edu  
The Office of Graduate and Postdoctoral Affairs provides university-wide support for all graduate 
students and academic programs, with a focus on supporting graduate student success at 
Carnegie Mellon.
Answer: "
"What can modeling the conversation with entity graphs be used for?
","['jamie callan_197d5fbc3764ff18186275545d0764d5b1c7659b_metadata.txt', 'jamie callan_197d5fbc3764ff18186275545d0764d5b1c7659b_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What can modeling the conversation with entity graphs be used for?

Context: Faculty Name: jamie callan
Paperid: 197d5fbc3764ff18186275545d0764d5b1c7659b
Title: Conversational Search with Random Walks over Entity Graphs
Year: 2023
Abstract: The entities that emerge during a conversation can be used to model topics, but not all entities are equally useful for this task. Modeling the conversation with entity graphs and predicting each entity's centrality in the conversation provides additional information that improves the retrieval of answer passages for the current question. Experiments show that using random walks to estimate entity centrality on conversation entity graphs improves top precision answer passage ranking over competitive transformer-based baselines.
Authors: Gustavo Gonçalves, João Magalhães, Jamie Callan
Venue: International Conference on the Theory of Information Retrieval
Tldr: {'model': 'tldr@v2.0.0', 'text': 'Experiments show that using random walks to estimate entity centrality on conversation entity graphs improves top precision answer passage ranking over competitive transformer-based baselines.'}
Url: https://dl.acm.org/doi/pdf/10.1145/3578337.3605125
Title: Conversational Search with RandomWalks over Entity Graphs
Authors: Gustavo Gonçalves, João Magalhães, Jamie Callan
Section: 6.2 Entity Graphs over Conversation Turns
40. This confirms the previously seen low divergence for 2019, to the left of the vertical line of Figure 3, between the relevant entity set and the retrieved entity set. To the right of the vertical line of Figure 3 as the divergence between sets increases, the EC model performance also decreases across all systems that use more than 40 passages to build the graph. After this point, the introduced noisy entities lead to sharp drops in performance across all graph sizes, showing the need for a balance between graph size, and ranking depth. Large graphs with many non-relevant entities for the passages to be reranked, or on the other hand, small graphs that do not cover the relevant entities of the passages to be reranked will lead to deficient results. Another interesting observation, that confirms the observations so far, is that as the graph size increases, there are faster diminishing returns as the reranking depth is also increased. That is, for a graph built with 100 passages (Graph-100 - pink line in Fig. 2), there is too much noise in the graph to rerank more than 40 passages, thus the centrality ranking signals perform worse than the BERT baselines. This observation ties back to Figure 3, where the median graph size built with 100 passages has approximately 500 entities (in Figure 3 when Nr. of Passages in Ranking = 100, the Median Entity Count is ≈ 500). Many of these entities will be noise as we can see from the gap between the lines in Figure 3.
Answer: "
"Who is ther first author of the paper Cross-Modal Fine-Tuning: Align then Refine?
","['graham neubig_03c7f61b0c6bc4c480ed6da4e9d7dda104ddfec3_metadata.txt', 'graham neubig_03c7f61b0c6bc4c480ed6da4e9d7dda104ddfec3_content_2.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who is ther first author of the paper Cross-Modal Fine-Tuning: Align then Refine?

Context: Faculty Name: graham neubig
Paperid: 03c7f61b0c6bc4c480ed6da4e9d7dda104ddfec3
Title: Cross-Modal Fine-Tuning: Align then Refine
Year: 2023
Abstract: Fine-tuning large-scale pretrained models has led to tremendous progress in well-studied modalities such as vision and NLP. However, similar gains have not been observed in many other modalities due to a lack of relevant pretrained models. In this work, we propose ORCA, a general cross-modal fine-tuning framework that extends the applicability of a single large-scale pretrained model to diverse modalities. ORCA adapts to a target task via an align-then-refine workflow: given the target input, ORCA first learns an embedding network that aligns the embedded feature distribution with the pretraining modality. The pretrained model is then fine-tuned on the embedded data to exploit the knowledge shared across modalities. Through extensive experiments, we show that ORCA obtains state-of-the-art results on 3 benchmarks containing over 60 datasets from 12 modalities, outperforming a wide range of hand-designed, AutoML, general-purpose, and task-specific methods. We highlight the importance of data alignment via a series of ablation studies and demonstrate ORCA's utility in data-limited regimes.
Authors: Junhong Shen, Liam Li, L. Dery, Corey Staten, M. Khodak, Graham Neubig, Ameet Talwalkar
Venue: International Conference on Machine Learning
Tldr: {'model': 'tldr@v2.0.0', 'text': ""This work proposes ORCA, a general cross-modal fine-tuning framework that extends the applicability of a single large-scale pretrained model to diverse modalities and highlights the importance of data alignment via a series of ablation studies and demonstrates ORCA's utility in data-limited regimes.""}
Url: http://arxiv.org/pdf/2302.05738
Title: Cross-Modal Fine-Tuning: Align then Refine
Authors: Junhong Shen, Liam Li, Lucio M. Dery, Corey Staten, Mikhail Khodak, Graham Neubig, Ameet Talwalkar
Section: 4.1. A Breadth Perspective: Can Pretrained Models Transfer Across Modalities?
Now, we evaluate the opposite approach, focusing on two tasks: DeepSEA (1D) and Spherical (2D). This evaluation is straightforward to perform by switching the model bodies, since the embedder architecture of ORCA handles all input transformations needed to obtain the sequence features. The results are shown in Table 13 in the Appendix. We see that fine-tuned RoBERTa outperforms Swin on the 1D task, possibly because the DeepSEA data (genomics sequences) are structured more like language than images with discrete units of information and general grammatical rules. More crucially, for both tasks, models with smaller final OTDDs have better fine-tuning accuracy. This suggests a way of selecting pretrained models by comparing the optimized OTDDs and picking the one with the smallest value. Apart from these three key insights, recall that one of our motivations for cross-modal fine-tuning is to help tasks with limited data, where training models from scratch is difficult. Indeed, for vanilla fine-tuning, a small amount of data may not give enough signal to update the pretrained weights, but it is possible to learn a good embedder first with ORCA, which can then make fine-tuning easier. In Figure 4 (right), we vary the dataset size and find that the performance gain of ORCA increases as the dataset size decreases. Meanwhile, using ORCA allows us to match the performance of naive fine-tuning on 3× amount of data. Thus, it can benefit model development in domains where data collection is costly. Beyond the cross-modal setting, we also verify ORCA’s efficacy for in-modality transfer in Appendix A.8.1.
Answer: "
"In summer 2024, When do the May Mini-5 and Semester classes begin?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In summer 2024, When do the May Mini-5 and Semester classes begin?

Context: On Monday, 05 May, 2025, during the Spring 2025 (S25) semester, Semester & Mini-4 Faculty Course Evaluations close is observed.
On Tuesday, 06 May, 2025, during the Spring 2025 (S25) semester, Make-Up Final Examinations is observed.
On Wednesday, 08 May, 2025, during the Spring 2025 (S25) semester, Graduating Students' Final Grades Due by 4 pm is observed.
On Sunday, 11 May, 2025, during the Spring 2025 (S25) semester, Commencement is observed.
On Tuesday, 13 May, 2025, during the Spring 2025 (S25) semester, Final Grades Due by 4 pm is observed.
On Tuesday, 20 May, 2025, during the Spring 2025 (S25) semester, Spring Deans' Lists Posted is observed.
On Monday, 12 May, 2025, during the Summer 2025 (M25) semester, Summer Session All, One & Mini-5 Classes Begin is observed.
On Friday, 16 May, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 add, audit, & tuition adjustment drop deadline (1) is observed.
On Friday, 23 May, 2025, during the Summer 2025 (M25) semester, Summer All add, audit, & tuition adjustment drop deadline (1) is observed.
On Friday, 26 May, 2025, during the Summer 2025 (M25) semester, Memorial Day is observed, leading to University Closed & No Classes.
On Tuesday, 27 May, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 09 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 pass/no pass & withdrawal deadline (3) is observed.
On Friday, 13 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Faculty Course Evaluations open is observed.
From 04 May, 2024,Saturday to 05 May, 2024,Sunday marks the Reading Days for Spring 2024 (S24) semester.
On Monday, 06 May, 2024, during the Spring 2024 (S24) semester, Final Examinations is observed.
On Monday, 06 May, 2024, during the Spring 2024 (S24) semester, Semester & Mini-4 Faculty Course Evaluations close is observed.
On Tuesday, 07 May, 2024, during the Spring 2024 (S24) semester, Make-Up Final Examinations is observed.
On Wednesday, 08 May, 2024, during the Spring 2024 (S24) semester, Graduating Final Grades Due by 4 pm is observed.
On Sunday, 12 May, 2024, during the Spring 2024 (S24) semester, Commencement is observed.
On Tuesday, 14 May, 2024, during the Spring 2024 (S24) semester, Final Grades Due by 4 pm is observed.
On Tuesday, 21 May, 2023, during the Spring 2024 (S24) semester, Spring Deans' Lists Posted is observed.
On Monday, 13 May, 2024, during the Summer One 2024 (M24) semester, Semester & Mini-5 Classes Begin is observed.
On Friday, 17 May, 2024, during the Summer One 2024 (M24) semester, Mini-5 add, audit, & tuition adjustment drop deadline (1) is observed.
On Friday, 24 May, 2024, during the Summer One 2024 (M24) semester, Semester add, audit, & tuition adjustment drop deadline (1) is observed.
On Monday, 27 May, 2024, during the Summer One 2024 (M24) semester, Memorial Day is observed, leading to University Closed & No Classes.
On Tuesday, 28 May, 2024, during the Summer One 2024 (M24) semester, Mini-5 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 07 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 pass/no pass & withdrawal deadline (3) is observed.
Answer: "
"What is the MOS-Q achieved by the HF-GAN on the VoxCeleb test set?
","['eric xing_19ce6aa0fd8a7b7decd836326e2e3fe8222bae22_metadata.txt', 'eric xing_6721244fad7f4790272be86e8b165fccd69578ab_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the MOS-Q achieved by the HF-GAN on the VoxCeleb test set?

Context: Faculty Name: eric xing
Paperid: 19ce6aa0fd8a7b7decd836326e2e3fe8222bae22
Title: Visible and Infrared Image Fusion of Forest Fire Scenes Based on Generative Adversarial Networks with Multi-Classification and Multi-Level Constraints
Year: 2023
Abstract: Aimed at addressing deficiencies in existing image fusion methods, this paper proposed a multi-level and multi-classification generative adversarial network (GAN)-based method (MMGAN) for fusing visible and infrared images of forest fire scenes (the surroundings of firefighters), which solves the problem that GANs tend to ignore visible contrast ratio information and detailed infrared texture information. The study was based on real-time visible and infrared image data acquired by visible and infrared binocular cameras on forest firefighters’ helmets. We improved the GAN by, on the one hand, splitting the input channels of the generator into gradient and contrast ratio paths, increasing the depth of convolutional layers, and improving the extraction capability of shallow networks. On the other hand, we designed a discriminator using a multi-classification constraint structure and trained it against the generator in a continuous and adversarial manner to supervise the generator, generating better-quality fused images. Our results indicated that compared to mainstream infrared and visible image fusion methods, including anisotropic diffusion fusion (ADF), guided filtering fusion (GFF), convolutional neural networks (CNN), FusionGAN, and dual-discriminator conditional GAN (DDcGAN), the MMGAN model was overall optimal and had the best visual effect when applied to image fusions of forest fire surroundings. Five of the six objective metrics were optimal, and one ranked second-to-optimal. The image fusion speed was more than five times faster than that of the other methods. The MMGAN model significantly improved the quality of fused images of forest fire scenes, preserved the contrast ratio information of visible images and the detailed texture information of infrared images of forest fire scenes, and could accurately reflect information on forest fire scene surroundings.
Faculty Name: eric xing
Paperid: 6721244fad7f4790272be86e8b165fccd69578ab
Title: KD-DLGAN: Data Limited Image Generation via Knowledge Distillation
Year: 2023
Abstract: Generative Adversarial Networks (GANs) rely heavily on large-scale training data for training high-quality image generation models. With limited training data, the GAN discriminator often suffers from severe overfitting which directly leads to degraded generation especially in generation diversity. Inspired by the recent advances in knowledge distillation (KD), we propose KD-DLGAN, a knowledge-distillation based generation framework that introduces pre-trained vision-language models for training effective data-limited generation models. KD-DLGAN consists of two innovative designs. The first is aggregated generative KD that mitigates the discriminator overfitting by challenging the discriminator with harder learning tasks and distilling more generalizable knowledge from the pre-trained models. The second is correlated generative KD that improves the generation diversity by distilling and preserving the diverse image-text correlation within the pre-trained models. Extensive experiments over multiple benchmarks show that KD-DLGAN achieves superior image generation with limited training data. In addition, KD-DLGAN complements the state-of-the-art with consistent and substantial performance gains. Note that codes will be released.
Authors: Kaiwen Cui, Yingchen Yu, Fangneng Zhan, Shengcai Liao, Shijian Lu1, Eric P. Xing
Venue: Computer Vision and Pattern Recognition
Tldr: {'model': 'tldr@v2.0.0', 'text': 'KD-DLGAN is proposed, a knowledge-distillation based generation framework that introduces pre-trained vision-language models for training effective data-limited generation models and achieves superior image generation with limited training data.'}
Url: https://arxiv.org/pdf/2303.17158
Answer: "
"Which country does LTI have a special PhD program with?
","['program_info_Dual-DegreePhDinLanguageandInformationTechnologies(PortugalPartnership).txt', 'program_info_PhDinLanguageandInformationTechnology.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which country does LTI have a special PhD program with?

Context: Academic Program Name:
Dual-Degree Ph.D. in Language and Information Technologies (Portugal Partnership)

Website:
https://lti.cs.cmu.edu/academics/phd-programs/dual-degree-phd-lti.html

Overview:
The LTI offers a dual-degree Ph.D. in Language and Information Technologies in cooperation with:
Universidade de Aveiro (Ph.D. in Computer Engineering), Universidade do Minho (Ph.D. in Informatics)  and the Universidade do Porto (FCUP, Ph.D. in Computer Science and FEUP, Ph.D. in Computer Science) as part of MAPi;
Universidade de Lisboa, Faculdade de Ciências – FCUL (Ph.D. in Informatics) 
Universidade de Lisboa, Instituto Superior Técnico – IST  (Ph.D. in Computer Science and Engineering, Ph.D. in Electrical and Computer Engineering, Ph.D. in Information Security)
Universidade Nova de Lisboa, Faculdade de Ciências e Tecnologia – FCTUNL (Ph.D. in Computer Science)
Universidade de Coimbra, Faculdade de Ciências e Tecnologia – FCTUC (Ph.D. in Information Science and Technology)
Students jointly enrolled in the LTI Ph.D program spend a year in Portugal, then two years at Carnegie Mellon taking classes in linguistics, computer science, statistical learning and task orientation.
After completing the majority of their academic requirements, students return to Portugal for the next two years to conduct extensive research, ultimately leading to a dissertation topic that will be publicly defended. One adviser from each institution co-supervises their student’s progress and helps to define their final thesis topic.

Requirements:
Students participating in the dual-degree program will spend their first year in Portugal, followed by two years in Pittsburgh to complete their coursework. They will complete a maximum of eight courses with a proper balance of focus areas (linguistics, computer science, statistical/learning and task orientation). After that, they will return to Portugal for their last two years, pursuing research and completing their dissertation. For more, see the Carnegie Mellon | Portugal page.

Curriculum:
While in the dual Ph.D. program, your schedule may look like this.
Academic Program Name:
Ph.D. in Language and Information Technology

Website:
https://lti.cs.cmu.edu/academics/phd-programs/phd-lti.html

Overview:
In order to obtain your Ph.D. in Language and Information Technologies, you need to pass 96 units (generally, eight courses) of graduate courses that fulfill these requirements: At least 72 units of LTI courses: Must include one class in each LTI focus area. At least 24 units of SCS courses. At least two lab courses in two different research areas. Here's a sample of what your five-year schedule might look like: Fall Spring Summer Year 1 Grammars and Lexicons Algorithms for NLP Directed Study Search Engines or Machine Learning for Text Mining Machine Translation Directed Study Required Research Year 2 Software Engineering for LT (I) Speech Understanding Self-Paced Lab Directed Study Software Engineering for LT (II) Self-Paced Lab Directed Study Required Research Year 3 Directed Research Directed Research Directed Research Year 4 Directed Research Directed Research Directed Research Year 5 Directed Research Directed Research Directed Research

Requirements:
In order to obtain your Ph.D. in Language and Information Technologies, you need to pass 96 units (generally, eight courses) of graduate courses that fulfill these requirements: At least 72 units of LTI courses: Must include one class in each LTI focus area. At least 24 units of SCS courses. At least two lab courses in two different research areas. Here's a sample of what your five-year schedule might look like: Fall Spring Summer Year 1 Grammars and Lexicons Algorithms for NLP Directed Study Search Engines or Machine Learning for Text Mining Machine Translation Directed Study Required Research Year 2 Software Engineering for LT (I) Speech Understanding Self-Paced Lab Directed Study Software Engineering for LT (II) Self-Paced Lab Directed Study Required Research Year 3 Directed Research Directed Research Directed Research Year 4 Directed Research Directed Research Directed Research Year 5 Directed Research Directed Research Directed Research

Curriculum:
In order to obtain your Ph.D. in Language and Information Technologies, you need to pass 96 units (generally, eight courses) of graduate courses that fulfill these requirements: At least 72 units of LTI courses: Must include one class in each LTI focus area. At least 24 units of SCS courses. At least two lab courses in two different research areas.
Answer: "
"How many teams participated in the IWSLT 2023 shared tasks?
","['shinji watanabe_f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b_metadata.txt', 'shinji watanabe_f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b_content_3.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many teams participated in the IWSLT 2023 shared tasks?

Context: Faculty Name: shinji watanabe
Paperid: f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b
Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN
Year: 2023
Abstract: This paper reports on the shared tasks organized by the 20th IWSLT Conference. The shared tasks address 9 scientific challenges in spoken language translation: simultaneous and offline translation, automatic subtitling and dubbing, speech-to-speech translation, multilingual, dialect and low-resource speech translation, and formality control. The shared tasks attracted a total of 38 submissions by 31 teams. The growing interest towards spoken language translation is also witnessed by the constantly increasing number of shared task organizers and contributors to the overview paper, almost evenly distributed across industry and academia.
Authors: Sweta Agrawal, Antonios Anastasopoulos, L. Bentivogli, Ondrej Bojar, Claudia Borg, Marine Carpuat, R. Cattoni, Mauro Cettolo, Mingda Chen, William Chen, K. Choukri, Alexandra Chronopoulou, Anna Currey, T. Declerck, Qianqian Dong, Kevin Duh, Y. Estève, Marcello Federico, Souhir Gahbiche, B. Haddow, B. Hsu, Phu Mon Htut, H. Inaguma, Dávid Javorský, J. Judge, Yasumasa Kano, Tom Ko, Rishu Kumar, Peng Li, Xutai Ma, Prashant Mathur, E. Matusov, Paul McNamee, John P. McCrae, Kenton Murray, Maria Nadejde, Satoshi Nakamura, Matteo Negri, H. Nguyen, J. Niehues, Xing Niu, Atul Kr.
Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN
Authors: Milind Agarwal, Sweta Agrawal, Antonios Anastasopoulos, Luisa Bentivogli, Ondřej Bojar, Claudia Borg, Marine Carpuat, Roldano Cattoni, Mauro Cettolo, Mingda Chen, William Chen, Khalid Choukri, Alexandra Chronopoulou, Anna Currey, Thierry Declerck, Qianqian Dong, Kevin Duh, Yannick Estève, Marcello Federico, Souhir Gahbiche, Barry Haddow, Benjamin Hsu, Phu Mon Htut, Hirofumi Inaguma, Dávid Javorský, John Judge, Yasumasa Kano, Tom Ko, Rishu Kumar, Pengwei Li, Xutai Ma, Prashant Mathur, Evgeny Matusov, Paul McNamee, John P. McCrae, Kenton Murray, Maria Nadejde, Satoshi Nakamura, Matteo Negri, Ha Nguyen, Jan Niehues, Xing Niu, Atul Kr. Ojha, John E. Ortega, Proyag Pal, Juan Pino, Lonneke van der Plas, Peter Polák, Elijah Rippeth, Elizabeth Salesky, Jiatong Shi, Matthias Sperber, Sebastian Stüker, Katsuhito Sudoh, Yun Tang, Brian Thompson, Kevin Tran, Marco Turchi, Alex Waibel, Mingxuan Wang, Shinji Watanabe, Rodolfo Zevallos
Section: 8.4 Results
systems. General Observations As in previous years, the low-resource shared task proved particularly challenging for the participants, but there are several encouraging signs that further reinforce the need for more research in the area. First, more teams than ever participated in the shared task, showing a continued interest in the field. Second, we note that for the language pair that was repeated from last year (Tamasheq– French), almost all submissions outperformed last year’s best submission, with an accuracy increase of more than 17 BLEU points in the unconstrained setting.
Answer: "
"What is the DAE achieved by the CRL-COM (D) system from the paper Improving Factuality of Abstractive Summarization via Contrastive Reward Learning, on the XSUM dataset?
","['graham neubig_f640e89fcede075b4bde3b2fa0dc78f591589ba3_content_0.txt', 'graham neubig_f640e89fcede075b4bde3b2fa0dc78f591589ba3_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the DAE achieved by the CRL-COM (D) system from the paper Improving Factuality of Abstractive Summarization via Contrastive Reward Learning, on the XSUM dataset?

Context: Title: Improving Factuality of Abstractive Summarization via Contrastive Reward Learning
Authors: I-Chun Chern, Zhiruo Wang, Sanjan Das, Bhavuk Sharma, Pengfei Liu, Graham Neubig
Section: Acknowledgements
We would like to thank Yixin Liu for helpful discussion on BRIO. We would also like to thank Tanya Goyal for helpful discussion on DAE.
Faculty Name: graham neubig
Paperid: f640e89fcede075b4bde3b2fa0dc78f591589ba3
Title: Improving Factuality of Abstractive Summarization via Contrastive Reward Learning
Year: 2023
Abstract: Modern abstractive summarization models often generate summaries that contain hallucinated or contradictory information. In this paper, we propose a simple but effective contrastive learning framework that incorporates recent developments in reward learning and factuality metrics. Empirical studies demonstrate that the proposed framework enables summarization models to learn from feedback of factuality metrics using contrastive reward learning, leading to more factual summaries by human evaluations. This suggests that further advances in learning and evaluation algorithms can feed directly into providing more factual summaries. Code and human evaluation results will be publicly available at \url{https://github.com/EthanC111/factuality_summarization}.
Authors: Ethan Chern, Zhiruo Wang, Sanjan Das, Bhavuk Sharma, Pengfei Liu, Graham Neubig
Venue: TRUSTNLP
Tldr: {'model': 'tldr@v2.0.0', 'text': 'Empirical studies demonstrate that the proposed framework enables summarization models to learn from feedback of factuality metrics using contrastive reward learning, leading to more factual summaries by human evaluations, suggesting that further advances in learning and evaluation algorithms can feed directly into providing morefactuality summaries.'}
Url: https://arxiv.org/pdf/2307.04507
Answer: "
"Which fraternity entered a keg of beer mounted on four wheels in 1960 buggy?
","['cmubuggy_d403.txt', 'cmubuggy_d403.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which fraternity entered a keg of beer mounted on four wheels in 1960 buggy?

Context: This year's buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It's one of the few times spectators can see buggies up close. Dave Singh explains how Fringe builds its buggies. Each team keeps its actual processes a secret, but each buggy has certain features such as a body, pushbar, wheels and driving and braking mechanisms. Behind Singh are other buggies built by Fringe. Behind the Wheel Drivers, have the closest connection to the vehicles, aside from the mechanics who spent countless hours building and maintaining individual buggies. While races are brief, each driver must log a number of practice runs to qualify to race. Teams practice nights and weekends throughout the fall and spring semesters as weather permits. Buggies are often built around drivers, so the fit can be snug, and drivers are often less than 5 feet, 3 inches in height. Most — but not all — are women. Tishya Girdhar, a junior in neuroscience, is social chair for Fringe. She came to CMU wanting to drive, but first she had to get over being claustrophobic. She started as a mechanic and got behind the wheel for last year's Sweepstakes. ""Our team's philosophy is to teach everyone how to do everything,"" Girdhar said. ""But I came to CMU wanting to drive. I wanted to drive so badly."" Amy Chen demonstrates how a driver is positioned inside of Boson, a Fringe buggy built in 2016. Safety is a top priority for teams. All drivers are required to have five pieces of safety gear: mouth guard, goggles, a harness that includes three points of contact to the body of the buggy, gloves and a helmet. And during practice runs and races, flaggers man the course to let drivers know if it's safe to proceed. The streets of the race course haven't changed, but the condition of the course varies due to potholes, which can make or break a fast run. The last course records were set in 2017, but this year potholes are making for some challenging conditions, said Amy Chen, a senior in psychology who has been driving for four years.
An mural celebrating Buggy can be found in CMU's Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions. Sweepstakes Slang Buggy: Vehicle being raced and also a nickname for the competition. Chute: A section of the freeroll portion of the buggy course (near the southwestern end of Frew Street at its intersection with Schenley Drive) where buggies make the sharp righthand turn from Schenley Drive onto Frew Street. Chute Flagger: Team member who provides a signal for buggy drivers to know when to start the right-hand turn from Schenley Drive onto Frew Street. Driver: Person who travels with a buggy and controls the vehicles via steering and braking systems. Pushbar: Structure attached to a buggy that a person pushes to propel that buggy forward. Pusher: Person who propels a buggy via a pushbar along one of the five hills of the buggy course. Shell: Entire outer structure or covering of a buggy that determines that buggy's aerodynamic characteristics. Transition: Procedure whereby one pusher finishes pushing a buggy and the next pusher in sequence starts to push that same buggy. Members of Fringe, celebrating their 50th year of Buggy racing, allowed a rare visit into its shop for a behind-the-scenes tour. This year Fringe is planning to roll four different vehicles, built and maintained in the Fringe workshop in the basement of the East Campus Garage, known as the ""Froom."" ""We're used to saying everything with 'fr' in front of it but when we say something in front of other people, it gets them confused,"" said Fringe head mechanic Dave Singh, who will graduate in May with a bachelor's degree in mechanical engineering and biomedical engineering. Fringe vehicles often are named with the letter ""B,"" like Boson, Blueshift, Bissa and Bumper. Other teams, such as Apex, often use names that connotate fire, while the SDC (Student Dormitory Committee) team, uses names such as Vice, Bane, Avarice and Malice. This year's buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It's one of the few times spectators can see buggies up close.
Answer: "
"When was the buggy course laid out in lanes for the first time?
","['cmubuggy_d403.txt', 'cmubuggy_d403.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When was the buggy course laid out in lanes for the first time?

Context: This year's buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It's one of the few times spectators can see buggies up close. Dave Singh explains how Fringe builds its buggies. Each team keeps its actual processes a secret, but each buggy has certain features such as a body, pushbar, wheels and driving and braking mechanisms. Behind Singh are other buggies built by Fringe. Behind the Wheel Drivers, have the closest connection to the vehicles, aside from the mechanics who spent countless hours building and maintaining individual buggies. While races are brief, each driver must log a number of practice runs to qualify to race. Teams practice nights and weekends throughout the fall and spring semesters as weather permits. Buggies are often built around drivers, so the fit can be snug, and drivers are often less than 5 feet, 3 inches in height. Most — but not all — are women. Tishya Girdhar, a junior in neuroscience, is social chair for Fringe. She came to CMU wanting to drive, but first she had to get over being claustrophobic. She started as a mechanic and got behind the wheel for last year's Sweepstakes. ""Our team's philosophy is to teach everyone how to do everything,"" Girdhar said. ""But I came to CMU wanting to drive. I wanted to drive so badly."" Amy Chen demonstrates how a driver is positioned inside of Boson, a Fringe buggy built in 2016. Safety is a top priority for teams. All drivers are required to have five pieces of safety gear: mouth guard, goggles, a harness that includes three points of contact to the body of the buggy, gloves and a helmet. And during practice runs and races, flaggers man the course to let drivers know if it's safe to proceed. The streets of the race course haven't changed, but the condition of the course varies due to potholes, which can make or break a fast run. The last course records were set in 2017, but this year potholes are making for some challenging conditions, said Amy Chen, a senior in psychology who has been driving for four years.
Buggy Races Keep Rolling at Carnegie Mellon - News - Carnegie Mellon University Carnegie Mellon University ——— Search Search Search this site only News News › Stories › Archives › 2019 › April › Buggy Races Keep Rolling at Carnegie Mellon April 10, 2019 Buggy Races Keep Rolling at Carnegie Mellon In its 99th year, the tradition is a Spring Carnival treat By Heidi Opdyke opdyke(through)andrew.cmu.edu Media Inquiries Julie Mattera Marketing and Communications jmattera(through)cmu.edu 412-268-2902 Sweepstakes, also known as the Buggy Races, has come a long way at Carnegie Mellon University. The slick, torpedo-like vessels carrying drivers with nerves of steel are a far cry from the two-man teams that once changed places halfway through a race and rode in everything from rain barrels with bicycle wheels to three-wheeled ash cans 99 years ago. Today, it takes six people to maneuver the .84 -mile course around Schenley Park's Flagstaff Hill. But while five pushers and a driver navigate the course's hills, dozens of people are needed to make a successful race happen. A year of planning goes into just over two minutes of racing. The Machine The basics of a buggy are straightforward, but teams are often secretive in how they build the machines, in particular the way they brake, steer and what types of wheels are used. Each has a body, pushbar for runners to move the machine up the hills, wheels, a safety harness and driving and braking mechanisms. Some also include fairings, a type of housing around the wheels that help reduce drag, make the vehicle quieter and just looks cool. Fairings have been a key feature for the Fringe team in recent years, which is celebrating its 50th anniversary of Buggy. ""We have a reputation of being the quietest on the course,"" said Diya Nuxoll, who wrapped up her bachelor's degree in mechanical engineering in December and is working on an advanced studies master's degree in design and manufacturing. Nuxoll leads the Fringe team as one of two head mechanics. An mural celebrating Buggy can be found in CMU's Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions. Sweepstakes Slang Buggy: Vehicle being raced and also a nickname for the competition.
Answer: "
"In summer 2024, When is Juneteenth observed and what is the University's policy on classes?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In summer 2024, When is Juneteenth observed and what is the University's policy on classes?

Context: On Friday, 07 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 pass/no pass & withdrawal deadline (3) is observed.
On Friday, 14 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Faculty Course Evaluations open is observed.
On Tuesday, 18 June, 2024, during the Summer One 2024 (M24) semester, Semester course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Wednesday, 19 June, 2022, during the Summer One 2024 (M24) semester, Juneteenth is observed, leading to University Closed & No Classes.
On Thursday, 20 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Last Day of Classes is observed.
On Thursday, 20 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 voucher deadline (4) is observed.
On Friday, 21 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Final Exams is observed.
On Friday, 21 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Faculty Course Evaluations close is observed.
On Monday, 24 June, 2024, during the Summer One 2024 (M24) semester, Mini-6 First Day of Classes is observed.
On Tuesday, 25 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Final Grades Due by 4 pm is observed.
On Friday, 28 June, 2024, during the Summer One 2024 (M24) semester, Mini-6 add, audit, & tuition adjustment drop deadline (1) is observed.
On Thursday, 04 July, 2024, during the Summer One 2024 (M24) semester, Independence Day is observed, leading to University Closed & No Classes.
On Friday, 05 July, 2024, during the Summer One 2024 (M24) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Friday, 13 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Faculty Course Evaluations open is observed.
On Tuesday, 17 June, 2025, during the Summer 2025 (M25) semester, Summer All course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Wednesday, 18 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Last Day of Classes is observed.
On Wednesday, 18 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 voucher deadline is observed.
On Thursday, 19 June, 2025, during the Summer 2025 (M25) semester, Juneteenth is observed, leading to University Closed & No Classes.
On Friday, 20 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Final Exams is observed.
On Friday, 20 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Faculty Course Evaluations close is observed.
On Monday, 23 June, 2025, during the Summer 2025 (M25) semester, Summer Two & Mini-6 First Day of Classes is observed.
On Tuesday, 24 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Final Grades Due by 4 pm is observed.
On Friday, 27 June, 2025, during the Summer 2025 (M25) semester, Summer Two & Mini-6 add, audit, & tuition adjustment drop deadline (1) is observed.
On Thursday, 03 July, 2025, during the Summer 2025 (M25) semester, Summer All pass/no pass & withdrawal grade deadline (3) is observed.
On Friday, 04 July, 2025, during the Summer 2025 (M25) semester, Independence Day is observed, leading to University Closed & No Classes.
On Monday, 07 July, 2025, during the Summer 2025 (M25) semester, Summer Two & Mini-6 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
Answer: "
"What is the DialDoc 2023 shared task about?
","['teruko mitamura_444737639aeea4e1e616509e368afb0bae8f89d6_metadata.txt', 'eric nyberg_444737639aeea4e1e616509e368afb0bae8f89d6_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the DialDoc 2023 shared task about?

Context: Faculty Name: teruko mitamura
Paperid: 444737639aeea4e1e616509e368afb0bae8f89d6
Title: Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting for Multilingual Document-Grounded QA
Year: 2023
Abstract: The DialDoc 2023 shared task has expanded the document-grounded dialogue task to encompass multiple languages, despite having limited annotated data. This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-agnostic approach is superior. Additionally, the study investigates the impact of query rewriting techniques using large language models, such as ChatGPT, on multilingual, document-grounded question-answering systems. The experiments conducted demonstrate that, for the examples examined, query rewriting does not enhance performance compared to the original queries. This failure is due to topic switching in final dialogue turns and irrelevant topics being considered for query rewriting.
Authors: Srinivas Gowriraj, Soham Dinesh Tiwari, Mitali Potnis, Srijan Bansal, T. Mitamura, Eric Nyberg
Venue: Workshop on Document-grounded Dialogue and Conversational Question Answering
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-gnostic approach is superior.'}
Url: https://aclanthology.org/2023.dialdoc-1.11.pdf
Faculty Name: eric nyberg
Paperid: 444737639aeea4e1e616509e368afb0bae8f89d6
Title: Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting for Multilingual Document-Grounded QA
Year: 2023
Abstract: The DialDoc 2023 shared task has expanded the document-grounded dialogue task to encompass multiple languages, despite having limited annotated data. This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-agnostic approach is superior. Additionally, the study investigates the impact of query rewriting techniques using large language models, such as ChatGPT, on multilingual, document-grounded question-answering systems. The experiments conducted demonstrate that, for the examples examined, query rewriting does not enhance performance compared to the original queries. This failure is due to topic switching in final dialogue turns and irrelevant topics being considered for query rewriting.
Authors: Srinivas Gowriraj, Soham Dinesh Tiwari, Mitali Potnis, Srijan Bansal, T. Mitamura, Eric Nyberg
Venue: Workshop on Document-grounded Dialogue and Conversational Question Answering
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-gnostic approach is superior.'}
Url: https://aclanthology.org/2023.dialdoc-1.11.pdf
Answer: "
"What LTI professor was on ""KIT’s Multilingual Speech Translation System for IWSLT 2023""?
","['alexander waibel_papers.txt', 'shinji watanabe_d24d60719e90e69749a75c160cb760d1d9fca44a_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What LTI professor was on ""KIT’s Multilingual Speech Translation System for IWSLT 2023""?

Context: List of 2023 Open Access papers by alexander waibel are:
AdapITN: A Fast, Reliable, and Dynamic Adaptive Inverse Text Normalization
Train Global, Tailor Local: Minimalist Multilingual Translation into Endangered Languages
Towards Efficient Simultaneous Speech Translation: CUNI-KIT System for Simultaneous Track at IWSLT 2023
SYNTACC : Synthesizing Multi-Accent Speech By Weight Factorization
KIT’s Multilingual Speech Translation System for IWSLT 2023
Convoifilter: A case study of doing cocktail party speech recognition
Continually learning new languages
Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff
End-to-End Evaluation for Low-Latency Simultaneous Speech Translation
FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN
Incremental Learning of Humanoid Robot Behavior from Natural Interaction and Large Language Models
Title: Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff
Authors: Peter Polák, Brian Yan, Shinji Watanabe, Alex Waibel, Ondřej Bojar
Section: 6. References
[1] C. Fügen, A. Waibel, and M. Kolss, “Simultaneous translation of lectures and speeches,” Machine translation, vol. 21, pp. 209– 252, 2007. [2] A. Anastasopoulos et al., “FINDINGS OF THE IWSLT 2021 EVALUATION CAMPAIGN,” in Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021), 2021, pp. 1–29. [3] A. Anastasopoulos et al., “FINDINGS OF THE IWSLT 2022 EVALUATION CAMPAIGN,” in Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022), 2022. [4] M. Ma et al., “Stacl: Simultaneous translation with implicit anticipation and controllable latency using prefix-to-prefix framework,” in Proc. ACL, 2019, pp. 3025–3036. [5] X. Ma, J. Pino, and P. Koehn, “SimulMT to SimulST: Adapting simultaneous text translation to end-to-end simultaneous speech translation,” in Proc. ACL, 2020, pp. 582–587. [6] D. Liu, G. Spanakis, and J. Niehues, “Low-Latency Sequenceto-Sequence Speech Recognition and Translation by Partial Hypothesis Selection,” in Proc. Interspeech, 2020, pp. 3620–3624. [7] P. Polák et al., “CUNI-KIT System for Smultaneous Speech Translation Task at IWSLT 2022,” in Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022), 2022. [8] S. Watanabe et al., “Hybrid ctc/attention architecture for endto-end speech recognition,” IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp.
Answer: "
"In ""CONVOIFILTER: A CASE STUDY OF DOING COCKTAIL PARTY SPEECH RECOGNITION"" what 3 letter metric was reduced from 80% to 26.4%?
","['shinji watanabe_6c33625c7b0ffc37955921a145531d9d4eaee713_metadata.txt', 'bhiksha raj_255bad49d29202e2d255926ab0983c125dcce835_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In ""CONVOIFILTER: A CASE STUDY OF DOING COCKTAIL PARTY SPEECH RECOGNITION"" what 3 letter metric was reduced from 80% to 26.4%?

Context: Faculty Name: shinji watanabe
Paperid: 6c33625c7b0ffc37955921a145531d9d4eaee713
Title: Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation
Year: 2023
Abstract: Neural speech separation has made remarkable progress and its integration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we explore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recognition performance from the case with filterbank features. To further improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separation and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%).
Authors: Yoshiki Masuyama, Xuankai Chang, Wangyou Zhang, Samuele Cornell, Zhongqiu Wang, Nobutaka Ono, Y. Qian, Shinji Watanabe
Venue: IEEE Workshop on Applications of Signal Processing to Audio and Acoustics
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end, and employs the recent self-supervised learning representation (SSLR) as a feature and improves the recognition performance from the case with filterbank features.'}
Url: https://arxiv.org/pdf/2307.12231
Title: EVALUATING SPEECH SYNTHESIS BY TRAINING RECOGNIZERS ON SYNTHETIC SPEECH
Authors: Dareen Alharthi, Roshan Sharma, Hira Dhamyal, Soumi Maiti, Bhiksha Raj, Rita Singh
Section: 4.4. Evaluation Metrics
MOS-Naturalness (MOS-N) : We conducted a crowdsourced Mean Opinion Score (MOS) evaluation to assess the naturalness of synthetic speech generated by each system, in comparison to real speech. We obtained 50 sentences from the LibriTTS test-clean dataset and another 50 from the LibriTTS test-other dataset, resulting in a total of 100 samples each for real speech, MQTTS, YourTTS and StyleTTS. Each sample was evaluated by 10 raters, who were instructed to rate the naturalness of the speech on a scale of 1 to 5, with 1 indicating poor and 5 indicating excellent quality. MOS-Intelligibility(MOS-I): We assessed intelligibility of spoken words by using nonsense sentences [26], effectively eliminating sentence structure and grammar from the evaluation. This absence of structure allowed listeners to only focus on the quality of the synthesized speech and not be distracted by the grammar. Participants were presented with a choice between the original sentence and a transcription generated by the Whisper-medium. We specifically selected 60 sentences with relatively high Word Error Rate (WER) from a pool of 200 random sentences generated by ChatGPT [27]. Among these, 30 sentences were short (less than 10 words), while the other 30 were long. This allowed us to evaluate the impact of sentence length variation on intelligibility. We generated synthetic speech using the three TTS systems for the 60 sentences using a test-clean set as a reference for the model’s speaker and style encoder. We used WebMushar [28] to create a test form along with Prolific for crowd-sourcing. Intelligibility of Synthetic Speech using WER from Pretrained ASR: We computed the WER for synthetic speech generated by three different systems using the Whisper medium multilingual. This model is pre-trained on real speech and evaluated on synthetic speech. This setting of training / testing demonstrates the traditional way that speech synthesis evaluation is performed. This evaluation was performed on both the test-clean and test-other datasets from LibriTTS.
Answer: "
"Who is the PhD Program Director for the LTI PhD degree?
","['handbook_phd_2023-2024.txt', 'handbook_phd_2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who is the PhD Program Director for the LTI PhD degree?

Context: degree are: 
Jamie Callan 
Ph.D. Program Director 
Professor 
GHC 5419 
callan@cs.cmu.edu 
412-268-4525 
Stacey Young 
Ph.D. Academic Program Manager 
LTI Graduate Program Manager 
GHC 6415 
staceyy@cs.cmu.edu  
412-268-2623 
Mona Diab 
LTI Director 
Professor 
GHC 5723 
mdiab@andrew.cmu.edu 
412-268-3669 
 
Joan Axelson 
Office Manager 
GHC 5405 
jaxelson@andrew.cmu.edu 
412-268-7517 
 
Julie Nys 
Employment Processes Manger 
GHC 5405 
jnys@andrew.cmu.edu 
412-268-3515 
1.3 
University Policies and Expectations 
It is the responsibility of each member of the Carnegie Mellon community to be familiar with 
university policies and guidelines. In addition to this departmental graduate student handbook, 
the following resources are available to assist you in understanding community expectations. 
 The Word/Student Handbook: 
https://www.cmu.edu/student-affairs/theword//index.html 
 Academic Integrity Policy: 
 
https://www.cmu.edu/policies/student-and- student-life/academic-
integrity.html 
LTI Ph.D. Graduate Student Handbook 
Page 11 
 
 University Policies Website:  
https://www.cmu.edu/policies/ 
 Office of Graduate and Postdoctoral Affairs: 
https://www.cmu.edu/graduate/policies/index.html 
Due to the changing nature of conditions and expectations surrounding public health and safety 
requirements please visit https://www.cmu.edu/coronavirus/ for the most up to date information.
 
Please see Appendix A for additional information about The Word and University 
resources. 
1.4 
The Academic Calendar  
The Academic calendar can be found at https://www.cmu.edu/hub/calendar/index.html and 
provides information on all deadlines, including registration dates, class start dates, add/drop 
deadlines, exam dates, and more.  
Some doctoral course-sections follow a separate Academic Calendar.
For information about Carnegie Mellon requirements and 
policies, please see the universitys handbook The Word, the Office of Graduate and Postdoctoral 
Affairs web page, the Office of the Dean of Students web page, and other resources contained in 
Appendix A of this handbook. 
Welcome! We hope that your time here is a life-changing experience. 
1.1 
Degrees Offered 
The Language Technologies Institute offers two Ph.D. programs and four Master degrees. 
 Ph.D. in Language and Information Technologies (LTI Ph.D.) 
 Dual-Degree Ph.D. in Language and Information Technologies (CMU-PT Ph.D.) 
 Masters in Language Technologies (MLT) 
 Master of Science in Intelligent Information Systems (MIIS) 
 Master of Computational Data Science (MCDS) 
 Master of Science in Artificial Intelligence and Innovation (MSAII) 
LTI Ph.D. Graduate Student Handbook 
Page 10 
 
This handbook applies to the LTI Ph.D. 
The Ph.D. in Language and Information Technologies (LTI Ph.D.) is focused on understanding 
and extending the state of the art in computational linguistics, natural language processing, 
dialogue systems, information retrieval, machine translation, speech processing, video 
understanding, multimodal systems, automated reasoning, and other topics related to analysis 
and understanding of unstructured information (e.g., machine learning, and software engineering 
of intelligent systems). 
1.2 
Department Personnel 
The people responsible for administering the LTI Ph.D. degree are: 
Jamie Callan 
Ph.D. Program Director 
Professor 
GHC 5419 
callan@cs.cmu.edu 
412-268-4525 
Stacey Young 
Ph.D.
Answer: "
"What is the title of LTI's text mining course?
","['program_info_LanguageTechnologiesConcentration.txt', 'carolyn_rose.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the title of LTI's text mining course?

Context: Academic Program Name:
Language Technologies Concentration

Website:
https://lti.cs.cmu.edu/academics/lt-concentration.html

Overview:
Human language technologies have become an increasingly central component of computer science. Information retrieval, machine translation and speech technology are used daily by the general public, while text mining, natural language processing and language-based tutoring are common within more specialized professional or educational environments. The LTI prepares students for this world by offering a minor that gives you the opportunity to not only learn about language technologies, but to also apply that knowledge through a directed project.

Requirements:
Students interested in the language technologies minor must complete our prerequisite courses with an average grade of B (3.0) or better before applying to the program. (Students who do not meet this average must submit a letter of explanation along with their application.) Prerequisites include:
Principles of Imperative Computation (15-122)
Principles of Functional Programming (15-150)
We also strongly encourage candidates to take:
Differential and Integral Calculus (21-120) and Integration and Approximation (21-122)
Matrices and Linear Transformations (21-241) or Matrix Theory (21-242)
Probability and Computing (15-259) or Probability (21-325) or Probability Theory for Computer Scientists (36-218) or Introduction to Probability Theory (36-225)

Curriculum:
The Language Technologies Concentration requires that SCS students complete one core course and their choice of three elective courses of at least 9 units each. The electives can be chosen from a specific set of stand-alone courses. In addition to the four courses, students are required to do an undergraduate research project for at least 9 units to complete their concentration.
Course Requirements for Undergraduate Minor
Carolyn Rosé
Professor, Language Technologies Institute
Human-Computer Interaction Institute
Contact
5415 —Gates & Hillman Centers
Email
412-268-7130
Research Area
Computer-Supported Collaborative Learning/MOOCs, Information Retrieval, Text Mining and Analytics, Language Technologies for Education, Natural Language Processing and Computational Linguistics

Education
Ph.D. in Language and Information Technology
1997
Employer Upon Graduation: 
Faculty, LTI CMU
Research
My research focuses on better understanding the social and pragmatic nature of conversation, and using this understanding to build computational systems that improve the efficacy of conversation both between people, and between people and computers. In order to pursue these goals, I invoke approaches from computational discourse analysis and text mining, conversational agents, and computer-supported collaborative learning. I ground my research in the fields of language technologies and human-computer interaction, and am fortunate to work closely with students and post-docs from the LTI and the Human-Computer Interaction Institute, as well as to direct my own lab, TELEDIA. My group’s highly interdisciplinary work, published in 160 peer-reviewed publications, is represented in the top venues in five fields: language technologies, learning sciences, cognitive science, educational technology and human-computer interaction.

An exciting direction of my group's work is spearheading a satellite working group to support social interaction for learning in MOOCs with EdX called DANCE. My research toward this end has birthed and substantially contributed to the growth of two thriving interrelated research areas: automated analysis of collaborative learning processes and dynamic support for collaborative learning. Both areas use intelligent conversational agents to support collaborative learning in a context-sensitive way.

All of my work draws insight from rich theoretical models from sociolinguistics and discourse analysis, and pares them down to precise operationalizations that capture the most important essence for achieving impact. I always start by investigating how conversation works and formalizing this understanding in models that are precise enough to be reproducible and that demonstrate explanatory power in connection with outcomes with real-world value. The next step is to adapt, extend and apply machine learning and text mining technologies that leverage this deep understanding to build computational models capable of automatically applying these constructs to naturally occurring language interactions. Finally, with the technology to automatically monitor naturalistic language communication in place, we can build interventions with real-world benefits.
Answer: "
"What is the procedure whereby one pusher finishes pushing a buggy and the next pusher in sequence starts to push that same buggy?
","['cmubuggy_d403.txt', 'cmubuggy_d403.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the procedure whereby one pusher finishes pushing a buggy and the next pusher in sequence starts to push that same buggy?

Context: An mural celebrating Buggy can be found in CMU's Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions. Sweepstakes Slang Buggy: Vehicle being raced and also a nickname for the competition. Chute: A section of the freeroll portion of the buggy course (near the southwestern end of Frew Street at its intersection with Schenley Drive) where buggies make the sharp righthand turn from Schenley Drive onto Frew Street. Chute Flagger: Team member who provides a signal for buggy drivers to know when to start the right-hand turn from Schenley Drive onto Frew Street. Driver: Person who travels with a buggy and controls the vehicles via steering and braking systems. Pushbar: Structure attached to a buggy that a person pushes to propel that buggy forward. Pusher: Person who propels a buggy via a pushbar along one of the five hills of the buggy course. Shell: Entire outer structure or covering of a buggy that determines that buggy's aerodynamic characteristics. Transition: Procedure whereby one pusher finishes pushing a buggy and the next pusher in sequence starts to push that same buggy. Members of Fringe, celebrating their 50th year of Buggy racing, allowed a rare visit into its shop for a behind-the-scenes tour. This year Fringe is planning to roll four different vehicles, built and maintained in the Fringe workshop in the basement of the East Campus Garage, known as the ""Froom."" ""We're used to saying everything with 'fr' in front of it but when we say something in front of other people, it gets them confused,"" said Fringe head mechanic Dave Singh, who will graduate in May with a bachelor's degree in mechanical engineering and biomedical engineering. Fringe vehicles often are named with the letter ""B,"" like Boson, Blueshift, Bissa and Bumper. Other teams, such as Apex, often use names that connotate fire, while the SDC (Student Dormitory Committee) team, uses names such as Vice, Bane, Avarice and Malice. This year's buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It's one of the few times spectators can see buggies up close.
Buggy Races Keep Rolling at Carnegie Mellon - News - Carnegie Mellon University Carnegie Mellon University ——— Search Search Search this site only News News › Stories › Archives › 2019 › April › Buggy Races Keep Rolling at Carnegie Mellon April 10, 2019 Buggy Races Keep Rolling at Carnegie Mellon In its 99th year, the tradition is a Spring Carnival treat By Heidi Opdyke opdyke(through)andrew.cmu.edu Media Inquiries Julie Mattera Marketing and Communications jmattera(through)cmu.edu 412-268-2902 Sweepstakes, also known as the Buggy Races, has come a long way at Carnegie Mellon University. The slick, torpedo-like vessels carrying drivers with nerves of steel are a far cry from the two-man teams that once changed places halfway through a race and rode in everything from rain barrels with bicycle wheels to three-wheeled ash cans 99 years ago. Today, it takes six people to maneuver the .84 -mile course around Schenley Park's Flagstaff Hill. But while five pushers and a driver navigate the course's hills, dozens of people are needed to make a successful race happen. A year of planning goes into just over two minutes of racing. The Machine The basics of a buggy are straightforward, but teams are often secretive in how they build the machines, in particular the way they brake, steer and what types of wheels are used. Each has a body, pushbar for runners to move the machine up the hills, wheels, a safety harness and driving and braking mechanisms. Some also include fairings, a type of housing around the wheels that help reduce drag, make the vehicle quieter and just looks cool. Fairings have been a key feature for the Fringe team in recent years, which is celebrating its 50th anniversary of Buggy. ""We have a reputation of being the quietest on the course,"" said Diya Nuxoll, who wrapped up her bachelor's degree in mechanical engineering in December and is working on an advanced studies master's degree in design and manufacturing. Nuxoll leads the Fringe team as one of two head mechanics. An mural celebrating Buggy can be found in CMU's Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions. Sweepstakes Slang Buggy: Vehicle being raced and also a nickname for the competition.
Answer: "
"Where do the rehearsals for The Kiltie Band take place?
","['kiltieband_d406.txt', 'kiltieband_d406.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Where do the rehearsals for The Kiltie Band take place?

Context: The Kiltie Band
The Kiltie Band began in 1908 with a group of just seven students dedicated to supporting Carnegie Tech football, and today's Kiltie Band continues a tradition of excellence originated over a century ago. The Band took the field for its first official performance on November 25th, 1922, on what would have been Andrew Carnegie's 87th birthday.
The Kiltie Marching Band consists of musicians and colorguard and plays at all Carnegie Mellon home football games. There is a new show every week with music ranging from traditional marches and oldies to current pop tunes and jazz standards.
After football season, the Kiltie Concert Band plays a holiday concert and two spring concerts, including a performance at Carnegie Mellon’s Spring Carnival. Additionally, a part of the band gets together and acts as a pep band for the basketball teams.
The Kiltie Band is a campus organization recognized by Student Senate and administered by the Department of Athletics, Physical Education and Recreation. Membership is open to all members of the campus/community without audition. Participants must do their best to promote the success of the ensemble through attendance, attitude, and dependability.
FAQs
Q: When are rehearsals?
A: Football season: Mondays and Thursdays from 5:30 p.m.-7:30 p.m. and Game Day 9:30-11:00a.m. Concert Season: Mondays and Thursdays from 5:30-6:30 p.m.
Location is the CUC Studio Theater.
Q: When are performances?
A: The Kiltie Band performs at all home football games, a holiday concert, and two spring concerts. Times and dates for all performances are announced at the first practice.
Q: Are there auditions?
A: No, any member of the campus community with music experience is able to join the Kiltie Band!
Q: Do I have to memorize music?
A: No, the music is changed for every show. You should invest in a good and trusty lyre.
Q: When is the first rehearsal?
A: On the first day of class (Monday) at 5:30 p.m. in the CUC Studio Theater. If you have questions, please email the Direcator at
jolisar@andrew.cmu.edu
.
Q: Can I borrow a school instrument?
A: There are several Kiltie-owned instruments available for your use.
in the CUC Studio Theater. If you have questions, please email the Direcator at
jolisar@andrew.cmu.edu
.
Q: Can I borrow a school instrument?
A: There are several Kiltie-owned instruments available for your use. Loans begin at 4:30 p.m. before the first rehearsal.
Q: What do they wear under those kilts?
A: Join and you’ll find out!
For more information, visit the
Kiltie Band website
.
Interested in Joining?
Please email the following information to Kiltie Band Director
Jeremy Olisar.
Name
High School
Address at Carnegie Mellon (if known)
Home address
Cell number
Home number
Whether you plan on being in the band or colorguard
If in the band what instrument(s) you play
Whether you need to borrow an instrument or equipment
To see and hear the band in action, visit our
YouTube channel
.
Answer: "
"What are the multimodal capabilities of the proposed model in ""Generating Images with Multimodal Language Models""?
","['daniel fried_6fb5c0eff3696ef252aca9638e10176ecce7cecb_metadata.txt', 'daniel fried_2b2d9ee18507aa89a7f1ecba0bf68844c7d9aa75_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the multimodal capabilities of the proposed model in ""Generating Images with Multimodal Language Models""?

Context: Faculty Name: daniel fried
Paperid: 6fb5c0eff3696ef252aca9638e10176ecce7cecb
Title: Generating Images with Multimodal Language Models
Year: 2023
Abstract: We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text -- outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence.
Authors: Jing Yu Koh, Daniel Fried, R. Salakhutdinov
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work proposes a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces, and exhibits a wider range of capabilities compared to prior multimodal language models.'}
Url: https://arxiv.org/pdf/2305.17216
Faculty Name: daniel fried
Paperid: 2b2d9ee18507aa89a7f1ecba0bf68844c7d9aa75
Title: Grounding Language Models to Images for Multimodal Generation
Year: 2023
Abstract: We propose an efﬁcient method to ground pre-trained text-only language models to the visual domain, enabling them to process and generate arbitrarily interleaved image-and-text data. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and ﬁnetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text inter-leaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.
Authors: Jing Yu Koh, R. Salakhutdinov, Daniel Fried
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': None}
Url: http://arxiv.org/pdf/2301.13823
Answer: "
"The MOSAIC framework from the paper titled ""MOSAIC: Learning Unified Multi-Sensory Object Property Representations for Robot Perception"" was evaluated on two task families. What are these task families?
","['yonatan bisk_69b8cd15966c4c9c3e44e71769e557f1c87fb3f9_metadata.txt', 'yonatan bisk_69b8cd15966c4c9c3e44e71769e557f1c87fb3f9_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: The MOSAIC framework from the paper titled ""MOSAIC: Learning Unified Multi-Sensory Object Property Representations for Robot Perception"" was evaluated on two task families. What are these task families?

Context: Faculty Name: yonatan bisk
Paperid: 69b8cd15966c4c9c3e44e71769e557f1c87fb3f9
Title: MOSAIC: Learning Unified Multi-Sensory Object Property Representations for Robot Perception
Year: 2023
Abstract: A holistic understanding of object properties across diverse sensory modalities (e.g., visual, audio, and haptic) is essential for tasks ranging from object categorization to complex manipulation. Drawing inspiration from cognitive science studies that emphasize the significance of multi-sensory integration in human perception, we introduce MOSAIC (Multimodal Object property learning with Self-Attention and Interactive Comprehension), a novel framework designed to facilitate the learning of unified multi-sensory object property representations. While it is undeniable that visual information plays a prominent role, we acknowledge that many fundamental object properties extend beyond the visual domain to encompass attributes like texture, mass distribution, or sounds, which significantly influence how we interact with objects. In MOSAIC, we leverage this profound insight by distilling knowledge from multimodal foundation models and aligning these representations not only across vision but also haptic and auditory sensory modalities. Through extensive experiments on a dataset where a humanoid robot interacts with 100 objects across 10 exploratory behaviors, we demonstrate the versatility of MOSAIC in two task families: object categorization and object-fetching tasks. Our results underscore the efficacy of MOSAIC's unified representations, showing competitive performance in category recognition through a simple linear probe setup and excelling in the fetch object task under zero-shot transfer conditions. This work pioneers the application of sensory grounding in foundation models for robotics, promising a significant leap in multi-sensory perception capabilities for autonomous systems. We have released the code, datasets, and additional results: https://github.com/gtatiya/MOSAIC.
Authors: Gyan Tatiya, Jonathan M Francis, Ho-Hsiang Wu, Yonatan Bisk, Jivko Sinapov
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work pioneers the application of sensory grounding in foundation models for robotics, promising a significant leap in multi-sensory perception capabilities for autonomous systems.'}
Url: https://arxiv.org/pdf/2309.08508
Title: MOSAIC: Learning Unified Multi-Sensory Object Property Representations for Robot Learning via Interactive Perception
Authors: Gyan Tatiya, Jonathan Francis, Ho-Hsiang Wu, Yonatan Bisk, Jivko Sinapov
Section: VI. CONCLUSION AND FUTURE WORK
We introduced the MOSAIC framework to enable robots to generate versatile, multimodal representations through interactive object perception and to leverage these unified representations across various downstream robot learning tasks. Through extensive performance evaluation, we have showcased the effectiveness of these unified representations in tasks such as category recognition, using a simple linear probe setup, and the fetch object task under zero-shot conditions. Moving forward, there are several exciting directions for future research. Firstly, we plan to consider the transfer of unified representations across different robot morphologies, enabling a broader range of robots to benefit from this technology. Furthermore, we envision settings where interactive behaviors are learned and composed, alongside the tasks we considered in this paper, thereby further increasing the efficacy of object exploration. These future endeavors hold the potential to further enhance the utility of unified representations in robotics and expand their applications across a multitude of scenarios and environments. One limitation in our current study is that, for the fetch object task, we evaluated using a zero-shot transfer condition rather than a learning-based approach to find the target object. For future work, it would be important to explore learningbased policies for solving the fetch object task, potentially increasing the versatility and adaptability of our framework.
Answer: "
"Scotty was officially accepted as CMU's first mascot in which year?
","['tartanfacts_d404.txt', 'mascot_d405_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Scotty was officially accepted as CMU's first mascot in which year?

Context: In keeping with tradition, students, faculty, staff and alumni voted to name the new official mascot Scotty. But it's not just the costumed mascot that voters named. The live dog is also known as Scotty to the Carnegie Mellon community.
About Scotty
|
Celebrating Scotty
|
Performer Requests
Department Quick Facts
Name of School: Carnegie Mellon University
City/Zip: Pittsburgh, PA 15213
Founded: 1900
Enrollment (undergrad): 6,982- Fall 2021
Enrollment (graduate and doctoral): 7,062 - Fall 2021
Alumni: 102,577
Nickname: Tartans
Mascot: Scottie Dog
School Colors: Cardinal and Gray
Football Stadium and Track and Field: Gesling Stadium
Capacity: 3,500
Surface: FieldTurf
Basketball/Volleyball Gym: Wiegand Gymnasium
Capacity: 500
Soccer Stadium: CMU Soccer Field
Capacity: 250
Surface: FieldTurf
Affiliation: NCAA Division III
Conference: University Athletic Association | Presidents' Athletic Conference (Football Only)
President: Dr. Farnam Jahanian
Alma Mater, Year: University of Texas at Austin
Athletic Director: Dr. Josh Centor
Alma Mater, Year: Brandeis, 2004
Athletic Department Phone: 412-268-8054
Mailing Address:  5000 Forbes Avenue / Pittsburgh, PA 15213
{
  ""description"": ""div class=\""content\"">\n<h2>About Scotty</h2>\n<table border=\""0\"" align=\""right\"">\n<tbody>\n<tr>\n<td style=\""padding: 0px;\"" width=\""900\"">\n<p>The Scottish terri ..."",
  ""viewport"": ""width=device-width, initial-scale=1"",
  ""msapplication-TileColor"": ""#c41230"",
  ""msapplication-config"": ""/assets/favicons/browserconfig.xml"",
  ""theme-color"": ""#c41230"",
  ""fb:app_id"": ""280467664480"",
  ""og:locale"": ""en_US"",
  ""og:title"": ""Carnegie Mellon University Athletics"",
  ""dcterms.title"": ""Carnegie Mellon University Athletics"",
  ""og:description"": ""Carnegie Mellon University Athletics"",
  ""dcterms.description"": ""Carnegie Mellon University Athletics"",
  ""og:image"": ""https://athletics.cmu.edu/images/setup/thumbnail_default.jpg?max_width=1200&max_height=675"",
  "" og:image:alt"": ""Carnegie Mellon University Athletics thumbnail"",
  ""og:site_name"": ""Carnegie Mellon University Athletics"",
  ""og:url"": ""https://athletics.cmu.edu/athletics/mascot/about"",
  ""dcterms.identifier"": ""https://athletics.cmu.edu/athletics/mascot/about"",
  ""og:type"": ""article"",
  ""dcterms.type"": ""article""
}
Topic category is mascot
Answer: "
"What does KALE use to convert dense representations into a sparse set?
","['jamie callan_1e0a4ff0c5d2d850ff5907e310ffcedc9cad9718_content_2.txt', 'jamie callan_1e0a4ff0c5d2d850ff5907e310ffcedc9cad9718_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What does KALE use to convert dense representations into a sparse set?

Context: Title: KALE: Using a K-Sparse Projector for Lexical Expansion
Authors: Luís Borges, Bruno Martins, Jamie Callan
Section: 3 THE KALE APPROACH
it was beneficial to connect the dense representation output by the projector into the ranking loss, instead of directly connecting the sparsified vector. Since TopK only extracts a small number of dimensions, the sparse vector likely had insufficient information to provide stable learning compared to the dense vector, and thus this dense vector was kept as the student. The regularization term promoting equipartitioning takes as input the same vectors that go into the M3SE loss. Given the dense vectors, the sum of the weights in each dimension is computed, as well as the sum of all the weights in the batch. Dividing these values returns the current weight distribution across the dimensions. Ideally, the distribution should be uniform, which translates to: dims = |𝐵 |∑︁ 𝑖 B𝑖 , 𝑤𝑒𝑖𝑔ℎ𝑡 = |𝑉 |∑︁ 𝑣 𝑑𝑖𝑚𝑠𝑣, (2) Equipartition(B) = KL ( dims 𝑤𝑒𝑖𝑔ℎ𝑡 , 1 𝐷 ) + KL ( 1 𝐷 , dims 𝑤𝑒𝑖𝑔ℎ𝑡 ) . (3) In the previous equations, KL is the Kullback-Leibler divergence, |𝐵 | is the batch size, and |𝑉 | is the vocabulary size, which matches the dimensionality of the input dense vectors. Vectors and matrices are boldfaced, while scalars are italicized. The term 1 𝐷 denotes a uniform distribution over the 𝐷 dimensions. With equipartitioning, the KALE loss is the sum of the two individual components: Loss = M3SE + Equipartition. (4)
Faculty Name: jamie callan
Paperid: 1e0a4ff0c5d2d850ff5907e310ffcedc9cad9718
Title: KALE: Using a K-Sparse Projector for Lexical Expansion
Year: 2023
Abstract: Recent research has proposed retrieval approaches based on sparse representations and inverted indexes, with terms produced by neural language models and leveraging the advantages from both neural retrieval and lexical matching. This paper proposes KALE, a new lightweight method of this family that uses a small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary. The KALE vocabulary captures semantic concepts than perform well when used in isolation, and perform better when extending the original lexical vocabulary, this way improving first-stage retrieval accuracy. Experiments with the MSMARCOv1 passage retrieval dataset, the TREC Deep Learning dataset, and BEIR datasets, examined the effectiveness of KALE under varying conditions. Results show that the KALE terms can replace the original lexical vocabulary, with gains in accuracy and efficiency. Combining KALE with the original lexical vocabulary, or with other learned terms, can further improve retrieval accuracy with only a modest increase in computational cost.
Authors: Luís Borges, Bruno Martins, Jamie Callan
Venue: International Conference on the Theory of Information Retrieval
Tldr: {'model': 'tldr@v2.0.0', 'text': 'KALE is a new lightweight method that uses a small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary, which can replace the original lexical vocabulary with gains in accuracy and efficiency.'}
Url: https://dl.acm.org/doi/pdf/10.1145/3578337.3605131
Answer: "
"According to the MSAII handbook, who is the associate dean for masters programs?
","['handbook-msaii-2022-2023.txt', 'handbook-msaii-2022-2023.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: According to the MSAII handbook, who is the associate dean for masters programs?

Context: Students are encouraged to consult with relevant university faculty and staff as soon as possible 
as they begin making plans regarding time away.   Students must contact the Office of the Dean 
of Student Affairs to register for Maternity Accommodations.  Students will complete an 
information form and meet with a member of Dean’s Office staff to determine resources and 
33 
 
procedures appropriate for the individual student. Planning for the student’s discussion with 
her academic contact(s) (advisor, associate dean, etc.) will be reviewed during this meeting. 
11.6 Statute of Limitations 
As outlined in the Master’s Students Statute of Limitations, www.cmu.edu/policies/student-
and-student-life/masters-students-statute-of-limitations.html, students must complete all 
requirements for the master’s degree within a maximum of seven years from original 
matriculation as a master’s student, or less if required by a more restrictive department, school 
or college policy.  Once this time-to-degree limit has lapsed, the person may resume work 
towards a master’s degree only if newly admitted to a currently offered master’s degree program 
under criteria determined by that program. 
Under extraordinary circumstances, such as leave of absence, military or public service, family or 
parental leave, or temporary disability, a school or college may, upon the relevant department's 
recommendation and with the written approval of the dean (or designate), defer the lapse for a 
period commensurate with the duration of that interruption. Students who are pursuing a 
master’s degree as part-time students for all semesters of their program, as approved by their 
program, may also appeal to their program or department for extension of the time to degree 
limit.  
Any request for a waiver of the statute of limitations must be approved by the Department Head 
and by the SCS Associate Dean for Masters Programs. The waiver request must explain the 
exceptional circumstances that warrant an extension. For cases in which a waiver is granted, 
the waiver will cover specific courses and will specify a time period for completion of the 
program. 
See also the ‘Duration of Study’ policy. 
11.7 Residency Requirement 
The MSAII is a full-time, in-residence program conducted only on the Pittsburgh campus.
6 Academic Calendar 
The Academic Calendar can be found at https://www.cmu.edu/hub/calendar/index.html and 
provides information on all deadlines including registration dates, class start dates, add/drop 
deadlines, exam dates and more. 
7 School and Departmental Information 
The following are key personnel with whom you may interact during your time at Carnegie 
Mellon: 
 
Martial Hebert 
 
 
 
 
 
Dean, School of Computer Science 
9 
 
 
University Professor 
 
GHC 6105 
 
412-268-5704 
 
hebert@cs.cmu.edu  
https://www.cs.cmu.edu/~hebert/  
 
David Garlan 
Associate Dean for Master’s Programs, SCS 
Professor 
TCS 420 
garlan@cs.cmu.edu 
http://www.cs.cmu.edu/~garlan/academics.htm  
 
Carolyn Rosé  
 
 
 
 
Interim Director, Language Technologies Institute 
 
Professor 
 
GHC 5415 
 
412-268-7130 
 
cprose@cs.cmu.edu 
8 MSAII Policies 
The Language Technologies Institute (LTI) has prepared this statement of policies, program 
requirements, guidance, process and procedures for students in the M.S. in Artificial Intelligence 
and Innovation (MSAII) program. A copy of this handbook is also available online at the 
program website.   
The University Student guide, The Word, which details university-wide policies, is also 
available online at www.cmu.edu/student-affairs/theword.     
Additional resources specific to graduate students can be found at 
www.cmu.edu/graduate/policies/    
8.1 The MSAII Degree 
The M.S. in Artificial Intelligence and Innovation (MSAII) is a professional master's program 
offered by the Language Technologies Institute (LTI) in the School of Computer Science at 
Carnegie Mellon University. This program seeks to educate students in the innovative use of 
artificial intelligence to create practical solutions in a wide variety of application areas. As 
artificial intelligence matures, there is a great need for students who can envision, design, plan 
and deliver solutions that integrate AI technologies such as deep learning, natural language 
processing, robotics and big data analytics into new applications.  The program stresses both 
10 
 
intrapreneurship and entrepreneurship and encourages students to develop ideas for startups 
and for introduction into established companies.
Answer: "
"In fall 2023, What is the last day of classes for Mini-2, Semester, and Mini-2?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, What is the last day of classes for Mini-2, Semester, and Mini-2?

Context: From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mini-2 Classes Begin is observed.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mid-Semester & Mini-1 grades due by 4 pm is observed.
On Friday, 27 October, 2023, during the Fall 2023 (F23) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
On Tuesday, 07 November, 2023, during the Fall 2023 (F23) semester, Democracy Day is observed, leading to No Classes, except Evening classes after 5 pm will still meet.
On Saturday, 11 November, 2023, during the Fall 2023 (F23) semester, Homecoming is observed.
From 13 November, 2023,Monday to 17 November, 2023,Friday marks the Spring 2024 Registration Week for Fall 2023 (F23) semester.
On Monday, 13 November, 2023, during the Fall 2023 (F23) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Wednesday, 15 November, 2023, during the Fall 2023 (F23) semester, Mini-2 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 22 November, 2023,Wednesday to 24 November, 2023,Friday marks the Thanksgiving Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Mini-2 pass/no pass & withdrawal deadline (2) is observed.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Faculty Course Evaluations open is observed.
On Friday, 08 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Last Day of Classes is observed.
On Friday, 08 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Last Day of Classes is observed.
On Friday, 08 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 voucher deadline (4) is observed.
From 11 December, 2023,Monday to 12 December, 2023,Tuesday marks the Final Exams for Fall 2023 (F23) semester.
On Wednesday, 13 December, 2023, during the Fall 2023 (F23) semester, Reading Day is observed.
From 14 December, 2023,Thursday to 15 December, 2023,Friday marks the Final Exams for Fall 2023 (F23) semester.
On Saturday, 16 December, 2023, during the Fall 2023 (F23) semester, Reading Day is observed.
On Sunday, 17 December, 2023, during the Fall 2023 (F23) semester, Final Exams is observed.
On Monday, 18 December, 2023, during the Fall 2023 (F23) semester, Make-Up Final Exams is observed.
On Monday, 18 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Faculty Course Evaluations close is observed.
On Wednesday, 20 December, 2023, during the Fall 2023 (F23) semester, Final Grades Due by 4 pm is observed.
From 23 December, 2023,Saturday to 02 January, 2024,Tuesday marks the Winter Break for Fall 2023 (F23) semester which leads to University Closed.
On Monday, 08 January, 2023, during the Fall 2023 (F23) semester, Fall Deans' Lists Posted is observed.
On Monday, 15 January, 2024, during the Spring 2024 (S24) semester, Martin Luther King Day is observed, leading to No Classes.
On Tuesday, 16 January, 2024, during the Spring 2024 (S24) semester, First Day of Class is observed.
On Monday, 22 January, 2024, during the Spring 2024 (S24) semester, Mini-3 add, audit & tuition adjustment drop deadline (1) is observed.
Answer: "
"What number do all of the LTI classes start with?
","['handbook_phd_2023-2024.txt', 'mlt-student-handbook-2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What number do all of the LTI classes start with?

Context: Unless otherwise specified, ""course"" means an actual 
classroom course, not credit given for research or independent study. Note that the LTI allows 
any one MLD (10-XXX) graduate course to count as an ""LTI course"".  
An SCS course is any 12-unit course with a course number indicating a unit of the School of 
Computer Science (including LTI); a 6-unit course with such a number counts as one-half of an 
SCS course. Unless otherwise specified, ""course"" means an actual classroom course, not credit 
given for research or independent study. Note: Recommended electives that are technically 
LTI Ph.D. Graduate Student Handbook 
Page 15 
 
outside of the SCS now count towards this requirement; for example, Digital Signal Processing in 
ECE. Please see the Program Director for approval of electives as SCS.  
LTI Focus Areas are sets of courses defined on the LTI course webpage under Course 
Categories. If a student believes a new course should be added to a Focus Area, they should make 
a request to the Ph.D. Program Director. They will decide, with advice from faculty in the 
appropriate area, whether it should be in the Focus Area. If approved, it will be added to the LTI 
Focus Area webpage.  
A Task-Orientation Focus Course is a course belonging to that LTI Focus Area, as listed on 
the LTIs Course Categories webpage.  
An LTI lab course is a course in the list of lab courses defined on the LTIs Course Categories 
webpage. 
3.1.2 Grade Requirements 
Students must demonstrate their mastery of material taught in courses and their success in 
applying their skills in directed study by satisfying the following grade requirements. 
Minimum grade: Only courses with a grade of B (3.0) or higher are counted as satisfying a degree 
requirement. 
Pass/fail: Pass/fail grades are not permitted for courses and projects used to satisfy a degree 
requirement. 
3.1.3 Proficiency Requirements 
The LTI Ph.D. does not require any Qualifying Exams. Instead, a LTI Ph.D. student is required to 
demonstrate proficiency in the following four areas.
Students are strongly 
encouraged to finish the thesis work within one (1) year following the semester they 
enroll for the first Masters Thesis course. 
 
 
 
 
MLT Graduate Student Handbook 
Page 17 
 
4.4 Definitions of LTI Terminology 
We define here some of the terms as used in this handbook: 
 An LTI course is any 12-unit course with a number of 11-XXX; a 6-unit course with an 
11-XXX counts as 1/2 of an LTI course. Unless otherwise specified, ""course"" means an 
actual classroom course, not credit given for research or independent study. Note that 
we will allow any one MLD (10-xxx) graduate-level course to count as an LTI course. 
 An SCS course is any 12-unit course with a course number indicating a unit of the 
School of Computer Science (including LTI); a 6-unit course with such a number counts 
as 1/2 of an SCS course. Unless otherwise specified, ""course"" means an actual classroom 
course, not credit given for research or independent study. Note: Recommended 
electives that are technically outside of the SCS now count towards this requirement; for 
example, Digital Signal Processing in ECE. Please see the Program Director for approval 
of electives as SCS. 
 LTI Focus Areas are sets of courses defined on the LTI course webpage under Course 
Categories. If a student believes a new course should be added to a Focus Area, they 
should notify the Chair of the LTI Graduate Programs. He will decide, with advice from 
faculty in the appropriate area, whether it should be in the Focus Area, and if approved 
it will be added to the LTI Focus Area webpage. 
o A Task-Orientation Focus Course is simply a course belonging to that LTI Focus 
Area, as listed on the Course Categories webpage. 
 An LTI lab course is simply a course in the list of lab courses defined in the LTI Course 
Categories webpage. 
4.5 Recommended Electives outside of SCS  
Students are free to take elective courses outside the SCS, at Carnegie Mellon or cross-
registered at the University of Pittsburgh, as long as the student fulfills the requirements of 
their program as described above.
Answer: "
"Who is the last author on WebArena?
","['daniel fried_e41482f4ee984f17382f6cdd900df094d928be06_content_1.txt', 'graham neubig_e41482f4ee984f17382f6cdd900df094d928be06_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who is the last author on WebArena?

Context: Title: WEBARENA: A REALISTIC WEB ENVIRONMENT FOR BUILDING AUTONOMOUS AGENTS
Authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig
Section: 3.2 EVALUATION ANNOTATION
the fifth example in Table 1, first obtains the URL of the latest post by examining the last state in the state sequence s. Then it navigates to the corresponding post page and obtains the post’s content by running the Javascript “document.querySelector(‘.submission__inner’).outerText”. Subsequently, we annotate keywords that need to exist within the located content. For example, the evaluation verifies if the post is correctly posted in the “nyc” subreddit by examining the URL of Function ID Intent Eval Implementation rinfo(a ∗, â) 1 Tell me the name of the customer whohas the most cancellations in the history exact_match(â, “Samantha Jones”) 2 Find the customer name andemail with phone number 8015551212 must_include(â, “Sean Miller”) must_include(â, “sean@gmail.com”) 3 Compare walking and driving time from AMC Waterfront to Randyland fuzzy_match(â, “walking: 2h58min”) fuzzy_match(â, “driving: 21min”) rprog(s) 4 Checkout merge requests assigned to me url=locate_current_url(s) exact_match(URL, “gitlab.com/merge_ requests?assignee_username=byteblaze”) 5 Post to ask “whether Ineed a car in NYC” url=locate_latest_post_url(s) body=locate_latest_post_body(s) must_include(URL, “/f/nyc”) must_include(body,“a car in NYC”) Table 1: We introduce two evaluation approaches. rinfo (top) measures the correctness of performing information-seeking tasks. It compares the predicted answer â with the annotated reference a∗ with three implementations. rprog (bottom) programmatically checks whether the intermediate states during the executions possess the anticipated properties specified by the intent. the post and if the post contains the requested content by examining the post content.
Title: WEBARENA: A REALISTIC WEB ENVIRONMENT FOR BUILDING AUTONOMOUS AGENTS
Authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig
Section: 3.2 EVALUATION ANNOTATION
the fifth example in Table 1, first obtains the URL of the latest post by examining the last state in the state sequence s. Then it navigates to the corresponding post page and obtains the post’s content by running the Javascript “document.querySelector(‘.submission__inner’).outerText”. Subsequently, we annotate keywords that need to exist within the located content. For example, the evaluation verifies if the post is correctly posted in the “nyc” subreddit by examining the URL of Function ID Intent Eval Implementation rinfo(a ∗, â) 1 Tell me the name of the customer whohas the most cancellations in the history exact_match(â, “Samantha Jones”) 2 Find the customer name andemail with phone number 8015551212 must_include(â, “Sean Miller”) must_include(â, “sean@gmail.com”) 3 Compare walking and driving time from AMC Waterfront to Randyland fuzzy_match(â, “walking: 2h58min”) fuzzy_match(â, “driving: 21min”) rprog(s) 4 Checkout merge requests assigned to me url=locate_current_url(s) exact_match(URL, “gitlab.com/merge_ requests?assignee_username=byteblaze”) 5 Post to ask “whether Ineed a car in NYC” url=locate_latest_post_url(s) body=locate_latest_post_body(s) must_include(URL, “/f/nyc”) must_include(body,“a car in NYC”) Table 1: We introduce two evaluation approaches. rinfo (top) measures the correctness of performing information-seeking tasks. It compares the predicted answer â with the annotated reference a∗ with three implementations. rprog (bottom) programmatically checks whether the intermediate states during the executions possess the anticipated properties specified by the intent. the post and if the post contains the requested content by examining the post content.
Answer: "
"In spring 2024, When do Mini-4 faculty course evaluations close?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, When do Mini-4 faculty course evaluations close?

Context: On Wednesday, 05 February, 2025, during the Spring 2025 (S25) semester, Mini-3 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 17 February, 2025, during the Spring 2025 (S25) semester, Mini-3 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 17 February, 2025, during the Spring 2025 (S25) semester, Mini-3 Faculty Course Evaluations open is observed.
On Monday, 24 February, 2025, during the Spring 2025 (S25) semester, Semester course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 28 February, 2025, during the Spring 2025 (S25) semester, Mini-3 Last Day of Classes is observed.
On Friday, 28 February, 2025, during the Spring 2025 (S25) semester, Mini-3 voucher deadline (4) is observed.
On Saturday, 01 March, 2025, during the Spring 2025 (S25) semester, Mini-3 Exams is observed.
On Saturday, 01 March, 2025, during the Spring 2025 (S25) semester, Mini-3 Faculty Course Evaluations close is observed.
From 03 March, 2025,Monday to 07 March, 2023,Friday marks the Spring Break for Spring 2025 (S25) semester which leads to No Classes.
On Monday, 10 March, 2025, during the Spring 2025 (S25) semester, First day of Mini-4 Classes is observed.
On Tuesday, 11 March, 2025, during the Spring 2025 (S25) semester, Summer 2025 Registration Opens is observed.
On Wednesday, 12 March, 2025, during the Spring 2025 (S25) semester, Mid-Semester & Mini-3 grades due by 4 pm is observed.
On Friday, 14 March, 2025, during the Spring 2025 (S25) semester, Mini-4 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 31 March, 2025, during the Spring 2025 (S25) semester, Semester pass/no pass & withdrawal deadline is observed.
On Monday, 05 May, 2025, during the Spring 2025 (S25) semester, Semester & Mini-4 Faculty Course Evaluations close is observed.
On Tuesday, 06 May, 2025, during the Spring 2025 (S25) semester, Make-Up Final Examinations is observed.
On Wednesday, 08 May, 2025, during the Spring 2025 (S25) semester, Graduating Students' Final Grades Due by 4 pm is observed.
On Sunday, 11 May, 2025, during the Spring 2025 (S25) semester, Commencement is observed.
On Tuesday, 13 May, 2025, during the Spring 2025 (S25) semester, Final Grades Due by 4 pm is observed.
On Tuesday, 20 May, 2025, during the Spring 2025 (S25) semester, Spring Deans' Lists Posted is observed.
On Monday, 12 May, 2025, during the Summer 2025 (M25) semester, Summer Session All, One & Mini-5 Classes Begin is observed.
On Friday, 16 May, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 add, audit, & tuition adjustment drop deadline (1) is observed.
On Friday, 23 May, 2025, during the Summer 2025 (M25) semester, Summer All add, audit, & tuition adjustment drop deadline (1) is observed.
On Friday, 26 May, 2025, during the Summer 2025 (M25) semester, Memorial Day is observed, leading to University Closed & No Classes.
On Tuesday, 27 May, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 09 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 pass/no pass & withdrawal deadline (3) is observed.
On Friday, 13 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Faculty Course Evaluations open is observed.
Answer: "
"In the IWSLT 2023 paper titled ""Evaluating Multilingual Speech Translation Under Realistic Conditions with Resegmentation and Terminology"", authors tackle the task of technical speech translation. What evaluation metrics were reported for translation?
","['mona diab_c5849f406e8263806a84e1a407ec0e0fe131bd5c_metadata.txt', 'mona diab_c5849f406e8263806a84e1a407ec0e0fe131bd5c_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the IWSLT 2023 paper titled ""Evaluating Multilingual Speech Translation Under Realistic Conditions with Resegmentation and Terminology"", authors tackle the task of technical speech translation. What evaluation metrics were reported for translation?

Context: Faculty Name: mona diab
Paperid: c5849f406e8263806a84e1a407ec0e0fe131bd5c
Title: Evaluating Multilingual Speech Translation under Realistic Conditions with Resegmentation and Terminology
Year: 2023
Abstract: We present the ACL 60/60 evaluation sets for multilingual translation of ACL 2022 technical presentations into 10 target languages. This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics.
Authors: Elizabeth Salesky, Kareem Darwish, Mohamed Al-Badrashiny, Mona T. Diab, J. Niehues
Venue: International Workshop on Spoken Language Translation
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics.'}
Url: https://aclanthology.org/2023.iwslt-1.2.pdf
Title: Evaluating Multilingual Speech Translation Under Realistic Conditions with Resegmentation and Terminology
Authors: Elizabeth Salesky, Kareem Darwish, Mohamed Al-Badrashiny, Mona Diab, Jan Niehues
Section: 4.1 Segmentation
making the dataset standard scoring with resegmentation. 12chrF for individual languages is shown in Table 6.
Answer: "
"Which professor from LTI worked on the paper Advancing Regular Language Reasoning in Linear Recurrent Neural Networks?
","['alexander rudnicky_06a8f2e3c4266196b008851f1ec7ef9f340809da_content_1.txt', 'alexander rudnicky_06a8f2e3c4266196b008851f1ec7ef9f340809da_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which professor from LTI worked on the paper Advancing Regular Language Reasoning in Linear Recurrent Neural Networks?

Context: Title: Advancing Regular Language Reasoning in Linear Recurrent Neural Networks
Authors: Ting-Han Fan, Ta-Chung Chi, Alexander I. Rudnicky
Section: 1 Introduction
Pair, and Modular Arithmetic.
Faculty Name: alexander rudnicky
Paperid: 06a8f2e3c4266196b008851f1ec7ef9f340809da
Title: Advancing Regular Language Reasoning in Linear Recurrent Neural Networks
Year: 2023
Abstract: In recent studies, linear recurrent neural networks (LRNNs) have achieved Transformer-level performance in natural language modeling and long-range modeling while offering rapid parallel training and constant inference costs. With the resurged interest in LRNNs, we study whether they can learn the hidden rules in training sequences, such as the grammatical structures of regular language. We theoretically analyze some existing LRNNs and discover their limitations on regular language. Motivated by the analysis, we propose a new LRNN equipped with a block-diagonal and input-dependent transition matrix. Experiments suggest that the proposed model is the only LRNN that can perform length extrapolation on regular language tasks such as Sum, Even Pair, and Modular Arithmetic.
Authors: Ting-Han Fan, Ta-Chung Chi, Alexander I. Rudnicky
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work theoretically analyze some existing LRNNs and proposes a new LRNN equipped with a block-diagonal and input-dependent transition matrix that is the only LRNN that can perform length extrapolation on regular language tasks such as Sum, Even Pair, and Modular Arithmetic.'}
Url: https://arxiv.org/pdf/2309.07412
Answer: "
"How many credits is Linguistics Lab worth?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many credits is Linguistics Lab worth?

Context: In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Linguistics Lab' with Course ID 11723 and Section A offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin, Mortensen located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Human Language for Artificial Intelligence' with Course ID 11724 and Section A offers 12.0 units. The Class meets Monday Wednesday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin located in Building WEH, Room 5409.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Meaning in Language Lab (Self Paced)' with Course ID 11726 and Section A offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rose located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Multilingual Natural Language Processing.' with Course ID 11737 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Li located in Building DH, Room 1212.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Machine Learning for Text and Graph-based Mining' with Course ID 11741 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Yang located in Building HH, Room B131.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Machine Learning for Text and Graph-based Mining' with Course ID 11741 and Section PP offers 12.0 units. The Class meets Schedule will be added between NA and NA ET.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Linguistics Lab' with Course ID 11723 and Section A offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin, Mortensen located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Human Language for Artificial Intelligence' with Course ID 11724 and Section A offers 12.0 units. The Class meets Monday Wednesday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin located in Building WEH, Room 5409.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Meaning in Language Lab (Self Paced)' with Course ID 11726 and Section A offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rose located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Multilingual Natural Language Processing.' with Course ID 11737 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Li located in Building DH, Room 1212.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Machine Learning for Text and Graph-based Mining' with Course ID 11741 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Yang located in Building HH, Room B131.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Machine Learning for Text and Graph-based Mining' with Course ID 11741 and Section PP offers 12.0 units. The Class meets Schedule will be added between NA and NA ET.
Answer: "
"How do buggies move forward in the beginning of the race?
","['cmubuggy_d403.txt', 'cmubuggy_d403.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How do buggies move forward in the beginning of the race?

Context: An mural celebrating Buggy can be found in CMU's Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions. Sweepstakes Slang Buggy: Vehicle being raced and also a nickname for the competition. Chute: A section of the freeroll portion of the buggy course (near the southwestern end of Frew Street at its intersection with Schenley Drive) where buggies make the sharp righthand turn from Schenley Drive onto Frew Street. Chute Flagger: Team member who provides a signal for buggy drivers to know when to start the right-hand turn from Schenley Drive onto Frew Street. Driver: Person who travels with a buggy and controls the vehicles via steering and braking systems. Pushbar: Structure attached to a buggy that a person pushes to propel that buggy forward. Pusher: Person who propels a buggy via a pushbar along one of the five hills of the buggy course. Shell: Entire outer structure or covering of a buggy that determines that buggy's aerodynamic characteristics. Transition: Procedure whereby one pusher finishes pushing a buggy and the next pusher in sequence starts to push that same buggy. Members of Fringe, celebrating their 50th year of Buggy racing, allowed a rare visit into its shop for a behind-the-scenes tour. This year Fringe is planning to roll four different vehicles, built and maintained in the Fringe workshop in the basement of the East Campus Garage, known as the ""Froom."" ""We're used to saying everything with 'fr' in front of it but when we say something in front of other people, it gets them confused,"" said Fringe head mechanic Dave Singh, who will graduate in May with a bachelor's degree in mechanical engineering and biomedical engineering. Fringe vehicles often are named with the letter ""B,"" like Boson, Blueshift, Bissa and Bumper. Other teams, such as Apex, often use names that connotate fire, while the SDC (Student Dormitory Committee) team, uses names such as Vice, Bane, Avarice and Malice. This year's buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It's one of the few times spectators can see buggies up close.
This year's buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It's one of the few times spectators can see buggies up close. Dave Singh explains how Fringe builds its buggies. Each team keeps its actual processes a secret, but each buggy has certain features such as a body, pushbar, wheels and driving and braking mechanisms. Behind Singh are other buggies built by Fringe. Behind the Wheel Drivers, have the closest connection to the vehicles, aside from the mechanics who spent countless hours building and maintaining individual buggies. While races are brief, each driver must log a number of practice runs to qualify to race. Teams practice nights and weekends throughout the fall and spring semesters as weather permits. Buggies are often built around drivers, so the fit can be snug, and drivers are often less than 5 feet, 3 inches in height. Most — but not all — are women. Tishya Girdhar, a junior in neuroscience, is social chair for Fringe. She came to CMU wanting to drive, but first she had to get over being claustrophobic. She started as a mechanic and got behind the wheel for last year's Sweepstakes. ""Our team's philosophy is to teach everyone how to do everything,"" Girdhar said. ""But I came to CMU wanting to drive. I wanted to drive so badly."" Amy Chen demonstrates how a driver is positioned inside of Boson, a Fringe buggy built in 2016. Safety is a top priority for teams. All drivers are required to have five pieces of safety gear: mouth guard, goggles, a harness that includes three points of contact to the body of the buggy, gloves and a helmet. And during practice runs and races, flaggers man the course to let drivers know if it's safe to proceed. The streets of the race course haven't changed, but the condition of the course varies due to potholes, which can make or break a fast run. The last course records were set in 2017, but this year potholes are making for some challenging conditions, said Amy Chen, a senior in psychology who has been driving for four years.
Answer: "
"In the MCDS degree, what are the two names of the tracks you can take in your plan of study? One of these is a 16 month degree and the other is a 20 month degree. Answer with the names of the two tracks with an 'and' in between.
","['program_info_MasterofComputationalDataScience.txt', 'mcds-student-handbook-2023_2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the MCDS degree, what are the two names of the tracks you can take in your plan of study? One of these is a 16 month degree and the other is a 20 month degree. Answer with the names of the two tracks with an 'and' in between.

Context: Academic Program Name:
Master of Computational Data Science

Website:
https://lti.cs.cmu.edu/academics/masters-programs/mcds.html

Overview:
The MCDS degree focuses on engineering and deploying large-scale information systems. Our comprehensive curriculum equips you with the skills and knowledge to develop the layers of technology involved in the next generation of massive information system deployments and analyze the data these systems generate. When you graduate, you’ll have a unified vision of these systems from your core courses; internship experience; and semester-long, group-oriented capstone project. MCDS graduates are sought-after software engineers, data scientists and project managers at leading information technology, software services and social media companies.

Requirements:
The MCDS program offers three majors: Systems, Analytics, and Human-Centered Data Science. All three require the same total number of course credits, split among required core courses, electives, data science seminar and capstone courses specifically defined for each major. The degree can also be earned in two different ways, depending on the length of time you spend working on it. Regardless of the timing option, all MCDS students must complete a minimum of 144 units to graduate.
Here are the options:
Standard Timing — a 16-month degree consisting of study for fall and spring semesters, a summer internship, and fall semester of study. Each semester comprises a minimum of 48 units. This timing is typical for most students. Students graduate in December.
Extended Timing — a 20-month degree consisting of study for fall and spring semesters, a summer internship, and a second year of fall and spring study. Each semester comprises a minimum of 36 units. Students graduate in May.
For a complete overview of the MCDS requirements read the MCDS Handbook.

Curriculum:
To earn an MCDS degree, students must pass courses in the core curriculum, the MCDS seminar, a concentration area, and electives. Students must also complete a capstone project in which they work on a research project at CMU or on an industry-sponsored project.
In total, students must complete 144 eligible units of study, including eight 12-unit courses, two 12-unit seminar courses, and one 24-unit capstone course. Students must choose at minimum five core courses. The remainder of the 12-unit courses with course numbers 600 or greater can be electives chosen from the SCS course catalog.
Any 
additional non-prerequisite units taken beyond the 144 units are also considered 
electives. 
 
To maintain full-time enrollment status, a student must enroll in a minimum of 36 
course units per semester. A student may not take more than 60 units per 
semester, without permission from their academic advisor. Students must 
maintain full-time enrollment status (minimum of 36 units) in their final semester. 
3.3.4 Prerequisite Core Course 
All MCDS students are expected to pass the 11-637 Foundations of Computational Data 
Science course by the end of their first semester. Each student must pass 11-637 with a 
grade of “B” or better.  
3.3.5 Plan of study 
The degree consists of two timing options based on the length of time the student spends 
working on the degree. The student chooses their timing at the start of the degree 
program (for visa requirements). Changes in timing are possible with the approval of the 
Director of the degree program and successful visa extension application with CMU’s 
Office of International Education. Note that all degree options consist of the same amount 
of coursework: 
 
 
 
Professional Preparation Track – a 16-month degree consisting of study for Fall 
and Spring semesters, a summer internship, and Fall semester of study. Each 
semester consists of a minimum of 48 units of study. This timing is typical for most 
students. The student graduates in December. 
 
Research Preparation Track – a 20-month degree consisting of study for Fall and 
Spring semesters, a summer internship, and a second year of Fall and Spring 
study. Each semester consists of a minimum of 36 units of study. This timing is 
designed for students interested in extending their time at CMU for developing 
applied research skills in preparation for further graduate study or research-
oriented employment. Note that the per-semester course load is lower, but the 
total cost is higher since four semesters of tuition are paid. This timing is also 
 
 
15
recommended for students interested in pursuing a PhD after graduation. The 
student graduates in May.
Answer: "
"What is the title of the ethics course offered at LTI?
","['handbook_phd_2023-2024.txt', 'handbook_phd_2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the title of the ethics course offered at LTI?

Context: It is the 
responsibility of the entire community to establish and maintain the integrity of our university.  
Carnegie Mellon University educates its students to become professionals who will serve society 
with integrity. The university also creates and disseminates new knowledge and expressions of 
knowledge in ways that benefit society.  
Carnegie Mellon strives to serve the changing needs of society through the three primary goals 
outlined in its mission statement: To create and disseminate knowledge and art through research 
and artistic expression, teaching and learning and transfer to society; to serve students by 
teaching them leadership, problem- solving skills, and the values of quality, ethical behavior, 
responsibility to society and commitments to work; and to pursue the advantages provided by a 
diverse community, open to the exchange of ideas, where discovery and artistic creativity can 
flourish.  
In any presentation, creative, artistic or research, it is the ethical responsibility of each student to 
identify the conceptual sources of the work submitted. Failure to do so is dishonest and is the 
basis for a charge of cheating or plagiarism, which is subject to disciplinary action. 
Please review the University Policy on Academic Integrity. 
The policy includes the University expectations around academic integrity and provides 
definitions of cheating, plagiarism, and unauthorized assistance.  
A review of the Universitys Academic Disciplinary Actions procedures is also recommended.  
Important note: The LTI implements the above policys option of conven[ing] a disciplinary 
hearing according to the procedures of the department/program. Our procedure is as follows: A 
first violation is grounds for dismissal from the graduate program. If we decide not to immediately 
dismiss, the first violation will result in the student being on disciplinary probation.  
If the student commits a second violation while on probation, the penalty is dismissal from the 
graduate program. 
These procedures outline the process for investigating, reporting, and adjudicating violations of 
the University Policy on Academic Integrity. The procedures also outline the appeal process. 
Please see the Appeals of Course Level Action section of The Word and the Office of Community 
Standards & Integrity web page for more information. 
LTI Ph.D.
Some doctoral course-sections follow a separate Academic Calendar. 
1.5 
Carnegie Mellon University Statement of Assurance  
Carnegie Mellon University does not discriminate in admission, employment, or administration 
of its programs or activities on the basis of race, color, national origin, sex, handicap or disability, 
age, sexual orientation, gender identity, religion, creed, ancestry, belief, veteran status, or genetic 
information.  Furthermore, Carnegie Mellon University does not discriminate and is required not 
to discriminate in violation of federal, state, or local laws or executive orders. 
Inquiries concerning the application of a compliance with this statement should be directed to 
the university ombudsperson, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA 
15213 (412-268-1018). Obtain general information about Carnegie Mellon University by calling 
412-268-2000. 
Carnegie Mellon University publishes an annual campus security and fire safety report describing 
the universitys security, alcohol and drug, sexual assault, and fire safety policies, and containing 
statistics about the number and type of crimes committed on campus, and the number and cause 
of fires in campus residence facilities during the preceding three years. You can obtain a copy by 
contacting the Carnegie Mellon Police Department at 412-268-2323. The annual security and fire 
safety report also is available online at www.cmu.edu/police/annualreports . 
Information regarding the applicable grievance procedures for alleged violations 
of the Statement of Assurance is available at 
https://www.cmu.edu/policies/forms-and-documents/soa-violations.pdf.   
The Office for Institutional Equity and Title IX may be reached at 412-268-7125  
LTI Ph.D. Graduate Student Handbook 
Page 12 
 
or  institutionalequity@cmu.edu. 
1.6 
The Carnegie Mellon Code 
Students at Carnegie Mellon, because they are members of an academic community dedicated to 
the achievement of excellence, are expected to meet the highest standards of personal, ethical, and 
moral conduct possible. 
 These standards require personal integrity, a commitment to honesty without compromise, as 
well as truth without equivocation and a willingness to place the good of the community above 
the good of the self. Obligations once undertaken must be met, commitments kept.
Answer: "
"What is ESPnet-ST-v2?
","['shinji watanabe_dab8e7dc79085774eea58bcb9ea2ed0ee20377eb_metadata.txt', 'shinji watanabe_dab8e7dc79085774eea58bcb9ea2ed0ee20377eb_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is ESPnet-ST-v2?

Context: Faculty Name: shinji watanabe
Paperid: dab8e7dc79085774eea58bcb9ea2ed0ee20377eb
Title: ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit
Year: 2023
Abstract: ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broadening interests of the spoken language translation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simultaneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) – each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language translation toolkits. This toolkit offers state-of-the-art architectures such as transducers, hybrid CTC/attention, multi-decoders with searchable intermediates, time-synchronous blockwise CTC/attention, Translatotron models, and direct discrete unit models. In this paper, we describe the overall design, example models for each task, and performance benchmarking behind ESPnet-ST-v2, which is publicly available at https://github.com/espnet/espnet.
Authors: Brian Yan, Jiatong Shi, Yun Tang, H. Inaguma, Yifan Peng, Siddharth Dalmia, Peter Pol'ak, Patrick Fernandes, Dan Berrebbi, Tomoki Hayashi, Xiaohui Zhang, Zhaoheng Ni, Moto Hira, Soumi Maiti, J. Pino, Shinji Watanabe
Venue: Annual Meeting of the Association for Computational Linguistics
Tldr: {'model': 'tldr@v2.0.0', 'text': 'The overall design, example models for each task, and performance benchmarking behind ESPnet-ST-v2 are described, which is publicly available at https://github.com/espnet/esp net.'}
Url: https://arxiv.org/pdf/2304.04596
Title: ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit
Authors: Brian Yan, Jiatong Shi, Yun Tang, Hirofumi Inaguma, Yifan Peng, Siddharth Dalmia, Peter Polák, Patrick Fernandes, Dan Berrebbi, Tomoki Hayashi, Xiaohui Zhang, Zhaoheng Ni, Moto Hira, Soumi Maiti, Juan Pino, Shinji Watanabe
Section: A.1 Reproducibility
Table 9 shows the hyperparameters for the models presented in §5. All of our data preparation scripts are available in ESPnet: https://github.com/ espnet/espnet/tree/master/egs2.
Answer: "
"What is the target duration of the LTI PhD program in years?
","['handbook_phd_2023-2024.txt', 'program_info_PhDinLanguageandInformationTechnology.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the target duration of the LTI PhD program in years?

Context: 4 
Ph.D. Academic Policies 
4.1 
Duration of Study 
The target duration of the LTI Ph.D. is five years, although six years is also common. 
Carnegie Mellon graduate students are expected to complete their degree requirements within 
the standard length of time for their program of study as outlined in this Graduate Student 
Handbook. Upon completion of the graduate program degree requirements, the degree will be 
certified by the students academic program in the semester in which the student completes the 
requirements. 
Early Completion 
The Ph.D. is granted when all degree requirements are satisfied. Some students complete the 
program in fewer than five years. 
LTI Ph.D. Graduate Student Handbook 
Page 24 
 
Extended or Longer-than-Standard Competition 
Longer-than-standard degree completion may occur due to academic interruptions in making 
progress toward the degree as defined by the academic program, interruptions of full-time study 
or progress towards the degree due to serious, documented medical issues, or other unusual or 
unforeseen circumstances. 
Doctoral students who require an extended period to complete their degree requirements must 
consult with their academic program and are subject to the CMU Policy on Doctoral Student 
Status (https://www.cmu.edu/policies/student-and-student-life/doctoral-student-status.html), 
specifically the Time to Degree. 
4.1.1 Department Policy on Double Counting Courses 
An LTI Ph.D. student who uses courses taken as a Masters degree student (at Carnegie Mellon 
or elsewhere) toward their program requirements cannot use those same courses toward any 
other Masters degree offered by the school. The LTI (like other SCS units) allows its Ph.D. 
students who have passed the requirements for an LTI Masters degree to receive the masters 
degree without any additional work. Any other sharing of coursework by an LTI student between 
more than one CMU degree (e.g., receiving an MLD Masters degree that includes courses taken 
as an LTI Ph.D. student) must be explicitly approved by the LTI, on a case-by-case basis, in 
advance.  
4.1.2 External Internships  
The LTI provides summer support for its Ph.D. students, so Ph.D. students are expected to do 
research at Carnegie Mellon during the summer. However, outside experience can be a valuable 
educational experience, Ph.D.
Academic Program Name:
Ph.D. in Language and Information Technology

Website:
https://lti.cs.cmu.edu/academics/phd-programs/phd-lti.html

Overview:
In order to obtain your Ph.D. in Language and Information Technologies, you need to pass 96 units (generally, eight courses) of graduate courses that fulfill these requirements: At least 72 units of LTI courses: Must include one class in each LTI focus area. At least 24 units of SCS courses. At least two lab courses in two different research areas. Here's a sample of what your five-year schedule might look like: Fall Spring Summer Year 1 Grammars and Lexicons Algorithms for NLP Directed Study Search Engines or Machine Learning for Text Mining Machine Translation Directed Study Required Research Year 2 Software Engineering for LT (I) Speech Understanding Self-Paced Lab Directed Study Software Engineering for LT (II) Self-Paced Lab Directed Study Required Research Year 3 Directed Research Directed Research Directed Research Year 4 Directed Research Directed Research Directed Research Year 5 Directed Research Directed Research Directed Research

Requirements:
In order to obtain your Ph.D. in Language and Information Technologies, you need to pass 96 units (generally, eight courses) of graduate courses that fulfill these requirements: At least 72 units of LTI courses: Must include one class in each LTI focus area. At least 24 units of SCS courses. At least two lab courses in two different research areas. Here's a sample of what your five-year schedule might look like: Fall Spring Summer Year 1 Grammars and Lexicons Algorithms for NLP Directed Study Search Engines or Machine Learning for Text Mining Machine Translation Directed Study Required Research Year 2 Software Engineering for LT (I) Speech Understanding Self-Paced Lab Directed Study Software Engineering for LT (II) Self-Paced Lab Directed Study Required Research Year 3 Directed Research Directed Research Directed Research Year 4 Directed Research Directed Research Directed Research Year 5 Directed Research Directed Research Directed Research

Curriculum:
In order to obtain your Ph.D. in Language and Information Technologies, you need to pass 96 units (generally, eight courses) of graduate courses that fulfill these requirements: At least 72 units of LTI courses: Must include one class in each LTI focus area. At least 24 units of SCS courses. At least two lab courses in two different research areas.
Answer: "
"How many courses are offered by BXA Intercollege Degree Programs in Spring 2024 (exclude the BXA Studio courses)?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many courses are offered by BXA Intercollege Degree Programs in Spring 2024 (exclude the BXA Studio courses)?

Context: In Semester Spring 2024, from the department of BXA Intercollege Degree Programs, the subject titled 'BXA Undergraduate Research Project' with Course ID 52390 and Section A offers 3-12 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Murray, Hagan located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of BXA Intercollege Degree Programs, the subject titled 'BXA Seminar III: Deconstructing Disciplines' with Course ID 52392 and Section A offers 9.0 units. The Class meets Monday between 06:00PM and 06:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Marcum, Hagan located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of BXA Intercollege Degree Programs, the subject titled 'BXA Seminar IV: Capstone Project Research' with Course ID 52401 and Section A offers VAR units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Murray, Hagan located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of BXA Intercollege Degree Programs, the subject titled 'BXA Seminar V: Capstone Project Production' with Course ID 52402 and Section A offers 9.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Murray, Hagan located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of BXA Intercollege Degree Programs, the subject titled 'BXA Internship' with Course ID 52590 and Section A offers 3-12 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hagan, Murray located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Biological Sciences, the subject titled 'Frontiers, Analysis, and Discovery in Biological Sciences' with Course ID 03117 and Section NA offers 6.0 units.
In Semester Spring 2024, from the department of BXA Intercollege Degree Programs, the subject titled 'BXA Undergraduate Research Project' with Course ID 52390 and Section A offers 3-12 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Murray, Hagan located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of BXA Intercollege Degree Programs, the subject titled 'BXA Seminar III: Deconstructing Disciplines' with Course ID 52392 and Section A offers 9.0 units. The Class meets Monday between 06:00PM and 06:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Marcum, Hagan located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of BXA Intercollege Degree Programs, the subject titled 'BXA Seminar IV: Capstone Project Research' with Course ID 52401 and Section A offers VAR units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Murray, Hagan located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of BXA Intercollege Degree Programs, the subject titled 'BXA Seminar V: Capstone Project Production' with Course ID 52402 and Section A offers 9.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Murray, Hagan located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of BXA Intercollege Degree Programs, the subject titled 'BXA Internship' with Course ID 52590 and Section A offers 3-12 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hagan, Murray located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Biological Sciences, the subject titled 'Frontiers, Analysis, and Discovery in Biological Sciences' with Course ID 03117 and Section NA offers 6.0 units.
Answer: "
"How does SenteCon affect predictive performance on downstream tasks?
","['louis philippe morency_47a4ac301820c3ea7da4efb8e2466cc6468ad631_metadata.txt', 'chenyan xiong_bfa7f7bec1c4553c6c382ec2dbf4f889d7fa6e4f_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How does SenteCon affect predictive performance on downstream tasks?

Context: Faculty Name: louis philippe morency
Paperid: 47a4ac301820c3ea7da4efb8e2466cc6468ad631
Title: SenteCon: Leveraging Lexicons to Learn Human-Interpretable Language Representations
Year: 2023
Abstract: Although deep language representations have become the dominant form of language featurization in recent years, in many settings it is important to understand a model's decision-making process. This necessitates not only an interpretable model but also interpretable features. In particular, language must be featurized in a way that is interpretable while still characterizing the original text well. We present SenteCon, a method for introducing human interpretability in deep language representations. Given a passage of text, SenteCon encodes the text as a layer of interpretable categories in which each dimension corresponds to the relevance of a specific category. Our empirical evaluations indicate that encoding language with SenteCon provides high-level interpretability at little to no cost to predictive performance on downstream tasks. Moreover, we find that SenteCon outperforms existing interpretable language representations with respect to both its downstream performance and its agreement with human characterizations of the text.
Authors: Victoria Lin, Louis-Philippe Morency
Venue: Annual Meeting of the Association for Computational Linguistics
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work presents SenteCon, a method for introducing human interpretability in deep language representations that outperforms existing interpretable language representations with respect to both its downstream performance and its agreement with human characterizations of the text.'}
Url: http://arxiv.org/pdf/2305.14728
Title: COMPLEQA: Benchmarking the Impacts of Knowledge Graph Completion Methods on Question Answering
Authors: Donghan Yu, Yu Gu, Chenyan Xiong, Yiming Yang
Section: 4 Experiments
The F1 score, in this context, is a more direct measure of the quality of predicted triplets compared to MRR. To delve further, we measured the overlap of predicted triplets among various models and found that no model’s predictions, correct or incorrect, fully encompassed the other’s. For example, in a 20% incomplete KG, ComplEx and RotatE had about 80% shared correct predictions and 30% shared incorrect ones. In this case, despite ComplEx’s superior performance in KGC, it doesn’t solely determine QA performance as various predicted triplets impact QA differently, and this impact may not align well with their contribution to KGC performance. This discrepancy points to the need for KGC methods that optimize both KG completion and downstream task performance.
Answer: "
"What are some of the under-served languages currently identified by GlobalBench?
","['graham neubig_17605c43ca3eb982c99642052ddc21a93d116594_metadata.txt', 'graham neubig_17605c43ca3eb982c99642052ddc21a93d116594_content_2.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are some of the under-served languages currently identified by GlobalBench?

Context: Faculty Name: graham neubig
Paperid: 17605c43ca3eb982c99642052ddc21a93d116594
Title: GlobalBench: A Benchmark for Global Progress in Natural Language Processing
Year: 2023
Abstract: Despite the major advances in NLP, significant disparities in NLP system performance across languages still exist. Arguably, these are due to uneven resource allocation and sub-optimal incentives to work on less resourced languages. To track and further incentivize the global development of equitable language technology, we introduce GlobalBench. Prior multilingual benchmarks are static and have focused on a limited number of tasks and languages. In contrast, GlobalBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring accuracy, GlobalBench also tracks the estimated per-speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served languages are the ones with a relatively high population, but nonetheless overlooked by composite multilingual benchmarks (like Punjabi, Portuguese, and Wu Chinese). Currently, GlobalBench covers 966 datasets in 190 languages, and has 1,128 system submissions spanning 62 languages.
Authors: Yueqi Song, Catherine Cui, Simran Khanuja, Pengfei Liu, FAHIM FAISAL, Alissa Ostapenko, Genta Indra Winata, Alham Fikri Aji, Samuel Cahyawijaya, Yulia Tsvetkov, Antonios Anastasopoulos, Graham Neubig
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work introduces GlobalBench, an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages and tracks the estimated per-speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world.'}
Url: http://arxiv.org/pdf/2305.14716
Title: GlobalBench: A Benchmark for Global Progress in Natural Language Processing
Authors: Yueqi Song, Catherine Cui, Simran Khanuja, Pengfei Liu, Fahim Faisal, Alissa Ostapenko, Genta Indra Winata, Alham Fikri Aji, Samuel Cahyawijaya, Yulia Tsvetkov, Antonios Anastasopoulos, Graham Neubig
Section: 2 GlobalBench Design Principles
when there is support for only one language, G = 1. Formally, if performance of a language for a task is yi, (i = 1 ... n where n is the total number of languages), and is indexed in non-decreasing order (yi ≤ yi+1), then the Gini coefficient (G) can be calculated as: G = 1 n ( n + 1− 2 ∑n i=1(n + 1− i)yi∑n i=1 yi ) (4) For each task, we obtain equity values, calculated using the maximum performance of submitted systems for each language in a task. For languages that are not supported by any dataset, we assume the system performance to be zero. The global equity values for each task are in Table 2. Apart from the above, we keep track of system performances (F1/Accuracy/BLEU etc.). For each task, we take the system output with the highest performance among all system outputs with the same language, and provide a ranking of languages with the highest system performances. We also maintain a ranking of most under-served languages (sorted based on utilities), for reasons detailed below. 2.3 Incentivization: Reward Improvement We can estimate current global progress in language technologies using the demographic and lin- guistic weighted global averages, and the global equity values. In GlobalBench, we also encourage development of systems that improve upon these metrics. We accomplish this in two ways: First, we identify areas with the greatest potential for improvement, i.e., we identify the most under-served languages. We choose a parameter of τ = 0.4 to better strike a balance between demographic- and linguistic-weighted utility. Languages farthest from the ideal τ -weighted utility are expected to be most under-served. Hence, (1− τ -weighted utility) of a language gives us this measure.
Answer: "
"What is the full name of the conference where the paper GameQA: Gamified Mobile App Platform for Building Multiple-Domain Question-Answering Datasets, got published?
","['eric nyberg_0f008e07d601e8f21d1df5db3d36e85484840083_metadata.txt', 'eric nyberg_papers.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the full name of the conference where the paper GameQA: Gamified Mobile App Platform for Building Multiple-Domain Question-Answering Datasets, got published?

Context: Faculty Name: eric nyberg
Paperid: 0f008e07d601e8f21d1df5db3d36e85484840083
Title: GameQA: Gamified Mobile App Platform for Building Multiple-Domain Question-Answering Datasets
Year: 2023
Abstract: The methods used to create many of the well-known Question-Answering (QA) datasets are hard to replicate for low-resource languages. A commonality amongst these methods is hiring annotators to source answers from the internet by querying a single answer source, such as Wikipedia. Applying these methods for low-resource languages can be problematic since there is no single large answer source for these languages. Consequently, this can result in a high ratio of unanswered questions, since the amount of information in any single source is limited. To address this problem, we developed a novel crowd-sourcing platform to gather multiple-domain QA data for low-resource languages. Our platform, which consists of a mobile app and a web API, gamifies the data collection process. We successfully released the app for Icelandic (a low-resource language with about 350,000 native speakers) to build a dataset which rivals large QA datasets for high-resource languages both in terms of size and ratio of answered questions. We have made the platform open source with instructions on how to localize and deploy it to gather data for other low-resource languages.
Authors: Njall Skarphedinsson, Breki Gudmundsson, Steinar Smari, M. Lárusdóttir, H. Einarsson, Abuzar Khan, Eric Nyberg, H. Loftsson
Venue: Conference of the European Chapter of the Association for Computational Linguistics
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work developed a novel crowd-sourcing platform to gather multiple-domain QA data for low-resource languages and successfully released the app for Icelandic to build a dataset which rivals large QA datasets for high- resource languages both in terms of size and ratio of answered questions.'}
Url: https://aclanthology.org/2023.eacl-demo.18.pdf
List of 2023 Open Access papers by eric nyberg are:
GameQA: Gamified Mobile App Platform for Building Multiple-Domain Question-Answering Datasets
InPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers
Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting for Multilingual Document-Grounded QA
Chain-of-Skills: A Configurable Model for Open-Domain Question Answering
A super wear-resistant coating for Mg alloys achieved by plasma electrolytic oxidation and discontinuous deposition
Difference-Masking: Choosing What to Mask in Continued Pretraining
Using Implicit Feedback to Improve Question Generation
Answer: "
"In fall 2023, When is unit 02613 on Mondays, Wednesdays, and Fridays?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, When is unit 02613 on Mondays, Wednesdays, and Fridays?

Context: In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Professional Issues for Computational and Automated Scientists' with Course ID 02602 and Section A offers 3.0 units. The Class meets Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Brasier, DeBlasio located in Building POS, Room 152.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Algorithms and Advanced Data Structures' with Course ID 02613 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 09:00AM and 09:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Yu located in Building DH, Room 2315.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'String Algorithms' with Course ID 02614 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kingsford located in Building CFA, Room 102.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Essential Mathematics and Statistics for Scientists' with Course ID 02680 and Section NA offers 9.0 units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Lugo-Martinez, DeBlasio located in Building POS, Room 153.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Essential Mathematics and Statistics for Scientists' with Course ID 02680 and Section A offers 9.0 units. The Class meets Friday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor DeBlasio, Lugo-Martinez located in Building SH, Room 105.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'M.S. Thesis Research' with Course ID 02700 and Section A offers 1-12 units. The Class meets Schedule will be added between NA and NA ET.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Professional Issues for Computational and Automated Scientists' with Course ID 02602 and Section A offers 3.0 units. The Class meets Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Brasier, DeBlasio located in Building POS, Room 152.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Algorithms and Advanced Data Structures' with Course ID 02613 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 09:00AM and 09:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Yu located in Building DH, Room 2315.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'String Algorithms' with Course ID 02614 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kingsford located in Building CFA, Room 102.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Essential Mathematics and Statistics for Scientists' with Course ID 02680 and Section NA offers 9.0 units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Lugo-Martinez, DeBlasio located in Building POS, Room 153.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Essential Mathematics and Statistics for Scientists' with Course ID 02680 and Section A offers 9.0 units. The Class meets Friday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor DeBlasio, Lugo-Martinez located in Building SH, Room 105.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'M.S. Thesis Research' with Course ID 02700 and Section A offers 1-12 units. The Class meets Schedule will be added between NA and NA ET.
Answer: "
"Who were the instructors for 11667?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who were the instructors for 11667?

Context: In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Foundations of Computational Data I' with Course ID 11671 and Section A3 offers 6.0 units. The Class meets Thursday between 07:30PM and 08:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rose located in Building CMU, Room REMOTE.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Foundations of Computational Data Science II:' with Course ID 11672 and Section NA offers 6.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Foundations of Computational Data Science II' with Course ID 11672 and Section A4 offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rose located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'AI Venture Studio:' with Course ID 11681 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'AI Venture Studio' with Course ID 11681 and Section A offers 12.0 units. The Class meets Friday between 02:00PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ammirati, Paulisick located in Building POS, Room 152.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning' with Course ID 11685 and Section Lec 1 offers 12.0 units. The Class meets Monday Wednesday between 08:00AM and 09:20AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ramakrishnan, Singh located in Building BH, Room A51.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Foundations of Computational Data I' with Course ID 11671 and Section A3 offers 6.0 units. The Class meets Thursday between 07:30PM and 08:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rose located in Building CMU, Room REMOTE.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Foundations of Computational Data Science II:' with Course ID 11672 and Section NA offers 6.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Foundations of Computational Data Science II' with Course ID 11672 and Section A4 offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rose located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'AI Venture Studio:' with Course ID 11681 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'AI Venture Studio' with Course ID 11681 and Section A offers 12.0 units. The Class meets Friday between 02:00PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ammirati, Paulisick located in Building POS, Room 152.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning' with Course ID 11685 and Section Lec 1 offers 12.0 units. The Class meets Monday Wednesday between 08:00AM and 09:20AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ramakrishnan, Singh located in Building BH, Room A51.
Answer: "
"Which LTI faculty were involved in the WebArena paper?
","['graham neubig_e41482f4ee984f17382f6cdd900df094d928be06_metadata.txt', 'daniel fried_e41482f4ee984f17382f6cdd900df094d928be06_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which LTI faculty were involved in the WebArena paper?

Context: Faculty Name: graham neubig
Paperid: e41482f4ee984f17382f6cdd900df094d928be06
Title: WebArena: A Realistic Web Environment for Building Autonomous Agents
Year: 2023
Abstract: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.
Authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper builds an environment for language-guided agents that is highly realistic and reproducible, and creates an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.'}
Url: https://arxiv.org/pdf/2307.13854
Faculty Name: daniel fried
Paperid: e41482f4ee984f17382f6cdd900df094d928be06
Title: WebArena: A Realistic Web Environment for Building Autonomous Agents
Year: 2023
Abstract: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.
Authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper builds an environment for language-guided agents that is highly realistic and reproducible, and creates an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.'}
Url: https://arxiv.org/pdf/2307.13854
Answer: "
"In spring 2024, What is the day and time of course 17645-F?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, What is the day and time of course 17645-F?

Context: On Monday, 31 March, 2025, during the Spring 2025 (S25) semester, Semester pass/no pass & withdrawal deadline is observed.
On Wednesday, 02 April, 2025, during the Spring 2025 (S25) semester, Mini-4 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 03 April, 2025,Thursday to 05 April, 2023,Saturday marks the Spring Carnival for Spring 2025 (S25) semester which leads to No Classes.
From 07 April, 2025,Monday to 11 April, 2023,Friday marks the Fall 2025 Registration Week for Spring 2025 (S25) semester.
On Monday, 14 April, 2025, during the Spring 2025 (S25) semester, Mini-4 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 14 April, 2025, during the Spring 2025 (S25) semester, Semester & Mini-4 Faculty Course Evaluations open is observed.
On Friday, 25 April, 2025, during the Spring 2025 (S25) semester, Last Day of Classes is observed.
On Friday, 25 April, 2025, during the Spring 2025 (S25) semester, Semester & Mini-4 voucher deadline (4) is observed.
From 28 April, 2025,Monday to 29 April, 2023,Tuesday marks the Final Examinations for Spring 2025 (S25) semester.
On Wednesday, 30 April, 2025, during the Spring 2025 (S25) semester, Reading Day is observed.
From 01 May, 2025,Thursday to 02 May, 2023,Friday marks the Final Examinations for Spring 2025 (S25) semester.
From 03 May, 2025,Saturday to 04 May, 2023,Sunday marks the Reading Days for Spring 2025 (S25) semester.
On Monday, 05 May, 2025, during the Spring 2025 (S25) semester, Final Examinations is observed.
On Monday, 05 May, 2025, during the Spring 2025 (S25) semester, Semester & Mini-4 Faculty Course Evaluations close is observed.
From 04 May, 2024,Saturday to 05 May, 2024,Sunday marks the Reading Days for Spring 2024 (S24) semester.
On Monday, 06 May, 2024, during the Spring 2024 (S24) semester, Final Examinations is observed.
On Monday, 06 May, 2024, during the Spring 2024 (S24) semester, Semester & Mini-4 Faculty Course Evaluations close is observed.
On Tuesday, 07 May, 2024, during the Spring 2024 (S24) semester, Make-Up Final Examinations is observed.
On Wednesday, 08 May, 2024, during the Spring 2024 (S24) semester, Graduating Final Grades Due by 4 pm is observed.
On Sunday, 12 May, 2024, during the Spring 2024 (S24) semester, Commencement is observed.
On Tuesday, 14 May, 2024, during the Spring 2024 (S24) semester, Final Grades Due by 4 pm is observed.
On Tuesday, 21 May, 2023, during the Spring 2024 (S24) semester, Spring Deans' Lists Posted is observed.
On Monday, 13 May, 2024, during the Summer One 2024 (M24) semester, Semester & Mini-5 Classes Begin is observed.
On Friday, 17 May, 2024, during the Summer One 2024 (M24) semester, Mini-5 add, audit, & tuition adjustment drop deadline (1) is observed.
On Friday, 24 May, 2024, during the Summer One 2024 (M24) semester, Semester add, audit, & tuition adjustment drop deadline (1) is observed.
On Monday, 27 May, 2024, during the Summer One 2024 (M24) semester, Memorial Day is observed, leading to University Closed & No Classes.
On Tuesday, 28 May, 2024, during the Summer One 2024 (M24) semester, Mini-5 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 07 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 pass/no pass & withdrawal deadline (3) is observed.
Answer: "
"Who are all of the tenure-track associate professors in LTI?
","['mona_diab.txt', 'handbook_phd_2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who are all of the tenure-track associate professors in LTI?

Context: Mona Diab
LTI Director and Tenured Professor, Language Technologies Institute
Research Area : Fairness and Ethics in Language Technology
Contact: 5723 Gates & Hillman Centers
Email : mdiab@andrew.cmu.edu
Phone : 412-268-3669
Address
5000 Forbes Avenue
Pittsburgh, PA 15213
The Office of the Assistant Vice Provost for Graduate Education (AVPGE) offers a robust 
schedule of professional development opportunities. Some are geared towards a specific 
population (masters students, Ph.D. students at the beginning of their program, graduate 
students seeking tenure track positions, etc.) and others are open to all graduate students (time 
management, balancing, staying healthy). A full schedule of programs can be found at: 
http://www.cmu.edu/graduate/. 
6.8 
University Libraries 
http://search.library.cmu.edu 
There are three main libraries at Carnegie Mellon:  Hunt Library, Mellon Library, and the Sorrells 
Library with the combined mission of providing access and help to graduate students in finding 
the information needed, teaching graduate students to evaluate available information and use 
reliable sources. The libraries digital resources and services, including off-campus/ wireless 
access to databases and e-journals, offer online access. There are also two neighboring libraries 
open to Carnegie Mellon graduate students: Carnegie Library of Pittsburgh and University of 
Pittsburgh Libraries. Visit the University Libraries website for information about all mentioned 
library locations and hours, on-line resources, and FAQs. 
LTI Ph.D. Graduate Student Handbook 
Page 37 
 
6.9 
Computing Services 
Computing Services is located in Cyert Hall  Room 285.  Computing Services develops, 
maintains, and supports the computing infrastructure for Carnegie Mellon students, faculty 
members and staff members. This includes the campus wired and wireless networks, public 
computer labs or clusters, cable television and telephone services, computing related 
documentation and support through the Help Center. In addition, Computing Services provides 
standard classroom technologies for over 100 lecture halls, classrooms, and seminar rooms across 
campus. The website contains addition information regarding The Help Center hours, location 
and contact information, computing cluster hours and location, the Carnegie Mellon web portal, 
computing security and policies and guidelines. Students can email the Help Center at it-
help@cmu.edu or call 412-268-4357 (HELP) with questions and for assistance.
Answer: "
"What are the components of HomeRobot?
","['yonatan bisk_3b0c02955e88f5862e61b560c7f70ba8cf235b1d_metadata.txt', 'yonatan bisk_3b0c02955e88f5862e61b560c7f70ba8cf235b1d_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the components of HomeRobot?

Context: Faculty Name: yonatan bisk
Paperid: 3b0c02955e88f5862e61b560c7f70ba8cf235b1d
Title: HomeRobot: Open-Vocabulary Mobile Manipulation
Year: 2023
Abstract: HomeRobot (noun): An affordable compliant robot that navigates homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future research work improve performance. See videos on our website: https://ovmm.github.io/.
Title: HomeRobot: Open-Vocabulary Mobile Manipulation
Authors: Sriram Yenamandra, Arun Ramachandran, Karmesh Yadav, Austin Wang, Mukul Khanna, Theophile Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander William Clegg, John Turner, Zsolt Kira, Manolis Savva, Angel Chang, Devendra Singh Chaplot, Dhruv Batra, Roozbeh Mottaghi, Yonatan Bisk, Chris Paxton
Section: H.2 Robot Setup
Visualizing The Robot We use RVIZ, a part of ROS, to visualize results and progress. Fig. 22 shows three different outputs from our system: on the far left, an image from the test environment being processed by Detic; in the center, a top-down map generated by the navigation planner described in Sec. E.2; and on the right, an image from RVIZ with the point cloud from the robot’s head camera registered against the 2D lidar map created by Hector SLAM. One advantage of the HomeRobot stack is that it is designed to work with existing debugging tools - especially ROS [113]. ROS is a widely-used framework for robotics software development that comes with a lot of online resources, official support from Hello Robot, and a rich and thriving open-source community with wide industry backing.
Answer: "
"In fall 2023, What is the title of course 05360?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, What is the title of course 05360?

Context: In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Interaction Design for Human-Computer Interaction' with Course ID 05360 and Section D offers 12.0 units. The Class meets Tuesday Thursday between 12:30PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Matthews, Saunders located in Building 4SC, Room 104.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Interaction Design Fundamentals:' with Course ID 05360 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Interaction Design for Human-Computer Interaction' with Course ID 05360 and Section E offers 12.0 units. The Class meets Tuesday Thursday between 05:00PM and 07:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Shelly, Beck located in Building 4SC, Room 104.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Prototyping Algorithmic Experiences' with Course ID 05380 and Section A offers 15.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Holstein located in Building GHC, Room 4301.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Designing Human Centered Software:' with Course ID 05391 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Designing Human-Centered Software' with Course ID 05391 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Interaction Design for Human-Computer Interaction' with Course ID 05360 and Section D offers 12.0 units. The Class meets Tuesday Thursday between 12:30PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Matthews, Saunders located in Building 4SC, Room 104.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Interaction Design Fundamentals:' with Course ID 05360 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Interaction Design for Human-Computer Interaction' with Course ID 05360 and Section E offers 12.0 units. The Class meets Tuesday Thursday between 05:00PM and 07:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Shelly, Beck located in Building 4SC, Room 104.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Prototyping Algorithmic Experiences' with Course ID 05380 and Section A offers 15.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Holstein located in Building GHC, Room 4301.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Designing Human Centered Software:' with Course ID 05391 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Designing Human-Centered Software' with Course ID 05391 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET.
Answer: "
"In 2011, an IBM computer defeated human champions on the “Jeopardy!” game show. What was the name of this computer?
","['25things_d400.txt', '25things_d400.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In 2011, an IBM computer defeated human champions on the “Jeopardy!” game show. What was the name of this computer?

Context: An algorithm developed by CMU scientists is close to enabling a nationwide network that would match living kidney donors with potential recipients whom they've never met in real life. 21. RNA sequencing via videogames, 2010 Thanks to crowdsourcing, science isn’t just for scientists any more. People without formal training in molecular biology are producing new insights into genetic encoding through a videogame called EteRNA, developed by researchers at CMU and Stanford, that allows players to fold and shift RNA molecules to solve on-screen puzzles. 22. Language learning software, 2010Learning a second language has always been challenging, but a CMU spinoff called Duolingo is proving that it doesn’t have to be expensive. Duolingo has developed language tutoring software that enables users to learn Spanish, English, Italian, German, Portuguese or French for free through its website and mobile apps. In the process, Duolingo users are helping to translate the Web. 23. Question-answering computers, 2011 Searching the Web for information is rarely as simple as asking a question in plain English. So-called “question-answering” machines moved from laboratories to TV screens when an IBM computer called “Watson” defeated two human champions on the game show “Jeopardy!” At the heart of Watson was computer architecture developed by CMU’s Eric Nyberg and his students. 24. Encrypting online information, 2012 Credit card numbers and other data used online is safer thanks to an encryption scheme developed by CMU alumna Shafi Goldwasser (S’79). She shared the 2012 Turing Award for her role in developing practical encoding schemes that are difficult to break. 25. Smart, adaptable traffic signals, 2012 Smart traffic lights developed at CMU’s Robotics Institute are saving time and energy, and cutting down on the amount of air pollution created by idling cars. First rolled out in Pittsburgh’s East Liberty neighborhood, the new signals are being studied around the country.
Clarke would go on to receive the Turing Award for his role in the development of model checking. 16. CAPTCHAs, 2000 “Spam” and malicious attacks were a growing problem on the Internet when hackers developed automated “bots” that could sign up for email and other Web services without human intervention. Luis von Ahn (CS’03,’05), Nick Hopper (CS’04), John Langford (CS’02) and CMU professor Manuel Blum invented a “Completely Automated Public Turing Test to tell Computers and Humans Apart,” or CAPTCHA, to help foil the bots. A later variation, reCAPTCHA, is helping digitize old books and newspapers. 17. Robotic video cameras, 2001When Baltimore Ravens quarterback Trent Dilfer dropped back to pass, TV viewers of Super Bowl XXXV saw something they’d never seen before --- the motion froze and the view suddenly rotated to show Dilfer’s opposite side. CBS called it Eyevision. The synchronized system of robotic cameras and advanced image processing was the brainchild of CMU’s Takeo Kanade, one of many advances he pioneered in computer vision. 18. Self-driving vehicles, 2007Carnegie Mellon’s early attempts at self-driving vehicles progressed slowly, creeping around Pittsburgh’s Schenley Park in the late 1990s. But they were going full-throttle by the time CMU’s self-driving SUV, named BOSS, won the 2007 DARPA Urban Challenge road race. 19. Thought reading programs, 2007Your brain reacts in different ways, depending on what words you’re thinking about --- ways that are measurable with magnetic-resonance imaging, or MRI, machines. CMU researchers Tom Mitchell and Marcel Just are decoding those brain scans and are making progress at reading people’s thoughts. 20. Kidney donor matching, 2008 Organ transplants save lives every day, but more people could likely be saved if it was easier to match recipients with donors who are unrelated. An algorithm developed by CMU scientists is close to enabling a nationwide network that would match living kidney donors with potential recipients whom they've never met in real life. 21. RNA sequencing via videogames, 2010 Thanks to crowdsourcing, science isn’t just for scientists any more.
Answer: "
"What is the semantic notion used as a case study in ""Syntax and Semantics Meet in the “Middle”: Probing the Syntax-Semantics Interface of LMs Through Agentivity""?
","['graham neubig_c5207241406586f4263b235667e004b71ea68953_content_2.txt', 'lori levin_c5207241406586f4263b235667e004b71ea68953_content_2.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the semantic notion used as a case study in ""Syntax and Semantics Meet in the “Middle”: Probing the Syntax-Semantics Interface of LMs Through Agentivity""?

Context: Title: Syntax and Semantics Meet in the “Middle”: Probing the Syntax-Semantics Interface of LMs Through Agentivity
Authors: Lindia Tjuatja, Emmy Liu, Lori Levin, Graham Neubig
Section: 1 Introduction
the alternation shown in example (1). We then compare the performance of LMs to both human judgements and corpus statistics. Probing for LMs for their knowledge of agentivity in syntactic constructions as in (1) and (2) is a particularly insightful case study as it allows us to explore three interconnected questions in a highly controlled syntactic setting: I. Do models display sensitivity to aspects of word-level semantics independent of syntactic context, and is such sensitivity aligned with human judgements? (§3.1) II. Can models employ lexical semantics to determine the appropriate semantics of a sentence where the syntax is ambiguous between readings (as in 1)? (§3.2) III. Can models determine the semantics of a sentence from syntax, disregarding lexical semantics when necessary (as in 2)? (§3.3) Additionally, the relatively infrequent pairings of semantic function and syntactic form of sentences such as (1b) are also interesting from a learnability and acquisition perspective for both LMs and humans. How both come to process and acquire exceptions to a general “rule” has been a topic of debate since early connectionist models (Rumelhart and McClelland, 1986). Hence, knowledge of LM capabilities in acquiring and processing these linguistic anomalies may serve as valuable insight to linguists, cognitive scientists, and NLP practitioners alike.
Title: Syntax and Semantics Meet in the “Middle”: Probing the Syntax-Semantics Interface of LMs Through Agentivity
Authors: Lindia Tjuatja, Emmy Liu, Lori Levin, Graham Neubig
Section: 1 Introduction
the alternation shown in example (1). We then compare the performance of LMs to both human judgements and corpus statistics. Probing for LMs for their knowledge of agentivity in syntactic constructions as in (1) and (2) is a particularly insightful case study as it allows us to explore three interconnected questions in a highly controlled syntactic setting: I. Do models display sensitivity to aspects of word-level semantics independent of syntactic context, and is such sensitivity aligned with human judgements? (§3.1) II. Can models employ lexical semantics to determine the appropriate semantics of a sentence where the syntax is ambiguous between readings (as in 1)? (§3.2) III. Can models determine the semantics of a sentence from syntax, disregarding lexical semantics when necessary (as in 2)? (§3.3) Additionally, the relatively infrequent pairings of semantic function and syntactic form of sentences such as (1b) are also interesting from a learnability and acquisition perspective for both LMs and humans. How both come to process and acquire exceptions to a general “rule” has been a topic of debate since early connectionist models (Rumelhart and McClelland, 1986). Hence, knowledge of LM capabilities in acquiring and processing these linguistic anomalies may serve as valuable insight to linguists, cognitive scientists, and NLP practitioners alike.
Answer: "
"If you wanted to take Arts & Community Development in Fall 2023, what were the course numbers of the courses offered?
","['metadata_course_fall_23.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: If you wanted to take Arts & Community Development in Fall 2023, what were the course numbers of the courses offered?

Context: In Semester Fall 2023, from the department of Arts & Entertainment Management, the subject titled 'Arts & Community Development' with Course ID 93832 and Section A1 offers 6.0 units. The Class meets Monday Wednesday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor McDill located in Building HBH, Room 2008.
In Semester Fall 2023, from the department of Arts & Entertainment Management, the subject titled 'Artistic Development in the Music Industry' with Course ID 93837 and Section L offers 3.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Los Angeles, California location,led by experienced instructor Grinberg located in Building TBA, Room None.
In Semester Fall 2023, from the department of Arts & Entertainment Management, the subject titled 'Arts Management Professional Seminar I' with Course ID 93847 and Section A offers 0.0 units. The Class meets Thursday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Spangler, Bowser located in Building HBH, Room 1202.
In Semester Fall 2023, from the department of Arts & Entertainment Management, the subject titled 'Film Economics, Marketing & Distribution' with Course ID 93851 and Section L offers 12.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Los Angeles, California location,led by experienced instructor Baker located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Arts & Entertainment Management, the subject titled 'Production Management' with Course ID 93852 and Section L offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Los Angeles, California location,led by experienced instructor Harrison, Lammi located in Building TBA, Room None.
In Semester Fall 2023, from the department of Arts & Entertainment Management, the subject titled 'Digital Innovation and Entrepreneurship' with Course ID 93853 and Section L offers 6.0 units. The Class meets Schedule will be added between NA and NA ET.
In Semester Fall 2023, from the department of Arts & Entertainment Management, the subject titled 'Museum Operations' with Course ID 93807 and Section A1 offers 6.0 units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Haldeman located in Building HBH, Room 1208.
In Semester Fall 2023, from the department of Arts & Entertainment Management, the subject titled 'Public Art' with Course ID 93809 and Section A2 offers 6.0 units. The Class meets Monday Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rearick located in Building HBH, Room 1004.
In Semester Fall 2023, from the department of Arts & Entertainment Management, the subject titled 'Producing a Performing Arts Season' with Course ID 93811 and Section A2 offers 6.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Crawford located in Building HBH, Room 1208.
In Semester Fall 2023, from the department of Arts & Entertainment Management, the subject titled 'Disruptive Technologies in Arts Enterprises' with Course ID 93830 and Section A offers 6.0 units. The Class meets Thursday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Crawford located in Building HBH, Room 1202.
In Semester Fall 2023, from the department of Arts & Entertainment Management, the subject titled 'Introduction to Business Law for Arts Managers' with Course ID 93831 and Section A offers 12.0 units. The Class meets Monday between 06:30PM and 09:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Gurwin located in Building HBH, Room 1202.
In Semester Fall 2023, from the department of Arts & Entertainment Management, the subject titled 'Arts & Community Development' with Course ID 93832 and Section A1 offers 6.0 units.
Answer: "
"What are the 4 common MCDS core courses? List them in the following format: course number - title; course number - title; ...
","['mcds-student-handbook-2023_2024.txt', 'program_info_MasterofComputationalDataScience.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the 4 common MCDS core courses? List them in the following format: course number - title; course number - title; ...

Context: Note that the per-semester course load is lower, but the 
total cost is higher since four semesters of tuition are paid. This timing is also 
 
 
15
recommended for students interested in pursuing a PhD after graduation. The 
student graduates in May. 
3.3.6.1 MCDS Curriculum 
All MCDS students must complete 144 units of graduate study which satisfy the 
following curriculum: 
 
11-637 - Foundations of Computational Data Science 
 
Four (4) additional MCDS Core Courses (10-601 Introduction to Machine 
Learning; 05-839 Interactive Data Science; 15-619 Cloud Computing; 11-631 Data 
Science Seminar; 48 units) 
 
Three courses (3) from one area of concentration curriculum (36 units) 
 
Three (3) MCDS Capstone courses (11-634, 11-635 and 11-632) (36 units) 
 
One (1) Elective: any graduate level course 600 and above in the School of 
Computer Science (12 units) 
 
3.3.6.2 Common MCDS Core Courses 
All MCDS students are required to complete four common core courses in their first two 
semesters: 
 
10-601 - Machine Learning 
 
15-619 - Cloud Computing 
 
05-839 - Interactive Data Science 
 
11-631 - Data Science Seminar 
 
3.3.6.3 Areas of Concentration 
In addition to the common MCDS core, all students must complete at least one area of 
concentration, which consists of three courses in Analytics, Systems, or Human-
Centered Data Science. Students consult with their academic advisor and choose one or 
more areas of concentration during their first semester, in preparation for enrolling in 
Spring classes.
Students must choose at minimum five core courses. The remainder of the 12-unit courses with course numbers 600 or greater can be electives chosen from the SCS course catalog. Any additional non-prerequisite units taken beyond the 144 units are also considered electives.
Students who plan to select the Systems concentration may wish to enroll in 15-513 “Introduction to Computing Systems” during the summer session preceding their enrollment in the program; this course is a prerequisite for many advanced Systems courses, so it should be completed during Summer if you wish to enroll in advanced Systems courses in the Fall.
Click here to see the MCDS Course Map.
Some example courses of study are included below.
Example 1: Analytics Major, 16 Months
Example 2: Systems Major, 16 Months
Example 3: Human-Centered Data Science Major, 16 Months
Answer: "
"What number do all of the Computational Biology classes start with?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What number do all of the Computational Biology classes start with?

Context: In Semester Spring 2024, from the department of Computational Biology, the subject titled 'Introduction to Computational Biology' with Course ID 02250 and Section Lec 1 offers 12.0 units. The Class meets Monday Wednesday Friday between 03:00PM and 03:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor DeBlasio located in Building DH, Room 1212.
In Semester Spring 2024, from the department of Computational Biology, the subject titled 'Introduction to Computational Biology' with Course ID 02250 and Section Lec 2 offers 12.0 units. The Class meets Sunday Tuesday between 11:30AM and 12:45PM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Ganapathiraju located in Building CMB, Room 1031.
In Semester Spring 2024, from the department of Computational Biology, the subject titled 'Introduction to Computational Biology' with Course ID 02250 and Section A offers 12.0 units. The Class meets Friday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor DeBlasio located in Building DH, Room 1112.
In Semester Spring 2024, from the department of Computational Biology, the subject titled 'Introduction to Computational Biology' with Course ID 02250 and Section B offers 12.0 units. The Class meets Friday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor DeBlasio located in Building GHC, Room 5222.
In Semester Spring 2024, from the department of Computational Biology, the subject titled 'Introduction to Computational Biology' with Course ID 02250 and Section W offers 12.0 units. The Class meets Thursday between 11:30AM and 12:45PM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Ganapathiraju located in Building CMB, Room 1031.
In Semester Spring 2024, from the department of Computational Biology, the subject titled 'Great Ideas in Computational Biology' with Course ID 02251 and Section Lec 1 offers 12.0 units. The Class meets Monday Wednesday between 12:30PM and 01:50PM ET.
In Semester Spring 2024, from the department of Computational Biology, the subject titled 'Introduction to Computational Biology' with Course ID 02250 and Section Lec 1 offers 12.0 units. The Class meets Monday Wednesday Friday between 03:00PM and 03:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor DeBlasio located in Building DH, Room 1212.
In Semester Spring 2024, from the department of Computational Biology, the subject titled 'Introduction to Computational Biology' with Course ID 02250 and Section Lec 2 offers 12.0 units. The Class meets Sunday Tuesday between 11:30AM and 12:45PM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Ganapathiraju located in Building CMB, Room 1031.
In Semester Spring 2024, from the department of Computational Biology, the subject titled 'Introduction to Computational Biology' with Course ID 02250 and Section A offers 12.0 units. The Class meets Friday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor DeBlasio located in Building DH, Room 1112.
In Semester Spring 2024, from the department of Computational Biology, the subject titled 'Introduction to Computational Biology' with Course ID 02250 and Section B offers 12.0 units. The Class meets Friday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor DeBlasio located in Building GHC, Room 5222.
In Semester Spring 2024, from the department of Computational Biology, the subject titled 'Introduction to Computational Biology' with Course ID 02250 and Section W offers 12.0 units. The Class meets Thursday between 11:30AM and 12:45PM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Ganapathiraju located in Building CMB, Room 1031.
In Semester Spring 2024, from the department of Computational Biology, the subject titled 'Great Ideas in Computational Biology' with Course ID 02251 and Section Lec 1 offers 12.0 units. The Class meets Monday Wednesday between 12:30PM and 01:50PM ET.
Answer: "
"What's the cost in us dollars per program for the masters degrees in language technologies if you submit after the early deadline?
","['mcds-student-handbook-2023_2024.txt', 'handbook-msaii-2022-2023.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What's the cost in us dollars per program for the masters degrees in language technologies if you submit after the early deadline?

Context: Language Technologies Institute / School of Computer Science 
Graduate Student Handbook 
Academic Year 2023-2024  
Master of Computational Data Science Program 
 
Last revision date: July 20, 2023 
 
The information contained in this graduate handbook template focuses on the 
resources and locations available at the Carnegie Mellon Pittsburgh Campus. 
 
 
1
Table of Contents 
1 Welcome . 6 
1.1 The MCDS Degree . 6 
1.2 Vision . 7 
1.3 Mission . 7 
1.4 MCDS Contact Information. 8 
1.5 University Policies and Expectations . 9 
1.6 Carnegie Mellon University Statement of Assurance . 9 
1.7 The Carnegie Mellon Code . 10 
2 The Language Technologies Institute . 10 
2.1 Main Office . 10 
2.2 Photocopies and Printers . 11 
2.3 Office Space for MS Students . 11 
2.4 Computers for MS Students . 11 
3 MCDS Degree Completion and Certification . 11 
3.1 CMU Degree Completion and Statute of Limitations . 11 
Early Completion 
11 
Extended or Longer-than-Standard Completion 
12 
Policy on Master’s Student Statute of Limitations 
12 
Additional Guidance for Students 
12 
3.2 Full-time Status . 13 
3.3 MCDS Degree Enrollment Process and Related Information . 13 
3.3.1 Duration of the degree program 
13 
3.3.2 Residency requirements 
13 
3.3.3 Degree Certification: Course requirements and related policies/protocols 
13 
3.3.4 Prerequisite Core Course 
14 
3.3.5 Plan of study 
14 
3.3.6.1 MCDS Curriculum 
15 
3.3.6.2 Common MCDS Core Courses 
15 
3.3.6.3 Areas of Concentration 
15 
3.3.6.4 MCDS Capstone Courses 
16 
3.3.10 Capstone project 
17 
3.3.11 Elective courses 
17 
3.3.12 Undergraduate courses 
17 
3.3.13 Independent study course 
18 
 
 
2
3.3.14 Double counting courses 
18 
3.3.15 Courses outside of the School of Computer Science 
18 
3.3.16 Grades 
18 
3.3.17 Student Review,
6 Academic Calendar 
The Academic Calendar can be found at https://www.cmu.edu/hub/calendar/index.html and 
provides information on all deadlines including registration dates, class start dates, add/drop 
deadlines, exam dates and more. 
7 School and Departmental Information 
The following are key personnel with whom you may interact during your time at Carnegie 
Mellon: 
 
Martial Hebert 
 
 
 
 
 
Dean, School of Computer Science 
9 
 
 
University Professor 
 
GHC 6105 
 
412-268-5704 
 
hebert@cs.cmu.edu  
https://www.cs.cmu.edu/~hebert/  
 
David Garlan 
Associate Dean for Master’s Programs, SCS 
Professor 
TCS 420 
garlan@cs.cmu.edu 
http://www.cs.cmu.edu/~garlan/academics.htm  
 
Carolyn Rosé  
 
 
 
 
Interim Director, Language Technologies Institute 
 
Professor 
 
GHC 5415 
 
412-268-7130 
 
cprose@cs.cmu.edu 
8 MSAII Policies 
The Language Technologies Institute (LTI) has prepared this statement of policies, program 
requirements, guidance, process and procedures for students in the M.S. in Artificial Intelligence 
and Innovation (MSAII) program. A copy of this handbook is also available online at the 
program website.   
The University Student guide, The Word, which details university-wide policies, is also 
available online at www.cmu.edu/student-affairs/theword.     
Additional resources specific to graduate students can be found at 
www.cmu.edu/graduate/policies/    
8.1 The MSAII Degree 
The M.S. in Artificial Intelligence and Innovation (MSAII) is a professional master's program 
offered by the Language Technologies Institute (LTI) in the School of Computer Science at 
Carnegie Mellon University. This program seeks to educate students in the innovative use of 
artificial intelligence to create practical solutions in a wide variety of application areas. As 
artificial intelligence matures, there is a great need for students who can envision, design, plan 
and deliver solutions that integrate AI technologies such as deep learning, natural language 
processing, robotics and big data analytics into new applications.  The program stresses both 
10 
 
intrapreneurship and entrepreneurship and encourages students to develop ideas for startups 
and for introduction into established companies.
Answer: "
"In fall 2023, When does unit 02601 take place on Fridays?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, When does unit 02601 take place on Fridays?

Context: In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Professional Issues for Computational and Automated Scientists' with Course ID 02602 and Section A offers 3.0 units. The Class meets Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Brasier, DeBlasio located in Building POS, Room 152.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Algorithms and Advanced Data Structures' with Course ID 02613 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 09:00AM and 09:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Yu located in Building DH, Room 2315.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'String Algorithms' with Course ID 02614 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kingsford located in Building CFA, Room 102.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Essential Mathematics and Statistics for Scientists' with Course ID 02680 and Section NA offers 9.0 units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Lugo-Martinez, DeBlasio located in Building POS, Room 153.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Essential Mathematics and Statistics for Scientists' with Course ID 02680 and Section A offers 9.0 units. The Class meets Friday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor DeBlasio, Lugo-Martinez located in Building SH, Room 105.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'M.S. Thesis Research' with Course ID 02700 and Section A offers 1-12 units. The Class meets Schedule will be added between NA and NA ET.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Professional Issues for Computational and Automated Scientists' with Course ID 02602 and Section A offers 3.0 units. The Class meets Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Brasier, DeBlasio located in Building POS, Room 152.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Algorithms and Advanced Data Structures' with Course ID 02613 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 09:00AM and 09:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Yu located in Building DH, Room 2315.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'String Algorithms' with Course ID 02614 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kingsford located in Building CFA, Room 102.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Essential Mathematics and Statistics for Scientists' with Course ID 02680 and Section NA offers 9.0 units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Lugo-Martinez, DeBlasio located in Building POS, Room 153.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'Essential Mathematics and Statistics for Scientists' with Course ID 02680 and Section A offers 9.0 units. The Class meets Friday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor DeBlasio, Lugo-Martinez located in Building SH, Room 105.
In Semester Fall 2023, from the department of Computational Biology, the subject titled 'M.S. Thesis Research' with Course ID 02700 and Section A offers 1-12 units. The Class meets Schedule will be added between NA and NA ET.
Answer: "
"What is the name of the proposed method that extends WavLM's joint prediction and denoising to 40k hours of data across 136 languages?
","['shinji watanabe_01a819f7155bb87c32f1e4c13d9439c080e6aa97_metadata.txt', 'shinji watanabe_01a819f7155bb87c32f1e4c13d9439c080e6aa97_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the name of the proposed method that extends WavLM's joint prediction and denoising to 40k hours of data across 136 languages?

Context: Faculty Name: shinji watanabe
Paperid: 01a819f7155bb87c32f1e4c13d9439c080e6aa97
Title: Joint Prediction and Denoising for Large-Scale Multilingual Self-Supervised Learning
Year: 2023
Abstract: Multilingual self-supervised learning (SSL) has often lagged behind state-of-the-art (SOTA) methods due to the expenses and complexity required to handle many languages. This further harms the reproducibility of SSL, which is already limited to few research groups due to its resource usage. We show that more powerful techniques can actually lead to more efficient pre-training, opening SSL to more research groups. We propose WavLabLM, which extends WavLM’s joint prediction and denoising to 40k hours of data across 136 languages. To build WavLabLM, we devise a novel multi-stage pre-training method, designed to address the language imbalance of multilingual data. WavLabLM achieves comparable performance to XLS-R on ML-SUPERB with less than $10 \%$ of the training data, making SSL realizable with academic compute. We show that further efficiency can be achieved with a vanilla HuBERT Base model, which can maintain $94 \%$ of XLS-R’s performance with only $3 \%$ of the data, 4 GPUs, and limited trials. We open-source all code and models in ESPnet.
Authors: William Chen, Jiatong Shi, Brian Yan, Dan Berrebbi, Wangyou Zhang, Yifan Peng, Xuankai Chang, Soumi Maiti, Shinji Watanabe
Venue: Automatic Speech Recognition & Understanding
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work proposes WavLabLM, which extends WavLM’s joint prediction and denoising to 40k hours of data across 136 languages, and devise a novel multi-stage pre-training method, designed to address the language imbalance of multilingual data.'}
Url: https://arxiv.org/pdf/2309.15317
Title: JOINT PREDICTION AND DENOISING FOR LARGE-SCALE MULTILINGUAL SELF-SUPERVISED LEARNING
Authors: William Chen, Jiatong Shi, Brian Yan, Dan Berrebbi, Wangyou Zhang, Yifan Peng, Xuankai Chang, Soumi Maiti, Shinji Watanabe
Section: 8. REFERENCES
[1] A. Radford et al., “Robust speech recognition via largescale weak supervision,” arXiv preprint arXiv:2212.04356, 2022. [2] T. Brown et al., “Language models are few-shot learners,” in Proc. NeurIPS, vol. 33, 2020, pp. 1877–1901. [3] S. Chen et al., “WavLM: Large-scale self-supervised pre-training for full stack speech processing,” IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505–1518, 2022. [4] A. Mohamed et al., “Self-supervised speech representation learning: A review,” IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1179– 1210, 2022. [5] X. Chang et al., “An exploration of self-supervised pretrained representations for end-to-end speech recognition,” in Proc. ASRU, 2021, pp. 228–235. [6] A. Chowdhery et al., “PaLM: Scaling language modeling with pathways,” arXiv preprint arXiv:2204.02311, 2022. [7] T. L. Scao et al., “BLOOM: A 176b-parameter openaccess multilingual language model,” arXiv preprint arXiv:2211.05100, 2022. [8] S. Black et al., “GPT-neox-20b: An open-source autoregressive language model,” in Challenges & Perspectives in Creating Large Language Models, 2022. [9] A. Srivastava et al., “Beyond the imitation game: Quantifying and extrapolating the capabilities of language models,” arXiv preprint arXiv:2206.04615, 2022.
Answer: "
"What year was Fully Unsupervised Topic Clustering of Unlabelled Spoken Audio Using Self-Supervised Representation Learning and Topic Model published?
","['shinji watanabe_debb65ab30ceef2faef0e4af560a67f2abd03d14_metadata.txt', 'shinji watanabe_papers.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What year was Fully Unsupervised Topic Clustering of Unlabelled Spoken Audio Using Self-Supervised Representation Learning and Topic Model published?

Context: Faculty Name: shinji watanabe
Paperid: debb65ab30ceef2faef0e4af560a67f2abd03d14
Title: Fully Unsupervised Topic Clustering of Unlabelled Spoken Audio Using Self-Supervised Representation Learning and Topic Model
Year: 2023
Abstract: Unsupervised topic clustering of spoken audio is an important research topic for zero-resourced unwritten languages. A classical approach is to find a set of spoken terms from only the audio based on dynamic time warping or generative modeling (e.g., hidden Markov model), and apply a topic model to classify topics. The spoken term discovery is the most important and difficult part. In this paper, we propose to combine self-supervised representation learning (SSRL) methods as a component of spoken term discovery and probabilistic topic models. Most SSRL methods pre-train a model which predicts high-quality pseudo labels generated from an audio-only corpus. These pseudo labels can be used to produce a sequence of pseudo subwords by applying deduplication and a subword model. Then, we apply a topic model based on latent Dirichlet allocation for these pseudo-subword sequences in an unsupervised manner. The clustering performance is evaluated on the Fisher corpus using normalized mutual information. We confirm the improvement of the proposed method and its effectiveness compared to an existing approach using dynamic time warping and topic models although the experimental setups are not directly comparable.
Authors: Takashi Maekaku, Yuya Fujita, Xuankai Chang, Shinji Watanabe
Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes to combine self-supervised representation learning (SSRL) methods as a component of spoken term discovery and probabilistic topic models and confirms the improvement of the proposed method and its effectiveness compared to an existing approach using dynamic time warping and topic models.'}
Url: N/A
February to March 2023
Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study
Joint Modelling of Spoken Language Understanding Tasks with Integrated Dialog History
Enhancing End-to-End Conversational Speech Translation Through Target Language Context Utilization
Challenges of Corporate Alliance CLOMA toward Plastic Litter
The Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction
Reproducing Whisper-Style Training Using An Open-Source Toolkit And Publicly Available Data
The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios
Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing
ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit
Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge
Fully Unsupervised Topic Clustering of Unlabelled Spoken Audio Using Self-Supervised Representation Learning and Topic Model
TAPLoss: A Temporal Acoustic Parameter Loss for Speech Enhancement
End-to-End Speech Recognition: A Survey
The Multimodal Information Based Speech Processing (Misp) 2022 Challenge: Audio-Visual Diarization And Recognition
DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models
An external quality assessment feasibility study; cross laboratory comparison of haemagglutination inhibition assay and microneutralization assay performance for seasonal influenza serology testing: A FLUCOP study
Multi-Channel Speaker Extraction with Adversarial Training: The Wavlab Submission to The Clarity ICASSP 2023 Grand Challenge
AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual Representation Models
Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation
Deep Speech Synthesis from MRI-Based Articulatory Representations
FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN
Speech collage: code-switched audio generation by collaging monolingual corpora
Exploration on HuBERT with Multiple Resolutions
A Randomized, Double-Blind,
Answer: "
"Which instructors co-taught On-Device Machine Learning last fall?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which instructors co-taught On-Device Machine Learning last fall?

Context: The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ramakrishnan located in Building DH, Room A302.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Machine Learning for Signal Processing' with Course ID 11755 and Section B offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ramakrishnan located in Building CMU, Room REMOTE.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'On-Device Machine Learning:' with Course ID 11767 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'On-Device Machine Learning' with Course ID 11767 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Bisk, Strubell located in Building DH, Room 1212.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Large-Scale Multi-Media Analysis:' with Course ID 11775 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Large-Scale Multi-Media Analysis' with Course ID 11775 and Section A offers 12.0 units. The Class meets Monday Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hauptmann located in Building GHC, Room 4307.
The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ramakrishnan located in Building DH, Room A302.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Machine Learning for Signal Processing' with Course ID 11755 and Section B offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ramakrishnan located in Building CMU, Room REMOTE.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'On-Device Machine Learning:' with Course ID 11767 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'On-Device Machine Learning' with Course ID 11767 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Bisk, Strubell located in Building DH, Room 1212.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Large-Scale Multi-Media Analysis:' with Course ID 11775 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Large-Scale Multi-Media Analysis' with Course ID 11775 and Section A offers 12.0 units. The Class meets Monday Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hauptmann located in Building GHC, Room 4307.
Answer: "
"Which class did Shinji Watanabe teach in Fall 2024?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which class did Shinji Watanabe teach in Fall 2024?

Context: In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Machine Learning for Text and Graph-based Mining' with Course ID 11741 and Section PP offers 12.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Lisbon, Portugal location,led by experienced instructor Yang located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Search Engines:' with Course ID 11742 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Search Engines' with Course ID 11742 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Callan located in Building MI, Room 348.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Self-Paced Lab: IR' with Course ID 11743 and Section A offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Callan located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Speech Recognition and Understanding' with Course ID 11751 and Section A offers 12.0 units. The Class meets Monday Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Watanabe located in Building GHC, Room 4307.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Machine Learning for Signal Processing' with Course ID 11755 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ramakrishnan located in Building DH, Room A302.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Machine Learning for Text and Graph-based Mining' with Course ID 11741 and Section PP offers 12.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Lisbon, Portugal location,led by experienced instructor Yang located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Search Engines:' with Course ID 11742 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Search Engines' with Course ID 11742 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Callan located in Building MI, Room 348.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Self-Paced Lab: IR' with Course ID 11743 and Section A offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Callan located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Speech Recognition and Understanding' with Course ID 11751 and Section A offers 12.0 units. The Class meets Monday Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Watanabe located in Building GHC, Room 4307.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Machine Learning for Signal Processing' with Course ID 11755 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ramakrishnan located in Building DH, Room A302.
Answer: "
"In spring 2024, What is the title of course 17537?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, What is the title of course 17537?

Context: In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Principles of Software Construction: Objects, Design, and Concurrency' with Course ID 17514 and Section A offers 12.0 units. The Class meets Wednesday between 09:00AM and 09:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Aldrich, Garrod, Lacomis located in Building WEH, Room 5328.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Principles of Software Construction: Objects, Design, and Concurrency' with Course ID 17514 and Section B offers 12.0 units. The Class meets Wednesday between 10:00AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Aldrich, Garrod, Lacomis located in Building PH, Room 125B.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Principles of Software Construction: Objects, Design, and Concurrency' with Course ID 17514 and Section C offers 12.0 units. The Class meets Wednesday between 11:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Aldrich, Garrod, Lacomis located in Building WEH, Room 5316.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Principles of Software Construction: Objects, Design, and Concurrency' with Course ID 17514 and Section D offers 12.0 units. The Class meets Wednesday between 12:00PM and 12:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Aldrich, Garrod, Lacomis located in Building POS, Room 147.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Principles of Software Construction: Objects, Design, and Concurrency' with Course ID 17514 and Section E offers 12.0 units. The Class meets Wednesday between 01:00PM and 01:50PM ET.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Principles of Software Construction: Objects, Design, and Concurrency' with Course ID 17514 and Section A offers 12.0 units. The Class meets Wednesday between 09:00AM and 09:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Aldrich, Garrod, Lacomis located in Building WEH, Room 5328.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Principles of Software Construction: Objects, Design, and Concurrency' with Course ID 17514 and Section B offers 12.0 units. The Class meets Wednesday between 10:00AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Aldrich, Garrod, Lacomis located in Building PH, Room 125B.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Principles of Software Construction: Objects, Design, and Concurrency' with Course ID 17514 and Section C offers 12.0 units. The Class meets Wednesday between 11:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Aldrich, Garrod, Lacomis located in Building WEH, Room 5316.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Principles of Software Construction: Objects, Design, and Concurrency' with Course ID 17514 and Section D offers 12.0 units. The Class meets Wednesday between 12:00PM and 12:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Aldrich, Garrod, Lacomis located in Building POS, Room 147.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Principles of Software Construction: Objects, Design, and Concurrency' with Course ID 17514 and Section E offers 12.0 units. The Class meets Wednesday between 01:00PM and 01:50PM ET.
Answer: "
"Are guests allowed to play in the tennis court?
","['Apr-13_Eventno_54_50thReunionDinnerReception1974.txt', 'Apr-12_Eventno_15_Booth.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Are guests allowed to play in the tennis court?

Context: Event: 50th Reunion Dinner Reception (1974)
Date: 4/13/24
Time: 6:00 PM-9:00 PM ET
Participants/Audience: Reunion Exclusive 
Event Details: 
Reconnect with former classmates at this signature 50th Reunion celebration for the Class of 1974 at the beautiful Pittsburgh Golf Club. Cocktail reception followed by dinner. 

Note: Registration required. This event is open to the Class of 1974, Half Century Tartans and their guests only.

Meal Selections and Cost

Meal selection options: Beef, salmon or vegetarian
Cost
Now through Feb. 23: $94 per person.
Feb. 24 through Apr. 5: $99 per person.
Event: Booth!
Date: 4/12/24
Time: 11:00 AM-11:00 PM ET
Participants/Audience: Open to entire CMU community 
Event Details: 
Booth is #9 on the map.

This year’s theme is “Arcade: Let the Games Begin.” Be sure to check out every booth and cast your vote for the top choice in each category. 

Note: No registration required. No event fee. This event is open to the entire CMU community and their guests.

Weekend hours:

Thursday: 3-11 p.m.
Friday & Saturday: 11 a.m.-11 p.m.
Answer: "
"In spring 2024, What is the title of course 15050?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, What is the title of course 15050?

Context: In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section O offers 12.0 units. The Class meets Wednesday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section P offers 12.0 units. The Class meets Wednesday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section Q offers 12.0 units. The Class meets Wednesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section R offers 12.0 units. The Class meets Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section S offers 12.0 units. The Class meets Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5210.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section T offers 12.0 units. The Class meets Wednesday between 07:00PM and 08:20PM ET.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section O offers 12.0 units. The Class meets Wednesday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section P offers 12.0 units. The Class meets Wednesday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section Q offers 12.0 units. The Class meets Wednesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section R offers 12.0 units. The Class meets Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5207.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section S offers 12.0 units. The Class meets Wednesday between 05:00PM and 06:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Erdmann, Kaynar located in Building GHC, Room 5210.
In Semester Spring 2024, from the department of Computer Science, the subject titled 'Principles of Functional Programming' with Course ID 15150 and Section T offers 12.0 units. The Class meets Wednesday between 07:00PM and 08:20PM ET.
Answer: "
"When was the first Interfraternity Sweepstakes Race held?
","['Apr-12_Eventno_2_PreliminarySweepstakesRaces.txt', 'Apr-13_Eventno_3_SweepstakesFinalRaces.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When was the first Interfraternity Sweepstakes Race held?

Context: Event: Preliminary Sweepstakes Races
Date: 4/12/24
Time: 8:00 AM-12:00 PM ET
Participants/Audience: Open to entire CMU community 
Event Details: 
Come watch the CMU tradition where teams compete in an exciting sport best described as a mix between a relay race and bobsled! 

Can't make it back to campus? We'll livestream the races through cmuTV. You may also tune into WRCT Radio to hear play-by-play. Register for calendar hold during registration. If you're interested in being a part of our live chat, join our BAA Discord at cmubuggy.org/chat. 

Note: No registration required. No event fee. This event is open to the entire CMU community and their guests.
Event: Sweepstakes Final Races
Date: 4/13/24
Time: 8:00 AM-12:00 PM ET
Participants/Audience: Open to entire CMU community 
Event Details: 
Watch the exciting conclusion of this year's Sweepstakes races as teams take on the final heats that stand between them and the championship title. 

Can't make it back to campus? We'll livestream the races through cmuTV. You may also tune into WRCT Radio to hear the play-by-play. If you're interested in being a part of our live chat, join our BAA Discord at cmubuggy.org/chat. 

Note: No registration required. No event fee. This event is open to the entire CMU community and their guests.
Answer: "
"How many people co-authored the paper COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive Statements?
","['maarten sap_185ace5661963e2e1eb998e739e4110272a6bb43_content_0.txt', 'maarten sap_185ace5661963e2e1eb998e739e4110272a6bb43_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many people co-authored the paper COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive Statements?

Context: Title: COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive Statements
Authors: Xuhui Zhou, Hao Zhu, Akhila Yerukola, Thomas Davidson, Jena D. Hwang, Swabha Swayamdipta, Maarten Sap
Section: C GPT-3 prompts used in this paper
The example prompts for generating likely contexts are in Figure 8. The example prompts for generating adversarial contexts are in Figure 9. The example prompts for generating the likely explanations are in Figure 10. 11https://github.com/huggingface/transformers
Faculty Name: maarten sap
Paperid: 185ace5661963e2e1eb998e739e4110272a6bb43
Title: COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive Statements
Year: 2023
Abstract: Warning: This paper contains content that may be offensive or upsetting. Understanding the harms and offensiveness of statements requires reasoning about the social and situational context in which statements are made. For example, the utterance""your English is very good""may implicitly signal an insult when uttered by a white man to a non-white colleague, but uttered by an ESL teacher to their student would be interpreted as a genuine compliment. Such contextual factors have been largely ignored by previous approaches to toxic language detection. We introduce COBRA frames, the first context-aware formalism for explaining the intents, reactions, and harms of offensive or biased statements grounded in their social and situational context. We create COBRACORPUS, a dataset of 33k potentially offensive statements paired with machine-generated contexts and free-text explanations of offensiveness, implied biases, speaker intents, and listener reactions. To study the contextual dynamics of offensiveness, we train models to generate COBRA explanations, with and without access to the context. We find that explanations by context-agnostic models are significantly worse than by context-aware ones, especially in situations where the context inverts the statement's offensiveness (29% accuracy drop). Our work highlights the importance and feasibility of contextualized NLP by modeling social factors.
Authors: Xuhui Zhou, Haojie Zhu, Akhila Yerukola, Thomas Davidson, Jena D. Hwang, Swabha Swayamdipta, Maarten Sap
Venue: Annual Meeting of the Association for Computational Linguistics
Tldr: {'model': 'tldr@v2.0.0', 'text': 'COBRA frames are introduced, the first context-aware formalism for explaining the intents, reactions, and harms of offensive or biased statements grounded in their social and situational context, and the importance and feasibility of contextualized NLP by modeling social factors are highlighted.'}
Url: http://arxiv.org/pdf/2306.01985
Answer: "
"In the CSurF paper, what evaluation metrics were reported for MSMARCO?
","['graham neubig_4d74a5048b884e8bb3842240abf98915c619c8f8_content_1.txt', 'jamie callan_6b7eefa15c0a461afeab4fa13cf862c5340fdc2a_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the CSurF paper, what evaluation metrics were reported for MSMARCO?

Context: Title: Multi-Dimensional Evaluation of Text Summarization with In-Context Learning
Authors: Sameer Jain, Vaishakh Keshava, Swarnashree Mysore Sathyendra, Patrick Fernandes, Pengfei Liu, Graham Neubig, Chunting Zhou
Section: 1 Introduction
increased. Finally, in light of the recent work (Goyal et al., 2022) that points to the misalignment of existing evaluation metrics with human preference in evaluating zeroshot summaries generated by LLMs such as GPT-3 (Brown et al., 2020), we study the effectiveness of ICE in evaluating zero-shot summaries generated by GPT-3. We find that ICE evaluations agree closely with human judgments on such summaries.
0) 0.352 0.159 0.420 0.692 0.329 0.690 0.310 0.505 0.833 0.154 0.690 0.232 0.696 0.466 CSurF𝐻𝑁 (oracle) 0.521 0.192 0.458 0.729 0.370 0.713 0.353 0.550 0.867 0.161 0.717 0.264 0.739 0.510 (Best 𝛾 ) 0.0 0.3 0.1 0.2 0.2 0.4 0.1 0.2 0.2 0.3 0.2 0.2 0.2 Figure 3: Lexical form frequency and weights for CSurF. The red dotted curve denotes weight distribution. Model trained with 𝜆𝑞=𝜆𝑑=1e-2. X axis denotes percentage of corpus CSFs. These experiments demonstrate that the CSurF retrieval process can be highly efficient due to the sparsity of CSF match signals. To look into the properties of the CSF generation and matching processes, we analyze and plot the distribution of corpus CSFs’ lexical form frequency and expansion weight in Figure 3. Compared to the lexical form frequency distribution of the original text, CSurF is trained to simultaneously expand meaningful lexical surface forms but also prune existing lexical terms with low term importance, and removes a significant proportion of tokens with the highest occurrence frequency, most of which do not carry important contextual meaning such as stop words. This leads to the aforementioned comparison where CSurF requires lower scoring operations per query than COIL-tok despite having ""longer"" queries or documents. Post-hoc index pruning with 𝛼 = 0.5 further removes redundant matching signals of lexical forms at all frequencies, resulting in a significant decrease of matching operations without major influence in retrieval performance. For instance, after training and post-hoc pruning, the five most frequent terms in the MSMARCO passage set (""the"", ""of"", ""and"", ""in"", ""to"") are removed by over 98.5% compared to their original corpus term frequency.
Answer: "
"Where is Multimodal Fusion Interactions: A Study of Human and Automatic Quantification published?
","['louis philippe morency_90b09bdb1bd78875ee8d8d324a568a36955e4765_metadata.txt', 'louis philippe morency_90b09bdb1bd78875ee8d8d324a568a36955e4765_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Where is Multimodal Fusion Interactions: A Study of Human and Automatic Quantification published?

Context: Faculty Name: louis philippe morency
Paperid: 90b09bdb1bd78875ee8d8d324a568a36955e4765
Title: Multimodal Fusion Interactions: A Study of Human and Automatic Quantification
Year: 2023
Abstract: In order to perform multimodal fusion of heterogeneous signals, we need to understand their interactions: how each modality individually provides information useful for a task and how this information changes in the presence of other modalities. In this paper, we perform a comparative study of how humans annotate two categorizations of multimodal interactions: (1) partial labels, where different annotators annotate the label given the first, second, and both modalities, and (2) counterfactual labels, where the same annotator annotates the label given the first modality before asking them to explicitly reason about how their answer changes when given the second. We further propose an alternative taxonomy based on (3) information decomposition, where annotators annotate the degrees of redundancy: the extent to which modalities individually and together give the same predictions, uniqueness: the extent to which one modality enables a prediction that the other does not, and synergy: the extent to which both modalities enable one to make a prediction that one would not otherwise make using individual modalities. Through experiments and annotations, we highlight several opportunities and limitations of each approach and propose a method to automatically convert annotations of partial and counterfactual labels to information decomposition, yielding an accurate and efficient method for quantifying multimodal interactions.
Authors: P. Liang, Yun Cheng, R. Salakhutdinov, Louis-Philippe Morency
Venue: International Conference on Multimodal Interaction
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A comparative study of how humans annotate two categorizations of multimodal interactions is performed and a method to automatically convert annotations of partial and counterfactual labels to information decomposition is proposed, yielding an accurate and efficient method for quantifying multimodals interactions.'}
Url: https://dl.acm.org/doi/pdf/10.1145/3577190.3614151
Title: Multimodal Fusion Interactions: A Study of Human and Automatic sQuantification
Authors: Paul Pu Liang, Yun Cheng, Ruslan Salakhutdinov, Louis-Philippe Morency
Section: A HUMAN ANNOTATION DETAILS
Participation in all annotations was fully voluntary and we obtained consent from all participants prior to annotations. The authors manually took anonymous notes on all results and feedback in such a manner that the identities of annotators cannot readily be ascertained directly or through identifers linked to the subjects. Participants were not the authors nor in the same research groups as the authors, but they all hold or are working towards a graduate degree in a STEM feld and have knowledge of machine learning. None of the participants knew about this project before their session and each participant only interacted with the setting they were involved in. We sample 50 datapoints from each of the 5 datasets in Table 1 and give them to a total of 18 diferent annotators: • 3 annotators for direct annotation of interactions, • 3 annotators for partial labeling of �1, �2, and �12, • 3 annotators for counterfactual, labeling �1 frst then �1+2, • 3 annotators for counterfactual, labeling �2 frst then �2+1. All annotations were performed via google spreadsheets. A.1 Annotating partial labels We asked 3 annotators to predict the partial labels in a randomized setting. For each annotator, we asked them to annotate �1 then �2 given only modality 1 or 2 respectively, and fnally � given both modalities. This completion order is designed on purpose to minimize possible memorization of the data so that the annotators can provide completely independent unimodal and multimodal predictions on the label. When annotating the visual modality of the video datasets, we explicitly require the annotators to mute the audio and predict the partial labels based only on the video frames. After that, all annotators are asked to provide a confdence score on a scale of 0 (no confdence) to 5 (high confdence) about their annotations. The confdence scale is applied to all annotation settings below.
Answer: "
"What version of ChatGPT is used to extract facts in the FacTool paper?
","['graham neubig_7a5b44ea10a51708e18786595c8d70b18950da11_metadata.txt', 'shinji watanabe_8f0a24d1678e4d0e584b0932196cd257d5c53c7d_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What version of ChatGPT is used to extract facts in the FacTool paper?

Context: Faculty Name: graham neubig
Paperid: 7a5b44ea10a51708e18786595c8d70b18950da11
Title: FacTool: Factuality Detection in Generative AI - A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios
Year: 2023
Abstract: The emergence of generative pre-trained models has facilitated the synthesis of high-quality text, but it has also posed challenges in identifying factual errors in the generated text. In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models. (2) Generated texts tend to be lengthy and lack a clearly defined granularity for individual facts. (3) There is a scarcity of explicit evidence available during the process of fact checking. With the above challenges in mind, in this paper, we propose FacTool, a task and domain agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and scientific literature review) show the efficacy of the proposed method. We release the code of FacTool associated with ChatGPT plugin interface at https://github.com/GAIR-NLP/factool .
Authors: Ethan Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, Pengfei Liu
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes FacTool, a task and domain agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT), and demonstrates the efficacy of the proposed method.'}
Url: https://arxiv.org/pdf/2307.13528
Faculty Name: shinji watanabe
Paperid: 8f0a24d1678e4d0e584b0932196cd257d5c53c7d
Title: Improving Audio Captioning Models with Fine-grained Audio Features, Text Embedding Supervision, and LLM Mix-up Augmentation
Year: 2023
Abstract: Automated audio captioning (AAC) aims to generate informative descriptions for various sounds from nature and/or human activities. In recent years, AAC has quickly attracted research interest, with state-of-the-art systems now relying on a sequence-to-sequence (seq2seq) backbone powered by strong models such as Transformers. Following the macro-trend of applied machine learning research, in this work, we strive to improve the performance of seq2seq AAC models by extensively leveraging pretrained models and large language models (LLMs). Specifically, we utilize BEATs to extract fine-grained audio features. Then, we employ Instructor LLM to fetch text embeddings of captions, and infuse their language-modality knowledge into BEATs audio features via an auxiliary InfoNCE loss function. Moreover, we propose a novel data augmentation method that uses ChatGPT to produce caption mix-ups (i.e., grammatical and compact combinations of two captions) which, together with the corresponding audio mixtures, increase not only the amount but also the complexity and diversity of training data. During inference, we propose to employ nucleus sampling and a hybrid reranking algorithm, which has not been explored in AAC research. Combining our efforts, our model achieves a new state-of-the-art 32.6 SPIDEr-FL score on the Clotho evaluation split, and wins the 2023 DCASE AAC challenge.
Authors: Shih-Lun Wu, Xuankai Chang, G. Wichern, Jee-weon Jung, Franccois G. Germain, Jonathan Le Roux, Shinji Watanabe
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work utilizes BEATs to extract fine-grained audio features and proposes a novel data augmentation method that uses ChatGPT to produce caption mix-ups which increase not only the amount but also the complexity and diversity of training data.'}
Url: https://arxiv.org/pdf/2309.17352
Answer: "
"Where is FACTORCL published?
","['louis philippe morency_e1b2a35a000ca296c32284b323c7e36a28fe0693_metadata.txt', 'louis philippe morency_e1b2a35a000ca296c32284b323c7e36a28fe0693_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Where is FACTORCL published?

Context: Faculty Name: louis philippe morency
Paperid: e1b2a35a000ca296c32284b323c7e36a28fe0693
Title: Factorized Contrastive Learning: Going Beyond Multi-view Redundancy
Year: 2023
Abstract: In a wide range of multimodal tasks, contrastive learning has become a particularly appealing approach since it can successfully learn representations from abundant unlabeled data with only pairing information (e.g., image-caption or video-audio pairs). Underpinning these approaches is the assumption of multi-view redundancy - that shared information between modalities is necessary and sufficient for downstream tasks. However, in many real-world settings, task-relevant information is also contained in modality-unique regions: information that is only present in one modality but still relevant to the task. How can we learn self-supervised multimodal representations to capture both shared and unique information relevant to downstream tasks? This paper proposes FactorCL, a new multimodal representation learning method to go beyond multi-view redundancy. FactorCL is built from three new contributions: (1) factorizing task-relevant information into shared and unique representations, (2) capturing task-relevant information via maximizing MI lower bounds and removing task-irrelevant information via minimizing MI upper bounds, and (3) multimodal data augmentations to approximate task relevance without labels. On large-scale real-world datasets, FactorCL captures both shared and unique information and achieves state-of-the-art results on six benchmarks
Authors: P. Liang, Zihao Deng, Martin Q. Ma, James Y. Zou, Louis-Philippe Morency, R. Salakhutdinov
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'FactorCL is a new multimodal representation learning method to go beyond multi-view redundancy and captures both shared and unique information and achieves state-of-the-art results on six benchmarks.'}
Url: http://arxiv.org/pdf/2306.05268
Title: FACTORIZED CONTRASTIVE LEARNING: Going Beyond Multi-view Redundancy
Authors: Paul Pu Liang, Zihao Deng, Martin Q. Ma, James Zou, Louis-Philippe Morency, Ruslan Salakhutdinov
Section: D.3 Additional analysis and results
Fusion experiments: In Table 6 and 7 we present more detailed results on the Multibench [44] and IRFL [87] datasets computed from 5 independent runs. FACTORCL significantly outperforms the baselines that do not capture both shared and unique information in both supervised and selfsupervised settings, particularly on MUSTARD (where unique information expresses sarcasm, such as sardonic facial expressions or ironic tone of voice), and on MIMIC (with unique health indicators and sensor readings). There are also big improvements on the two sentiment analysis datasets MOSEI and MOSI, with 6.8% and 21.9% increases respectively when compared to SupCon [40]. In Table 7, we also see that FACTORCL substantially improves the state-of-the-art in classifying images and figurative captions which are not literally descriptive of the image on IRFL, outperforming zero-shot and fine-tuned CLIP [60] as well as continued pre-training baselines on top of CLIP. While the supervised version gives the best results overall, the self-supervised version with our proposed unique augmentations also performs better than independent augmentations, indicating that in the case without label information, we should always try to find and use unique augmentations when possible. In our experiments, we use word masking for text augmentations. For independent image augmentations, we use cropping, flipping, and color jittering. The unique augmentation simply removes the cropping operation, as illustrated in Figure 4 in the main text. Additional experiments on high shared information and low unique information: In Table 8 we include additional results using our method on the CIFAR10 [42] and MNIST [19] datasets. Our method outperforms the self-supervised contrastive learning on both datasets as expected, and roughly maintains the same performance as supervised contrastive learning. Therefore, in cases with abundant shared information (two modalities with high shared information or two different views generated from augmentations), our method recovers the performance of existing methods that do not capture unique information.
Answer: "
"What is the average performance improvement of Prompt2Model over gpt-3.5-turbo LLM?
","['graham neubig_e69684fb06a7b1fe621d7ef0c97fc2ca0e122c43_metadata.txt', 'chenyan xiong_38aaf8a29df6deeff0bf64cc835d242a25b10337_content_2.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the average performance improvement of Prompt2Model over gpt-3.5-turbo LLM?

Context: Faculty Name: graham neubig
Paperid: e69684fb06a7b1fe621d7ef0c97fc2ca0e122c43
Title: Prompt2Model: Generating Deployable Models from Natural Language Instructions
Year: 2023
Abstract: Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment. This is done through a multi-step process of retrieval of existing datasets and pretrained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains models that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model reliability before deployment. Prompt2Model is available open-source at https://github.com/neulab/prompt2model.
Authors: Vijay Viswanathan, Chenyang Zhao, Amanda Bertsch, Tongshuang Sherry Wu, Graham Neubig
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment.'}
Url: https://arxiv.org/pdf/2308.12261
Title: Model-Generated Pretraining Signals Improves Zero-Shot Generalization of Text-to-Text Transformers
Authors: Linyuan Gong, Chenyan Xiong, Xiaodong Liu, Payal Bajaj, Yiqing Xie, Alvin Cheung, Jianfeng Gao, Xia Song
Section: A Appendix
NVIDIA V100 (16GB) GPUs. Prompt-finetuning each large++ model takes about 70 hours on 64x NVIDIA V100 (16GB) GPUs. A.7 Full Results on T0 Eval Figure 8 results of METRO-T0 versus our T0 baseline and T03B by Sanh et al. (2022) on all 9 tasks in the T0 Eval benchmark. The results shows that METRO-T0LARGE++, having only 775M parameters, 1https://github.com/facebookresearch/fairseq 2https://github.com/bigscience-workshop/promptsource 3https://huggingface.co/docs/transformers/index 4https://github.com/bigscience-workshop/t-zero consistently outperforms T03B over all tasks on the T0 Eval benchmark. A.8 Evaluation on MMLU The prompt template used to evaluate our models MMLU is the prompt template from the AI2 Reasoning Challenge (AI2-ARC) concatenated with 5 passages in MS MARCO (Nguyen et al., 2016). These 5 passages are selected via dense retrival using T5-ANCE (Ge et al., 2023; Ni et al., 2021), which maps a query to a single vector to retrieve similar passage from the corpus. Adding densely-retrieved passages to prompts is a standard approach to enhance LM’s performance on zero-shot prompting. This approach is named retrieval augmentation. All T0 and METRO-T0 results reported in Table 3 are evaluated using this prompt template with retrieval augmentation. On the other hand, all Flan-T5 results reported in Table 3 are numbers reported in their paper. For each model, we take the maximum score of the reported “direct” prompting performance and the “chain-ofthought (CoT)” prompting performance. Both prompt templates are not publicly available as of the time this paper is written. As a result, Table 3 involves comparisons across multiple prompt templates.
Answer: "
"What determines a buggy's aerodynamic characteristics?
","['cmubuggy_d403.txt', 'cmubuggy_d403.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What determines a buggy's aerodynamic characteristics?

Context: An mural celebrating Buggy can be found in CMU's Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions. Sweepstakes Slang Buggy: Vehicle being raced and also a nickname for the competition. Chute: A section of the freeroll portion of the buggy course (near the southwestern end of Frew Street at its intersection with Schenley Drive) where buggies make the sharp righthand turn from Schenley Drive onto Frew Street. Chute Flagger: Team member who provides a signal for buggy drivers to know when to start the right-hand turn from Schenley Drive onto Frew Street. Driver: Person who travels with a buggy and controls the vehicles via steering and braking systems. Pushbar: Structure attached to a buggy that a person pushes to propel that buggy forward. Pusher: Person who propels a buggy via a pushbar along one of the five hills of the buggy course. Shell: Entire outer structure or covering of a buggy that determines that buggy's aerodynamic characteristics. Transition: Procedure whereby one pusher finishes pushing a buggy and the next pusher in sequence starts to push that same buggy. Members of Fringe, celebrating their 50th year of Buggy racing, allowed a rare visit into its shop for a behind-the-scenes tour. This year Fringe is planning to roll four different vehicles, built and maintained in the Fringe workshop in the basement of the East Campus Garage, known as the ""Froom."" ""We're used to saying everything with 'fr' in front of it but when we say something in front of other people, it gets them confused,"" said Fringe head mechanic Dave Singh, who will graduate in May with a bachelor's degree in mechanical engineering and biomedical engineering. Fringe vehicles often are named with the letter ""B,"" like Boson, Blueshift, Bissa and Bumper. Other teams, such as Apex, often use names that connotate fire, while the SDC (Student Dormitory Committee) team, uses names such as Vice, Bane, Avarice and Malice. This year's buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It's one of the few times spectators can see buggies up close.
This year's buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It's one of the few times spectators can see buggies up close. Dave Singh explains how Fringe builds its buggies. Each team keeps its actual processes a secret, but each buggy has certain features such as a body, pushbar, wheels and driving and braking mechanisms. Behind Singh are other buggies built by Fringe. Behind the Wheel Drivers, have the closest connection to the vehicles, aside from the mechanics who spent countless hours building and maintaining individual buggies. While races are brief, each driver must log a number of practice runs to qualify to race. Teams practice nights and weekends throughout the fall and spring semesters as weather permits. Buggies are often built around drivers, so the fit can be snug, and drivers are often less than 5 feet, 3 inches in height. Most — but not all — are women. Tishya Girdhar, a junior in neuroscience, is social chair for Fringe. She came to CMU wanting to drive, but first she had to get over being claustrophobic. She started as a mechanic and got behind the wheel for last year's Sweepstakes. ""Our team's philosophy is to teach everyone how to do everything,"" Girdhar said. ""But I came to CMU wanting to drive. I wanted to drive so badly."" Amy Chen demonstrates how a driver is positioned inside of Boson, a Fringe buggy built in 2016. Safety is a top priority for teams. All drivers are required to have five pieces of safety gear: mouth guard, goggles, a harness that includes three points of contact to the body of the buggy, gloves and a helmet. And during practice runs and races, flaggers man the course to let drivers know if it's safe to proceed. The streets of the race course haven't changed, but the condition of the course varies due to potholes, which can make or break a fast run. The last course records were set in 2017, but this year potholes are making for some challenging conditions, said Amy Chen, a senior in psychology who has been driving for four years.
Answer: "
"How many times larger was the monoT5-3B ranker compared to the MiniLM ranker used in the InPars-Light study?
","['eric nyberg_3a30217c4115777fb30c182c97cc77d34d065556_metadata.txt', 'eric nyberg_3a30217c4115777fb30c182c97cc77d34d065556_content_2.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many times larger was the monoT5-3B ranker compared to the MiniLM ranker used in the InPars-Light study?

Context: Faculty Name: eric nyberg
Paperid: 3a30217c4115777fb30c182c97cc77d34d065556
Title: InPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers
Year: 2023
Abstract: We carried out a reproducibility study of InPars, which is a method for unsupervised training of neural rankers (Bonifacio et al., 2022). As a by-product, we developed InPars-light, which is a simple-yet-effective modification of InPars. Unlike InPars, InPars-light uses 7x-100x smaller ranking models and only a freely available language model BLOOM, which -- as we found out -- produced more accurate rankers compared to a proprietary GPT-3 model. On all five English retrieval collections (used in the original InPars study) we obtained substantial (7%-30%) and statistically significant improvements over BM25 (in nDCG and MRR) using only a 30M parameter six-layer MiniLM-30M ranker and a single three-shot prompt. In contrast, in the InPars study only a 100x larger monoT5-3B model consistently outperformed BM25, whereas their smaller monoT5-220M model (which is still 7x larger than our MiniLM ranker) outperformed BM25 only on MS MARCO and TREC DL 2020. In the same three-shot prompting scenario, our 435M parameter DeBERTA v3 ranker was at par with the 7x larger monoT5-3B (average gain over BM25 of 1.3 vs 1.32): In fact, on three out of five datasets, DeBERTA slightly outperformed monoT5-3B. Finally, these good results were achieved by re-ranking only 100 candidate documents compared to 1000 used by Bonifacio et al. (2022). We believe that InPars-light is the first truly cost-effective prompt-based unsupervised recipe to train and deploy neural ranking models that outperform BM25. Our code and data is publicly available.
Title: InPars-Light: Cost-Effective Unsupervised Training of Effi- cient Rankers
Authors: Leonid Boytsov, Preksha Patel, Vivek Sourabh, Riddhi Nisar, Sayani Kundu, Ramya Ramanathan, Eric Nyberg
Section: 5 Results
the best-seed outcomes which are presented in § A.3 Table 5. For MiniLM-L6-30M, the all-domain pre-training improves the best-seed accuracy in all cases. For DeBERTA-v3-435M, there is either a substantial degradation or a small decrease/increase that is not statistically significant (denoted by super-script label “c”). Thus, our biggest model—unlike a 15x smaller MiniLM-L6-30M—does not benefit from all-domain pretraining. However, there is no substantial degradation either. Supervised transfer learning with optional unsupervised fine-tuning. We found that our ranking models trained on MS MARCO (both MiniLM-L6-30M and DeBERTA-v3-435M) transferred well to other collections in almost all the cases. However, monoT5 models trained on MS MARCO are still substantially more accurate. According to Table 1, the average gains over BM25 are (1) 1.21 for MiniLM-30M vs. 1.46 for monoT5-200M and (2) 1.42 for DeBERTA-v3-435M vs. 1.59 for monoT5-3B. In that, this gap is not reduced by fine-tuning using synthetically generated data. This is different from the fully unsupervised scenario described above, where MiniLM-L6-30M often outperforms monoT5-220M while DeBERTA-v3-435M is at par with monoT5-3B. This is in line with prior findings that large ranking models have better zero-shot transferring effectiveness (Ni et al., 2021; Rosa et al., 2022). However, using multi-billion parameter models pre-trained on MS MARCO in a commercial setting is problematic from both efficiency and legal standpoints. In particular, MS MARCO has a research-only license.12. Model-type ablation.
Answer: "
"What are the benefits of using IPA over fine-tuning?
","['shinji watanabe_ef567580e167c3e7c546345df93d644be5d4f66f_content_0.txt', 'graham neubig_03c7f61b0c6bc4c480ed6da4e9d7dda104ddfec3_content_2.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the benefits of using IPA over fine-tuning?

Context: Overall, the variation in layer usage for different tasks, models, and modalities strongly motivates the use of the learnable weightedsum technique for evaluation, instead of suboptimally evaluating the final layer alone. 5. HOW DOES INTERMEDIATE-TASK FINE-TUNING AFFECT PERFORMANCE? Studies in natural language processing show that pretrained language models can be improved by initial fine-tuning on an intermediate task, followed by further fine-tuning on the target task [43, 44]. In previous sections, we focus on assessing models pretrained in a self-supervised manner. However, model creators often release models variants that are fine-tuned further for performing specific downstream tasks. For example, MAViL adds 3 Transformer fusion layers after the audio and video encoders, and the whole model is finetuned on (audio&video, class) pairs for audio event classification. We hypothesize that these supervised models variants may provide improved representations for speech/audio tasks after intermediate-task training. In order to support our hypothesis, we additionally evaluate fully fine-tuned variants of AV-HuBERT and MAViL on our benchmark, to determine when intermediate-task fine-tuning is beneficial. The variant of AV-HuBERT uses the same architecture, and is fine-tuned on 433 hours of (video, text) pairs from LRS3-TED to perform visual speech recognition, whereas the MAViL variant is fine-tuned on the entirety of AudioSet-2M. Experiment results are shown
Title: Cross-Modal Fine-Tuning: Align then Refine
Authors: Junhong Shen, Liam Li, Lucio M. Dery, Corey Staten, Mikhail Khodak, Graham Neubig, Ameet Talwalkar
Section: 4.1. A Breadth Perspective: Can Pretrained Models Transfer Across Modalities?
Now, we evaluate the opposite approach, focusing on two tasks: DeepSEA (1D) and Spherical (2D). This evaluation is straightforward to perform by switching the model bodies, since the embedder architecture of ORCA handles all input transformations needed to obtain the sequence features. The results are shown in Table 13 in the Appendix. We see that fine-tuned RoBERTa outperforms Swin on the 1D task, possibly because the DeepSEA data (genomics sequences) are structured more like language than images with discrete units of information and general grammatical rules. More crucially, for both tasks, models with smaller final OTDDs have better fine-tuning accuracy. This suggests a way of selecting pretrained models by comparing the optimized OTDDs and picking the one with the smallest value. Apart from these three key insights, recall that one of our motivations for cross-modal fine-tuning is to help tasks with limited data, where training models from scratch is difficult. Indeed, for vanilla fine-tuning, a small amount of data may not give enough signal to update the pretrained weights, but it is possible to learn a good embedder first with ORCA, which can then make fine-tuning easier. In Figure 4 (right), we vary the dataset size and find that the performance gain of ORCA increases as the dataset size decreases. Meanwhile, using ORCA allows us to match the performance of naive fine-tuning on 3× amount of data. Thus, it can benefit model development in domains where data collection is costly. Beyond the cross-modal setting, we also verify ORCA’s efficacy for in-modality transfer in Appendix A.8.1.
Answer: "
"What is StarCoderBase trained on?
","['daniel fried_3e4085e5869f1b7959707a1e1d7d273b6057eb4e_metadata.txt', 'daniel fried_3e4085e5869f1b7959707a1e1d7d273b6057eb4e_content_3.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is StarCoderBase trained on?

Context: Faculty Name: daniel fried
Paperid: 3e4085e5869f1b7959707a1e1d7d273b6057eb4e
Title: StarCoder: may the source be with you!
Year: 2023
Abstract: The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.
Title: StarCoder: may the source be with you!
Authors: Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nour Fahmy, Urvashi Bhattacharyya, Wenhao Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jan Ebert, Tri Dao, Mayank Mishra, Alex Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries
Section: 1 Introduction
AI documentation such as model cards (Mitchell et al., 2019); – We incorporate a new attribution tool into the VSCode demo that can help users detect and locate model generations that may have been copied from the training set. This is achieved through a two-step process that involves a lightweight membership check followed by a search over a BM25 index (Section 9); and – We have significantly improved the PII redaction pipeline by collecting a PII dataset containing 12,000 files with 22,950 annotated entities. We fine-tuned our own encoder model (StarEncoder) on this dataset, resulting in a robust PII detection model (Section 4).
Answer: "
"Who is the current head coach of men's basketball?
","['Apr-13_Eventno_33_SchoolofArchitecturePavilionDedicationAlumniOpenHouse.txt', 'shinji watanabe_8bc617c9139648d7a92991d70c671230bac7b2e2_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who is the current head coach of men's basketball?

Context: Event: School of Architecture Pavilion Dedication & Alumni Open House
Date: 4/13/24
Time: 1:00 PM-2:30 PM ET
Participants/Audience: Open to School of Architecture Community 
Event Details: 
Drop by the official Spring Carnival entrance pavilion to meet School of Architecture Head Omar Khan and current students who designed and built this NOMAS-led project. 

Note: Registration required. Walk-ins are welcome as space permits. No event fee. This event is open to the Architecture community and their guests.
Faculty Name: shinji watanabe
Paperid: 8bc617c9139648d7a92991d70c671230bac7b2e2
Title: AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head
Year: 2023
Abstract: Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT in solving AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. Our system is publicly available at \url{https://github.com/AIGC-Audio/AudioGPT}.
Authors: Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jia-Bin Huang, Jinglin Liu, Yixiang Ren, Zhou Zhao, Shinji Watanabe
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A multi-modal AI system named AudioGPT is proposed, which complements LLMs with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue.'}
Url: http://arxiv.org/pdf/2304.12995
Answer: "
"In spring 2024, What is the deadline for adding or dropping a Mini-3 course with tuition adjustment?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, What is the deadline for adding or dropping a Mini-3 course with tuition adjustment?

Context: On Monday, 22 January, 2024, during the Spring 2024 (S24) semester, Mini-3 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 29 January, 2024, during the Spring 2024 (S24) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Wednesday, 07 February, 2024, during the Spring 2024 (S24) semester, Mini-3 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 19 February, 2024, during the Spring 2024 (S24) semester, Mini-3 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 19 February, 2024, during the Spring 2024 (S24) semester, Mini-3 Faculty Course Evaluations open is observed.
On Monday, 26 February, 2024, during the Spring 2024 (S24) semester, Semester course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 01 March, 2024, during the Spring 2024 (S24) semester, Mini-3 Last Day of Classes is observed.
On Friday, 01 March, 2024, during the Spring 2024 (S24) semester, Mini-3 voucher deadline (4) is observed.
On Saturday, 02 March, 2024, during the Spring 2024 (S24) semester, Mini-3 Exams is observed.
On Saturday, 02 March, 2024, during the Spring 2024 (S24) semester, Mini-3 Faculty Course Evaluations close is observed.
From 04 March, 2024,Monday to 08 March, 2024,Friday marks the Spring Break for Spring 2024 (S24) semester which leads to No Classes.
On Monday, 11 March, 2024, during the Spring 2024 (S24) semester, First day of Mini-4 Classes is observed.
On Monday, 11 March, 2024, during the Spring 2024 (S24) semester, Mid-Semester & Mini-3 grades due by 4 pm is observed.
On Tuesday, 12 March, 2024, during the Spring 2024 (S24) semester, Summer 2024 Registration Opens is observed.
On Wednesday, 05 February, 2025, during the Spring 2025 (S25) semester, Mini-3 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 17 February, 2025, during the Spring 2025 (S25) semester, Mini-3 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 17 February, 2025, during the Spring 2025 (S25) semester, Mini-3 Faculty Course Evaluations open is observed.
On Monday, 24 February, 2025, during the Spring 2025 (S25) semester, Semester course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 28 February, 2025, during the Spring 2025 (S25) semester, Mini-3 Last Day of Classes is observed.
On Friday, 28 February, 2025, during the Spring 2025 (S25) semester, Mini-3 voucher deadline (4) is observed.
On Saturday, 01 March, 2025, during the Spring 2025 (S25) semester, Mini-3 Exams is observed.
On Saturday, 01 March, 2025, during the Spring 2025 (S25) semester, Mini-3 Faculty Course Evaluations close is observed.
From 03 March, 2025,Monday to 07 March, 2023,Friday marks the Spring Break for Spring 2025 (S25) semester which leads to No Classes.
On Monday, 10 March, 2025, during the Spring 2025 (S25) semester, First day of Mini-4 Classes is observed.
On Tuesday, 11 March, 2025, during the Spring 2025 (S25) semester, Summer 2025 Registration Opens is observed.
On Wednesday, 12 March, 2025, during the Spring 2025 (S25) semester, Mid-Semester & Mini-3 grades due by 4 pm is observed.
On Friday, 14 March, 2025, during the Spring 2025 (S25) semester, Mini-4 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 31 March, 2025, during the Spring 2025 (S25) semester, Semester pass/no pass & withdrawal deadline is observed.
Answer: "
"What was the total number of submissions for the IWSLT 2023 shared tasks?
","['shinji watanabe_f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b_metadata.txt', 'shinji watanabe_f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b_content_3.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What was the total number of submissions for the IWSLT 2023 shared tasks?

Context: Faculty Name: shinji watanabe
Paperid: f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b
Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN
Year: 2023
Abstract: This paper reports on the shared tasks organized by the 20th IWSLT Conference. The shared tasks address 9 scientific challenges in spoken language translation: simultaneous and offline translation, automatic subtitling and dubbing, speech-to-speech translation, multilingual, dialect and low-resource speech translation, and formality control. The shared tasks attracted a total of 38 submissions by 31 teams. The growing interest towards spoken language translation is also witnessed by the constantly increasing number of shared task organizers and contributors to the overview paper, almost evenly distributed across industry and academia.
Authors: Sweta Agrawal, Antonios Anastasopoulos, L. Bentivogli, Ondrej Bojar, Claudia Borg, Marine Carpuat, R. Cattoni, Mauro Cettolo, Mingda Chen, William Chen, K. Choukri, Alexandra Chronopoulou, Anna Currey, T. Declerck, Qianqian Dong, Kevin Duh, Y. Estève, Marcello Federico, Souhir Gahbiche, B. Haddow, B. Hsu, Phu Mon Htut, H. Inaguma, Dávid Javorský, J. Judge, Yasumasa Kano, Tom Ko, Rishu Kumar, Peng Li, Xutai Ma, Prashant Mathur, E. Matusov, Paul McNamee, John P. McCrae, Kenton Murray, Maria Nadejde, Satoshi Nakamura, Matteo Negri, H. Nguyen, J. Niehues, Xing Niu, Atul Kr.
Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN
Authors: Milind Agarwal, Sweta Agrawal, Antonios Anastasopoulos, Luisa Bentivogli, Ondřej Bojar, Claudia Borg, Marine Carpuat, Roldano Cattoni, Mauro Cettolo, Mingda Chen, William Chen, Khalid Choukri, Alexandra Chronopoulou, Anna Currey, Thierry Declerck, Qianqian Dong, Kevin Duh, Yannick Estève, Marcello Federico, Souhir Gahbiche, Barry Haddow, Benjamin Hsu, Phu Mon Htut, Hirofumi Inaguma, Dávid Javorský, John Judge, Yasumasa Kano, Tom Ko, Rishu Kumar, Pengwei Li, Xutai Ma, Prashant Mathur, Evgeny Matusov, Paul McNamee, John P. McCrae, Kenton Murray, Maria Nadejde, Satoshi Nakamura, Matteo Negri, Ha Nguyen, Jan Niehues, Xing Niu, Atul Kr. Ojha, John E. Ortega, Proyag Pal, Juan Pino, Lonneke van der Plas, Peter Polák, Elijah Rippeth, Elizabeth Salesky, Jiatong Shi, Matthias Sperber, Sebastian Stüker, Katsuhito Sudoh, Yun Tang, Brian Thompson, Kevin Tran, Marco Turchi, Alex Waibel, Mingxuan Wang, Shinji Watanabe, Rodolfo Zevallos
Section: 8.4 Results
systems. General Observations As in previous years, the low-resource shared task proved particularly challenging for the participants, but there are several encouraging signs that further reinforce the need for more research in the area. First, more teams than ever participated in the shared task, showing a continued interest in the field. Second, we note that for the language pair that was repeated from last year (Tamasheq– French), almost all submissions outperformed last year’s best submission, with an accuracy increase of more than 17 BLEU points in the unconstrained setting.
Answer: "
"When does the Spring 2025 course registeration start for sophomores?
","['Meta-Data-Calendar-2425.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When does the Spring 2025 course registeration start for sophomores?

Context: Day: 'Monday', Event: 'Mini-3 Faculty Course Evaluations open', Semester: 'Spring 2025 (S25)'
Date: '2025-02-24', Day: 'Monday', Event: 'Semester course drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Spring 2025 (S25)'
Date: '2025-02-28', Day: 'Friday', Event: 'Mini-3 Last Day of Classes ', Semester: 'Spring 2025 (S25)'
Date: '2025-02-28', Day: 'Friday', Event: 'Mini-3 voucher deadline (4)', Semester: 'Spring 2025 (S25)'
Date: '2025-03-01', Day: 'Saturday', Event: 'Mini-3 Exams', Semester: 'Spring 2025 (S25)'
Date: '2025-03-01', Day: 'Saturday', Event: 'Mini-3 Faculty Course Evaluations close', Semester: 'Spring 2025 (S25)'
Start Date: '2025-03-03', End Date: '2023-03-07', Days: 'Monday to Friday', Event: 'Spring Break; No Classes', Semester: 'Spring 2025 (S25)'
Date: '2025-03-10', Day: 'Monday', Event: 'First day of Mini-4 Classes', Semester: 'Spring 2025 (S25)'
Date: '2025-03-11', Day: 'Tuesday', Event: 'Summer 2025 Registration Opens', Semester: 'Spring 2025 (S25)'
Date: '2025-03-12', Day: 'Wednesday', Event: 'Mid-Semester & Mini-3 grades due by 4 pm', Semester: 'Spring 2025 (S25)'
Date: '2025-03-14', Day: 'Friday', Event: 'Mini-4 add, audit & tuition adjustment drop deadline (1)', Semester: 'Spring 2025 (S25)'
Date: '2025-03-31', Day: 'Monday', Event: 'Semester pass/no pass & withdrawal deadline', Semester: 'Spring 2025 (S25)'
Date: '2025-04-02', Day: 'Wednesday', Event: 'Mini-4 course drop deadline; withdrawal grade assigned after this date (2)',
On Tuesday, 12 March, 2024, during the Spring 2024 (S24) semester, Summer 2024 Registration Opens is observed.
On Friday, 15 March, 2024, during the Spring 2024 (S24) semester, Mini-4 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 01 April, 2024, during the Spring 2024 (S24) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Wednesday, 03 April, 2024, during the Spring 2024 (S24) semester, Mini-4 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 11 April, 2024,Thursday to 13 April, 2024,Saturday marks the Spring Carnival for Spring 2024 (S24) semester which leads to No Classes.
From 15 April, 2024,Monday to 19 April, 2024,Friday marks the Fall 2024 Registration Week for Spring 2024 (S24) semester.
On Monday, 15 April, 2024, during the Spring 2024 (S24) semester, Mini-4 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 15 April, 2024, during the Spring 2024 (S24) semester, Semester & Mini-4 Faculty Course Evaluations open is observed.
On Friday, 26 April, 2024, during the Spring 2024 (S24) semester, Last Day of Classes is observed.
On Friday, 26 April, 2024, during the Spring 2024 (S24) semester, Semester & Mini-4 voucher deadline (4) is observed.
From 29 April, 2024,Monday to 30 April, 2024,Tuesday marks the Final Examinations for Spring 2024 (S24) semester.
On Wednesday, 01 May, 2024, during the Spring 2024 (S24) semester, Reading Day is observed.
From 02 May, 2024,Thursday to 03 May, 2024,Friday marks the Final Examinations for Spring 2024 (S24) semester.
From 04 May, 2024,Saturday to 05 May, 2024,Sunday marks the Reading Days for Spring 2024 (S24) semester.
Answer: "
"At what conference was Exploration on HuBERT with Multiple Resolutions published?
","['shinji watanabe_fa75ef55e04e3b25b8af56435478c2fd17403ce8_content_1.txt', 'shinji watanabe_fa75ef55e04e3b25b8af56435478c2fd17403ce8_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: At what conference was Exploration on HuBERT with Multiple Resolutions published?

Context: Title: Exploration on HuBERT with Multiple Resolutions
Authors: Jiatong Shi, Yun Tang, Hirofumi Inaguma, Hongyu Gong, Juan Pino, Shinji Watanabe
Section: 6. References
on sa-unet,” in 2019 4th International Conference on Mechanical, Control and Computer Engineering (ICMCCE), 2019, pp. 818–8183. [21] T. Zhao et al., “Unet++-based multi-channel speech dereverberation and distant speech recognition,” in 2021 12th International Symposium on Chinese Spoken Language Processing (ISCSLP), 2021, pp. 1–5. [22] R. Li et al., “Unet-tts: Improving unseen speaker and style transfer in one-shot voice cloning,” in Proc. ICASSP, 2022, pp. 8327– 8331. [23] X. Xiang, X. Zhang, and H. Chen, “A nested u-net with selfattention and dense connectivity for monaural speech enhancement,” IEEE Signal Processing Letters, vol. 29, pp. 105–109, 2021. [24] Y. Xian et al., “A multi-scale feature recalibration network for end-to-end single channel speech enhancement,” IEEE Journal of Selected Topics in Signal Processing, vol. 15, no. 1, pp. 143– 155, 2020. [25] G. Liu et al., “Cp-GAN: Context pyramid generative adversarial network for speech enhancement,” in Proc. ICASSP, 2020, pp. 6624–6628. [26] X. Xiang, X. Zhang, and H. Chen, “A convolutional network with multi-scale and attention mechanisms for end-to-end single-channel speech enhancement,” IEEE Signal Processing Letters, vol. 28, pp. 1455–1459, 2021. [27] J. Shi et al., “Bridging speech and textual pre-trained models with unsupervised ASR,” arXiv preprint arXiv:2211.03025, 2022. [28] V. Panayotov et al., “Librispeech: An asr corpus based on public domain audio books,” in Proc.
Title: Exploration on HuBERT with Multiple Resolutions
Authors: Jiatong Shi, Yun Tang, Hirofumi Inaguma, Hongyu Gong, Juan Pino, Shinji Watanabe
Section: 6. References
[1] A. Mohamed et al., “Self-supervised speech representation learning: A review,” IEEE Journal of Selected Topics in Signal Processing, 2022. [2] W.-N. Hsu et al., “HuBERT: Self-supervised speech representation learning by masked prediction of hidden units,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451–3460, 2021. [3] S.-w. Yang et al., “SUPERB: Speech Processing Universal PERformance Benchmark,” in Proc. Interspeech, 2021, pp. 1194– 1198. [4] H.-S. Tsai et al., “SUPERB-SG: Enhanced speech processing universal performance benchmark for semantic and generative capabilities,” in Proc. ACL, 2022, pp. 8479–8492. [5] T.-h. Feng et al., “SUPERB@ SLT 2022: Challenge on generalization and efficiency of self-supervised speech representation learning,” in Proc. SLT, 2023, pp. 1096–1103. [6] X. Chang et al., “An exploration of self-supervised pretrained representations for end-to-end speech recognition,” in Proc. ASRU, 2021, pp. 228–235. [7] D. Berrebbi et al., “Combining Spectral and Self-Supervised Features for Low Resource Speech Recognition and Translation,” in Proc. Interspeech, 2022, pp. 3533–3537. [8] S. H. Mallidi and H. Hermansky, “Novel neural network based fusion for multistream asr,” in Proc. ICASSP, 2016, pp. 5680– 5684. [9] S. H. R. Mallidi et al., “A practical and efficient multistream framework for noise robust speech recognition,” Ph.D. dissertation, Johns Hopkins University, 2018.
Answer: "
"How many credits is Human Language for AI worth?
","['graham neubig_74b05bba46db21e589a2cc0f916f81069b0368ef_metadata.txt', 'carolyn rose_27ca2d927421035e10b48c96a96db32224f1f8e6_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many credits is Human Language for AI worth?

Context: Faculty Name: graham neubig
Paperid: 74b05bba46db21e589a2cc0f916f81069b0368ef
Title: Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation
Year: 2023
Abstract: Many recent advances in natural language generation have been fueled by training large language models on internet-scale data. However, this paradigm can lead to models that generate toxic, inaccurate, and unhelpful content, and automatic evaluation metrics often fail to identify these behaviors. As models become more capable, human feedback is an invaluable signal for evaluating and improving models. This survey aims to provide an overview of the recent research that has leveraged human feedback to improve natural language generation. First, we introduce an encompassing formalization of feedback, and identify and organize existing research into a taxonomy following this formalization. Next, we discuss how feedback can be described by its format and objective, and cover the two approaches proposed to use feedback (either for training or decoding): directly using the feedback or training feedback models. We also discuss existing datasets for human-feedback data collection, and concerns surrounding feedback collection. Finally, we provide an overview of the nascent field of AI feedback, which exploits large language models to make judgments based on a set of principles and minimize the need for human intervention.
Authors: Patrick Fernandes, Aman Madaan, Emmy Liu, António Farinhas, Pedro Henrique Martins, Amanda Bertsch, José G. C. de Souza, Shuyan Zhou, Tongshuang Sherry Wu, Graham Neubig, André F. T. Martins
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'An overview of the recent research that has leveraged human feedback to improve natural language generation and the nascent field of AI feedback, which exploits large language models to make judgments based on a set of principles and minimize the need for human intervention is provided.'}
Url: http://arxiv.org/pdf/2305.00955
Faculty Name: carolyn rose
Paperid: 27ca2d927421035e10b48c96a96db32224f1f8e6
Title: Exploring Artificial Intelligence in English Language Arts with StoryQ
Year: 2023
Abstract: Exploring Artificial Intelligence (AI) in English Language Arts (ELA) with StoryQ is a 10-hour curriculum module designed for high school ELA classes. The module introduces students to fundamental AI concepts and essential machine learning workflow using StoryQ, a web-based GUI environment for Grades 6-12 learners. In this module, students work with unstructured text data and learn to train, test, and improve text classification models such as intent recognition, clickbait filter, and sentiment analysis. As they interact with machine-learning language models deeply, students also gain a nuanced understanding of language and how to wield it, not just as a data structure, but as a tool in our human-human encounters as well. The current version contains eight lessons, all delivered through a full-featured online learning and teaching platform. Computers and Internet access are required to implement the module. The module was piloted in an ELA class in the Spring of 2022, and the student learning outcomes were positive. The module is currently undergoing revision and will be further tested and improved in Fall 2022.
Authors: J. Chao, Rebecca Ellis, Shiyan Jiang, C. Rosé, W. Finzer, Can Tatar, James Fiacco, Kenia Wiedemann
Venue: AAAI Conference on Artificial Intelligence
Tldr: {'model': 'tldr@v2.0.0', 'text': None}
Url: https://ojs.aaai.org/index.php/AAAI/article/download/26899/26671
Answer: "
"Who is the LTI director?
","['mona_diab.txt', 'handbook_phd_2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who is the LTI director?

Context: Mona Diab
LTI Director and Tenured Professor, Language Technologies Institute
Research Area : Fairness and Ethics in Language Technology
Contact: 5723 Gates & Hillman Centers
Email : mdiab@andrew.cmu.edu
Phone : 412-268-3669
Address
5000 Forbes Avenue
Pittsburgh, PA 15213
For information about Carnegie Mellon requirements and 
policies, please see the universitys handbook The Word, the Office of Graduate and Postdoctoral 
Affairs web page, the Office of the Dean of Students web page, and other resources contained in 
Appendix A of this handbook. 
Welcome! We hope that your time here is a life-changing experience. 
1.1 
Degrees Offered 
The Language Technologies Institute offers two Ph.D. programs and four Master degrees. 
 Ph.D. in Language and Information Technologies (LTI Ph.D.) 
 Dual-Degree Ph.D. in Language and Information Technologies (CMU-PT Ph.D.) 
 Masters in Language Technologies (MLT) 
 Master of Science in Intelligent Information Systems (MIIS) 
 Master of Computational Data Science (MCDS) 
 Master of Science in Artificial Intelligence and Innovation (MSAII) 
LTI Ph.D. Graduate Student Handbook 
Page 10 
 
This handbook applies to the LTI Ph.D. 
The Ph.D. in Language and Information Technologies (LTI Ph.D.) is focused on understanding 
and extending the state of the art in computational linguistics, natural language processing, 
dialogue systems, information retrieval, machine translation, speech processing, video 
understanding, multimodal systems, automated reasoning, and other topics related to analysis 
and understanding of unstructured information (e.g., machine learning, and software engineering 
of intelligent systems). 
1.2 
Department Personnel 
The people responsible for administering the LTI Ph.D. degree are: 
Jamie Callan 
Ph.D. Program Director 
Professor 
GHC 5419 
callan@cs.cmu.edu 
412-268-4525 
Stacey Young 
Ph.D.
Answer: "
"When did buggy rules change to include a permanent driver and four pushers along the course?
","['cmubuggy_d403.txt', 'cmubuggy_d403.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When did buggy rules change to include a permanent driver and four pushers along the course?

Context: An mural celebrating Buggy can be found in CMU's Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions. Sweepstakes Slang Buggy: Vehicle being raced and also a nickname for the competition. Chute: A section of the freeroll portion of the buggy course (near the southwestern end of Frew Street at its intersection with Schenley Drive) where buggies make the sharp righthand turn from Schenley Drive onto Frew Street. Chute Flagger: Team member who provides a signal for buggy drivers to know when to start the right-hand turn from Schenley Drive onto Frew Street. Driver: Person who travels with a buggy and controls the vehicles via steering and braking systems. Pushbar: Structure attached to a buggy that a person pushes to propel that buggy forward. Pusher: Person who propels a buggy via a pushbar along one of the five hills of the buggy course. Shell: Entire outer structure or covering of a buggy that determines that buggy's aerodynamic characteristics. Transition: Procedure whereby one pusher finishes pushing a buggy and the next pusher in sequence starts to push that same buggy. Members of Fringe, celebrating their 50th year of Buggy racing, allowed a rare visit into its shop for a behind-the-scenes tour. This year Fringe is planning to roll four different vehicles, built and maintained in the Fringe workshop in the basement of the East Campus Garage, known as the ""Froom."" ""We're used to saying everything with 'fr' in front of it but when we say something in front of other people, it gets them confused,"" said Fringe head mechanic Dave Singh, who will graduate in May with a bachelor's degree in mechanical engineering and biomedical engineering. Fringe vehicles often are named with the letter ""B,"" like Boson, Blueshift, Bissa and Bumper. Other teams, such as Apex, often use names that connotate fire, while the SDC (Student Dormitory Committee) team, uses names such as Vice, Bane, Avarice and Malice. This year's buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It's one of the few times spectators can see buggies up close.
This year's buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It's one of the few times spectators can see buggies up close. Dave Singh explains how Fringe builds its buggies. Each team keeps its actual processes a secret, but each buggy has certain features such as a body, pushbar, wheels and driving and braking mechanisms. Behind Singh are other buggies built by Fringe. Behind the Wheel Drivers, have the closest connection to the vehicles, aside from the mechanics who spent countless hours building and maintaining individual buggies. While races are brief, each driver must log a number of practice runs to qualify to race. Teams practice nights and weekends throughout the fall and spring semesters as weather permits. Buggies are often built around drivers, so the fit can be snug, and drivers are often less than 5 feet, 3 inches in height. Most — but not all — are women. Tishya Girdhar, a junior in neuroscience, is social chair for Fringe. She came to CMU wanting to drive, but first she had to get over being claustrophobic. She started as a mechanic and got behind the wheel for last year's Sweepstakes. ""Our team's philosophy is to teach everyone how to do everything,"" Girdhar said. ""But I came to CMU wanting to drive. I wanted to drive so badly."" Amy Chen demonstrates how a driver is positioned inside of Boson, a Fringe buggy built in 2016. Safety is a top priority for teams. All drivers are required to have five pieces of safety gear: mouth guard, goggles, a harness that includes three points of contact to the body of the buggy, gloves and a helmet. And during practice runs and races, flaggers man the course to let drivers know if it's safe to proceed. The streets of the race course haven't changed, but the condition of the course varies due to potholes, which can make or break a fast run. The last course records were set in 2017, but this year potholes are making for some challenging conditions, said Amy Chen, a senior in psychology who has been driving for four years.
Answer: "
"In ""Pengi: An Audio Language Model for Audio Tasks,"" on how many downstream tasks is the model evaluated on?
","['rita singh_ad22af138fa1d1490cda0301abf8159a7c30c5a2_content_0.txt', 'rita singh_ad22af138fa1d1490cda0301abf8159a7c30c5a2_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In ""Pengi: An Audio Language Model for Audio Tasks,"" on how many downstream tasks is the model evaluated on?

Context: Title: Pengi: An Audio Language Model for Audio Tasks
Authors: Soham Deshmukh, Benjamin Elizalde, Rita Singh, Huaming Wang
Section: H Constrastive Learning and Generative Pretraining
We compare our model Pengi with CLAP [17], a state-of-the-art Zero-Shot model that has been evaluated on 16 downstream tasks. However, CLAP is trained on a smaller amount of audio-text data. This leads us to ask: “Is the improved performance due to the larger training data or the generative pretraining?”. We already know that generative pretraining allows us to perform open-ended tasks like Audio Captioning, AQA, which are not possible with contrastive models. But this does not tell us if: generative pretraining is beneficial for close-ended tasks like classification?. To answer this question, we train a CLAP model with the same data 4.1) that we use to train Pengi. We call this model CLAP*. Results. The results are shown in Table 15. We see generative pertaining (Pengi) outperforming contrastive learning (CLAP*) on average. Moreover, with generative pretraining, the model can perform open-ended tasks like Audio Captioning and Audio Question Answering. An interesting observation is Pengi outperforms human performance (81%) on ESC50. Humans have limitations inherent to how much information a participant can handle at once. In the case of ESC50, humans listen to the audio once, and have to remember the audio content, task description, and choose among 50 different classes. Moreover, listeners have different degrees of familiarity with prototypical content from different sound classes, whereas Pengi has been exposed to similar content during training. In a sense, Pengi is an expert listener, whereas the humans in the listening experiment were not.
Faculty Name: rita singh
Paperid: ad22af138fa1d1490cda0301abf8159a7c30c5a2
Title: Pengi: An Audio Language Model for Audio Tasks
Year: 2023
Abstract: In the domain of audio processing, Transfer Learning has facilitated the rise of Self-Supervised Learning and Zero-Shot Learning techniques. These approaches have led to the development of versatile models capable of tackling a wide array of tasks, while delivering state-of-the-art performance. However, current models inherently lack the capacity to produce the requisite language for open-ended tasks, such as Audio Captioning or Audio Question&Answering. We introduce Pengi, a novel Audio Language Model that leverages Transfer Learning by framing all audio tasks as text-generation tasks. It takes as input, an audio recording, and text, and generates free-form text as output. The input audio is represented as a sequence of continuous embeddings by an audio encoder. A text encoder does the same for the corresponding text input. Both sequences are combined as a prefix to prompt a pre-trained frozen language model. The unified architecture of Pengi enables open-ended tasks and close-ended tasks without any additional fine-tuning or task-specific extensions. When evaluated on 22 downstream tasks, our approach yields state-of-the-art performance in several of them. Our results show that connecting language models with audio models is a major step towards general-purpose audio understanding
Authors: Soham Deshmukh, Benjamin Elizalde, Rita Singh, Huaming Wang
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'Pengi is introduced, a novel Audio Language Model that leverages Transfer Learning by framing all audio tasks as text-generation tasks, and shows that connecting language models with audio models is a major step towards general-purpose audio understanding.'}
Url: http://arxiv.org/pdf/2305.11834
Answer: "
"In summer 2024, When is the deadline for Mini-5 vouchers?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-2324.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In summer 2024, When is the deadline for Mini-5 vouchers?

Context: On Friday, 07 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 pass/no pass & withdrawal deadline (3) is observed.
On Friday, 14 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Faculty Course Evaluations open is observed.
On Tuesday, 18 June, 2024, during the Summer One 2024 (M24) semester, Semester course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Wednesday, 19 June, 2022, during the Summer One 2024 (M24) semester, Juneteenth is observed, leading to University Closed & No Classes.
On Thursday, 20 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Last Day of Classes is observed.
On Thursday, 20 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 voucher deadline (4) is observed.
On Friday, 21 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Final Exams is observed.
On Friday, 21 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Faculty Course Evaluations close is observed.
On Monday, 24 June, 2024, during the Summer One 2024 (M24) semester, Mini-6 First Day of Classes is observed.
On Tuesday, 25 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Final Grades Due by 4 pm is observed.
On Friday, 28 June, 2024, during the Summer One 2024 (M24) semester, Mini-6 add, audit, & tuition adjustment drop deadline (1) is observed.
On Thursday, 04 July, 2024, during the Summer One 2024 (M24) semester, Independence Day is observed, leading to University Closed & No Classes.
On Friday, 05 July, 2024, during the Summer One 2024 (M24) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
Event: 'Mini-5 course drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-07', Day: 'Friday', Event: 'Mini-5 pass/no pass & withdrawal deadline (3)', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-14', Day: 'Friday', Event: 'Mini-5 Faculty Course Evaluations open', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-18', Day: 'Tuesday', Event: 'Semester course drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Summer One 2024 (M24)'
Date: '2022-06-19', Day: 'Wednesday', Event: 'Juneteenth; University Closed & No Classes', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-20', Day: 'Thursday', Event: 'Mini-5 Last Day of Classes', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-20', Day: 'Thursday', Event: 'Mini-5 voucher deadline (4)', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-21', Day: 'Friday', Event: 'Mini-5 Final Exams', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-21', Day: 'Friday', Event: 'Mini-5 Faculty Course Evaluations close ', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-24', Day: 'Monday', Event: 'Mini-6 First Day of Classes', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-25', Day: 'Tuesday', Event: 'Mini-5 Final Grades Due by 4 pm', Semester: 'Summer One 2024 (M24)'
Date: '2024-06-28', Day: 'Friday', Event: 'Mini-6 add, audit, & tuition adjustment drop deadline (1)', Semester: 'Summer One 2024 (M24)'
Date: '2024-07-04', Day: 'Thursday', Event: 'Independence Day; University Closed & No Classes',
Answer: "
"What is the name of the initiative introduced to track and incentivize the global development of equitable language technology?
","['graham neubig_17605c43ca3eb982c99642052ddc21a93d116594_metadata.txt', 'david mortensen_17fbffb05fa14e21d1c506fd5f0f568b955fe983_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the name of the initiative introduced to track and incentivize the global development of equitable language technology?

Context: Faculty Name: graham neubig
Paperid: 17605c43ca3eb982c99642052ddc21a93d116594
Title: GlobalBench: A Benchmark for Global Progress in Natural Language Processing
Year: 2023
Abstract: Despite the major advances in NLP, significant disparities in NLP system performance across languages still exist. Arguably, these are due to uneven resource allocation and sub-optimal incentives to work on less resourced languages. To track and further incentivize the global development of equitable language technology, we introduce GlobalBench. Prior multilingual benchmarks are static and have focused on a limited number of tasks and languages. In contrast, GlobalBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring accuracy, GlobalBench also tracks the estimated per-speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served languages are the ones with a relatively high population, but nonetheless overlooked by composite multilingual benchmarks (like Punjabi, Portuguese, and Wu Chinese). Currently, GlobalBench covers 966 datasets in 190 languages, and has 1,128 system submissions spanning 62 languages.
Authors: Yueqi Song, Catherine Cui, Simran Khanuja, Pengfei Liu, FAHIM FAISAL, Alissa Ostapenko, Genta Indra Winata, Alham Fikri Aji, Samuel Cahyawijaya, Yulia Tsvetkov, Antonios Anastasopoulos, Graham Neubig
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work introduces GlobalBench, an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages and tracks the estimated per-speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world.'}
Url: http://arxiv.org/pdf/2305.14716
Faculty Name: david mortensen
Paperid: 17fbffb05fa14e21d1c506fd5f0f568b955fe983
Title: Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models
Year: 2023
Abstract: Language models have graduated from being research prototypes to commercialized products offered as web APIs, and recent works have highlighted the multilingual capabilities of these products. The API vendors charge their users based on usage, more specifically on the number of ``tokens'' processed or generated by the underlying language models. What constitutes a token, however, is training data and model dependent with a large variance in the number of tokens required to convey the same information in different languages. In this work, we analyze the effect of this non-uniformity on the fairness of an API's pricing policy across languages. We conduct a systematic analysis of the cost and utility of OpenAI's language model API on multilingual benchmarks in 22 typologically diverse languages. We show evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results. These speakers tend to also come from regions where the APIs are less affordable to begin with. Through these analyses, we aim to increase transparency around language model APIs' pricing policies and encourage the vendors to make them more equitable.
Authors: Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David R. Mortensen, Noah A. Smith, Yulia Tsvetkov
Venue: Conference on Empirical Methods in Natural Language Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': ""This work conducts a systematic analysis of the cost and utility of OpenAI's language model API on multilingual benchmarks in 22 typologically diverse languages and shows evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results.""}
Url: http://arxiv.org/pdf/2305.13707
Answer: "
"What was David A. Tepper School of Business's original name?
","['fact_sheet_d407.txt', 'cmuhistory_d402.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What was David A. Tepper School of Business's original name?

Context: 13%  International
SCHOOLS AND COLLEGES
College of Engineering 
College of Fine Arts
Dietrich College of Humanities  
and Social Sciences
Heinz College of Information Systems 
and Public Policy
Mellon College of Science
School of Computer Science
Tepper School of Business
cmu.edu/assets/pdfs/cmu-fact-sheet.pdf    |   cmu.edu/about/awards.html
March 2020
BIG IDEAS THAT SHAPE YOUR WORLD START HERE
1,000+ companies greenlighted 
across the U.S. and internationally 
Cognitive tutors, which improve 
the math skills of 500,000+ 
students each year 
The first U.S. school to award  
a degree in drama in 1914
James Gosling (SCS 1983) 
invented the Java computer 
programming language.
The first exclusive higher 
education partner of the Tony 
Awards®, co-creating the first, 
national recognition program 
honoring K-12 theatre educators 
The first smile in an email, created 
in 1982 by professor Scott Fahlman
Kevlar Fiber, invented by alumna 
Stephanie Kwolek (MM 1946)
The Last Lecture, written  
by alumni Randy Pausch (SCS 1988)  
and Jeffrey Zaslow (DC 1980)
In 1919, CMU’s first Ph.D. was 
awarded to Mao Yisheng, father  
of Chinese bridge construction. 
Duolingo, invented by professor  
Luis von Ahn (CS 2003, 2005), had  
10 million downloads in 12 months 
and was named Apple’s 2013 app  
of the year.
:-)
LEADERSHIP
Farnam Jahanian  
President
David Coulter 
Chair, Board of Trustees
James H. Garrett Jr.
The first U.S. drama degree was awarded in 1914 at Carnegie Tech. It began offering graduate degrees. In 1919, the first doctorate (in civil engineering) was awarded to Mao Yisheng, a student from China. It laid the groundwork for a research institution, recruiting leading scientists, offering sponsored fellowships with government and industry leaders and pioneering nontraditional interdisciplinary research, which brought together physicists, chemists and metallurgists, for example. Interdisciplinary research would become the hallmark of Carnegie Mellon research. It initiated the 'Carnegie Plan' in 1938, a new curriculum that required science and engineer students to take courses in humanities and social sciences in order to better understand the needs of society. Carnegie died in 1919, but his vision for an educated public lived on after him. Carnegie Tech - Post-war Years With the end of World War II, the latter half of the 20th century brought unprecedented growth to Carnegie Tech. In 1956, the arrival of the first IBM computer to campus was revolutionary, initiating a university culture where information technology pervaded virtually all areas of study. University culture also changed in 1973 when Margaret Morrison closed and women joined their male peers in classrooms and dorms. The times were changing, and Tech positioned itself at the forefront, opening three new schools: 1948: The Graduate School of Industrial Administration, later renamed the David A. Tepper School of Business, focusing on quantitative analysis and pioneering the field of management science. 1968: School of Urban and Public Affairs, later renamed the H. John Heinz III College, providing graduate training for work in the public sector. 1986: School of Computer Science, pioneering computing and artificial intelligence, led by interdisciplinary efforts of Allen Newell and Herbert Simon. Carnegie Mellon University In 1967, Carnegie Tech merged with the Mellon Institute, a science research center founded by the Mellon family of Pittsburgh, to become known as Carnegie Mellon University. The merger built upon a long history of support from the Mellons. It allowed Carnegie Mellon to establish the last of its current pillars: the Mellon College of Science and the College of Humanities and Social Sciences, now known as Marianna Brown Dietrich College of Humanities and Social Sciences.
Answer: "
"In spring 2024, How many units is course 10605?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, How many units is course 10605?

Context: In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Independent Study: Research' with Course ID 10620 and Section A offers 612 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Shah located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Generative AI' with Course ID 10623 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Gormley, Li, Lipton located in Building GHC, Room 4401.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Practicum' with Course ID 10635 and Section I offers VAR units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Shah located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Reading and Research' with Course ID 10697 and Section A offers 1-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Shah located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Introduction to Machine Learning (PhD)' with Course ID 10701 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Chai located in Building TEP, Room 1403.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Advanced Deep Learning' with Course ID 10707 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Salakhutdinov located in Building DH, Room 2302.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Independent Study: Research' with Course ID 10620 and Section A offers 612 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Shah located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Generative AI' with Course ID 10623 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Gormley, Li, Lipton located in Building GHC, Room 4401.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Practicum' with Course ID 10635 and Section I offers VAR units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Shah located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Reading and Research' with Course ID 10697 and Section A offers 1-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Shah located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Introduction to Machine Learning (PhD)' with Course ID 10701 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Chai located in Building TEP, Room 1403.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Advanced Deep Learning' with Course ID 10707 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Salakhutdinov located in Building DH, Room 2302.
Answer: "
"Who propels a buggy via a pushbar along one of the five hills of the buggy course?
","['cmubuggy_d403.txt', 'cmubuggy_d403.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who propels a buggy via a pushbar along one of the five hills of the buggy course?

Context: An mural celebrating Buggy can be found in CMU's Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions. Sweepstakes Slang Buggy: Vehicle being raced and also a nickname for the competition. Chute: A section of the freeroll portion of the buggy course (near the southwestern end of Frew Street at its intersection with Schenley Drive) where buggies make the sharp righthand turn from Schenley Drive onto Frew Street. Chute Flagger: Team member who provides a signal for buggy drivers to know when to start the right-hand turn from Schenley Drive onto Frew Street. Driver: Person who travels with a buggy and controls the vehicles via steering and braking systems. Pushbar: Structure attached to a buggy that a person pushes to propel that buggy forward. Pusher: Person who propels a buggy via a pushbar along one of the five hills of the buggy course. Shell: Entire outer structure or covering of a buggy that determines that buggy's aerodynamic characteristics. Transition: Procedure whereby one pusher finishes pushing a buggy and the next pusher in sequence starts to push that same buggy. Members of Fringe, celebrating their 50th year of Buggy racing, allowed a rare visit into its shop for a behind-the-scenes tour. This year Fringe is planning to roll four different vehicles, built and maintained in the Fringe workshop in the basement of the East Campus Garage, known as the ""Froom."" ""We're used to saying everything with 'fr' in front of it but when we say something in front of other people, it gets them confused,"" said Fringe head mechanic Dave Singh, who will graduate in May with a bachelor's degree in mechanical engineering and biomedical engineering. Fringe vehicles often are named with the letter ""B,"" like Boson, Blueshift, Bissa and Bumper. Other teams, such as Apex, often use names that connotate fire, while the SDC (Student Dormitory Committee) team, uses names such as Vice, Bane, Avarice and Malice. This year's buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It's one of the few times spectators can see buggies up close.
Buggy Races Keep Rolling at Carnegie Mellon - News - Carnegie Mellon University Carnegie Mellon University ——— Search Search Search this site only News News › Stories › Archives › 2019 › April › Buggy Races Keep Rolling at Carnegie Mellon April 10, 2019 Buggy Races Keep Rolling at Carnegie Mellon In its 99th year, the tradition is a Spring Carnival treat By Heidi Opdyke opdyke(through)andrew.cmu.edu Media Inquiries Julie Mattera Marketing and Communications jmattera(through)cmu.edu 412-268-2902 Sweepstakes, also known as the Buggy Races, has come a long way at Carnegie Mellon University. The slick, torpedo-like vessels carrying drivers with nerves of steel are a far cry from the two-man teams that once changed places halfway through a race and rode in everything from rain barrels with bicycle wheels to three-wheeled ash cans 99 years ago. Today, it takes six people to maneuver the .84 -mile course around Schenley Park's Flagstaff Hill. But while five pushers and a driver navigate the course's hills, dozens of people are needed to make a successful race happen. A year of planning goes into just over two minutes of racing. The Machine The basics of a buggy are straightforward, but teams are often secretive in how they build the machines, in particular the way they brake, steer and what types of wheels are used. Each has a body, pushbar for runners to move the machine up the hills, wheels, a safety harness and driving and braking mechanisms. Some also include fairings, a type of housing around the wheels that help reduce drag, make the vehicle quieter and just looks cool. Fairings have been a key feature for the Fringe team in recent years, which is celebrating its 50th anniversary of Buggy. ""We have a reputation of being the quietest on the course,"" said Diya Nuxoll, who wrapped up her bachelor's degree in mechanical engineering in December and is working on an advanced studies master's degree in design and manufacturing. Nuxoll leads the Fringe team as one of two head mechanics. An mural celebrating Buggy can be found in CMU's Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions. Sweepstakes Slang Buggy: Vehicle being raced and also a nickname for the competition.
Answer: "
"Did Andy Warhol graduate from CMU? Answer with True or False
","['history_d401.txt', 'fact_sheet_d407.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Did Andy Warhol graduate from CMU? Answer with True or False

Context: The experiment, which lasted two years, was an unqualified success. Separately, and also in 1986, the Pittsburgh Supercomputing Center was created as a joint effort between CMU, the University of Pittsburgh and Westinghouse Electric Corp.SCS is officially formedCMU’s Faculty Senate in the fall of 1988 agreed to President Richard Cyert’s plan to elevate the Department of Computer Science to college status. In addition to the Computer Science Department, SCS also incorporated the Robotics Institute, the Center for Machine Translation, and researchers from the Information Technology Center, which had developed Andrew. On Dec. 13, 1988, Cyert (H’89) told faculty and staff that Habermann had been appointed CMU’s first Dean of Computer Science, effective Dec. 1, and that the School of Computer Science would soon begin operations. SCS made its formal debut on Dec. 22, 1988, with a reception in the Wherrett Room of Skibo Hall, CMU’s student union. The official announcement of CMU’s new “graduate School of Computer Science” was made Jan. 3, 1989.Undergraduate degrees beginFor several years, undergraduates interested in computer science pursued an “applied math/CS” bachelor’s degree offered by the Mathematics Department. CSD professor Mary Shaw (CS’72) led CMU’s first effort to design an undergraduate curriculum solely in computer science. She and her colleagues were guided by the Carnegie Plan—guidelines established in 1938 under Carnegie Tech President Robert Doherty (A’40, E’48, H’50), outlining the principles of a sound professional education. Drawing on Shaw’s plan and also on the work of other faculty members, an undergraduate program in computer science was created during the 1989-90 academic year. Seven CS majors were admitted to the program as sophomores. Another 73 undergraduates were admitted in 1990–91.
:-)
LEADERSHIP
Farnam Jahanian  
President
David Coulter 
Chair, Board of Trustees
James H. Garrett Jr. 
Provost
2 National Academy of Engineering       3 National Academy of Sciences        4 National Academy of Medicine
Won by alumni and current/former faculty
The leading university center 
for cybersecurity, providing 
support to more than 110 
centers around the world 
The world’s first university 
robotics department,  
founded in 1979
Alumnus Andy Warhol  
(CFA1949), pop artist pioneer  
and cultural icon
One of only 29 universities 
invited to be a member of the 
World Economic Forum’s 
Global University  
Leaders Forum
CMU.EDU
ECONOMIC IMPACT
• Attracting major companies —  
including Google, Intel, Uber and GE —  
to locate operations and create new  
jobs in Pittsburgh
• To date, the CMU community has 
launched more than 400 startups 
and created more than 152 spinoff 
companies.
• Contributing to the cultural and civic life of  
the city with performances, exhibitions 
and research collaborations
13
ACADEMY  
AWARDS
65
MEMBERS  
OF NAE2
146
EMMY 
AWARDS
20
MEMBERS 
OF NAS3
6
MEMBERS 
OF NAM4
13
TURING 
AWARDS
20
NOBEL 
LAUREATES
58
TONY 
AWARDS
May 2023
Answer: "
"What tasks does ESPnet-ST-v2 support?
","['shinji watanabe_dab8e7dc79085774eea58bcb9ea2ed0ee20377eb_metadata.txt', 'shinji watanabe_dab8e7dc79085774eea58bcb9ea2ed0ee20377eb_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What tasks does ESPnet-ST-v2 support?

Context: Faculty Name: shinji watanabe
Paperid: dab8e7dc79085774eea58bcb9ea2ed0ee20377eb
Title: ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit
Year: 2023
Abstract: ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broadening interests of the spoken language translation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simultaneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) – each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language translation toolkits. This toolkit offers state-of-the-art architectures such as transducers, hybrid CTC/attention, multi-decoders with searchable intermediates, time-synchronous blockwise CTC/attention, Translatotron models, and direct discrete unit models. In this paper, we describe the overall design, example models for each task, and performance benchmarking behind ESPnet-ST-v2, which is publicly available at https://github.com/espnet/espnet.
Authors: Brian Yan, Jiatong Shi, Yun Tang, H. Inaguma, Yifan Peng, Siddharth Dalmia, Peter Pol'ak, Patrick Fernandes, Dan Berrebbi, Tomoki Hayashi, Xiaohui Zhang, Zhaoheng Ni, Moto Hira, Soumi Maiti, J. Pino, Shinji Watanabe
Venue: Annual Meeting of the Association for Computational Linguistics
Tldr: {'model': 'tldr@v2.0.0', 'text': 'The overall design, example models for each task, and performance benchmarking behind ESPnet-ST-v2 are described, which is publicly available at https://github.com/espnet/esp net.'}
Url: https://arxiv.org/pdf/2304.04596
Title: ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit
Authors: Brian Yan, Jiatong Shi, Yun Tang, Hirofumi Inaguma, Yifan Peng, Siddharth Dalmia, Peter Polák, Patrick Fernandes, Dan Berrebbi, Tomoki Hayashi, Xiaohui Zhang, Zhaoheng Ni, Moto Hira, Soumi Maiti, Juan Pino, Shinji Watanabe
Section: 3.2 Key Features
(e.g. BLEU) to compare and rank n-best outputs from one or more models. For S2ST, neural vocoders are supported for both spectral and discrete inputs (Hayashi et al., 2020, 2021).
Answer: "
"In the Paaploss paper, what does the neural network estimator developed in the study predict?
","['shinji watanabe_611f9ee6eef0936462cd78f371798d0699951c59_metadata.txt', 'bhiksha raj_611f9ee6eef0936462cd78f371798d0699951c59_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the Paaploss paper, what does the neural network estimator developed in the study predict?

Context: Faculty Name: shinji watanabe
Paperid: 611f9ee6eef0936462cd78f371798d0699951c59
Title: Paaploss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement
Year: 2023
Abstract: Despite rapid advancement in recent years, current speech enhancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics. We identify temporal acoustic parameters – such as spectral tilt, spectral flux, shimmer, etc. – that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-specific weights for each feature, as the acoustic parameters are known to show different behavior in different phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement workflows in both time-domain and time-frequency domain, as measured by standard evaluation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement.
Authors: Muqiao Yang, Joseph Konan, David Bick, YUNYANG ZENG, Shuo Han, Anurag Kumar, Shinji Watanabe, B. Raj
Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A learning objective is proposed that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics, and a neural network estimator is developed that can accurately predict their time-series values across an utterance.'}
Url: https://arxiv.org/pdf/2302.08095
Faculty Name: bhiksha raj
Paperid: 611f9ee6eef0936462cd78f371798d0699951c59
Title: Paaploss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement
Year: 2023
Abstract: Despite rapid advancement in recent years, current speech enhancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics. We identify temporal acoustic parameters – such as spectral tilt, spectral flux, shimmer, etc. – that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-specific weights for each feature, as the acoustic parameters are known to show different behavior in different phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement workflows in both time-domain and time-frequency domain, as measured by standard evaluation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement.
Authors: Muqiao Yang, Joseph Konan, David Bick, YUNYANG ZENG, Shuo Han, Anurag Kumar, Shinji Watanabe, B. Raj
Venue: IEEE International Conference on Acoustics, Speech, and Signal Processing
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A learning objective is proposed that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics, and a neural network estimator is developed that can accurately predict their time-series values across an utterance.'}
Url: https://arxiv.org/pdf/2302.08095
Answer: "
"When did Kappa Kappa Gamma enter the first all-women’s team in buggy history?
","['cmubuggy_d403.txt', 'eric xing_279aeb0ffdaec08391f6a50695e2b01d6a148b7e_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When did Kappa Kappa Gamma enter the first all-women’s team in buggy history?

Context: This year's buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It's one of the few times spectators can see buggies up close. Dave Singh explains how Fringe builds its buggies. Each team keeps its actual processes a secret, but each buggy has certain features such as a body, pushbar, wheels and driving and braking mechanisms. Behind Singh are other buggies built by Fringe. Behind the Wheel Drivers, have the closest connection to the vehicles, aside from the mechanics who spent countless hours building and maintaining individual buggies. While races are brief, each driver must log a number of practice runs to qualify to race. Teams practice nights and weekends throughout the fall and spring semesters as weather permits. Buggies are often built around drivers, so the fit can be snug, and drivers are often less than 5 feet, 3 inches in height. Most — but not all — are women. Tishya Girdhar, a junior in neuroscience, is social chair for Fringe. She came to CMU wanting to drive, but first she had to get over being claustrophobic. She started as a mechanic and got behind the wheel for last year's Sweepstakes. ""Our team's philosophy is to teach everyone how to do everything,"" Girdhar said. ""But I came to CMU wanting to drive. I wanted to drive so badly."" Amy Chen demonstrates how a driver is positioned inside of Boson, a Fringe buggy built in 2016. Safety is a top priority for teams. All drivers are required to have five pieces of safety gear: mouth guard, goggles, a harness that includes three points of contact to the body of the buggy, gloves and a helmet. And during practice runs and races, flaggers man the course to let drivers know if it's safe to proceed. The streets of the race course haven't changed, but the condition of the course varies due to potholes, which can make or break a fast run. The last course records were set in 2017, but this year potholes are making for some challenging conditions, said Amy Chen, a senior in psychology who has been driving for four years.
Faculty Name: eric xing
Paperid: 279aeb0ffdaec08391f6a50695e2b01d6a148b7e
Title: The First LHAASO Catalog of Gamma-Ray Sources
Year: 2023
Abstract: 
 We present the first catalog of very-high-energy and ultra-high-energy gamma-ray sources detected by the Large High Altitude Air Shower Observatory. The catalog was compiled using 508 days of data collected by the Water Cherenkov Detector Array from 2021 March to 2022 September and 933 days of data recorded by the Kilometer Squared Array from 2020 January to 2022 September. This catalog represents the main result from the most sensitive large coverage gamma-ray survey of the sky above 1 TeV, covering decl. from −20° to 80°. In total, the catalog contains 90 sources with an extended size smaller than 2° and a significance of detection at >5σ. Based on our source association criteria, 32 new TeV sources are proposed in this study. Among the 90 sources, 43 sources are detected with ultra-high energy (E > 100 TeV) emission at >4σ significance level. We provide the position, extension, and spectral characteristics of all the sources in this catalog.
Authors: Z. Cao, F. Aharonian, Q. An, Axikegu, Y. Bai, Y. Bao, D. Bastieri, X. Bi, Y. Bi, J. Cai, Q. Cao, W. Cao, Z. Cao, J. Chang, J. Chang, A. Chen, E. Chen, Liang Chen, Lin Chen, Long Chen, M. Chen, M. Chen, Q. Chen, S. Chen, S. Z. Chen, T. Chen, Y. Chen, N. Cheng, Y. Cheng, M. Cui, S. Cui, X. Cui, Y. Cui, B. Dai, H. Dai, Z. Dai, Danzengluobu, D. Volpe, X. Dong, K. Duan, J. Fan, Y. Z. Fan, J. Fang, K. Fang, C. Feng, L. Feng, S. Feng, X. Feng, Y. Feng, S. Gabici, B. Gao, C.
Answer: "
"In fall 2023, What is the title of course 05318?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, What is the title of course 05318?

Context: In Semester Fall 2023, from the department of Institute for Politics and Strategy, the subject titled 'International Organizations and Law' with Course ID 84313 and Section A2 offers 6.0 units. The Class meets Monday between 02:30PM and 05:20PM ET. Students attend lectures at the Washington, District of Columbia:Dulles location,led by experienced instructor Grise located in Building MDC, Room 522.
In Semester Fall 2023, from the department of Institute for Politics and Strategy, the subject titled 'Military Strategy and Doctrine' with Course ID 84328 and Section A offers 9.0 units. The Class meets Monday Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Toukan located in Building POS, Room 146.
In Semester Fall 2023, from the department of Institute for Politics and Strategy, the subject titled 'Asian Strategies' with Course ID 84329 and Section A2 offers 6.0 units. The Class meets Monday Wednesday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Cronin located in Building WEH, Room 5320.
In Semester Fall 2023, from the department of Institute for Politics and Strategy, the subject titled 'US China Relations' with Course ID 84335 and Section A1 offers 6.0 units. The Class meets Wednesday between 05:30PM and 08:20PM ET. Students attend lectures at the Washington, District of Columbia:Dulles location,led by experienced instructor Hansen located in Building MDC, Room 522.
In Semester Fall 2023, from the department of Institute for Politics and Strategy, the subject titled 'Seminar in Public Policy Research' with Course ID 84339 and Section A offers 12.0 units. The Class meets Tuesday between 05:30PM and 08:20PM ET. Students attend lectures at the Washington, District of Columbia:Dulles location,led by experienced instructor Marcellino located in Building MDC, Room 522.
In Semester Fall 2023, from the department of Institute for Politics and Strategy, the subject titled 'Bias, Objectivity, and the Media's Role in Politics' with Course ID 84351 and Section A2 offers 6.0 units.
In Semester Fall 2023, from the department of Institute for Politics and Strategy, the subject titled 'International Organizations and Law' with Course ID 84313 and Section A2 offers 6.0 units. The Class meets Monday between 02:30PM and 05:20PM ET. Students attend lectures at the Washington, District of Columbia:Dulles location,led by experienced instructor Grise located in Building MDC, Room 522.
In Semester Fall 2023, from the department of Institute for Politics and Strategy, the subject titled 'Military Strategy and Doctrine' with Course ID 84328 and Section A offers 9.0 units. The Class meets Monday Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Toukan located in Building POS, Room 146.
In Semester Fall 2023, from the department of Institute for Politics and Strategy, the subject titled 'Asian Strategies' with Course ID 84329 and Section A2 offers 6.0 units. The Class meets Monday Wednesday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Cronin located in Building WEH, Room 5320.
In Semester Fall 2023, from the department of Institute for Politics and Strategy, the subject titled 'US China Relations' with Course ID 84335 and Section A1 offers 6.0 units. The Class meets Wednesday between 05:30PM and 08:20PM ET. Students attend lectures at the Washington, District of Columbia:Dulles location,led by experienced instructor Hansen located in Building MDC, Room 522.
In Semester Fall 2023, from the department of Institute for Politics and Strategy, the subject titled 'Seminar in Public Policy Research' with Course ID 84339 and Section A offers 12.0 units. The Class meets Tuesday between 05:30PM and 08:20PM ET. Students attend lectures at the Washington, District of Columbia:Dulles location,led by experienced instructor Marcellino located in Building MDC, Room 522.
In Semester Fall 2023, from the department of Institute for Politics and Strategy, the subject titled 'Bias, Objectivity, and the Media's Role in Politics' with Course ID 84351 and Section A2 offers 6.0 units.
Answer: "
"When does the Spring Carnival start in Spring 2025 semester?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When does the Spring Carnival start in Spring 2025 semester?

Context: On Monday, 31 March, 2025, during the Spring 2025 (S25) semester, Semester pass/no pass & withdrawal deadline is observed.
On Wednesday, 02 April, 2025, during the Spring 2025 (S25) semester, Mini-4 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 03 April, 2025,Thursday to 05 April, 2023,Saturday marks the Spring Carnival for Spring 2025 (S25) semester which leads to No Classes.
From 07 April, 2025,Monday to 11 April, 2023,Friday marks the Fall 2025 Registration Week for Spring 2025 (S25) semester.
On Monday, 14 April, 2025, during the Spring 2025 (S25) semester, Mini-4 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 14 April, 2025, during the Spring 2025 (S25) semester, Semester & Mini-4 Faculty Course Evaluations open is observed.
On Friday, 25 April, 2025, during the Spring 2025 (S25) semester, Last Day of Classes is observed.
On Friday, 25 April, 2025, during the Spring 2025 (S25) semester, Semester & Mini-4 voucher deadline (4) is observed.
From 28 April, 2025,Monday to 29 April, 2023,Tuesday marks the Final Examinations for Spring 2025 (S25) semester.
On Wednesday, 30 April, 2025, during the Spring 2025 (S25) semester, Reading Day is observed.
From 01 May, 2025,Thursday to 02 May, 2023,Friday marks the Final Examinations for Spring 2025 (S25) semester.
From 03 May, 2025,Saturday to 04 May, 2023,Sunday marks the Reading Days for Spring 2025 (S25) semester.
On Monday, 05 May, 2025, during the Spring 2025 (S25) semester, Final Examinations is observed.
On Monday, 05 May, 2025, during the Spring 2025 (S25) semester, Semester & Mini-4 Faculty Course Evaluations close is observed.
On Tuesday, 12 March, 2024, during the Spring 2024 (S24) semester, Summer 2024 Registration Opens is observed.
On Friday, 15 March, 2024, during the Spring 2024 (S24) semester, Mini-4 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 01 April, 2024, during the Spring 2024 (S24) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Wednesday, 03 April, 2024, during the Spring 2024 (S24) semester, Mini-4 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 11 April, 2024,Thursday to 13 April, 2024,Saturday marks the Spring Carnival for Spring 2024 (S24) semester which leads to No Classes.
From 15 April, 2024,Monday to 19 April, 2024,Friday marks the Fall 2024 Registration Week for Spring 2024 (S24) semester.
On Monday, 15 April, 2024, during the Spring 2024 (S24) semester, Mini-4 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 15 April, 2024, during the Spring 2024 (S24) semester, Semester & Mini-4 Faculty Course Evaluations open is observed.
On Friday, 26 April, 2024, during the Spring 2024 (S24) semester, Last Day of Classes is observed.
On Friday, 26 April, 2024, during the Spring 2024 (S24) semester, Semester & Mini-4 voucher deadline (4) is observed.
From 29 April, 2024,Monday to 30 April, 2024,Tuesday marks the Final Examinations for Spring 2024 (S24) semester.
On Wednesday, 01 May, 2024, during the Spring 2024 (S24) semester, Reading Day is observed.
From 02 May, 2024,Thursday to 03 May, 2024,Friday marks the Final Examinations for Spring 2024 (S24) semester.
From 04 May, 2024,Saturday to 05 May, 2024,Sunday marks the Reading Days for Spring 2024 (S24) semester.
Answer: "
"Which languages are included in the dataset released by ""Multi-lingual and Multi-cultural Figurative Language Understanding""?
","['graham neubig_d5dd7230cccace7e77095d3b5fd8394850f59170_metadata.txt', 'graham neubig_d5dd7230cccace7e77095d3b5fd8394850f59170_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which languages are included in the dataset released by ""Multi-lingual and Multi-cultural Figurative Language Understanding""?

Context: Faculty Name: graham neubig
Paperid: d5dd7230cccace7e77095d3b5fd8394850f59170
Title: Multi-lingual and Multi-cultural Figurative Language Understanding
Year: 2023
Abstract: Figurative language permeates human communication, but at the same time is relatively understudied in NLP. Datasets have been created in English to accelerate progress towards measuring and improving figurative language processing in language models (LMs). However, the use of figurative language is an expression of our cultural and societal experiences, making it difficult for these phrases to be universally applicable. In this work, we create a figurative language inference dataset, \datasetname, for seven diverse languages associated with a variety of cultures: Hindi, Indonesian, Javanese, Kannada, Sundanese, Swahili and Yoruba. Our dataset reveals that each language relies on cultural and regional concepts for figurative expressions, with the highest overlap between languages originating from the same region. We assess multilingual LMs' abilities to interpret figurative language in zero-shot and few-shot settings. All languages exhibit a significant deficiency compared to English, with variations in performance reflecting the availability of pre-training and fine-tuning data, emphasizing the need for LMs to be exposed to a broader range of linguistic and cultural variation during training.
Authors: Anubha Kabra, Emmy Liu, Simran Khanuja, Alham Fikri Aji, Genta Indra Winata, Samuel Cahyawijaya, Anuoluwapo Aremu, Perez Ogayo, Graham Neubig
Venue: Annual Meeting of the Association for Computational Linguistics
Tldr: {'model': 'tldr@v2.0.0', 'text': ""This work assesses multilingual LMs' abilities to interpret figurative language in zero-shot and few-shot settings, and reveals that each language relies on cultural and regional concepts for figurative expressions, with the highest overlap between languages originating from the same region.""}
Url: http://arxiv.org/pdf/2305.16171
Title: Multi-lingual and Multi-cultural Figurative Language Understanding
Authors: Anubha Kabra, Emmy Liu, Simran Khanuja, Alham Fikri Aji, Genta Indra Winata, Samuel Cahyawijaya, Anuoluwapo Aremu, Perez Ogayo, Graham Neubig
Section: 1 Introduction
culture, such as food, mythology, famous people, or plants and animals native to specific regions. We benchmark multilingual model performance (§5) and analyze model failures (§6), finding that zero-shot performance of multilingual models is relatively poor, especially for lower-resource languages. According to (Liu et al., 2021), main factors which poses challenges on the performance in such cases are cross-lingual transfer and concept shift across languages. However, we observe that concept shift seems to play a larger role due to culturally specific examples. Adding a few examples in the target language can improve performance of larger models, but this is more beneficial for lower-resource languages. This highlights the importance of including culturally relevant training data, particularly data that highlights not just the existence of a concept, but also how people view that concept within that culture.
Answer: "
"Which faculty were involved in the CSurF paper?
","['jamie callan_6b7eefa15c0a461afeab4fa13cf862c5340fdc2a_metadata.txt', 'jamie callan_6b7eefa15c0a461afeab4fa13cf862c5340fdc2a_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which faculty were involved in the CSurF paper?

Context: Faculty Name: jamie callan
Paperid: 6b7eefa15c0a461afeab4fa13cf862c5340fdc2a
Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms
Year: 2023
Abstract: Lexical exact-match systems perform text retrieval efficiently with sparse matching signals and fast retrieval through inverted lists, but naturally suffer from the mismatch between lexical surface form and implicit term semantics. This paper proposes to directly bridge the surface form space and the term semantics space in lexical exact-match retrieval via contextualized surface forms (CSF). Each CSF pairs a lexical surface form with a context source, and is represented by a lexical form weight and a contextualized semantic vector representation. This framework is able to perform sparse lexicon-based retrieval by learning to represent each query and document as a ""bag-of-CSFs"", simultaneously addressing two key factors in sparse retrieval: vocabulary expansion of surface form and semantic representation of term meaning. At retrieval time, it efficiently matches CSFs through exact-match of learned surface forms, and effectively scores each CSF pair via contextual semantic representations, leading to joint improvement in both term match and term scoring. Multiple experiments show that this approach successfully resolves the main mismatch issues in lexical exact-match retrieval and outperforms state-of-the-art lexical exact-match systems, reaching comparable accuracy as lexical all-to-all soft match systems as an efficient exact-match-based system.
Authors: Zhen Fan, Luyu Gao, Jamie Callan
Venue: International Conference on the Theory of Information Retrieval
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes to directly bridge the surface form space and the term semantics space in lexical exact-match retrieval via contextualized surface forms (CSF), reaching comparable accuracy as lexical all-to-all soft match systems as an efficient exact- match-based system.'}
Url: https://dl.acm.org/doi/pdf/10.1145/3578337.3605126
Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms
Authors: Zhen Fan, Luyu Gao, Jamie Callan
Section: 6 CONCLUSION
This paper proposes CSurF, which performs sparse lexicon-based retrieval through constructing and matching Contextualized Surface Forms. Its retrieval process combines efficient surface form exact match and fine-grained contextualized semantic scoring, which leads to maximized model capacity while maintaining the simplicity and efficiency of exact-match-based retrieval systems. CSurF extends current term-weight based learned sparse retrieval approaches with vector term representations. On experiments across multiple datasets and retrieval settings, CSurF is able to simultaneously bridge the vocabulary and semantic mismatch in exact-match retrieval, and achieve state-of-the-art retrieval performance for lexical exact-match systems. Ablation studies and analysis further demonstrate CSurF’s ability to jointly expandmeaningful surface forms and ground surface forms to underlying semantics, which leads to increased model capacity. We also propose a simple interpolation approach in out-of-domain retrieval settings, to analyze the effect of original text vs. expanded surface forms as well as the quality of lexical form expansion on different retrieval tasks. Compared to all-to-all soft-match retrievers, CSurF achieves comparable performance across all retrieval tasks as an exact-matchbased retrieval system. CSurF is able to learn sparse connections of the original query and document terms, resolving the key efficiency issue of lexical soft-match. The retrieval efficiency of CSurF can also be further optimized with different approaches including training regularization adjustment, post-hoc index pruning, and vector representation approximation or dimension control, without significantly affecting retrieval accuracy. We hope this work encourages more research on building effective, efficient, robust and knowledge-enhanced sparse retrieval systems in the real world, as well as exploring the connection and distinction among current retrieval frameworks and systems.
Answer: "
"In fall 2023, Who are the instructors for course 05380?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, Who are the instructors for course 05380?

Context: In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Interaction Design for Human-Computer Interaction' with Course ID 05360 and Section D offers 12.0 units. The Class meets Tuesday Thursday between 12:30PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Matthews, Saunders located in Building 4SC, Room 104.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Interaction Design Fundamentals:' with Course ID 05360 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Interaction Design for Human-Computer Interaction' with Course ID 05360 and Section E offers 12.0 units. The Class meets Tuesday Thursday between 05:00PM and 07:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Shelly, Beck located in Building 4SC, Room 104.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Prototyping Algorithmic Experiences' with Course ID 05380 and Section A offers 15.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Holstein located in Building GHC, Room 4301.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Designing Human Centered Software:' with Course ID 05391 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Designing Human-Centered Software' with Course ID 05391 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Interaction Design for Human-Computer Interaction' with Course ID 05360 and Section D offers 12.0 units. The Class meets Tuesday Thursday between 12:30PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Matthews, Saunders located in Building 4SC, Room 104.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Interaction Design Fundamentals:' with Course ID 05360 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Interaction Design for Human-Computer Interaction' with Course ID 05360 and Section E offers 12.0 units. The Class meets Tuesday Thursday between 05:00PM and 07:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Shelly, Beck located in Building 4SC, Room 104.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Prototyping Algorithmic Experiences' with Course ID 05380 and Section A offers 15.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Holstein located in Building GHC, Room 4301.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Designing Human Centered Software:' with Course ID 05391 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Designing Human-Centered Software' with Course ID 05391 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET.
Answer: "
"What kind of vulnerabilities do diffusion models have according to the paper ""Extracting Training Data from Diffusion Models""?
","['daphne ippolito_2e965b5d97c2d6fb4af284307735be39283792ba_metadata.txt', 'daphne ippolito_2e965b5d97c2d6fb4af284307735be39283792ba_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What kind of vulnerabilities do diffusion models have according to the paper ""Extracting Training Data from Diffusion Models""?

Context: Faculty Name: daphne ippolito
Paperid: 2e965b5d97c2d6fb4af284307735be39283792ba
Title: Extracting Training Data from Diffusion Models
Year: 2023
Abstract: Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.
Authors: Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramèr, B. Balle, Daphne Ippolito, Eric Wallace
Venue: USENIX Security Symposium
Tldr: {'model': 'tldr@v2.0.0', 'text': 'The results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.'}
Url: http://arxiv.org/pdf/2301.13188
Title: Extracting Training Data from Diffusion Models
Authors: Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramèr, Borja Balle, Daphne Ippolito, Eric Wallace, Ann Graham Lotz
Section: 9 Discussion and Conclusion
each wrote the corresponding sections of the paper. • Jamie performed the membership inference attacks and inpainting attacks on CIFAR-10 diffusion models, and Nicholas performed the diffusion extraction experiments; each wrote the corresponding sections of the paper. • Matthew ran experiments for canary memorization and wrote the corresponding section of the paper. • Florian and Vikash performed preliminary experiments on memorization in GANs, and Milad and Vikash ran the experiments included in the paper. • Milad ran the membership inference experiments on GANs. • Vikash ran extraction experiments on pretrained GANs. • Daphne and Florian improved figure clarity and presentation. • Daphne, Borja, and Eric edited the paper and contributed to paper framing. • Nicholas organized the project and wrote the initial paper draft. Acknowledgements and Conflicts of Interest The authors are grateful to Tom Goldstein, Olivia Wiles, Katherine Lee, Austin Tarango, Ian Wilbur, Jeff Dean, Andreas Terzis, Robin Rombach, and Andreas Blattmann for comments on early drafts of this paper. Nicholas, Milad, Matthew, and Daphne are employed at Google, and Jamie and Borja are employed at DeepMind, companies that both train large machine learning models (including diffusion models) on both public and private datasets. Eric Wallace is supported by the Apple Scholars in AI/ML Fellowship.
Answer: "
"In fall, who were the instructors for the Introduction to Deep Learning course at LTI?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall, who were the instructors for the Introduction to Deep Learning course at LTI?

Context: In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning' with Course ID 11485 and Section B offers 9.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ramakrishnan, Singh located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'LTI Minor Project - Seniors' with Course ID 11490 and Section A offers 12.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'LTI Minor Project - Advanced' with Course ID 11590 and Section A offers 12.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Coding & Algorithms Bootcamp:' with Course ID 11601 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Coding Boot Camp' with Course ID 11601 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Brown located in Building DH, Room 2315.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Natural Language Processing:' with Course ID 11611 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning' with Course ID 11485 and Section B offers 9.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ramakrishnan, Singh located in Building DNM, Room DNM.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'LTI Minor Project - Seniors' with Course ID 11490 and Section A offers 12.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'LTI Minor Project - Advanced' with Course ID 11590 and Section A offers 12.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Coding & Algorithms Bootcamp:' with Course ID 11601 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Coding Boot Camp' with Course ID 11601 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Brown located in Building DH, Room 2315.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Natural Language Processing:' with Course ID 11611 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
Answer: "
"In spring 2024, What is the title of course 17356?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, What is the title of course 17356?

Context: In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Software Engineering for Startups' with Course ID 17356 and Section B offers 12.0 units. The Class meets Friday between 11:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Wright, Brown located in Building GHC, Room 4301.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Software Engineering Practicum' with Course ID 17413 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Timperley located in Building WEH, Room 6403.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'AI Governance: Identifying & Mitigating Risks in Design & Dev of AI Solutions' with Course ID 17416 and Section A offers 6.0 units. The Class meets Wednesday between 10:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Sadeh located in Building HH, Room B131.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'AI Governance: Identifying & Mitigating Risks in Design & Dev of AI Solutions' with Course ID 17416 and Section B offers 6.0 units. The Class meets Wednesday between 10:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Sadeh located in Building CMU, Room REMOTE.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Building User-Focused Sensing Systems' with Course ID 17422 and Section A offers 12.0 units. The Class meets Monday Wednesday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Agarwal, Goel located in Building WEH, Room 5403.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Software Engineering for Startups' with Course ID 17356 and Section B offers 12.0 units. The Class meets Friday between 11:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Wright, Brown located in Building GHC, Room 4301.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Software Engineering Practicum' with Course ID 17413 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Timperley located in Building WEH, Room 6403.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'AI Governance: Identifying & Mitigating Risks in Design & Dev of AI Solutions' with Course ID 17416 and Section A offers 6.0 units. The Class meets Wednesday between 10:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Sadeh located in Building HH, Room B131.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'AI Governance: Identifying & Mitigating Risks in Design & Dev of AI Solutions' with Course ID 17416 and Section B offers 6.0 units. The Class meets Wednesday between 10:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Sadeh located in Building CMU, Room REMOTE.
In Semester Spring 2024, from the department of Software & Societal Systems, the subject titled 'Building User-Focused Sensing Systems' with Course ID 17422 and Section A offers 12.0 units. The Class meets Monday Wednesday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Agarwal, Goel located in Building WEH, Room 5403.
Answer: "
"How many courses is Sindi teaching in Spring 2024?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many courses is Sindi teaching in Spring 2024?

Context: The Class meets Monday between 10:00AM and 12:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Sindi located in Building MM, Room 303.
In Semester Spring 2024, from the department of Architecture, the subject titled 'New Pedagogies:' with Course ID 48614 and Section NA offers 9.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Architecture, the subject titled 'Material Regeneration' with Course ID 48614 and Section B offers 9.0 units. The Class meets Wednesday between 09:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kwon located in Building MM, Room 321.
In Semester Spring 2024, from the department of Architecture, the subject titled 'New Pedagogies:' with Course ID 48614 and Section NA offers 9.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Architecture, the subject titled 'A Multiple-Making Approach to Inquiry in Craft + Computation' with Course ID 48614 and Section C offers 9.0 units. The Class meets Friday between 09:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Noel located in Building MM, Room C4.
In Semester Spring 2024, from the department of Architecture, the subject titled 'Carnival Gateway Special Project' with Course ID 48616 and Section A4 offers VAR units. The Class meets Monday between 06:30PM and 07:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Khan located in Building CFA, Room 206A.
In Semester Spring 2024, from the department of Architecture, the subject titled 'Carnival Gateway Project Management' with Course ID 48617 and Section A offers VAR units. The Class meets Monday between 06:30PM and 07:50PM ET.
The Class meets Monday between 10:00AM and 12:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Sindi located in Building MM, Room 303.
In Semester Spring 2024, from the department of Architecture, the subject titled 'New Pedagogies:' with Course ID 48614 and Section NA offers 9.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Architecture, the subject titled 'Material Regeneration' with Course ID 48614 and Section B offers 9.0 units. The Class meets Wednesday between 09:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kwon located in Building MM, Room 321.
In Semester Spring 2024, from the department of Architecture, the subject titled 'New Pedagogies:' with Course ID 48614 and Section NA offers 9.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Architecture, the subject titled 'A Multiple-Making Approach to Inquiry in Craft + Computation' with Course ID 48614 and Section C offers 9.0 units. The Class meets Friday between 09:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Noel located in Building MM, Room C4.
In Semester Spring 2024, from the department of Architecture, the subject titled 'Carnival Gateway Special Project' with Course ID 48616 and Section A4 offers VAR units. The Class meets Monday between 06:30PM and 07:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Khan located in Building CFA, Room 206A.
In Semester Spring 2024, from the department of Architecture, the subject titled 'Carnival Gateway Project Management' with Course ID 48617 and Section A offers VAR units. The Class meets Monday between 06:30PM and 07:50PM ET.
Answer: "
"Which two institutes merged together to form the current day Carnegie Mellon University?
","['cmuhistory_d402.txt', 'cmuhistory_d402.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which two institutes merged together to form the current day Carnegie Mellon University?

Context: The merger built upon a long history of support from the Mellons. It allowed Carnegie Mellon to establish the last of its current pillars: the Mellon College of Science and the College of Humanities and Social Sciences, now known as Marianna Brown Dietrich College of Humanities and Social Sciences. In 2017, Carnegie Mellon celebrated the 50th anniversary of the Carnegie Tech-Mellon Institute merger, revisiting the shared vision of the founders and recognizing the impact it has had, and will continue to have, in the world of higher education, research and discovery. A Global Impact In its 115 years, Carnegie Mellon has soared to national and international leadership in higher education and research. A birthplace of innovation since its founding, it continues to be known for innovation, for solving real-world problems and for interdisciplinary collaboration. Its alumni can be found across the globe — from Tony Award winners to Nobel Prize and Turing Award winners, from CEOs to entrepreneurs, from professors to artists. In the 2000s, in response to demand for expanded international educational opportunities, Carnegie Mellon began offering degree programs outside of Pittsburgh. Today its global presence includes campuses in Qatar and Silicon Valley, Calif., more than a dozen degree-granting locations and more than 20 research partnerships such as Los Angeles; New York City; Washington, D.C.; Australia; China; Portugal and Rwanda. The Future CMU is positioned like never before to meet the challenges of the 21st century. In the coming years, the university will see the largest expansion to the Pittsburgh campus since 1900. At the intersection of technology and humanity, CMU research, innovation and creativity will continue to guide our future as a world-class university. As outlined in the Strategic Plan 2025, the university will focus on advancing the individual student experience, the broader Carnegie Mellon community experience, and the social impact of Carnegie Mellon throughout the world. Carnegie Mellon University challenges the curious and passionate to deliver work that matters.
The first U.S. drama degree was awarded in 1914 at Carnegie Tech. It began offering graduate degrees. In 1919, the first doctorate (in civil engineering) was awarded to Mao Yisheng, a student from China. It laid the groundwork for a research institution, recruiting leading scientists, offering sponsored fellowships with government and industry leaders and pioneering nontraditional interdisciplinary research, which brought together physicists, chemists and metallurgists, for example. Interdisciplinary research would become the hallmark of Carnegie Mellon research. It initiated the 'Carnegie Plan' in 1938, a new curriculum that required science and engineer students to take courses in humanities and social sciences in order to better understand the needs of society. Carnegie died in 1919, but his vision for an educated public lived on after him. Carnegie Tech - Post-war Years With the end of World War II, the latter half of the 20th century brought unprecedented growth to Carnegie Tech. In 1956, the arrival of the first IBM computer to campus was revolutionary, initiating a university culture where information technology pervaded virtually all areas of study. University culture also changed in 1973 when Margaret Morrison closed and women joined their male peers in classrooms and dorms. The times were changing, and Tech positioned itself at the forefront, opening three new schools: 1948: The Graduate School of Industrial Administration, later renamed the David A. Tepper School of Business, focusing on quantitative analysis and pioneering the field of management science. 1968: School of Urban and Public Affairs, later renamed the H. John Heinz III College, providing graduate training for work in the public sector. 1986: School of Computer Science, pioneering computing and artificial intelligence, led by interdisciplinary efforts of Allen Newell and Herbert Simon. Carnegie Mellon University In 1967, Carnegie Tech merged with the Mellon Institute, a science research center founded by the Mellon family of Pittsburgh, to become known as Carnegie Mellon University. The merger built upon a long history of support from the Mellons. It allowed Carnegie Mellon to establish the last of its current pillars: the Mellon College of Science and the College of Humanities and Social Sciences, now known as Marianna Brown Dietrich College of Humanities and Social Sciences.
Answer: "
"Who is the current assistnat coach of women's basketball?
","['miis-handbook_2023-2024.txt', 'mlt-student-handbook-2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who is the current assistnat coach of women's basketball?

Context: Additionally, the 
Center supports and connects historically underrepresented students and those who are first 
in their family to attend college in a setting where students’ differences and talents are 
appreciated and reinforced, both at the graduate and undergraduate level. Initiatives 
coordinated by the Center include, but are not limited to:  
• First generation/first in family to attend college programs 
• LGBTQ+ Initiatives  
• Race and ethnically focused programs, including Inter-University 
Graduate Students of Color Series (SOC) and PhD SOC Network  
• Women’s empowerment programs, including Graduate Women’s 
Gatherings (GWGs)  
A.1.5 Assistance for Individuals with Disabilities  
https://www.cmu.edu/disability-resources/  
The Office of Disability Resources at Carnegie Mellon University has a continued mission to 
provide physical, digital, and programmatic access to ensure that students with disabilities 
have equal access to their educational experience.  The Office works to ensure that qualified 
individuals receive reasonable accommodations as guaranteed by the Americans with 
Disabilities Act (ADA) and Section 504 of the Rehabilitation Act of 1973. Students who would 
like to receive accommodations can begin the process through Disability Resources' secure 
online portal or email access@andrew.cmu.edu to begin the interactive accommodation 
Process. 
Students with physical, sensory, cognitive, or emotional disabilities are encouraged to self-
identify with the Office of Disability Resources and request needed accommodations. Any 
questions about the process can be directed to access@andrew.cmu.edu, or call (412) 268- 
6121.  
A.1.6 Eberly Center for Teaching Excellence & Educational Innovation  
https://www.cmu.edu/teaching/  
The Eberly Center offers a wide variety of confidential, consultation services and professional 
MIIS Graduate Student Handbook 
Page 42 
 
development programs to support graduate students as teaching assistants or instructors of 
record during their time at Carnegie Mellon University and as future faculty members at other 
institutions. Regardless of one's current or future teaching context and duties, Eberly’s goal is 
to disseminate evidence-based teaching strategies in ways that are accessible and actionable. 
Programs and services include campus-wide Graduate Student Instructor Orientation events 
and our Future Faculty Program, both of which are designed to help participants be effective 
and efficient in their teaching roles.
Additionally, the 
Center supports and connects historically underrepresented students and those who are first in 
their family to attend college in a setting where students differences and talents are 
appreciated and reinforced, both at the graduate and undergraduate level. Initiatives 
coordinated by the Center include, but are not limited to: 
 First generation/first in family to attend college programs 
 LGBTQ+ initiatives 
 Race and ethnically-focused programs, including Inter-University Graduate Students of 
Color Series (SOC) and PhD SOC Network 
 Womens empowerment programs, including Graduate Womens Gatherings (GWGs) 
 Transgender and non-binary student programs 
A.1.4 Eberly Center for Teaching Excellence & Educational Innovation 
www.cmu.edu/teaching 
We offer a wide variety of confidential, consultation services and professional development 
programs to support graduate students as teaching assistants or instructors of record during 
their time at Carnegie Mellon University and as future faculty members at other institutions. 
Regardless of ones current or future teaching context and duties, our goal is to disseminate 
evidence-based teaching strategies in ways that are accessible and actionable. Programs and 
services include campus-wide Graduate Student Instructor Orientation events and our Future 
Faculty Program, both of which are designed to help participants be effective and efficient in 
their teaching roles. The Eberly Center also assists departments in creating and conducting 
customized programs to meet the specific needs of their graduate student instructors. Specific 
information about Eberly Center support for graduate students can be found at: 
www.cmu.edu/teaching/graduatestudentsupport/index.html.    
A.1.5 Carnegie Mellon Ethics Hotline 
https://www.cmu.edu/hr/resources/ethics-hotline.html  
 
The health, safety, and well-being of the university community are top priorities at Carnegie 
Mellon University.
Answer: "
"Who is teaching the question answering course at LTI?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who is teaching the question answering course at LTI?

Context: Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Introduction to Question Answering' with Course ID 11697 and Section A offers 12.0 units. The Class meets Monday Wednesday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Nyberg, Mitamura located in Building PH, Room A18A.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'LTI Colloquium' with Course ID 11700 and Section A offers 6.0 units. The Class meets Friday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Bisk, Diaz located in Building POS, Room A35.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'LTI Colloquium' with Course ID 11700 and Section PP offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Lisbon, Portugal location,led by experienced instructor Diaz, Bisk located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Advanced Natural Language Processing' with Course ID 11711 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Frederking, Fried located in Building TEP, Room 1403.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Graduate Seminar on Dialog Processing' with Course ID 11716 and Section A offers 6.0 units. The Class meets Tuesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rudnicky located in Building PH, Room A21A.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Linguistics Lab' with Course ID 11723 and Section A offers 6.0 units.
Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Introduction to Question Answering' with Course ID 11697 and Section A offers 12.0 units. The Class meets Monday Wednesday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Nyberg, Mitamura located in Building PH, Room A18A.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'LTI Colloquium' with Course ID 11700 and Section A offers 6.0 units. The Class meets Friday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Bisk, Diaz located in Building POS, Room A35.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'LTI Colloquium' with Course ID 11700 and Section PP offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Lisbon, Portugal location,led by experienced instructor Diaz, Bisk located in Building TBA, Room None.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Advanced Natural Language Processing' with Course ID 11711 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Frederking, Fried located in Building TEP, Room 1403.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Graduate Seminar on Dialog Processing' with Course ID 11716 and Section A offers 6.0 units. The Class meets Tuesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rudnicky located in Building PH, Room A21A.
In Semester Fall 2023, from the department of Language Technologies Institute, the subject titled 'Linguistics Lab' with Course ID 11723 and Section A offers 6.0 units.
Answer: "
"Which LTI class is offered in Kigali, Rwanda?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which LTI class is offered in Kigali, Rwanda?

Context: In Semester Fall 2023, from the department of Information & Communication Technology, the subject titled 'Engineering AI Project Methods' with Course ID 04653 and Section A1 offers 6.0 units. The Class meets Monday Wednesday between 06:00PM and 07:50PM ET. Students attend lectures at the Kigali, Rwanda location,led by experienced instructor Brown located in Building CMR, Room F205.
In Semester Fall 2023, from the department of Information & Communication Technology, the subject titled 'Introduction to Probabilistic Graphical Model:' with Course ID 04654 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Information & Communication Technology, the subject titled 'Introduction to Probabilistic Graphical Model' with Course ID 04654 and Section J offers 12.0 units. The Class meets Tuesday Thursday between 03:00PM and 04:50PM ET. Students attend lectures at the Kigali, Rwanda location,led by experienced instructor Gueye, Mukamakuza located in Building CMR, Room F309.
In Semester Fall 2023, from the department of Information & Communication Technology, the subject titled 'Artificial Intelligence for Engineers' with Course ID 04655 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 10:00AM and 11:50AM ET. Students attend lectures at the Kigali, Rwanda location,led by experienced instructor Biyabani located in Building CMR, Room F205.
In Semester Fall 2023, from the department of Information & Communication Technology, the subject titled 'Artificial Intelligence for Engineers' with Course ID 04655 and Section NA offers 12.0 units. The Class meets Friday between 12:00PM and 01:00PM ET. Students attend lectures at the Kigali, Rwanda location,led by experienced instructor NA located in Building CMR, Room F205.
In Semester Fall 2023, from the department of Information & Communication Technology, the subject titled 'Vulnerability Assessment and Testing' with Course ID 04721 and Section A offers 12.0 units.
In Semester Fall 2023, from the department of Information & Communication Technology, the subject titled 'Engineering AI Project Methods' with Course ID 04653 and Section A1 offers 6.0 units. The Class meets Monday Wednesday between 06:00PM and 07:50PM ET. Students attend lectures at the Kigali, Rwanda location,led by experienced instructor Brown located in Building CMR, Room F205.
In Semester Fall 2023, from the department of Information & Communication Technology, the subject titled 'Introduction to Probabilistic Graphical Model:' with Course ID 04654 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Information & Communication Technology, the subject titled 'Introduction to Probabilistic Graphical Model' with Course ID 04654 and Section J offers 12.0 units. The Class meets Tuesday Thursday between 03:00PM and 04:50PM ET. Students attend lectures at the Kigali, Rwanda location,led by experienced instructor Gueye, Mukamakuza located in Building CMR, Room F309.
In Semester Fall 2023, from the department of Information & Communication Technology, the subject titled 'Artificial Intelligence for Engineers' with Course ID 04655 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 10:00AM and 11:50AM ET. Students attend lectures at the Kigali, Rwanda location,led by experienced instructor Biyabani located in Building CMR, Room F205.
In Semester Fall 2023, from the department of Information & Communication Technology, the subject titled 'Artificial Intelligence for Engineers' with Course ID 04655 and Section NA offers 12.0 units. The Class meets Friday between 12:00PM and 01:00PM ET. Students attend lectures at the Kigali, Rwanda location,led by experienced instructor NA located in Building CMR, Room F205.
In Semester Fall 2023, from the department of Information & Communication Technology, the subject titled 'Vulnerability Assessment and Testing' with Course ID 04721 and Section A offers 12.0 units.
Answer: "
"How many languages does ML-SUPERB cover?
","['shinji watanabe_090b284b2f8fc93ac3e7a92fc9f91bf4965ba75c_metadata.txt', 'shinji watanabe_bdf9ea3a67691e1b6a362f4019bf80c9cf31cecd_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many languages does ML-SUPERB cover?

Context: Faculty Name: shinji watanabe
Paperid: 090b284b2f8fc93ac3e7a92fc9f91bf4965ba75c
Title: ML-SUPERB: Multilingual Speech Universal PERformance Benchmark
Year: 2023
Abstract: Speech processing Universal PERformance Benchmark (SUPERB) is a leaderboard to benchmark the performance of Self-Supervised Learning (SSL) models on various speech processing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from high-resource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB benchmark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research.
Authors: Jiatong Shi, Dan Berrebbi, William Chen, Ho-Lam Chung, En-Pei Hu, Wei Huang, Xuankai Chang, Shang-Wen Li, Abdel-rahman Mohamed, Hung-yi Lee, Shinji Watanabe
Venue: Interspeech
Tldr: {'model': 'tldr@v2.0.0', 'text': None}
Url: https://arxiv.org/pdf/2305.10615
Faculty Name: shinji watanabe
Paperid: bdf9ea3a67691e1b6a362f4019bf80c9cf31cecd
Title: Findings of the 2023 ML-Superb Challenge: Pre-Training And Evaluation Over More Languages And Beyond
Year: 2023
Abstract: The 2023 Multilingual Speech Universal Performance Benchmark (ML-SUPERB) Challenge expands upon the acclaimed SUPERB framework, emphasizing self-supervised models in multilingual speech recognition and language identification. The challenge comprises a research track focused on applying ML-SUPERB to specific multilingual subjects, a Challenge Track for model submissions, and a New Language Track where language resource researchers can contribute and evaluate their low-resource language data in the context of the latest progress in multilingual speech recognition. The challenge garnered 12 model submissions and 54 language corpora, resulting in a comprehensive benchmark encompassing 154 languages. The findings indicate that merely scaling models is not the definitive solution for multilingual speech tasks, and a variety of speech/voice types present significant challenges in multilingual speech processing.
Authors: Jiatong Shi, William Chen, Dan Berrebbi, Hsiu-Hsuan Wang, Wei-Ping Huang, En-Pei Hu, Ho-Lam Chuang, Xuankai Chang, Yuxun Tang, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Shinji Watanabe
Venue: Automatic Speech Recognition & Understanding
Tldr: {'model': 'tldr@v2.0.0', 'text': 'The 2023 Multilingual Speech Universal Performance Benchmark (ML-SUPERB) Challenge expands upon the acclaimed SUPERB framework, emphasizing self-supervised models in multilingual speech recognition and language identification, resulting in a comprehensive benchmark encompassing 154 languages.'}
Url: https://arxiv.org/pdf/2310.05513
Answer: "
"Who is teaching the ""Ethics and Decision Making in Architecture"" in Spring 2024? 
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who is teaching the ""Ethics and Decision Making in Architecture"" in Spring 2024? 

Context: The Class meets Monday Wednesday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Torello located in Building MM, Room 409.
In Semester Spring 2024, from the department of Architecture, the subject titled 'Constructing Value(s): Economies of Design' with Course ID 48380 and Section A offers 6.0 units. The Class meets Tuesday between 09:00AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Volcy located in Building CFA, Room 214.
In Semester Spring 2024, from the department of Architecture, the subject titled 'Issues of Practice' with Course ID 48381 and Section A offers 6.0 units. The Class meets Thursday between 10:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Coppedge located in Building MM, Room 103.
In Semester Spring 2024, from the department of Architecture, the subject titled 'Ethics and Decision Making in Architecture' with Course ID 48383 and Section A offers 6.0 units. The Class meets Tuesday between 10:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Vavasis located in Building MM, Room 103.
In Semester Spring 2024, from the department of Architecture, the subject titled 'Advanced Synthesis Options Studio II' with Course ID 48410 and Section NA offers 18.0 units. The Class meets Tuesday Thursday between 01:00PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Arscott located in Building POS, Room 146.
In Semester Spring 2024, from the department of Architecture, the subject titled 'Advanced Synthesis Options Studio II' with Course ID 48410 and Section A offers 18.0 units. The Class meets Tuesday Thursday between 01:00PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Bista located in Building MM, Room 312.
The Class meets Monday Wednesday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Torello located in Building MM, Room 409.
In Semester Spring 2024, from the department of Architecture, the subject titled 'Constructing Value(s): Economies of Design' with Course ID 48380 and Section A offers 6.0 units. The Class meets Tuesday between 09:00AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Volcy located in Building CFA, Room 214.
In Semester Spring 2024, from the department of Architecture, the subject titled 'Issues of Practice' with Course ID 48381 and Section A offers 6.0 units. The Class meets Thursday between 10:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Coppedge located in Building MM, Room 103.
In Semester Spring 2024, from the department of Architecture, the subject titled 'Ethics and Decision Making in Architecture' with Course ID 48383 and Section A offers 6.0 units. The Class meets Tuesday between 10:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Vavasis located in Building MM, Room 103.
In Semester Spring 2024, from the department of Architecture, the subject titled 'Advanced Synthesis Options Studio II' with Course ID 48410 and Section NA offers 18.0 units. The Class meets Tuesday Thursday between 01:00PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Arscott located in Building POS, Room 146.
In Semester Spring 2024, from the department of Architecture, the subject titled 'Advanced Synthesis Options Studio II' with Course ID 48410 and Section A offers 18.0 units. The Class meets Tuesday Thursday between 01:00PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Bista located in Building MM, Room 312.
Answer: "
"What are some of the metrics incorporated in Pentathlon for efficiency evaluation?
","['emma strubell_84d20ad9f42d80dfd5130a6362d5422be8a6bdc3_metadata.txt', 'emma strubell_84d20ad9f42d80dfd5130a6362d5422be8a6bdc3_content_2.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are some of the metrics incorporated in Pentathlon for efficiency evaluation?

Context: Faculty Name: emma strubell
Paperid: 84d20ad9f42d80dfd5130a6362d5422be8a6bdc3
Title: Efficiency Pentathlon: A Standardized Arena for Efficiency Evaluation
Year: 2023
Abstract: Rising computational demands of modern natural language processing (NLP) systems have increased the barrier to entry for cutting-edge research while posing serious environmental concerns. Yet, progress on model efficiency has been impeded by practical challenges in model evaluation and comparison. For example, hardware is challenging to control due to disparate levels of accessibility across different institutions. Moreover, improvements in metrics such as FLOPs often fail to translate to progress in real-world applications. In response, we introduce Pentathlon, a benchmark for holistic and realistic evaluation of model efficiency. Pentathlon focuses on inference, which accounts for a majority of the compute in a model's lifecycle. It offers a strictly-controlled hardware platform, and is designed to mirror real-world applications scenarios. It incorporates a suite of metrics that target different aspects of efficiency, including latency, throughput, memory overhead, and energy consumption. Pentathlon also comes with a software library that can be seamlessly integrated into any codebase and enable evaluation. As a standardized and centralized evaluation platform, Pentathlon can drastically reduce the workload to make fair and reproducible efficiency comparisons. While initially focused on natural language processing (NLP) models, Pentathlon is designed to allow flexible extension to other fields. We envision Pentathlon will stimulate algorithmic innovations in building efficient models, and foster an increased awareness of the social and environmental implications in the development of future-generation NLP models.
Authors: Hao Peng, Qingqing Cao, Jesse Dodge, Matthew E. Peters, Jared Fernandez, Tom Sherborne, Kyle Lo, Sam Skjonsberg, Emma Strubell, Darrell Plessas, Iz Beltagy, Pete Walsh, Noah A. Smith, Hannaneh Hajishirzi
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': ""Pentathlon is a benchmark for holistic and realistic evaluation of model efficiency, which focuses on inference, which accounts for a majority of the compute in a model's lifecycle, and is designed to mirror real-world applications scenarios.""}
Url: https://arxiv.org/pdf/2307.09701
Title: Efficiency Pentathlon: A Standardized Arena for Efficiency Evaluation
Authors: Hao Peng, Qingqing Cao, Jesse Dodge, Matthew E. Peters, Jared Fernandez, Tom Sherborne, Kyle Lo, Sam Skjonsberg, Emma Strubell, Darrell Plessas, Iz Beltagy, Evan Pete Walsh, Noah A. Smith, Hannaneh Hajishirzi
Section: 3 Experiments
BLEU score, it takes a substantial hit in efficiency. Interestingly, despite being more than four times larger, WMT19-Meta achieves efficiency performance comparable to OPUS in latency, memory overhead, and energy consumption, and significantly outperforms it in terms of BLEU. However, it falls short of OPUS in throughput. This observation confirms that relying on a single efficiency metric risks oversimplifying the complex performance landscape of efficiency in practical applications. With ONNX, the models achieve over 20% improvements in latency and throughput in the singlestream scenario, accompanied by a significant reduction in memory and energy overhead. However, less efficiency improvement is observed in other scenarios with larger batch sizes. Larger models benefit more from FP16 quantization. By comparing Figures 2a and 2b, we observe that FP16 quantization improves all models’ efficiency performance (except #Params.), particularly memory overhead. Larger models appear to benefit more from quantization. As shown in Figures 2c and 2d, while OPUS experiences minimal efficiency gains from quantization apart from increased throughput, WMT21-Meta’s efficiency dramatically improves with FP16 quantization, nearly doubling throughput and reducing latency, memory overhead, and energy consumption by half or more. These results highlight the promise of advancing quantization techniques for larger models in order to improve the trade-off between accuracy and efficiency.
Answer: "
"How many parameters does the chain-of-skills model have?
","['eric nyberg_61354e45bca908ad08f24e44bd507b4e1c958e6f_metadata.txt', 'eric xing_5c577988ccebfea96de86678d04fd94fad367d2e_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many parameters does the chain-of-skills model have?

Context: Faculty Name: eric nyberg
Paperid: 61354e45bca908ad08f24e44bd507b4e1c958e6f
Title: Chain-of-Skills: A Configurable Model for Open-Domain Question Answering
Year: 2023
Abstract: The retrieval model is an indispensable component for real-world knowledge-intensive tasks, e.g., open-domain question answering (ODQA). As separate retrieval skills are annotated for different datasets, recent work focuses on customized methods, limiting the model transfer- ability and scalability. In this work, we propose a modular retriever where individual modules correspond to key skills that can be reused across datasets. Our approach supports flexible skill configurations based on the target domain to boost performance. To mitigate task interference, we design a novel modularization parameterization inspired by sparse Transformer. We demonstrate that our model can benefit from self-supervised pretraining on Wikipedia and fine-tuning using multiple ODQA datasets, both in a multi-task fashion. Our approach outperforms recent self-supervised retrievers in zero-shot evaluations and achieves state-of-the-art fine-tuned retrieval performance on NQ, HotpotQA and OTT-QA.
Authors: Kaixin Ma, Hao Cheng, Yu Zhang, Xiaodong Liu, Eric Nyberg, Jianfeng Gao
Venue: Annual Meeting of the Association for Computational Linguistics
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work proposes a modular retriever where individual modules correspond to key skills that can be reused across datasets and achieves state-of-the-art fine-tuned retrieval performance on NQ, HotpotQA and OTT-QA.'}
Url: http://arxiv.org/pdf/2305.03130
Recently, [WWS+22] introduced chain-of-thought prompting, directing models to clarify their reasoning over complex tasks, which was shown to enhance their accuracy. 44JASMINE [NAME+22] are Arabic GPT models ranging in sizes from 350M to 13B parameters, but these models have not been released to the public, and the arXiv paper describing them says the 6.7B and 13B models are still training. 45https://ai.google/static/documents/google-about-bard.pdf 46https://www.anthropic.com/index/introducing-claude Evaluating Large Language Models Large language models are proficient at generating coherent and fluent text, but have shortcomings in terms of factuality and reasoning skills. As a proxy to evaluate factuality, existing English large language models such as GPT-4 [Ope23] and LLaMA [TLI+23] use school exam questions [HBB+22] to understand how faithful the models are at providing knowledge. Evaluating commonsense reasoning abilities is also important, and is the target of datasets such as HellaSwag [ZHB+19], WinoGrande [SBBC21], ARC easy and challenge [CCE+18], and OpenBookQA [MCKS18]. Moreover, reasoning via programming is evaluated using HumanEval [CTJ+21] and MBPP [AON+21]. In Arabic NLP, existing benchmarks primarily focus on evaluating natural language understanding tasks. For instance, the ALUE benchmark [STG+21] encompasses semantic tasks such as irony detection [GKB+19], emotion classification [MBMSK18], sentiment classification [MBMSK18], offensive language [MDM+20] and hate speech identification [MDM+20]. Existing Arabic benchmarks, however, do not include knowledge and commonsense evaluation, posing a challenge for the assessment of Jais. In contrast, in other languages, researchers have effectively used methods such as machine translation or the construction of datasets in a similar manner to assess the knowledge proficiency and the commonsense understanding of language models [Ope23, LKW+23]. In this context, as detailed in Section 5, we used a combination of techniques, including crafting analogous datasets to those available for
Answer: "
"What LTI professor co-authored ""CONVOIFILTER: A CASE STUDY OF DOING COCKTAIL PARTY SPEECH RECOGNITION""?
","['alexander waibel_papers.txt', 'shinji watanabe_6c33625c7b0ffc37955921a145531d9d4eaee713_content_2.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What LTI professor co-authored ""CONVOIFILTER: A CASE STUDY OF DOING COCKTAIL PARTY SPEECH RECOGNITION""?

Context: List of 2023 Open Access papers by alexander waibel are:
AdapITN: A Fast, Reliable, and Dynamic Adaptive Inverse Text Normalization
Train Global, Tailor Local: Minimalist Multilingual Translation into Endangered Languages
Towards Efficient Simultaneous Speech Translation: CUNI-KIT System for Simultaneous Track at IWSLT 2023
SYNTACC : Synthesizing Multi-Accent Speech By Weight Factorization
KIT’s Multilingual Speech Translation System for IWSLT 2023
Convoifilter: A case study of doing cocktail party speech recognition
Continually learning new languages
Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff
End-to-End Evaluation for Low-Latency Simultaneous Speech Translation
FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN
Incremental Learning of Humanoid Robot Behavior from Natural Interaction and Large Language Models
Title: EXPLORING THE INTEGRATION OF SPEECH SEPARATION AND RECOGNITION WITH SELF-SUPERVISED LEARNING REPRESENTATION
Authors: Yoshiki Masuyama, Xuankai Chang, Wangyou Zhang, Samuele Cornell, Zhong-Qiu Wang, Nobutaka Ono, Yanmin Qian, Shinji Watanabe
Section: 6. REFERENCES
Zhang et al., “End-to-end dereverberation, beamforming, and speech recognition in a cocktail party,” IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 30, pp. 3173– 3188, 2022. [41] C. Boeddeker and et al., “Convolutive transfer function invariant SDR training criteria for multi-channel reverberant speech separation,” in Proc. ICASSP, 2021, pp. 8428–8432. [42] Y. J. Lu and et al., “Towards low-distortion multi-channel speech enhancement: The ESPNET-SE submission to the L3DAS22 challenge,” in Proc. ICASSP, 2022, pp. 9201– 9205. [43] T. von Neumann and et al., “End-to-end training of time domain audio separation and recognition,” in Proc. ICASSP, 2020, pp. 7004–7008. [44] W. Zhang et al., “End-to-end far-field speech recognition with unified dereverberation and beamforming,” in Proc. Interspeech, 2020, pp. 324–328. [45] J. Zhang et al., “Time-domain speech extraction with spatial information and multi speaker conditioning mechanism,” in Proc. ICASSP, 2021, pp. 6084–6088.
Answer: "
"What is the GitHub URL where MultiViz is available?
","['louis philippe morency_d01cc51c0d06583b809833a5f7ce71101d278528_metadata.txt', 'mona diab_99bfe503743c5ec8e16e50ab8438159cdb533a89_content_13.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the GitHub URL where MultiViz is available?

Context: Faculty Name: louis philippe morency
Paperid: d01cc51c0d06583b809833a5f7ce71101d278528
Title: MultiViz: Towards User-Centric Visualizations and Interpretations of Multimodal Models
Year: 2023
Abstract: The nature of human and computer interactions are inherently multimodal, which has led to substantial interest in building interpretable, interactive, and reliable multimodal interfaces. However, modern multimodal models and interfaces are typically black-box neural networks, which makes it challenging to understand their internal mechanics. How can we visualize their internal workings in order to empower stakeholders to visualize model behavior, perform model debugging, and promote trust in these models? Our paper proposes MultiViz, a method for analyzing the behavior of multimodal models via 4 stages: (1) unimodal importance, (2) cross-modal interactions, (3) multimodal representations and (4) multimodal prediction. MultiViz includes modular visualization tools for each stage before combining outputs from all stages through an interactive and human-in-the-loop API. Through user studies with 21 participants on 8 trained models across 6 real-world tasks, we show that the complementary stages in MultiViz together enable users to (1) simulate model predictions, (2) assign interpretable concepts to features, (3) perform error analysis on model misclassifications, and (4) use insights from error analysis to debug models. MultiViz is publicly available at https://github.com/pliang279/MultiViz, will be regularly updated with new visualization tools and metrics, and welcomes input from the community1.
Authors: P. Liang, Yiwei Lyu, Gunjan Chhablani, Nihal Jain, Zihao Deng, Xingbo Wang, Louis-Philippe Morency, R. Salakhutdinov
Venue: CHI Extended Abstracts
Tldr: {'model': 'tldr@v2.0.0', 'text': 'It is shown that the complementary stages in MultiViz together enable users to simulate model predictions, assign interpretable concepts to features, perform error analysis on model misclassifications, and use insights from error analysis to debug models.'}
Url: N/A
2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692. Potsawee Manakul, Adian Liusie, and Mark J. F. Gales. 2023. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. Gary Marcus and Future of Life Institute. 2023. Pause giant ai experiments: An open letter. Joshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906–1919, Online. Association for Computational Linguistics. Midjourney. 2022. https://www.midjourney.com. Niels Mündler, Jingxuan He, Slobodan Jenko, and Martin Vechev. 2023. Self-contradictory hallucinations of large language models: Evaluation, detection and mitigation. nVIDIA. 2023. https://nvidia.github.io/nemo/. NYT. https://www.nytimes.com/topic/company/twitter. OpenAI. 2022. Introducing chatgpt. OpenAI. 2023a. Gpt-4 technical report. OpenAI. 2023b. Our approach to ai safety. Allan Paivio. 2013. Dual coding theory, word abstractness, and emotion: a critical review of kousta et al.(2011). Jonas Pfeiffer, Francesco Piccinno, Massimo Nicosia, Xinyi Wang, Machel Reid, and Sebastian Ruder. 2023. mmt5: Modular multilingual pre-training solves source language hallucinations. Politifact. https://www.politifact.com/. Ofir Press, Noah A. Smith, and Mike Lewis. 2022. Train short, test long: Attention with linear biases enables input length extrapolation. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.
Answer: "
"What number do all of the Chemistry classes start with?
","['combined_metadata_final.txt', 'combined_metadata_final.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What number do all of the Chemistry classes start with?

Context: In Semester Summer One(All) 2024, from the department of Chemistry, the subject titled 'Study Abroad' with Course ID 09050 and Section S offers 0.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Stump located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Chemistry, the subject titled 'Modern Chemistry II' with Course ID 09106 and Section S offers 10.0 units. The Class meets Monday Tuesday Wednesday Thursday Friday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Sherwood located in Building DH, Room 2302.
In Semester Summer One(All) 2024, from the department of Chemistry, the subject titled 'Modern Chemistry II' with Course ID 09106 and Section NA offers 10.0 units. The Class meets Tuesday between 02:00PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor NA located in Building DH, Room 2302.
In Semester Summer One(All) 2024, from the department of Chemistry, the subject titled 'Organic Chemistry I' with Course ID 09217 and Section S offers 9.0 units. The Class meets Monday Tuesday Wednesday Thursday Friday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Silva located in Building DH, Room 2302.
In Semester Summer One(All) 2024, from the department of Chemistry, the subject titled 'Independent Study Chemistry' with Course ID 09435 and Section S offers 1.0 - 12.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Stump located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Chemistry, the subject titled 'Undergraduate Research' with Course ID 09445 and Section S offers 3-18 units. The Class meets Schedule will be added between NA and NA ET.
In Semester Summer One(All) 2024, from the department of Chemistry, the subject titled 'Undergraduate Research' with Course ID 09445 and Section S offers 3-18 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Stump located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Chemistry, the subject titled 'Chemical Research' with Course ID 09861 and Section R offers 0-99 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Noonan located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Chemistry, the subject titled 'Chemical Research' with Course ID 09861 and Section S offers 0-99 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Noonan located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Chemistry, the subject titled 'Doctoral Dissertation' with Course ID 09871 and Section S offers 5.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Noonan located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Chemistry, the subject titled 'Internship:' with Course ID 09990 and Section NA offers 0-99 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Summer One(All) 2024, from the department of Chemistry, the subject titled 'TBA' with Course ID 09990 and Section S offers 0-99 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Noonan located in Building TBA, Room None.
Answer: "
"How many authors from FACTORCL are from Carnegie Mellon University?
","['cmuhistory_d402_metadata.txt', 'mlt-student-handbook-2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many authors from FACTORCL are from Carnegie Mellon University?

Context: {
  ""viewport"": ""width=device-width, initial-scale=1.0"",
  ""description"": ""CMU \u2014 now a global research university \u2014 began when Andrew Carnegie famously stated, \""My Heart is in the Work,\"" and founded Carnegie Technical Schools in 1900."",
  ""author"": ""Carnegie Mellon University"",
  ""og:title"": ""History - CMU - Carnegie Mellon University"",
  ""og:description"": ""CMU \u2014 now a global research university \u2014 began when Andrew Carnegie famously stated, \""My Heart is in the Work,\"" and founded Carnegie Technical Schools in 1900."",
  ""og:type"": ""website"",
  ""og:url"": ""http://www.cmu.edu/about/history.html"",
  ""msapplication-TileColor"": ""#9f0000"",
  ""msapplication-TileImage"": ""//www.cmu.edu/favicon-144.png""
}
Topic category is cmuhistory
The Carnegie Mellon University transcript will include information on such courses as follows: 
Carnegie Mellon courses and courses taken through the university's cross-registration program 
will have grades recorded on the transcript and be factored into the QPA. All other courses will 
be recorded on this transcript indicating where the course was taken, but without grade. Such 
courses will not be taken into account for academic actions, honors, or QPA calculations. (Note: 
Suspended students may take courses elsewhere; however, they may receive transfer credit 
only if their colleges and department's policies allow this.) 
4.10 Academic Integrity 
Please review the University Policy on Academic Integrity: 
https://www.cmu.edu/policies/student-and-student-life/academic-integrity.html 
The policy includes the University expectations around academic integrity and provides 
definitions of cheating, plagiarism, and unauthorized assistance. 
A review of the Universitys Academic Disciplinary Actions procedures is also recommended. 
These procedures outline the process for investigating, reporting, and adjudicating violations of 
the University Policy on Academic Integrity. The procedures also outline the appeals process. 
https://www.cmu.edu/policies/student-and-student-life/academic-integrity.html 
In the midst of self-exploration, the high demands of a challenging academic environment can 
create situations where some students have difficulty exercising good judgment. Academic 
challenges can provide many opportunities for high standards to evolve if students actively 
reflect on these challenges and if the community supports discussions to aid in this process. It is 
the responsibility of the entire community to establish and maintain the integrity of our 
university. 
MLT Graduate Student Handbook 
Page 20 
 
Carnegie Mellon University educates its students to become professionals who will serve 
society with integrity. The university also creates and disseminates new knowledge and 
expressions of knowledge in ways that benefit society. Carnegie Mellon strives to serve the 
changing needs of society through the three primary goals outlined in its mission statement: to 
create and disseminate knowledge and art through research and artistic expression, teaching 
and learning and transfer to society; to serve students by teaching them leadership, problem-
solving skills, and the values of quality, ethical behavior, responsibility to society and 
commitments to work; and to pursue the advantages provided by a diverse community, open 
to the exchange of ideas, where discovery and artistic creativity can flourish.
Answer: "
"In spring 2024, When are Mid-Semester and Mini-3 grades due?
","['Meta-Data-Calendar-2425.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, When are Mid-Semester and Mini-3 grades due?

Context: Day: 'Monday', Event: 'Mini-3 Faculty Course Evaluations open', Semester: 'Spring 2025 (S25)'
Date: '2025-02-24', Day: 'Monday', Event: 'Semester course drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Spring 2025 (S25)'
Date: '2025-02-28', Day: 'Friday', Event: 'Mini-3 Last Day of Classes ', Semester: 'Spring 2025 (S25)'
Date: '2025-02-28', Day: 'Friday', Event: 'Mini-3 voucher deadline (4)', Semester: 'Spring 2025 (S25)'
Date: '2025-03-01', Day: 'Saturday', Event: 'Mini-3 Exams', Semester: 'Spring 2025 (S25)'
Date: '2025-03-01', Day: 'Saturday', Event: 'Mini-3 Faculty Course Evaluations close', Semester: 'Spring 2025 (S25)'
Start Date: '2025-03-03', End Date: '2023-03-07', Days: 'Monday to Friday', Event: 'Spring Break; No Classes', Semester: 'Spring 2025 (S25)'
Date: '2025-03-10', Day: 'Monday', Event: 'First day of Mini-4 Classes', Semester: 'Spring 2025 (S25)'
Date: '2025-03-11', Day: 'Tuesday', Event: 'Summer 2025 Registration Opens', Semester: 'Spring 2025 (S25)'
Date: '2025-03-12', Day: 'Wednesday', Event: 'Mid-Semester & Mini-3 grades due by 4 pm', Semester: 'Spring 2025 (S25)'
Date: '2025-03-14', Day: 'Friday', Event: 'Mini-4 add, audit & tuition adjustment drop deadline (1)', Semester: 'Spring 2025 (S25)'
Date: '2025-03-31', Day: 'Monday', Event: 'Semester pass/no pass & withdrawal deadline', Semester: 'Spring 2025 (S25)'
Date: '2025-04-02', Day: 'Wednesday', Event: 'Mini-4 course drop deadline; withdrawal grade assigned after this date (2)',
On Wednesday, 05 February, 2025, during the Spring 2025 (S25) semester, Mini-3 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 17 February, 2025, during the Spring 2025 (S25) semester, Mini-3 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 17 February, 2025, during the Spring 2025 (S25) semester, Mini-3 Faculty Course Evaluations open is observed.
On Monday, 24 February, 2025, during the Spring 2025 (S25) semester, Semester course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 28 February, 2025, during the Spring 2025 (S25) semester, Mini-3 Last Day of Classes is observed.
On Friday, 28 February, 2025, during the Spring 2025 (S25) semester, Mini-3 voucher deadline (4) is observed.
On Saturday, 01 March, 2025, during the Spring 2025 (S25) semester, Mini-3 Exams is observed.
On Saturday, 01 March, 2025, during the Spring 2025 (S25) semester, Mini-3 Faculty Course Evaluations close is observed.
From 03 March, 2025,Monday to 07 March, 2023,Friday marks the Spring Break for Spring 2025 (S25) semester which leads to No Classes.
On Monday, 10 March, 2025, during the Spring 2025 (S25) semester, First day of Mini-4 Classes is observed.
On Tuesday, 11 March, 2025, during the Spring 2025 (S25) semester, Summer 2025 Registration Opens is observed.
On Wednesday, 12 March, 2025, during the Spring 2025 (S25) semester, Mid-Semester & Mini-3 grades due by 4 pm is observed.
On Friday, 14 March, 2025, during the Spring 2025 (S25) semester, Mini-4 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 31 March, 2025, during the Spring 2025 (S25) semester, Semester pass/no pass & withdrawal deadline is observed.
Answer: "
"In the BASS paper from Interspeech 2023, what is the solution proposed to address the issue with training end-to-end speech summarization models on very large inputs?
","['rita singh_3bd320ddb25886417ae90011b00f13f5d558097b_metadata.txt', 'bhiksha raj_3bd320ddb25886417ae90011b00f13f5d558097b_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the BASS paper from Interspeech 2023, what is the solution proposed to address the issue with training end-to-end speech summarization models on very large inputs?

Context: Faculty Name: rita singh
Paperid: 3bd320ddb25886417ae90011b00f13f5d558097b
Title: BASS: Block-wise Adaptation for Speech Summarization
Year: 2023
Abstract: End-to-end speech summarization has been shown to improve performance over cascade baselines. However, such models are difficult to train on very large inputs (dozens of minutes or hours) owing to compute restrictions and are hence trained with truncated model inputs. Truncation leads to poorer models, and a solution to this problem rests in block-wise modeling, i.e., processing a portion of the input frames at a time. In this paper, we develop a method that allows one to train summarization models on very long sequences in an incremental manner. Speech summarization is realized as a streaming process, where hypothesis summaries are updated every block based on new acoustic information. We devise and test strategies to pass semantic context across the blocks. Experiments on the How2 dataset demonstrate that the proposed block-wise training method improves by 3 points absolute on ROUGE-L over a truncated input baseline.
Authors: Roshan Sharma, Kenneth Zheng, Siddhant Arora, Shinji Watanabe, Rita Singh, B. Raj
Venue: Interspeech
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper develops a method that allows one to train summarization models on very long sequences in an incremental manner and devise and test strategies to pass semantic context across the blocks.'}
Url: https://arxiv.org/pdf/2307.08217
Faculty Name: bhiksha raj
Paperid: 3bd320ddb25886417ae90011b00f13f5d558097b
Title: BASS: Block-wise Adaptation for Speech Summarization
Year: 2023
Abstract: End-to-end speech summarization has been shown to improve performance over cascade baselines. However, such models are difficult to train on very large inputs (dozens of minutes or hours) owing to compute restrictions and are hence trained with truncated model inputs. Truncation leads to poorer models, and a solution to this problem rests in block-wise modeling, i.e., processing a portion of the input frames at a time. In this paper, we develop a method that allows one to train summarization models on very long sequences in an incremental manner. Speech summarization is realized as a streaming process, where hypothesis summaries are updated every block based on new acoustic information. We devise and test strategies to pass semantic context across the blocks. Experiments on the How2 dataset demonstrate that the proposed block-wise training method improves by 3 points absolute on ROUGE-L over a truncated input baseline.
Authors: Roshan Sharma, Kenneth Zheng, Siddhant Arora, Shinji Watanabe, Rita Singh, B. Raj
Venue: Interspeech
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper develops a method that allows one to train summarization models on very long sequences in an incremental manner and devise and test strategies to pass semantic context across the blocks.'}
Url: https://arxiv.org/pdf/2307.08217
Answer: "
"What is the mechanism that is critical to language learning in young children?
","['graham neubig_e7b3b692b0816821aafc0d354749bc3802cbf6ac_metadata.txt', 'yonatan bisk_e7b3b692b0816821aafc0d354749bc3802cbf6ac_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the mechanism that is critical to language learning in young children?

Context: Faculty Name: graham neubig
Paperid: e7b3b692b0816821aafc0d354749bc3802cbf6ac
Title: Computational Language Acquisition with Theory of Mind
Year: 2023
Abstract: Unlike current state-of-the-art language models, young children actively acquire language through interactions with their surrounding environment and caretakers. One mechanism that has been argued to be critical to language learning is the ability to infer the mental states of other agents in social environments, coined Theory of Mind (ToM) by Premack&Woodruff (1978). Drawing inspiration from the modern operationalized versions of ToM implemented in Rabinowitz et al. (2018) and Zhu et al. (2021), we build language-learning agents equipped with ToM, and measure its effects on the learning process. We model ToM by giving the speaker agent an internal listener model that is trained alongside the speaker and used to rerank potential utterances. We experiment with varying task difficulty, hypothesizing that models will acquire more complex language to adapt to stronger environmental pressures. We find that training speakers with a highly weighted ToM listener component leads to performance gains in our image referential game setting. We also find some evidence that increasing task difficulty in the training process results in more fluent and precise utterances in evaluation. This suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition.
Authors: Andy T. Liu, Hao Zhu, Emmy Liu, Yonatan Bisk, Graham Neubig
Venue: International Conference on Learning Representations
Tldr: {'model': 'tldr@v2.0.0', 'text': ""It is found that training speakers with a highly weighted ToM listener component leads to performance gains in the authors' image referential game setting, and suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition.""}
Url: http://arxiv.org/pdf/2303.01502
Faculty Name: yonatan bisk
Paperid: e7b3b692b0816821aafc0d354749bc3802cbf6ac
Title: Computational Language Acquisition with Theory of Mind
Year: 2023
Abstract: Unlike current state-of-the-art language models, young children actively acquire language through interactions with their surrounding environment and caretakers. One mechanism that has been argued to be critical to language learning is the ability to infer the mental states of other agents in social environments, coined Theory of Mind (ToM) by Premack&Woodruff (1978). Drawing inspiration from the modern operationalized versions of ToM implemented in Rabinowitz et al. (2018) and Zhu et al. (2021), we build language-learning agents equipped with ToM, and measure its effects on the learning process. We model ToM by giving the speaker agent an internal listener model that is trained alongside the speaker and used to rerank potential utterances. We experiment with varying task difficulty, hypothesizing that models will acquire more complex language to adapt to stronger environmental pressures. We find that training speakers with a highly weighted ToM listener component leads to performance gains in our image referential game setting. We also find some evidence that increasing task difficulty in the training process results in more fluent and precise utterances in evaluation. This suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition.
Authors: Andy T. Liu, Hao Zhu, Emmy Liu, Yonatan Bisk, Graham Neubig
Venue: International Conference on Learning Representations
Tldr: {'model': 'tldr@v2.0.0', 'text': ""It is found that training speakers with a highly weighted ToM listener component leads to performance gains in the authors' image referential game setting, and suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition.""}
Url: http://arxiv.org/pdf/2303.01502
Answer: "
"How many authors are listed on the SPAE paper?
","['alexander hauptmann_376f494126d1ea4f571ea0263c43ac2b6331800a_content_1.txt', 'yonatan bisk_376f494126d1ea4f571ea0263c43ac2b6331800a_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many authors are listed on the SPAE paper?

Context: Title: SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs
Authors: Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A. Ross, Irfan Essa, Yonatan Bisk, Ming-Hsuan Yang, Kevin Murphy, Alexander G. Hauptmann, Lu Jiang
Section: C Additional Quantitative Results
saturates quite early.
Title: SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs
Authors: Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A. Ross, Irfan Essa, Yonatan Bisk, Ming-Hsuan Yang, Kevin Murphy, Alexander G. Hauptmann, Lu Jiang
Section: C Additional Quantitative Results
saturates quite early.
Answer: "
"What is the BERTScore achieved by BASS-adapt on the How-2 test set?
","['bhiksha raj_3bd320ddb25886417ae90011b00f13f5d558097b_content_2.txt', 'rita singh_3bd320ddb25886417ae90011b00f13f5d558097b_content_2.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the BERTScore achieved by BASS-adapt on the How-2 test set?

Context: Title: BASS: Block-wise Adaptation for Speech Summarization
Authors: Roshan Sharma, Kenneth Zheng, Rita Singh, Bhiksha Raj
Section: 7. References
Ann Arbor, Michigan: Association for Computational Linguistics, Jun. 2005, pp. 65–72. [Online]. Available: https://aclanthology.org/W05-0909 [27] T. Zhang*, V. Kishore*, F. Wu*, K. Q. Weinberger, and Y. Artzi, “Bertscore: Evaluating text generation with bert,” in International Conference on Learning Representations, 2020. [Online]. Available: https://openreview.net/forum?id=SkeHuCVFDr
Title: BASS: Block-wise Adaptation for Speech Summarization
Authors: Roshan Sharma, Kenneth Zheng, Rita Singh, Bhiksha Raj
Section: 7. References
Ann Arbor, Michigan: Association for Computational Linguistics, Jun. 2005, pp. 65–72. [Online]. Available: https://aclanthology.org/W05-0909 [27] T. Zhang*, V. Kishore*, F. Wu*, K. Q. Weinberger, and Y. Artzi, “Bertscore: Evaluating text generation with bert,” in International Conference on Learning Representations, 2020. [Online]. Available: https://openreview.net/forum?id=SkeHuCVFDr
Answer: "
"What was the model used for early buggies in the 1930s?
","['bhiksha raj_11c50900f50036fb3247be7c83849a8774a4ba60_content_0.txt', 'cmubuggy_d403.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What was the model used for early buggies in the 1930s?

Context: Title: FIXED INTER-NEURON COVARIABILITY INDUCES ADVERSARIAL ROBUSTNESS
Authors: Muhammad A. Shah, Bhiksha Raj
Section: 7. REFERENCES
[1] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus, “Intriguing properties of neural networks,” in ICLR, 2014. [2] Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry, “Adversarial examples are not bugs, they are features,” Advances in neural information processing systems, vol. 32, 2019. [3] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu, “Towards deep learning models resistant to adversarial attacks,” arXiv preprint arXiv:1706.06083, 2017. [4] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter, “Certified adversarial robustness via randomized smoothing,” in International Conference on Machine Learning. PMLR, 2019. [5] Hadi Salman, Mingjie Sun, Greg Yang, Ashish Kapoor, and J Zico Kolter, “Denoised smoothing: A provable defense for pretrained classifiers,” NeurIPS, vol. 33, pp. 21945–21957, 2020. [6] Muhammad A Shah and Bhiksha Raj, “Less is more: Training on low-fidelity images improves robustness to adversarial attacks,” in Submitted to ICLR., 2023, under review. [7] Muhammad A Shah, Raphael Olivier, and Bhiksha Raj, “Towards adversarial robustness via compact feature representations,” in ICASSP. IEEE, 2021. [8] Raphael Olivier, Bhiksha Raj, and Muhammad Shah, “Highfrequency adversarial defense for speech and audio,” in ICASSP. IEEE, 2021, pp. 2995–2999.
This year's buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It's one of the few times spectators can see buggies up close. Dave Singh explains how Fringe builds its buggies. Each team keeps its actual processes a secret, but each buggy has certain features such as a body, pushbar, wheels and driving and braking mechanisms. Behind Singh are other buggies built by Fringe. Behind the Wheel Drivers, have the closest connection to the vehicles, aside from the mechanics who spent countless hours building and maintaining individual buggies. While races are brief, each driver must log a number of practice runs to qualify to race. Teams practice nights and weekends throughout the fall and spring semesters as weather permits. Buggies are often built around drivers, so the fit can be snug, and drivers are often less than 5 feet, 3 inches in height. Most — but not all — are women. Tishya Girdhar, a junior in neuroscience, is social chair for Fringe. She came to CMU wanting to drive, but first she had to get over being claustrophobic. She started as a mechanic and got behind the wheel for last year's Sweepstakes. ""Our team's philosophy is to teach everyone how to do everything,"" Girdhar said. ""But I came to CMU wanting to drive. I wanted to drive so badly."" Amy Chen demonstrates how a driver is positioned inside of Boson, a Fringe buggy built in 2016. Safety is a top priority for teams. All drivers are required to have five pieces of safety gear: mouth guard, goggles, a harness that includes three points of contact to the body of the buggy, gloves and a helmet. And during practice runs and races, flaggers man the course to let drivers know if it's safe to proceed. The streets of the race course haven't changed, but the condition of the course varies due to potholes, which can make or break a fast run. The last course records were set in 2017, but this year potholes are making for some challenging conditions, said Amy Chen, a senior in psychology who has been driving for four years.
Answer: "
"Which two LLMs were explored in the SPAE paper?
","['alexander hauptmann_376f494126d1ea4f571ea0263c43ac2b6331800a_content_0.txt', 'yonatan bisk_376f494126d1ea4f571ea0263c43ac2b6331800a_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which two LLMs were explored in the SPAE paper?

Context: Title: SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs
Authors: Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A. Ross, Irfan Essa, Yonatan Bisk, Ming-Hsuan Yang, Kevin Murphy, Alexander G. Hauptmann, Lu Jiang
Section: D Additional Qualitative Examples
Token pyramid visualization. Fig. 13 shows tokenization and reconstruction samples by a 6-layer SPAE from ImageNet validation set. Key concepts are captured in the first few layers, whereas the later layers focus on the visual appearance. In the coffee machine example, many keywords are present to describe various aspects from the stove to the thermometer. In the parrot case, a single unified concept is repeatedly highlighted. Coarse-to-fine reconstruction. Fig. 14 shows reconstruction samples by SPAE-8 from ImageNet validation set. We compare the reconstructed images from layer 5 to layer 8 to demonstrate the coarse-to-fine progress. Conditional image interpolation. To the best of our knowledge, there have been no successful attempts that demonstrate generic image generation capability using a frozen LLM. To this end, we define a very simple setup to explore the interpolation capability of LLM, where the conditions are integers from 1 to 9. The target images are created with different pixel-space transformations detailed in . As shown in Fig. 15, images 1-4 and 6-9 are fed as context to produce image 5, where the model interpolates the variable property. Fig. 16 shows generated samples at 256×256 resolution under the same setup. Conditional image denoising. We use PAR decoding to produce the first 5 token layers with taskspecific conditions, followed by task-agnostic PNAR decoding to fill in layer 6. Fig. 17 visualizes the input pairs for the image-to-image generation examples in Figs. 7 and 9, with more examples in Fig. 18. Under the in-context denoising setup, the LLM generates novel images based on the provided context, where multiple different generations can be obtained. Multimodal outputs. Fig.
Title: SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs
Authors: Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A. Ross, Irfan Essa, Yonatan Bisk, Ming-Hsuan Yang, Kevin Murphy, Alexander G. Hauptmann, Lu Jiang
Section: D Additional Qualitative Examples
Token pyramid visualization. Fig. 13 shows tokenization and reconstruction samples by a 6-layer SPAE from ImageNet validation set. Key concepts are captured in the first few layers, whereas the later layers focus on the visual appearance. In the coffee machine example, many keywords are present to describe various aspects from the stove to the thermometer. In the parrot case, a single unified concept is repeatedly highlighted. Coarse-to-fine reconstruction. Fig. 14 shows reconstruction samples by SPAE-8 from ImageNet validation set. We compare the reconstructed images from layer 5 to layer 8 to demonstrate the coarse-to-fine progress. Conditional image interpolation. To the best of our knowledge, there have been no successful attempts that demonstrate generic image generation capability using a frozen LLM. To this end, we define a very simple setup to explore the interpolation capability of LLM, where the conditions are integers from 1 to 9. The target images are created with different pixel-space transformations detailed in . As shown in Fig. 15, images 1-4 and 6-9 are fed as context to produce image 5, where the model interpolates the variable property. Fig. 16 shows generated samples at 256×256 resolution under the same setup. Conditional image denoising. We use PAR decoding to produce the first 5 token layers with taskspecific conditions, followed by task-agnostic PNAR decoding to fill in layer 6. Fig. 17 visualizes the input pairs for the image-to-image generation examples in Figs. 7 and 9, with more examples in Fig. 18. Under the in-context denoising setup, the LLM generates novel images based on the provided context, where multiple different generations can be obtained. Multimodal outputs. Fig.
Answer: "
"What is the contact number of the Fitness Operations Manager?
","['handbook_phd_2023-2024.txt', 'mlt-student-handbook-2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the contact number of the Fitness Operations Manager?

Context: More information regarding these services, business hours, locations and 
contact 
information 
can 
be 
found 
on 
the 
Office 
of 
Tartan 
Ink 
website 
at:  
https://www.cmu.edu/tartanink/aboutus/index.html. 
6.16 University Center 
www.cmu.edu/university-center 
The Jared L. Cohon University Center is a centerpiece of the campus that provides a space for 
special events, physical fitness, student organizations and various activities, as well as 
LTI Ph.D. Graduate Student Handbook 
Page 39 
 
accommodating retail and dining services. As the campus crossroads, the University Center 
functions as a place for students to interact, get involved and enjoy new experiences. Visit the 
University Center website for information about campus eateries, ATMs and PNC Bank, fitness 
rooms and schedules, retail stores, scheduling University Center space, the public prayer room, 
student organizations, and the Wright-Rogal Chapel. 
The University Center Information Desk (first floor of the Cohon Center next to Wean Commons 
and Kirr Commons) is the location if you want to know about upcoming campus events or have 
questions about Carnegie Mellon in general, call the Information Desk at 412-268-2107. The 
Information Desk not only provides information about campus events, but also sells postage 
stamps, makes copies, sends faxes, distributes campus maps, manages a lost & found, and has 
information brochures about Pittsburgh and the campus. 
6.17 Athletic/Fitness Facilities 
 www.cmu.edu/athletics 
For the participant or the spectator, Carnegie Mellon offers intercollegiate athletics, intramural 
sports, physical education classes and club sports. The Athletics Department also offers aerobics 
classes in the University Center as well as occasional workshops and instruction related to fitness 
and health. The administrative offices are located in the Cohon University Fitness Center. 
The University Centers recreational facilities include an eight-lane pool, racquetball and squash 
courts, aerobics room, fitness center and gym for basketball and volleyball.
More information regarding these services, locations and 
contact information can be found in The Word at:  
www.cmu.edu/student-affairs/theword//campus_resources/copyprintmail.html.  
7.17 University Center 
www.cmu.edu/university-center 
The University Center is a centerpiece of the campus that provides a space for special events, 
physical fitness, student organizations and various activities, as well as accommodating retail 
and dining services. As the campus crossroads, the University Center functions as a place for 
students to interact, get involved and enjoy new experiences. Visit the University Center 
website for information about campus eateries, ATMs and PNC Bank, fitness rooms and 
schedules, retail stores, scheduling University Center space, the public prayer room, student 
organizations, and the Wright-Rogal Chapel. 
The University Center Information Desk is the location if you want to know about upcoming 
campus events or have questions about Carnegie Mellon in general. You can call the 
Information Desk at 412-268-2107. The Information Desk not only provides information about 
campus events, but also sells postage stamps, makes copies, sends faxes, distributes campus 
maps, manages a lost & found, and has informational brochures about Pittsburgh and the 
campus. 
7.18 Athletic/Fitness Facilities 
 www.cmu.edu/athletics 
For the participant or the spectator, Carnegie Mellon offers intercollegiate athletics, intramural 
sports, physical education classes and club sports. The Athletics Department also offers 
aerobics classes in the University Center, as well as occasional workshops and instruction 
related to fitness and health. The Athletics Office is located in the Skibo Gymnasium. 
MLT Graduate Student Handbook 
Page 36 
 
The University Centers recreational facilities include an eight-lane pool, racquetball and squash 
courts, class studios, a fully-equipped fitness center, and a gym for basketball and volleyball. All 
users must present a current Carnegie Mellon Card to use these facilities. 
7.19 CMU Alert 
www.cmu.edu/alert  
CMU Alert sends voice and/or text messages to registered phones in the event of a campus 
emergency that threatens public safety or during tests of the system in the spring and fall 
semesters. Students can register for CMU Alert through the website.
Answer: "
"How many components or phases are in the MultiBench toolkit pipeline?
","['louis philippe morency_40fb36ee67fdde99b196b4d1772de114aa821698_metadata.txt', 'louis philippe morency_40fb36ee67fdde99b196b4d1772de114aa821698_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many components or phases are in the MultiBench toolkit pipeline?

Context: Faculty Name: louis philippe morency
Paperid: 40fb36ee67fdde99b196b4d1772de114aa821698
Title: MultiZoo & MultiBench: A Standardized Toolkit for Multimodal Deep Learning
Year: 2023
Abstract: Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MultiZoo, a public toolkit consisting of standardized implementations of>20 core multimodal algorithms and MultiBench, a large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. Together, these provide an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, we offer a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MultiBench paves the way towards a better understanding of the capabilities and limitations of multimodal models, while ensuring ease of use, accessibility, and reproducibility. Our toolkits are publicly available, will be regularly updated, and welcome inputs from the community.
Authors: P. Liang, Yiwei Lyu, Xiang Fan, Arav Agarwal, Yun Cheng, Louis-Philippe Morency, R. Salakhutdinov
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'MultiZoo is released, a public toolkit consisting of standardized implementations of>20 core multimodal algorithms and MultiBench, a large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas that provide an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation.'}
Url: http://arxiv.org/pdf/2306.16413
Title: MULTIZOO & MULTIBENCH: A Standardized Toolkit for Multimodal Deep Learning
Authors: Paul Pu Liang, Yiwei Lyu, Xiang Fan, Arav Agarwal, Yun Cheng, Louis-Philippe Morency, Ruslan Salakhutdinov, Antti Honkela
Section: 4 Conclusion
In conclusion, we present MULTIZOO and MULTIBENCH, a large-scale open-source toolkit unifying previously disjoint efforts in multimodal research with a focus on ease of use, accessibility, and reproducibility, thereby enabling a deeper understanding of multimodal models. Through its unprecedented range of research areas, datasets, modalities, tasks, and evaluation metrics, our toolkit paves the way toward building more generalizable, lightweight, and robust multimodal models. These tools have already been used for new directions for visualizing trained models (Liang et al., 2023a), large-scale multimodal foundation models (Liang et al., 2023b), multimodal fusion methods (Huang et al., 2022; Xue and Marculescu, 2023), and other theoretical and empirical studies of multimodal learning in applications ranging from robotics (Li et al., 2022) and HCI (Wu et al., 2023) to IoT (Hou et al., 2023), remote sensing (Xiong et al., 2022), and healthcare (Suvon et al., 2022). Our toolkits are publicly available, regularly updated with new tasks and modeling paradigms, and welcome inputs from the community.
Answer: "
"How many scientific challenges in spoken language translation did the IWSLT 2023 shared tasks address?
","['shinji watanabe_f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b_metadata.txt', 'shinji watanabe_papers.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many scientific challenges in spoken language translation did the IWSLT 2023 shared tasks address?

Context: Faculty Name: shinji watanabe
Paperid: f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b
Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN
Year: 2023
Abstract: This paper reports on the shared tasks organized by the 20th IWSLT Conference. The shared tasks address 9 scientific challenges in spoken language translation: simultaneous and offline translation, automatic subtitling and dubbing, speech-to-speech translation, multilingual, dialect and low-resource speech translation, and formality control. The shared tasks attracted a total of 38 submissions by 31 teams. The growing interest towards spoken language translation is also witnessed by the constantly increasing number of shared task organizers and contributors to the overview paper, almost evenly distributed across industry and academia.
Authors: Sweta Agrawal, Antonios Anastasopoulos, L. Bentivogli, Ondrej Bojar, Claudia Borg, Marine Carpuat, R. Cattoni, Mauro Cettolo, Mingda Chen, William Chen, K. Choukri, Alexandra Chronopoulou, Anna Currey, T. Declerck, Qianqian Dong, Kevin Duh, Y. Estève, Marcello Federico, Souhir Gahbiche, B. Haddow, B. Hsu, Phu Mon Htut, H. Inaguma, Dávid Javorský, J. Judge, Yasumasa Kano, Tom Ko, Rishu Kumar, Peng Li, Xutai Ma, Prashant Mathur, E. Matusov, Paul McNamee, John P. McCrae, Kenton Murray, Maria Nadejde, Satoshi Nakamura, Matteo Negri, H. Nguyen, J. Niehues, Xing Niu, Atul Kr.
February to March 2023
Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study
Joint Modelling of Spoken Language Understanding Tasks with Integrated Dialog History
Enhancing End-to-End Conversational Speech Translation Through Target Language Context Utilization
Challenges of Corporate Alliance CLOMA toward Plastic Litter
The Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction
Reproducing Whisper-Style Training Using An Open-Source Toolkit And Publicly Available Data
The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios
Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing
ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit
Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge
Fully Unsupervised Topic Clustering of Unlabelled Spoken Audio Using Self-Supervised Representation Learning and Topic Model
TAPLoss: A Temporal Acoustic Parameter Loss for Speech Enhancement
End-to-End Speech Recognition: A Survey
The Multimodal Information Based Speech Processing (Misp) 2022 Challenge: Audio-Visual Diarization And Recognition
DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models
An external quality assessment feasibility study; cross laboratory comparison of haemagglutination inhibition assay and microneutralization assay performance for seasonal influenza serology testing: A FLUCOP study
Multi-Channel Speaker Extraction with Adversarial Training: The Wavlab Submission to The Clarity ICASSP 2023 Grand Challenge
AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual Representation Models
Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation
Deep Speech Synthesis from MRI-Based Articulatory Representations
FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN
Speech collage: code-switched audio generation by collaging monolingual corpora
Exploration on HuBERT with Multiple Resolutions
A Randomized, Double-Blind,
Answer: "
"Who is the first author of ""Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System""?
","['daphne ippolito_03fb535de5cfcf435705a079334ac60f501226ab_content_0.txt', 'daphne ippolito_03fb535de5cfcf435705a079334ac60f501226ab_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who is the first author of ""Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System""?

Context: Title: Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System
Authors: Daphne Ippolito, Nicholas Carlini, Katherine Lee, Milad Nasr, Yun William Yu
Section: G Additional Results
Table 6 gives the numbers used in Figure 2 in the main paper, as well as the root mean-square error between the true and estimated p values.
Faculty Name: daphne ippolito
Paperid: 03fb535de5cfcf435705a079334ac60f501226ab
Title: Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System
Year: 2023
Abstract: Neural language models are increasingly deployed into APIs and websites that allow a user to pass in a prompt and receive generated text. Many of these systems do not reveal generation parameters. In this paper, we present methods to reverse-engineer the decoding method used to generate text (i.e., top-_k_ or nucleus sampling). Our ability to discover which decoding strategy was used has implications for detecting generated text. Additionally, the process of discovering the decoding strategy can reveal biases caused by selecting decoding settings which severely truncate a model’s predicted distributions. We perform our attack on several families of open-source language models, as well as on production systems (e.g., ChatGPT).
Authors: Daphne Ippolito, Nicholas Carlini, Katherine Lee, Milad Nasr, Yun William Yu
Venue: International Conference on Natural Language Generation
Tldr: {'model': 'tldr@v2.0.0', 'text': 'Methods to reverse-engineer the decoding method used to generate text (i.e., top-_k_ or nucleus sampling) are presented, which has implications for detecting generated text.'}
Url: https://arxiv.org/pdf/2309.04858
Answer: "
"According to ChatGPT MT, what is the most important feature in determining ChatGPT's relative ability to translate a language?
","['david mortensen_11a571eaab42a6ffb1d938635a093315e392756d_metadata.txt', 'graham neubig_11a571eaab42a6ffb1d938635a093315e392756d_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: According to ChatGPT MT, what is the most important feature in determining ChatGPT's relative ability to translate a language?

Context: Faculty Name: david mortensen
Paperid: 11a571eaab42a6ffb1d938635a093315e392756d
Title: ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages
Year: 2023
Abstract: Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs’ MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world’s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language’s resource level is the most important feature in determining ChatGPT’s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.
Authors: Nathaniel R. Robinson, Perez Ogayo, David R. Mortensen, Graham Neubig
Venue: Conference on Machine Translation
Tldr: {'model': 'tldr@v2.0.0', 'text': ""This work presents the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark, and reveals that a language’s resource level is the most important feature in determining ChatGPT's relative ability to translate it, and suggests thatChatGPT is especially disadvantaged for LRLs and African languages.""}
Url: https://arxiv.org/pdf/2309.07423
Faculty Name: graham neubig
Paperid: 11a571eaab42a6ffb1d938635a093315e392756d
Title: ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages
Year: 2023
Abstract: Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs’ MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world’s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language’s resource level is the most important feature in determining ChatGPT’s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.
Authors: Nathaniel R. Robinson, Perez Ogayo, David R. Mortensen, Graham Neubig
Venue: Conference on Machine Translation
Tldr: {'model': 'tldr@v2.0.0', 'text': ""This work presents the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark, and reveals that a language’s resource level is the most important feature in determining ChatGPT's relative ability to translate it, and suggests thatChatGPT is especially disadvantaged for LRLs and African languages.""}
Url: https://arxiv.org/pdf/2309.07423
Answer: "
"What is the proposed approach in the paper ""Rethinking Voice-Face Correlation: A Geometry View""?
","['bhiksha raj_5a3307b2e64bbcaff1202e261b8a83f7d03418a8_content_0.txt', 'rita singh_5a3307b2e64bbcaff1202e261b8a83f7d03418a8_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the proposed approach in the paper ""Rethinking Voice-Face Correlation: A Geometry View""?

Context: Title: Rethinking Voice-Face Correlation: A Geometry View
Authors: Xiang Li, Yandong Wen, Muqiao Yang, Jinglu Wang, Rita Singh, Bhiksha Raj, Mohamed bin Zayed
Section: 5 CONCLUSION
In conclusion, this paper presents a novel approach to exploring the voice-face correlation by focusing on the geometric aspects of the face rather than relying on semantic cues such as gender, age, and emotion. The proposed voice-anthropometric measurement (AM)face paradigm identifies predictable facial AMs from the voice to guide 3D face reconstruction, which results in significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium. This approach not only eliminates the influence of unpredictable AMs but also offers a new perspective on voice-face correlation, which can be valuable for anthropometry science. The results of this study open up possibilities for future research in this area, such as developing more accurate voice-guided face synthesis techniques and a better understanding of the relationship between voice and facial geometry. Rethinking Voice-Face Correlation: A Geometry View Conference’23, July 2023, Ottawa, Canada
Title: Rethinking Voice-Face Correlation: A Geometry View
Authors: Xiang Li, Yandong Wen, Muqiao Yang, Jinglu Wang, Rita Singh, Bhiksha Raj, Mohamed bin Zayed
Section: 5 CONCLUSION
In conclusion, this paper presents a novel approach to exploring the voice-face correlation by focusing on the geometric aspects of the face rather than relying on semantic cues such as gender, age, and emotion. The proposed voice-anthropometric measurement (AM)face paradigm identifies predictable facial AMs from the voice to guide 3D face reconstruction, which results in significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium. This approach not only eliminates the influence of unpredictable AMs but also offers a new perspective on voice-face correlation, which can be valuable for anthropometry science. The results of this study open up possibilities for future research in this area, such as developing more accurate voice-guided face synthesis techniques and a better understanding of the relationship between voice and facial geometry. Rethinking Voice-Face Correlation: A Geometry View Conference’23, July 2023, Ottawa, Canada
Answer: "
"Which datasets were used in the evaluation of KALE method?
","['jamie callan_1e0a4ff0c5d2d850ff5907e310ffcedc9cad9718_metadata.txt', 'david mortensen_db14d05b18ec852f8afcd6d2d10bbd9eeaef8325_content_3.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which datasets were used in the evaluation of KALE method?

Context: Faculty Name: jamie callan
Paperid: 1e0a4ff0c5d2d850ff5907e310ffcedc9cad9718
Title: KALE: Using a K-Sparse Projector for Lexical Expansion
Year: 2023
Abstract: Recent research has proposed retrieval approaches based on sparse representations and inverted indexes, with terms produced by neural language models and leveraging the advantages from both neural retrieval and lexical matching. This paper proposes KALE, a new lightweight method of this family that uses a small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary. The KALE vocabulary captures semantic concepts than perform well when used in isolation, and perform better when extending the original lexical vocabulary, this way improving first-stage retrieval accuracy. Experiments with the MSMARCOv1 passage retrieval dataset, the TREC Deep Learning dataset, and BEIR datasets, examined the effectiveness of KALE under varying conditions. Results show that the KALE terms can replace the original lexical vocabulary, with gains in accuracy and efficiency. Combining KALE with the original lexical vocabulary, or with other learned terms, can further improve retrieval accuracy with only a modest increase in computational cost.
Authors: Luís Borges, Bruno Martins, Jamie Callan
Venue: International Conference on the Theory of Information Retrieval
Tldr: {'model': 'tldr@v2.0.0', 'text': 'KALE is a new lightweight method that uses a small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary, which can replace the original lexical vocabulary with gains in accuracy and efficiency.'}
Url: https://dl.acm.org/doi/pdf/10.1145/3578337.3605131
Title: PWESUITE: Phonetic Word Embeddings and Tasks They Facilitate
Authors: Vilém ZouharE, Kalvin Chang, Chenxuan Cui, Nathaniel CarlsonY, Nathaniel R. RobinsonC, Mrinmaya SachanE, David MortensenC
Section: 8. Bibliographical References
Liu. 2021. Incorporating syntactic and phonetic information into multimodal word embeddings using graph convolutional networks. In ICASSP International Conference on Acoustics, Speech and Signal Processing, pages 7588–7592. IEEE. Wenhao Zhu, Shuang Liu, Chaoming Liu, Xiaoya Yin, and Xiaping Xv. 2020. Learning multimodal word representations by explicitly embedding syntactic and phonetic information. IEEE Access, 8:223306–223315. Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzmán, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin. 2020. Unsupervised Cross-lingual Representation Learning at Scale. Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and Tomas Mikolov. 2018. Learning word vectors for 157 languages. In Proceedings of the International Conference on Language Resources and Evaluation (LREC 2018). Carnegie Mellon Speech Group. 2014. The Carnegie Mellon Pronouncing Dictionary 0.7b. Carnegie Mellon University. Benjamin Heinzerling and Michael Strube. 2018. BPEmb: Tokenization-free pre-trained subword embeddings in 275 languages. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). European Language Resources Association. Wenzek, Guillaume and Lachaux, Marie-Anne and Conneau, Alexis and Chaudhary, Vishrav and Guzmán, Francisco and Joulin, Armand and Grave, Edouard. 2020. CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data. European Language Resources Association.
Answer: "
"Who is the main instructor for the search engines course?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Who is the main instructor for the search engines course?

Context: In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Search Engines' with Course ID 11442 and Section A offers 9.0 units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Callan located in Building GHC, Room 4401.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning:' with Course ID 11485 and Section NA offers 9.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning' with Course ID 11485 and Section NA offers 9.0 units. The Class meets Monday Wednesday between 08:00AM and 09:20AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Singh, Ramakrishnan located in Building BH, Room A51.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning:' with Course ID 11485 and Section NA offers 9.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning' with Course ID 11485 and Section A offers 9.0 units. The Class meets Friday between 08:00AM and 09:20AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ramakrishnan, Singh located in Building BH, Room A51.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning' with Course ID 11485 and Section B offers 9.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Singh, Ramakrishnan located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Search Engines' with Course ID 11442 and Section A offers 9.0 units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Callan located in Building GHC, Room 4401.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning:' with Course ID 11485 and Section NA offers 9.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning' with Course ID 11485 and Section NA offers 9.0 units. The Class meets Monday Wednesday between 08:00AM and 09:20AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Singh, Ramakrishnan located in Building BH, Room A51.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning:' with Course ID 11485 and Section NA offers 9.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning' with Course ID 11485 and Section A offers 9.0 units. The Class meets Friday between 08:00AM and 09:20AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ramakrishnan, Singh located in Building BH, Room A51.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Introduction to Deep Learning' with Course ID 11485 and Section B offers 9.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Singh, Ramakrishnan located in Building DNM, Room DNM.
Answer: "
"In spring 2024, When do Mini-4 classes begin?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, When do Mini-4 classes begin?

Context: On Monday, 05 May, 2025, during the Spring 2025 (S25) semester, Semester & Mini-4 Faculty Course Evaluations close is observed.
On Tuesday, 06 May, 2025, during the Spring 2025 (S25) semester, Make-Up Final Examinations is observed.
On Wednesday, 08 May, 2025, during the Spring 2025 (S25) semester, Graduating Students' Final Grades Due by 4 pm is observed.
On Sunday, 11 May, 2025, during the Spring 2025 (S25) semester, Commencement is observed.
On Tuesday, 13 May, 2025, during the Spring 2025 (S25) semester, Final Grades Due by 4 pm is observed.
On Tuesday, 20 May, 2025, during the Spring 2025 (S25) semester, Spring Deans' Lists Posted is observed.
On Monday, 12 May, 2025, during the Summer 2025 (M25) semester, Summer Session All, One & Mini-5 Classes Begin is observed.
On Friday, 16 May, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 add, audit, & tuition adjustment drop deadline (1) is observed.
On Friday, 23 May, 2025, during the Summer 2025 (M25) semester, Summer All add, audit, & tuition adjustment drop deadline (1) is observed.
On Friday, 26 May, 2025, during the Summer 2025 (M25) semester, Memorial Day is observed, leading to University Closed & No Classes.
On Tuesday, 27 May, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 09 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 pass/no pass & withdrawal deadline (3) is observed.
On Friday, 13 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Faculty Course Evaluations open is observed.
From 04 May, 2024,Saturday to 05 May, 2024,Sunday marks the Reading Days for Spring 2024 (S24) semester.
On Monday, 06 May, 2024, during the Spring 2024 (S24) semester, Final Examinations is observed.
On Monday, 06 May, 2024, during the Spring 2024 (S24) semester, Semester & Mini-4 Faculty Course Evaluations close is observed.
On Tuesday, 07 May, 2024, during the Spring 2024 (S24) semester, Make-Up Final Examinations is observed.
On Wednesday, 08 May, 2024, during the Spring 2024 (S24) semester, Graduating Final Grades Due by 4 pm is observed.
On Sunday, 12 May, 2024, during the Spring 2024 (S24) semester, Commencement is observed.
On Tuesday, 14 May, 2024, during the Spring 2024 (S24) semester, Final Grades Due by 4 pm is observed.
On Tuesday, 21 May, 2023, during the Spring 2024 (S24) semester, Spring Deans' Lists Posted is observed.
On Monday, 13 May, 2024, during the Summer One 2024 (M24) semester, Semester & Mini-5 Classes Begin is observed.
On Friday, 17 May, 2024, during the Summer One 2024 (M24) semester, Mini-5 add, audit, & tuition adjustment drop deadline (1) is observed.
On Friday, 24 May, 2024, during the Summer One 2024 (M24) semester, Semester add, audit, & tuition adjustment drop deadline (1) is observed.
On Monday, 27 May, 2024, during the Summer One 2024 (M24) semester, Memorial Day is observed, leading to University Closed & No Classes.
On Tuesday, 28 May, 2024, during the Summer One 2024 (M24) semester, Mini-5 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 07 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 pass/no pass & withdrawal deadline (3) is observed.
Answer: "
"Has Professor Carolyn Rose worked on Automatic Essay Scoring?
","['carolyn rose_15d85036b15388bcb0199c83c01ba833e6095a31_metadata.txt', 'carolyn rose_papers.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Has Professor Carolyn Rose worked on Automatic Essay Scoring?

Context: Faculty Name: carolyn rose
Paperid: 15d85036b15388bcb0199c83c01ba833e6095a31
Title: Towards Extracting and Understanding the Implicit Rubrics of Transformer Based Automatic Essay Scoring Models
Year: 2023
Abstract: By aligning the functional components derived from the activations of transformer models trained for AES with external knowledge such as human-understandable feature groups, the proposed method improves the interpretability of a Longformer Automatic Essay Scoring (AES) system and provides tools for performing such analyses on further neural AES systems. The analysis focuses on models trained to score essays based on organization, main idea, support, and language. The findings provide insights into the models’ decision-making processes, biases, and limitations, contributing to the development of more transparent and reliable AES systems.
Authors: James Fiacco, David Adamson, C. Rosé
Venue: Workshop on Innovative Use of NLP for Building Educational Applications
Tldr: {'model': 'tldr@v2.0.0', 'text': 'The proposed method improves the interpretability of a Longformer Automatic Essay Scoring (AES) system and provides tools for performing such analyses on further neural AES systems.'}
Url: https://aclanthology.org/2023.bea-1.20.pdf
List of 2023 Open Access papers by carolyn rose are:
Enhancing student learning and achievement through orchestration of group processes and group composition
Editorial: Nine elements for robust collaborative learning analytics: A constructive collaborative critique
High school students’ data modeling practices and processes: from modeling unstructured data to evaluating automated decisions
Linguistic representations for fewer-shot relation extraction across domains
Using counterfactual contrast to improve compositional generalization for multi-step quantitative reasoning
Towards Extracting and Understanding the Implicit Rubrics of Transformer Based Automatic Essay Scoring Models
Exploring Artificial Intelligence in English Language Arts with StoryQ
Can a family caregiver provide an evidence‐based falls prevention intervention to the person with cognitive impairment or dementia they care for?
Hypermutator emergence in experimental Escherichia coli populations is stress-type dependent
Answer: "
"In the paper ""Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations"", what does ILL leverage for modeling the imprecise label information?
","['rita singh_8665c864d71df1e918d2010778fc06712f4e5550_metadata.txt', 'bhiksha raj_8665c864d71df1e918d2010778fc06712f4e5550_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the paper ""Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations"", what does ILL leverage for modeling the imprecise label information?

Context: Faculty Name: rita singh
Paperid: 8665c864d71df1e918d2010778fc06712f4e5550
Title: Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations
Year: 2023
Abstract: Learning with reduced labeling standards, such as noisy label, partial label, and multiple label candidates, which we generically refer to as \textit{imprecise} labels, is a commonplace challenge in machine learning tasks. Previous methods tend to propose specific designs for every emerging imprecise label configuration, which is usually unsustainable when multiple configurations of imprecision coexist. In this paper, we introduce imprecise label learning (ILL), a framework for the unification of learning with various imprecise label configurations. ILL leverages expectation-maximization (EM) for modeling the imprecise label information, treating the precise labels as latent variables.Instead of approximating the correct labels for training, it considers the entire distribution of all possible labeling entailed by the imprecise information. We demonstrate that ILL can seamlessly adapt to partial label learning, semi-supervised learning, noisy label learning, and, more importantly, a mixture of these settings. Notably, ILL surpasses the existing specified techniques for handling imprecise labels, marking the first unified framework with robust and effective performance across various challenging settings. We hope our work will inspire further research on this topic, unleashing the full potential of ILL in wider scenarios where precise labels are expensive and complicated to obtain.
Authors: Hao Chen, Ankit Shah, Jindong Wang, R. Tao, Yidong Wang, Xingxu Xie, Masashi Sugiyama, Rita Singh, B. Raj
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'Imprecise label learning (ILL) is introduced, a framework for the unification of learning with various imprecise label configurations, marking the first unified framework with robust and effective performance across various challenging settings.'}
Url: https://arxiv.org/pdf/2305.12715
Faculty Name: bhiksha raj
Paperid: 8665c864d71df1e918d2010778fc06712f4e5550
Title: Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations
Year: 2023
Abstract: Learning with reduced labeling standards, such as noisy label, partial label, and multiple label candidates, which we generically refer to as \textit{imprecise} labels, is a commonplace challenge in machine learning tasks. Previous methods tend to propose specific designs for every emerging imprecise label configuration, which is usually unsustainable when multiple configurations of imprecision coexist. In this paper, we introduce imprecise label learning (ILL), a framework for the unification of learning with various imprecise label configurations. ILL leverages expectation-maximization (EM) for modeling the imprecise label information, treating the precise labels as latent variables.Instead of approximating the correct labels for training, it considers the entire distribution of all possible labeling entailed by the imprecise information. We demonstrate that ILL can seamlessly adapt to partial label learning, semi-supervised learning, noisy label learning, and, more importantly, a mixture of these settings. Notably, ILL surpasses the existing specified techniques for handling imprecise labels, marking the first unified framework with robust and effective performance across various challenging settings. We hope our work will inspire further research on this topic, unleashing the full potential of ILL in wider scenarios where precise labels are expensive and complicated to obtain.
Authors: Hao Chen, Ankit Shah, Jindong Wang, R. Tao, Yidong Wang, Xingxu Xie, Masashi Sugiyama, Rita Singh, B. Raj
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'Imprecise label learning (ILL) is introduced, a framework for the unification of learning with various imprecise label configurations, marking the first unified framework with robust and effective performance across various challenging settings.'}
Url: https://arxiv.org/pdf/2305.12715
Answer: "
"What is the target duration of the LTI PhD program in months?
","['handbook_phd_2023-2024.txt', 'mlt-student-handbook-2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the target duration of the LTI PhD program in months?

Context: 4 
Ph.D. Academic Policies 
4.1 
Duration of Study 
The target duration of the LTI Ph.D. is five years, although six years is also common. 
Carnegie Mellon graduate students are expected to complete their degree requirements within 
the standard length of time for their program of study as outlined in this Graduate Student 
Handbook. Upon completion of the graduate program degree requirements, the degree will be 
certified by the students academic program in the semester in which the student completes the 
requirements. 
Early Completion 
The Ph.D. is granted when all degree requirements are satisfied. Some students complete the 
program in fewer than five years. 
LTI Ph.D. Graduate Student Handbook 
Page 24 
 
Extended or Longer-than-Standard Competition 
Longer-than-standard degree completion may occur due to academic interruptions in making 
progress toward the degree as defined by the academic program, interruptions of full-time study 
or progress towards the degree due to serious, documented medical issues, or other unusual or 
unforeseen circumstances. 
Doctoral students who require an extended period to complete their degree requirements must 
consult with their academic program and are subject to the CMU Policy on Doctoral Student 
Status (https://www.cmu.edu/policies/student-and-student-life/doctoral-student-status.html), 
specifically the Time to Degree. 
4.1.1 Department Policy on Double Counting Courses 
An LTI Ph.D. student who uses courses taken as a Masters degree student (at Carnegie Mellon 
or elsewhere) toward their program requirements cannot use those same courses toward any 
other Masters degree offered by the school. The LTI (like other SCS units) allows its Ph.D. 
students who have passed the requirements for an LTI Masters degree to receive the masters 
degree without any additional work. Any other sharing of coursework by an LTI student between 
more than one CMU degree (e.g., receiving an MLD Masters degree that includes courses taken 
as an LTI Ph.D. student) must be explicitly approved by the LTI, on a case-by-case basis, in 
advance.  
4.1.2 External Internships  
The LTI provides summer support for its Ph.D. students, so Ph.D. students are expected to do 
research at Carnegie Mellon during the summer. However, outside experience can be a valuable 
educational experience, Ph.D.
1.2 The MLT Degree 
The Master of Language Technologies (MLT) is a research-oriented Master of Science degree 
offered by the Language Technologies Institute (LTI), a graduate department in the School of 
Computer Science at Carnegie Mellon University.  The MLT program is a 24-month program 
consisting of courses, directed research, and an optional Masters' Thesis.  Typical research 
areas include speech processing, information retrieval, machine translation, natural language 
processing, machine learning, and computational biology.  Many MLT graduates continue on to 
PhD programs in the LTI or other leading universities.  Other graduates go on to work in the 
computer industry, many at major corporate research laboratories. 
There are significant differences between CMU's different departments and degree programs in 
philosophical approach, procedures, policies, and regulations. Each department issues a 
handbook that informs graduate students of their program requirements and procedures and 
ensures that students have written access to the standard information outlined below. This 
handbook describes the policies, procedures, and requirements for the Master of Language 
Technologies (MLT) degree. 
While this handbook is specific to your academic experience in the department, there are 
several other resources and offices graduate students are encouraged to consult during their 
tenure at Carnegie Mellon University.  Information about The Word, Carnegie Mellon University 
Student Handbook, the Office of the Assistant Vice Provost for Graduate Education, the Office 
of the Dean of Student Affairs and others are included in Appendix A of this handbook. 
All policies not explicitly described in this document conform to School of Computer Science 
(SCS) policies and university policies described in The Word, Carnegie Mellon University Student 
Handbook and at the University Policies website.
Answer: "
"In spring 2024, What is the voucher deadline for Mini-3?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, What is the voucher deadline for Mini-3?

Context: On Wednesday, 05 February, 2025, during the Spring 2025 (S25) semester, Mini-3 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 17 February, 2025, during the Spring 2025 (S25) semester, Mini-3 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 17 February, 2025, during the Spring 2025 (S25) semester, Mini-3 Faculty Course Evaluations open is observed.
On Monday, 24 February, 2025, during the Spring 2025 (S25) semester, Semester course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 28 February, 2025, during the Spring 2025 (S25) semester, Mini-3 Last Day of Classes is observed.
On Friday, 28 February, 2025, during the Spring 2025 (S25) semester, Mini-3 voucher deadline (4) is observed.
On Saturday, 01 March, 2025, during the Spring 2025 (S25) semester, Mini-3 Exams is observed.
On Saturday, 01 March, 2025, during the Spring 2025 (S25) semester, Mini-3 Faculty Course Evaluations close is observed.
From 03 March, 2025,Monday to 07 March, 2023,Friday marks the Spring Break for Spring 2025 (S25) semester which leads to No Classes.
On Monday, 10 March, 2025, during the Spring 2025 (S25) semester, First day of Mini-4 Classes is observed.
On Tuesday, 11 March, 2025, during the Spring 2025 (S25) semester, Summer 2025 Registration Opens is observed.
On Wednesday, 12 March, 2025, during the Spring 2025 (S25) semester, Mid-Semester & Mini-3 grades due by 4 pm is observed.
On Friday, 14 March, 2025, during the Spring 2025 (S25) semester, Mini-4 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 31 March, 2025, during the Spring 2025 (S25) semester, Semester pass/no pass & withdrawal deadline is observed.
On Monday, 22 January, 2024, during the Spring 2024 (S24) semester, Mini-3 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 29 January, 2024, during the Spring 2024 (S24) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Wednesday, 07 February, 2024, during the Spring 2024 (S24) semester, Mini-3 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 19 February, 2024, during the Spring 2024 (S24) semester, Mini-3 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 19 February, 2024, during the Spring 2024 (S24) semester, Mini-3 Faculty Course Evaluations open is observed.
On Monday, 26 February, 2024, during the Spring 2024 (S24) semester, Semester course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 01 March, 2024, during the Spring 2024 (S24) semester, Mini-3 Last Day of Classes is observed.
On Friday, 01 March, 2024, during the Spring 2024 (S24) semester, Mini-3 voucher deadline (4) is observed.
On Saturday, 02 March, 2024, during the Spring 2024 (S24) semester, Mini-3 Exams is observed.
On Saturday, 02 March, 2024, during the Spring 2024 (S24) semester, Mini-3 Faculty Course Evaluations close is observed.
From 04 March, 2024,Monday to 08 March, 2024,Friday marks the Spring Break for Spring 2024 (S24) semester which leads to No Classes.
On Monday, 11 March, 2024, during the Spring 2024 (S24) semester, First day of Mini-4 Classes is observed.
On Monday, 11 March, 2024, during the Spring 2024 (S24) semester, Mid-Semester & Mini-3 grades due by 4 pm is observed.
On Tuesday, 12 March, 2024, during the Spring 2024 (S24) semester, Summer 2024 Registration Opens is observed.
Answer: "
"For additional information about the LT concentration for undergraduates, who should you contact?
","['program_info_LanguageTechnologiesConcentration.txt', 'mcds-student-handbook-2023_2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: For additional information about the LT concentration for undergraduates, who should you contact?

Context: Academic Program Name:
Language Technologies Concentration

Website:
https://lti.cs.cmu.edu/academics/lt-concentration.html

Overview:
Human language technologies have become an increasingly central component of computer science. Information retrieval, machine translation and speech technology are used daily by the general public, while text mining, natural language processing and language-based tutoring are common within more specialized professional or educational environments. The LTI prepares students for this world by offering a minor that gives you the opportunity to not only learn about language technologies, but to also apply that knowledge through a directed project.

Requirements:
Students interested in the language technologies minor must complete our prerequisite courses with an average grade of B (3.0) or better before applying to the program. (Students who do not meet this average must submit a letter of explanation along with their application.) Prerequisites include:
Principles of Imperative Computation (15-122)
Principles of Functional Programming (15-150)
We also strongly encourage candidates to take:
Differential and Integral Calculus (21-120) and Integration and Approximation (21-122)
Matrices and Linear Transformations (21-241) or Matrix Theory (21-242)
Probability and Computing (15-259) or Probability (21-325) or Probability Theory for Computer Scientists (36-218) or Introduction to Probability Theory (36-225)

Curriculum:
The Language Technologies Concentration requires that SCS students complete one core course and their choice of three elective courses of at least 9 units each. The electives can be chosen from a specific set of stand-alone courses. In addition to the four courses, students are required to do an undergraduate research project for at least 9 units to complete their concentration.
Course Requirements for Undergraduate Minor
9 
31 Shuttle and Escort Services . 9 
32 The WORD . 10 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
6
1 Welcome 
Welcome to the Language Technologies Institute, Master of 
Computational Data Science Program. While this handbook is specific to 
your academic experience in the department, there are several other 
resources and offices graduate students are encouraged to consult during 
their tenure at Carnegie Mellon University. Information about The Word, 
the student handbook, the Office of Graduate and Postdoctoral Affairs, the 
Office of the Dean of Students, and others are included in Appendix A of 
this handbook. 
1.1 The MCDS Degree 
The MCDS Degree The Master of Computational Data Science (MCDS) 
degree is a professional Master of Science degree offered by the Language 
Technologies Institute (LTI), a department in the School of Computer 
Science at Carnegie Mellon University. The MCDS degree offers students 
with a Bachelor's degree the opportunity to improve their training with 
advanced study in Computer Science and Machine Learning. We cater to 
students with basic analytic skills and a strong aptitude for mathematics, 
programming, and logical reasoning. An undergraduate degree in 
Computer Science is not required. Most students will complete the 
program in three semesters; students coming from other disciplines and 
students focus on developing applied research skills in preparation for 
further graduate study or research-oriented employment may require an 
additional fourth semester. 
 
The MCDS Program offers a core curriculum and several concentrations; 
students entering the program enroll in core courses in their first 
semester and select further courses to satisfy at least one concentration 
(see Section 3.3.6). Students construct their own course of study, in 
consultation with their academic advisor, in order to satisfy broad 
guidelines. Thus, a student may tailor their coursework in a given 
concentration to follow a particular area of emphasis. The MCDS program 
is typically a 16-month program consisting of courses, seminars, a 
required Capstone Project and a required summer internship or practical 
training. While some MCDS graduates continue on to PhD programs in the 
LTI or other leading universities, most graduates go on to jobs in 
 
 
7
corporate research and development laboratories. 
 
The program consists entirely of coursework and a Capstone Project, and 
no Master’s Thesis is required.
Answer: "
"In spring 2024, When is the final deadline for withdrawing from a Mini-3 course?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, When is the final deadline for withdrawing from a Mini-3 course?

Context: On Wednesday, 05 February, 2025, during the Spring 2025 (S25) semester, Mini-3 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 17 February, 2025, during the Spring 2025 (S25) semester, Mini-3 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 17 February, 2025, during the Spring 2025 (S25) semester, Mini-3 Faculty Course Evaluations open is observed.
On Monday, 24 February, 2025, during the Spring 2025 (S25) semester, Semester course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 28 February, 2025, during the Spring 2025 (S25) semester, Mini-3 Last Day of Classes is observed.
On Friday, 28 February, 2025, during the Spring 2025 (S25) semester, Mini-3 voucher deadline (4) is observed.
On Saturday, 01 March, 2025, during the Spring 2025 (S25) semester, Mini-3 Exams is observed.
On Saturday, 01 March, 2025, during the Spring 2025 (S25) semester, Mini-3 Faculty Course Evaluations close is observed.
From 03 March, 2025,Monday to 07 March, 2023,Friday marks the Spring Break for Spring 2025 (S25) semester which leads to No Classes.
On Monday, 10 March, 2025, during the Spring 2025 (S25) semester, First day of Mini-4 Classes is observed.
On Tuesday, 11 March, 2025, during the Spring 2025 (S25) semester, Summer 2025 Registration Opens is observed.
On Wednesday, 12 March, 2025, during the Spring 2025 (S25) semester, Mid-Semester & Mini-3 grades due by 4 pm is observed.
On Friday, 14 March, 2025, during the Spring 2025 (S25) semester, Mini-4 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 31 March, 2025, during the Spring 2025 (S25) semester, Semester pass/no pass & withdrawal deadline is observed.
On Monday, 22 January, 2024, during the Spring 2024 (S24) semester, Mini-3 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 29 January, 2024, during the Spring 2024 (S24) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Wednesday, 07 February, 2024, during the Spring 2024 (S24) semester, Mini-3 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 19 February, 2024, during the Spring 2024 (S24) semester, Mini-3 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 19 February, 2024, during the Spring 2024 (S24) semester, Mini-3 Faculty Course Evaluations open is observed.
On Monday, 26 February, 2024, during the Spring 2024 (S24) semester, Semester course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 01 March, 2024, during the Spring 2024 (S24) semester, Mini-3 Last Day of Classes is observed.
On Friday, 01 March, 2024, during the Spring 2024 (S24) semester, Mini-3 voucher deadline (4) is observed.
On Saturday, 02 March, 2024, during the Spring 2024 (S24) semester, Mini-3 Exams is observed.
On Saturday, 02 March, 2024, during the Spring 2024 (S24) semester, Mini-3 Faculty Course Evaluations close is observed.
From 04 March, 2024,Monday to 08 March, 2024,Friday marks the Spring Break for Spring 2024 (S24) semester which leads to No Classes.
On Monday, 11 March, 2024, during the Spring 2024 (S24) semester, First day of Mini-4 Classes is observed.
On Monday, 11 March, 2024, during the Spring 2024 (S24) semester, Mid-Semester & Mini-3 grades due by 4 pm is observed.
On Tuesday, 12 March, 2024, during the Spring 2024 (S24) semester, Summer 2024 Registration Opens is observed.
Answer: "
"In which year did The Kiltie Band have their first official performance?
","['kiltieband_d406.txt', 'Apr-11_Eventno_21_KiltieBandAlumniStudentReception.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In which year did The Kiltie Band have their first official performance?

Context: The Kiltie Band
The Kiltie Band began in 1908 with a group of just seven students dedicated to supporting Carnegie Tech football, and today's Kiltie Band continues a tradition of excellence originated over a century ago. The Band took the field for its first official performance on November 25th, 1922, on what would have been Andrew Carnegie's 87th birthday.
The Kiltie Marching Band consists of musicians and colorguard and plays at all Carnegie Mellon home football games. There is a new show every week with music ranging from traditional marches and oldies to current pop tunes and jazz standards.
After football season, the Kiltie Concert Band plays a holiday concert and two spring concerts, including a performance at Carnegie Mellon’s Spring Carnival. Additionally, a part of the band gets together and acts as a pep band for the basketball teams.
The Kiltie Band is a campus organization recognized by Student Senate and administered by the Department of Athletics, Physical Education and Recreation. Membership is open to all members of the campus/community without audition. Participants must do their best to promote the success of the ensemble through attendance, attitude, and dependability.
FAQs
Q: When are rehearsals?
A: Football season: Mondays and Thursdays from 5:30 p.m.-7:30 p.m. and Game Day 9:30-11:00a.m. Concert Season: Mondays and Thursdays from 5:30-6:30 p.m.
Location is the CUC Studio Theater.
Q: When are performances?
A: The Kiltie Band performs at all home football games, a holiday concert, and two spring concerts. Times and dates for all performances are announced at the first practice.
Q: Are there auditions?
A: No, any member of the campus community with music experience is able to join the Kiltie Band!
Q: Do I have to memorize music?
A: No, the music is changed for every show. You should invest in a good and trusty lyre.
Q: When is the first rehearsal?
A: On the first day of class (Monday) at 5:30 p.m. in the CUC Studio Theater. If you have questions, please email the Direcator at
jolisar@andrew.cmu.edu
.
Q: Can I borrow a school instrument?
A: There are several Kiltie-owned instruments available for your use.
Event: Kiltie Band Alumni & Student Reception
Date: 4/11/24
Time: 4:30 PM-6:30 PM ET
Participants/Audience: Open to alumni and families of the Kiltie community 
Event Details: 
After the Kiltie Band Spring Carnival kick-off concert, current and former Kilties are invited for food and fun!

Note: Registration required. Walk-ins are welcome if space permits. No event fee. Open to the Kiltie community and guests.
Answer: "
"What is the name of the open-scientific collaboration working on the responsible development of Large Language Models for Code?
","['daniel fried_1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a_metadata.txt', 'daniel fried_3e4085e5869f1b7959707a1e1d7d273b6057eb4e_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the name of the open-scientific collaboration working on the responsible development of Large Language Models for Code?

Context: Faculty Name: daniel fried
Paperid: 1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a
Title: SantaCoder: don't reach for the stars!
Year: 2023
Abstract: The BigCode project is an open-scientific collaboration working on the responsible development of large language models for code. This tech report describes the progress of the collaboration until December 2022, outlining the current state of the Personally Identifiable Information (PII) redaction pipeline, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data. We train 1.1B parameter models on the Java, JavaScript, and Python subsets of The Stack and evaluate them on the MultiPL-E text-to-code benchmark. We find that more aggressive filtering of near-duplicates can further boost performance and, surprisingly, that selecting files from repositories with 5+ GitHub stars deteriorates performance significantly. Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and infilling on the Java, JavaScript, and Python portions of MultiPL-E, despite being a substantially smaller model. All models are released under an OpenRAIL license at https://hf.co/bigcode.
Faculty Name: daniel fried
Paperid: 3e4085e5869f1b7959707a1e1d7d273b6057eb4e
Title: StarCoder: may the source be with you!
Year: 2023
Abstract: The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.
Answer: "
"Which shared task does ""Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting for Multilingual Document-Grounded QA"" focus on?
","['eric nyberg_444737639aeea4e1e616509e368afb0bae8f89d6_metadata.txt', 'teruko mitamura_444737639aeea4e1e616509e368afb0bae8f89d6_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which shared task does ""Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting for Multilingual Document-Grounded QA"" focus on?

Context: Faculty Name: eric nyberg
Paperid: 444737639aeea4e1e616509e368afb0bae8f89d6
Title: Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting for Multilingual Document-Grounded QA
Year: 2023
Abstract: The DialDoc 2023 shared task has expanded the document-grounded dialogue task to encompass multiple languages, despite having limited annotated data. This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-agnostic approach is superior. Additionally, the study investigates the impact of query rewriting techniques using large language models, such as ChatGPT, on multilingual, document-grounded question-answering systems. The experiments conducted demonstrate that, for the examples examined, query rewriting does not enhance performance compared to the original queries. This failure is due to topic switching in final dialogue turns and irrelevant topics being considered for query rewriting.
Authors: Srinivas Gowriraj, Soham Dinesh Tiwari, Mitali Potnis, Srijan Bansal, T. Mitamura, Eric Nyberg
Venue: Workshop on Document-grounded Dialogue and Conversational Question Answering
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-gnostic approach is superior.'}
Url: https://aclanthology.org/2023.dialdoc-1.11.pdf
Faculty Name: teruko mitamura
Paperid: 444737639aeea4e1e616509e368afb0bae8f89d6
Title: Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting for Multilingual Document-Grounded QA
Year: 2023
Abstract: The DialDoc 2023 shared task has expanded the document-grounded dialogue task to encompass multiple languages, despite having limited annotated data. This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-agnostic approach is superior. Additionally, the study investigates the impact of query rewriting techniques using large language models, such as ChatGPT, on multilingual, document-grounded question-answering systems. The experiments conducted demonstrate that, for the examples examined, query rewriting does not enhance performance compared to the original queries. This failure is due to topic switching in final dialogue turns and irrelevant topics being considered for query rewriting.
Authors: Srinivas Gowriraj, Soham Dinesh Tiwari, Mitali Potnis, Srijan Bansal, T. Mitamura, Eric Nyberg
Venue: Workshop on Document-grounded Dialogue and Conversational Question Answering
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-gnostic approach is superior.'}
Url: https://aclanthology.org/2023.dialdoc-1.11.pdf
Answer: "
"In the paper ""A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech"", what type of data did the authors use to train their TTS systems?
","['alexander rudnicky_4b8d3ede673ddeab9dfb5184da6b748d7a526754_metadata.txt', 'shinji watanabe_4b8d3ede673ddeab9dfb5184da6b748d7a526754_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the paper ""A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech"", what type of data did the authors use to train their TTS systems?

Context: Faculty Name: alexander rudnicky
Paperid: 4b8d3ede673ddeab9dfb5184da6b748d7a526754
Title: A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech
Year: 2023
Abstract: Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human-level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregressive models, leading to unintelligible synthesis, and demonstrate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ablation analyses to identify the efficacy of our methods. We show that MQTTS outperforms existing TTS systems in several objective and subjective measures.
Authors: Li-Wei Chen, Shinji Watanabe, Alexander I. Rudnicky
Venue: AAAI Conference on Artificial Intelligence
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work introduces the MQTTS system, a Text-to-Speech system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality, and shows that MqTTS outperforms existing TTS systems in several objective and subjective measures.'}
Url: http://arxiv.org/pdf/2302.04215
Faculty Name: shinji watanabe
Paperid: 4b8d3ede673ddeab9dfb5184da6b748d7a526754
Title: A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech
Year: 2023
Abstract: Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human-level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregressive models, leading to unintelligible synthesis, and demonstrate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ablation analyses to identify the efficacy of our methods. We show that MQTTS outperforms existing TTS systems in several objective and subjective measures.
Authors: Li-Wei Chen, Shinji Watanabe, Alexander I. Rudnicky
Venue: AAAI Conference on Artificial Intelligence
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work introduces the MQTTS system, a Text-to-Speech system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality, and shows that MqTTS outperforms existing TTS systems in several objective and subjective measures.'}
Url: http://arxiv.org/pdf/2302.04215
Answer: "
"In spring 2024, When is Martin Luther King Day observed?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, When is Martin Luther King Day observed?

Context: On Friday, 08 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Last Day of Classes is observed.
On Friday, 08 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 voucher deadline (4) is observed.
From 11 December, 2023,Monday to 12 December, 2023,Tuesday marks the Final Exams for Fall 2023 (F23) semester.
On Wednesday, 13 December, 2023, during the Fall 2023 (F23) semester, Reading Day is observed.
From 14 December, 2023,Thursday to 15 December, 2023,Friday marks the Final Exams for Fall 2023 (F23) semester.
On Saturday, 16 December, 2023, during the Fall 2023 (F23) semester, Reading Day is observed.
On Sunday, 17 December, 2023, during the Fall 2023 (F23) semester, Final Exams is observed.
On Monday, 18 December, 2023, during the Fall 2023 (F23) semester, Make-Up Final Exams is observed.
On Monday, 18 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Faculty Course Evaluations close is observed.
On Wednesday, 20 December, 2023, during the Fall 2023 (F23) semester, Final Grades Due by 4 pm is observed.
From 23 December, 2023,Saturday to 02 January, 2024,Tuesday marks the Winter Break for Fall 2023 (F23) semester which leads to University Closed.
On Monday, 08 January, 2023, during the Fall 2023 (F23) semester, Fall Deans' Lists Posted is observed.
On Monday, 15 January, 2024, during the Spring 2024 (S24) semester, Martin Luther King Day is observed, leading to No Classes.
On Tuesday, 16 January, 2024, during the Spring 2024 (S24) semester, First Day of Class is observed.
On Monday, 22 January, 2024, during the Spring 2024 (S24) semester, Mini-3 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 11 December, 2024, during the Fall 2024 (F24) semester, Reading Day is observed.
From 12 December, 2024,Thursday to 13 December, 2023,Friday marks the Final Exams for Fall 2024 (F24) semester.
On Saturday, 14 December, 2024, during the Fall 2024 (F24) semester, Reading Day is observed.
On Sunday, 15 December, 2024, during the Fall 2024 (F24) semester, Final Exams is observed.
On Sunday, 15 December, 2024, during the Fall 2024 (F24) semester, Semester & Mini-2 Faculty Course Evaluations close is observed.
On Monday, 16 December, 2024, during the Fall 2024 (F24) semester, Make-Up Final Exams is observed.
On Wednesday, 18 December, 2024, during the Fall 2024 (F24) semester, Final Grades Due by 4 pm is observed.
From 24 December, 2024,Tuesday to 25 December, 2023,Wednesday marks the University Closed for Fall 2024 (F24) semester.
From 31 December, 2024,Tuesday to 01 January, 2023,Wednesday marks the University Closed for Fall 2024 (F24) semester.
On Monday, 06 January, 2025, during the Fall 2024 (F24) semester, Fall Deans' Lists Posted is observed.
On Monday, 13 January, 2025, during the Spring 2025 (S25) semester, First Day of Classes is observed.
On Friday, 17 January, 2025, during the Spring 2025 (S25) semester, Mini-3 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 20 January, 2025, during the Spring 2025 (S25) semester, Martin Luther King Day is observed, leading to No Classes & University Closed.
On Monday, 27 January, 2025, during the Spring 2025 (S25) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Wednesday, 05 February, 2025, during the Spring 2025 (S25) semester, Mini-3 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
Answer: "
"Which LTI faculty members are authors on the WebArena paper?
","['graham neubig_e41482f4ee984f17382f6cdd900df094d928be06_metadata.txt', 'daniel fried_e41482f4ee984f17382f6cdd900df094d928be06_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which LTI faculty members are authors on the WebArena paper?

Context: Faculty Name: graham neubig
Paperid: e41482f4ee984f17382f6cdd900df094d928be06
Title: WebArena: A Realistic Web Environment for Building Autonomous Agents
Year: 2023
Abstract: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.
Authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper builds an environment for language-guided agents that is highly realistic and reproducible, and creates an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.'}
Url: https://arxiv.org/pdf/2307.13854
Faculty Name: daniel fried
Paperid: e41482f4ee984f17382f6cdd900df094d928be06
Title: WebArena: A Realistic Web Environment for Building Autonomous Agents
Year: 2023
Abstract: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.
Authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper builds an environment for language-guided agents that is highly realistic and reproducible, and creates an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.'}
Url: https://arxiv.org/pdf/2307.13854
Answer: "
"Where is advanced NLP taught this semester?
","['program_info_MasterofScienceinArtificialIntelligenceandInnovation.txt', 'miis-handbook_2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Where is advanced NLP taught this semester?

Context: Knowledge Requirements (72 units)
This is a set of six rigorous courses to ensure that you are able to develop advanced AI applications.
11-601, Coding Bootcamp (12 units).  First fall semester.
10-601, Machine Learning (12 units),  First fall semester.  (Normally 11-691, Math for Machine Learning, which is not being offered in Fall 2019.)
10-605, Machine Learning with Large Datasets (12 units).  First spring semester.
11-611, Natural Language Processing (12 units).  Second fall semester.
A 12-unit course in AI, NLP, or ML.  Second fall semester.
11-785, Deep Learning (12 units).  Second spring semester.
Internship 
Every student is required to complete an industry internship during the summer between the first spring and second fall semesters.  Every student must register for the internship - 11-934 (MSAII Practicum Internship).  No tuition is charged for the internship.
Electives (36 units)
You must take at least three 12-unit elective courses or equivalent. The approved electives are listed below.  If you want to take any other course for elective credit, you must have the permission of the MSAII Director.  It is recommended to take one elective in the first fall semester, one or two in the first spring semester, one or two in the second fall semester and zero or one in the second spring semester.
11-641 Machine Learning for Text Mining
11-642 Search Engines
11-747 Neural Networks for NLP
11-755 Machine Learning for Signal Processing
11-777 Advanced Multimodal Machine Learning
10-605 Machine Learning with Large Datasets
10-608 Conversational Machine Learning
10-716 Advanced Machine Learning: Theory & Methods (was 10702)
15-624 Foundations of Cyber-Physical Systems
15-645 Database Systems
15-688 Practical Data Science
15-719 Advanced Cloud Computing
15-780 Graduate Artificial Intelligence
16-720 Computer Vision
16-725 Medical Image Analysis
16-722 Sensing and Sensors
16-824 Visual Learning and Recognition
17-637 Web Application Development
17-639 Management of Software Development
17-653 Managing Software Development
17-766 Software Engineering for Startups
02-604 Fundamentals of Bioinformatics
02-718 Computational Medicine
Introduction to Question Answering 
• 11-751, Speech Recognition and Understanding 
• 11-767, On-Service Machine Learning 
• 11-797, Question Answering 
• 11-830, Computational Ethics For NLP 
4.4.3 Breadth Courses:  Machine Learning 
• 11-641, Machine Learning for Text and Graph-Based Mining 
• 11-661, Language and Statistics 
• 11-663, Applied Machine Learning 
• 11-747, Neural Networks for NLP 
• 11-755, Machine Learning for Signal Processing 
• 11-761, Language and Statistics 
• 11-777, Multimodal Machine Learning 
MIIS Graduate Student Handbook 
Page 16 
 
• 11-785, Introduction to Deep Learning 
• 10-601, Introduction to Machine Learning (Master’s) 
• 10-605, Machine Learning with Large Datasets 
• 10-701, Introduction to Machine Learning (PhD) 
• 10-707, Advanced Deep Learning 
• 10-708, Probabilistic Graphical Models 
• 10-714, Deep Learning Systems 
• 10-715, Advanced Introduction to Machine Learning 
4.5 Practice Requirements 
A student must complete at least 66 practice-oriented course units and satisfy the following 
practice-oriented requirements for both MIIS-16 and MIIS-21 programs. 
1. Directed study requirement: Students must pass 24 units (typically 12 units x 2 
semesters) in directed study under the supervision of their advisor. Directed study is a 
structured, task-oriented form of independent study that provides deep, hands-on 
experience in a particular technology area and an opportunity to work closely with a 
member of the faculty. 
2. Internship requirement: Students must complete a one-semester (typically summer) 
internship at an organization (typically a company or government agency) approved by 
the MIIS Program Director. Internships are an opportunity to apply new skills in a 
professional setting and to learn about software development in a ‘real world’ 
organization. Students with prior professional experience may petition the MIIS 
Program Director to waive this requirement.  
MIIS students that do an internship during the summer semester are required to present 
their internship at a poster session at the beginning of the following Fall semester.
Answer: "
"In spring 2024, When are final examinations for the semester and Mini-4?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, When are final examinations for the semester and Mini-4?

Context: From 04 May, 2024,Saturday to 05 May, 2024,Sunday marks the Reading Days for Spring 2024 (S24) semester.
On Monday, 06 May, 2024, during the Spring 2024 (S24) semester, Final Examinations is observed.
On Monday, 06 May, 2024, during the Spring 2024 (S24) semester, Semester & Mini-4 Faculty Course Evaluations close is observed.
On Tuesday, 07 May, 2024, during the Spring 2024 (S24) semester, Make-Up Final Examinations is observed.
On Wednesday, 08 May, 2024, during the Spring 2024 (S24) semester, Graduating Final Grades Due by 4 pm is observed.
On Sunday, 12 May, 2024, during the Spring 2024 (S24) semester, Commencement is observed.
On Tuesday, 14 May, 2024, during the Spring 2024 (S24) semester, Final Grades Due by 4 pm is observed.
On Tuesday, 21 May, 2023, during the Spring 2024 (S24) semester, Spring Deans' Lists Posted is observed.
On Monday, 13 May, 2024, during the Summer One 2024 (M24) semester, Semester & Mini-5 Classes Begin is observed.
On Friday, 17 May, 2024, during the Summer One 2024 (M24) semester, Mini-5 add, audit, & tuition adjustment drop deadline (1) is observed.
On Friday, 24 May, 2024, during the Summer One 2024 (M24) semester, Semester add, audit, & tuition adjustment drop deadline (1) is observed.
On Monday, 27 May, 2024, during the Summer One 2024 (M24) semester, Memorial Day is observed, leading to University Closed & No Classes.
On Tuesday, 28 May, 2024, during the Summer One 2024 (M24) semester, Mini-5 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 07 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 31 March, 2025, during the Spring 2025 (S25) semester, Semester pass/no pass & withdrawal deadline is observed.
On Wednesday, 02 April, 2025, during the Spring 2025 (S25) semester, Mini-4 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 03 April, 2025,Thursday to 05 April, 2023,Saturday marks the Spring Carnival for Spring 2025 (S25) semester which leads to No Classes.
From 07 April, 2025,Monday to 11 April, 2023,Friday marks the Fall 2025 Registration Week for Spring 2025 (S25) semester.
On Monday, 14 April, 2025, during the Spring 2025 (S25) semester, Mini-4 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 14 April, 2025, during the Spring 2025 (S25) semester, Semester & Mini-4 Faculty Course Evaluations open is observed.
On Friday, 25 April, 2025, during the Spring 2025 (S25) semester, Last Day of Classes is observed.
On Friday, 25 April, 2025, during the Spring 2025 (S25) semester, Semester & Mini-4 voucher deadline (4) is observed.
From 28 April, 2025,Monday to 29 April, 2023,Tuesday marks the Final Examinations for Spring 2025 (S25) semester.
On Wednesday, 30 April, 2025, during the Spring 2025 (S25) semester, Reading Day is observed.
From 01 May, 2025,Thursday to 02 May, 2023,Friday marks the Final Examinations for Spring 2025 (S25) semester.
From 03 May, 2025,Saturday to 04 May, 2023,Sunday marks the Reading Days for Spring 2025 (S25) semester.
On Monday, 05 May, 2025, during the Spring 2025 (S25) semester, Final Examinations is observed.
On Monday, 05 May, 2025, during the Spring 2025 (S25) semester, Semester & Mini-4 Faculty Course Evaluations close is observed.
Answer: "
"What is the outer structure or covering of a buggy called?
","['cmubuggy_d403.txt', 'cmubuggy_d403.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the outer structure or covering of a buggy called?

Context: An mural celebrating Buggy can be found in CMU's Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions. Sweepstakes Slang Buggy: Vehicle being raced and also a nickname for the competition. Chute: A section of the freeroll portion of the buggy course (near the southwestern end of Frew Street at its intersection with Schenley Drive) where buggies make the sharp righthand turn from Schenley Drive onto Frew Street. Chute Flagger: Team member who provides a signal for buggy drivers to know when to start the right-hand turn from Schenley Drive onto Frew Street. Driver: Person who travels with a buggy and controls the vehicles via steering and braking systems. Pushbar: Structure attached to a buggy that a person pushes to propel that buggy forward. Pusher: Person who propels a buggy via a pushbar along one of the five hills of the buggy course. Shell: Entire outer structure or covering of a buggy that determines that buggy's aerodynamic characteristics. Transition: Procedure whereby one pusher finishes pushing a buggy and the next pusher in sequence starts to push that same buggy. Members of Fringe, celebrating their 50th year of Buggy racing, allowed a rare visit into its shop for a behind-the-scenes tour. This year Fringe is planning to roll four different vehicles, built and maintained in the Fringe workshop in the basement of the East Campus Garage, known as the ""Froom."" ""We're used to saying everything with 'fr' in front of it but when we say something in front of other people, it gets them confused,"" said Fringe head mechanic Dave Singh, who will graduate in May with a bachelor's degree in mechanical engineering and biomedical engineering. Fringe vehicles often are named with the letter ""B,"" like Boson, Blueshift, Bissa and Bumper. Other teams, such as Apex, often use names that connotate fire, while the SDC (Student Dormitory Committee) team, uses names such as Vice, Bane, Avarice and Malice. This year's buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It's one of the few times spectators can see buggies up close.
This year's buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It's one of the few times spectators can see buggies up close. Dave Singh explains how Fringe builds its buggies. Each team keeps its actual processes a secret, but each buggy has certain features such as a body, pushbar, wheels and driving and braking mechanisms. Behind Singh are other buggies built by Fringe. Behind the Wheel Drivers, have the closest connection to the vehicles, aside from the mechanics who spent countless hours building and maintaining individual buggies. While races are brief, each driver must log a number of practice runs to qualify to race. Teams practice nights and weekends throughout the fall and spring semesters as weather permits. Buggies are often built around drivers, so the fit can be snug, and drivers are often less than 5 feet, 3 inches in height. Most — but not all — are women. Tishya Girdhar, a junior in neuroscience, is social chair for Fringe. She came to CMU wanting to drive, but first she had to get over being claustrophobic. She started as a mechanic and got behind the wheel for last year's Sweepstakes. ""Our team's philosophy is to teach everyone how to do everything,"" Girdhar said. ""But I came to CMU wanting to drive. I wanted to drive so badly."" Amy Chen demonstrates how a driver is positioned inside of Boson, a Fringe buggy built in 2016. Safety is a top priority for teams. All drivers are required to have five pieces of safety gear: mouth guard, goggles, a harness that includes three points of contact to the body of the buggy, gloves and a helmet. And during practice runs and races, flaggers man the course to let drivers know if it's safe to proceed. The streets of the race course haven't changed, but the condition of the course varies due to potholes, which can make or break a fast run. The last course records were set in 2017, but this year potholes are making for some challenging conditions, said Amy Chen, a senior in psychology who has been driving for four years.
Answer: "
"In fall 2023, When are the final grades due for the semester?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, When are the final grades due for the semester?

Context: On Friday, 08 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Last Day of Classes is observed.
On Friday, 08 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 voucher deadline (4) is observed.
From 11 December, 2023,Monday to 12 December, 2023,Tuesday marks the Final Exams for Fall 2023 (F23) semester.
On Wednesday, 13 December, 2023, during the Fall 2023 (F23) semester, Reading Day is observed.
From 14 December, 2023,Thursday to 15 December, 2023,Friday marks the Final Exams for Fall 2023 (F23) semester.
On Saturday, 16 December, 2023, during the Fall 2023 (F23) semester, Reading Day is observed.
On Sunday, 17 December, 2023, during the Fall 2023 (F23) semester, Final Exams is observed.
On Monday, 18 December, 2023, during the Fall 2023 (F23) semester, Make-Up Final Exams is observed.
On Monday, 18 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Faculty Course Evaluations close is observed.
On Wednesday, 20 December, 2023, during the Fall 2023 (F23) semester, Final Grades Due by 4 pm is observed.
From 23 December, 2023,Saturday to 02 January, 2024,Tuesday marks the Winter Break for Fall 2023 (F23) semester which leads to University Closed.
On Monday, 08 January, 2023, during the Fall 2023 (F23) semester, Fall Deans' Lists Posted is observed.
On Monday, 15 January, 2024, during the Spring 2024 (S24) semester, Martin Luther King Day is observed, leading to No Classes.
On Tuesday, 16 January, 2024, during the Spring 2024 (S24) semester, First Day of Class is observed.
On Monday, 22 January, 2024, during the Spring 2024 (S24) semester, Mini-3 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 11 December, 2024, during the Fall 2024 (F24) semester, Reading Day is observed.
From 12 December, 2024,Thursday to 13 December, 2023,Friday marks the Final Exams for Fall 2024 (F24) semester.
On Saturday, 14 December, 2024, during the Fall 2024 (F24) semester, Reading Day is observed.
On Sunday, 15 December, 2024, during the Fall 2024 (F24) semester, Final Exams is observed.
On Sunday, 15 December, 2024, during the Fall 2024 (F24) semester, Semester & Mini-2 Faculty Course Evaluations close is observed.
On Monday, 16 December, 2024, during the Fall 2024 (F24) semester, Make-Up Final Exams is observed.
On Wednesday, 18 December, 2024, during the Fall 2024 (F24) semester, Final Grades Due by 4 pm is observed.
From 24 December, 2024,Tuesday to 25 December, 2023,Wednesday marks the University Closed for Fall 2024 (F24) semester.
From 31 December, 2024,Tuesday to 01 January, 2023,Wednesday marks the University Closed for Fall 2024 (F24) semester.
On Monday, 06 January, 2025, during the Fall 2024 (F24) semester, Fall Deans' Lists Posted is observed.
On Monday, 13 January, 2025, during the Spring 2025 (S25) semester, First Day of Classes is observed.
On Friday, 17 January, 2025, during the Spring 2025 (S25) semester, Mini-3 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 20 January, 2025, during the Spring 2025 (S25) semester, Martin Luther King Day is observed, leading to No Classes & University Closed.
On Monday, 27 January, 2025, during the Spring 2025 (S25) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Wednesday, 05 February, 2025, during the Spring 2025 (S25) semester, Mini-3 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
Answer: "
"What is one limitation of lexical exact-match systems?
","['jamie callan_6b7eefa15c0a461afeab4fa13cf862c5340fdc2a_metadata.txt', 'jamie callan_6b7eefa15c0a461afeab4fa13cf862c5340fdc2a_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is one limitation of lexical exact-match systems?

Context: Faculty Name: jamie callan
Paperid: 6b7eefa15c0a461afeab4fa13cf862c5340fdc2a
Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms
Year: 2023
Abstract: Lexical exact-match systems perform text retrieval efficiently with sparse matching signals and fast retrieval through inverted lists, but naturally suffer from the mismatch between lexical surface form and implicit term semantics. This paper proposes to directly bridge the surface form space and the term semantics space in lexical exact-match retrieval via contextualized surface forms (CSF). Each CSF pairs a lexical surface form with a context source, and is represented by a lexical form weight and a contextualized semantic vector representation. This framework is able to perform sparse lexicon-based retrieval by learning to represent each query and document as a ""bag-of-CSFs"", simultaneously addressing two key factors in sparse retrieval: vocabulary expansion of surface form and semantic representation of term meaning. At retrieval time, it efficiently matches CSFs through exact-match of learned surface forms, and effectively scores each CSF pair via contextual semantic representations, leading to joint improvement in both term match and term scoring. Multiple experiments show that this approach successfully resolves the main mismatch issues in lexical exact-match retrieval and outperforms state-of-the-art lexical exact-match systems, reaching comparable accuracy as lexical all-to-all soft match systems as an efficient exact-match-based system.
Authors: Zhen Fan, Luyu Gao, Jamie Callan
Venue: International Conference on the Theory of Information Retrieval
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper proposes to directly bridge the surface form space and the term semantics space in lexical exact-match retrieval via contextualized surface forms (CSF), reaching comparable accuracy as lexical all-to-all soft match systems as an efficient exact- match-based system.'}
Url: https://dl.acm.org/doi/pdf/10.1145/3578337.3605126
Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms
Authors: Zhen Fan, Luyu Gao, Jamie Callan
Section: 6 CONCLUSION
This paper proposes CSurF, which performs sparse lexicon-based retrieval through constructing and matching Contextualized Surface Forms. Its retrieval process combines efficient surface form exact match and fine-grained contextualized semantic scoring, which leads to maximized model capacity while maintaining the simplicity and efficiency of exact-match-based retrieval systems. CSurF extends current term-weight based learned sparse retrieval approaches with vector term representations. On experiments across multiple datasets and retrieval settings, CSurF is able to simultaneously bridge the vocabulary and semantic mismatch in exact-match retrieval, and achieve state-of-the-art retrieval performance for lexical exact-match systems. Ablation studies and analysis further demonstrate CSurF’s ability to jointly expandmeaningful surface forms and ground surface forms to underlying semantics, which leads to increased model capacity. We also propose a simple interpolation approach in out-of-domain retrieval settings, to analyze the effect of original text vs. expanded surface forms as well as the quality of lexical form expansion on different retrieval tasks. Compared to all-to-all soft-match retrievers, CSurF achieves comparable performance across all retrieval tasks as an exact-matchbased retrieval system. CSurF is able to learn sparse connections of the original query and document terms, resolving the key efficiency issue of lexical soft-match. The retrieval efficiency of CSurF can also be further optimized with different approaches including training regularization adjustment, post-hoc index pruning, and vector representation approximation or dimension control, without significantly affecting retrieval accuracy. We hope this work encourages more research on building effective, efficient, robust and knowledge-enhanced sparse retrieval systems in the real world, as well as exploring the connection and distinction among current retrieval frameworks and systems.
Answer: "
"In fall 2023, Who is the instructor for unit 02261 on Wednesdays?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, Who is the instructor for unit 02261 on Wednesdays?

Context: The Class meets Wednesday between 11:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kaestner, Lacomis located in Building WEH, Room 8427.
In Semester Fall 2023, from the department of Software & Societal Systems, the subject titled 'Principles of Software Construction: Objects, Design, and Concurrency' with Course ID 17214 and Section D offers 12.0 units. The Class meets Wednesday between 12:00PM and 12:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kaestner, Lacomis located in Building MI, Room 355.
In Semester Fall 2023, from the department of Software & Societal Systems, the subject titled 'Principles of Software Construction: Objects, Design, and Concurrency' with Course ID 17214 and Section E offers 12.0 units. The Class meets Wednesday between 01:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kaestner, Lacomis located in Building MI, Room 348.
In Semester Fall 2023, from the department of Software & Societal Systems, the subject titled 'Principles of Software Construction: Objects, Design, and Concurrency' with Course ID 17214 and Section F offers 12.0 units. The Class meets Wednesday between 02:00PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Lacomis, Kaestner located in Building WEH, Room 5304.
In Semester Fall 2023, from the department of Software & Societal Systems, the subject titled 'Special Topics in Societal Computing: Understanding Cyber Teams' with Course ID 17301 and Section A offers 9.0 units. The Class meets Monday Wednesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Dobson located in Building SH, Room 238.
In Semester Fall 2023, from the department of Software & Societal Systems, the subject titled 'Cryptocurrencies, Blockchains and Applications' with Course ID 17303 and Section A offers 9.0 units.
The Class meets Wednesday between 11:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kaestner, Lacomis located in Building WEH, Room 8427.
In Semester Fall 2023, from the department of Software & Societal Systems, the subject titled 'Principles of Software Construction: Objects, Design, and Concurrency' with Course ID 17214 and Section D offers 12.0 units. The Class meets Wednesday between 12:00PM and 12:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kaestner, Lacomis located in Building MI, Room 355.
In Semester Fall 2023, from the department of Software & Societal Systems, the subject titled 'Principles of Software Construction: Objects, Design, and Concurrency' with Course ID 17214 and Section E offers 12.0 units. The Class meets Wednesday between 01:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kaestner, Lacomis located in Building MI, Room 348.
In Semester Fall 2023, from the department of Software & Societal Systems, the subject titled 'Principles of Software Construction: Objects, Design, and Concurrency' with Course ID 17214 and Section F offers 12.0 units. The Class meets Wednesday between 02:00PM and 02:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Lacomis, Kaestner located in Building WEH, Room 5304.
In Semester Fall 2023, from the department of Software & Societal Systems, the subject titled 'Special Topics in Societal Computing: Understanding Cyber Teams' with Course ID 17301 and Section A offers 9.0 units. The Class meets Monday Wednesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Dobson located in Building SH, Room 238.
In Semester Fall 2023, from the department of Software & Societal Systems, the subject titled 'Cryptocurrencies, Blockchains and Applications' with Course ID 17303 and Section A offers 9.0 units.
Answer: "
"In which types of benchmarks has SoftMatch shown substantial improvements?
","['bhiksha raj_100da279ee981960884a12dfc5a0697c24ed315a_metadata.txt', 'bhiksha raj_papers.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In which types of benchmarks has SoftMatch shown substantial improvements?

Context: Faculty Name: bhiksha raj
Paperid: 100da279ee981960884a12dfc5a0697c24ed315a
Title: SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning
Year: 2023
Abstract: The critical challenge of Semi-Supervised Learning (SSL) is how to effectively leverage the limited labeled data and massive unlabeled data to improve the model's generalization performance. In this paper, we first revisit the popular pseudo-labeling methods via a unified sample weighting formulation and demonstrate the inherent quantity-quality trade-off problem of pseudo-labeling with thresholding, which may prohibit learning. To this end, we propose SoftMatch to overcome the trade-off by maintaining both high quantity and high quality of pseudo-labels during training, effectively exploiting the unlabeled data. We derive a truncated Gaussian function to weight samples based on their confidence, which can be viewed as a soft version of the confidence threshold. We further enhance the utilization of weakly-learned classes by proposing a uniform alignment approach. In experiments, SoftMatch shows substantial improvements across a wide variety of benchmarks, including image, text, and imbalanced classification.
Authors: Hao Chen, R. Tao, Yue Fan, Yidong Wang, Jindong Wang, B. Schiele, Xingxu Xie, B. Raj, M. Savvides
Venue: International Conference on Learning Representations
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper revisits the popular pseudo-labeling methods via a unified sample weighting formulation and demonstrates the inherent quantity-quality trade-off problem of pseudo-labels with thresholding, which may prohibit learning.'}
Url: http://arxiv.org/pdf/2301.10921
List of 2023 Open Access papers by bhiksha raj are:
FREDOM: Fairness Domain Adaptation Approach to Semantic Scene Understanding
UTOPIA: Unconstrained Tracking Objects without Preliminary Examination via Cross-Domain Adaptation
SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning
Fixed Inter-Neuron Covariability Induces Adversarial Robustness
Understanding political polarization using language models: A dataset and method
Prolonged school closure during the pandemic time in successive waves of COVID-19- vulnerability of children to sexual abuses – A case study in Tamil Nadu, India
BASS: Block-wise Adaptation for Speech Summarization
Understanding Political Polarisation using Language Models: A dataset and method
An Approach to Ontological Learning from Weak Labels
Rethinking Voice-Face Correlation: A Geometry View
Paaploss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement
Improving Perceptual Quality, Intelligibility,
Answer: "
"In spring 2024, Who is the instructor for Advanced Deep Learning?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2024, Who is the instructor for Advanced Deep Learning?

Context: In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Independent Study: Research' with Course ID 10620 and Section A offers 612 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Shah located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Generative AI' with Course ID 10623 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Gormley, Li, Lipton located in Building GHC, Room 4401.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Practicum' with Course ID 10635 and Section I offers VAR units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Shah located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Reading and Research' with Course ID 10697 and Section A offers 1-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Shah located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Introduction to Machine Learning (PhD)' with Course ID 10701 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Chai located in Building TEP, Room 1403.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Advanced Deep Learning' with Course ID 10707 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Salakhutdinov located in Building DH, Room 2302.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Independent Study: Research' with Course ID 10620 and Section A offers 612 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Shah located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Generative AI' with Course ID 10623 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Gormley, Li, Lipton located in Building GHC, Room 4401.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Practicum' with Course ID 10635 and Section I offers VAR units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Shah located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Reading and Research' with Course ID 10697 and Section A offers 1-48 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Shah located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Introduction to Machine Learning (PhD)' with Course ID 10701 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Chai located in Building TEP, Room 1403.
In Semester Spring 2024, from the department of Machine Learning, the subject titled 'Advanced Deep Learning' with Course ID 10707 and Section A offers 12.0 units. The Class meets Monday Wednesday Friday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Salakhutdinov located in Building DH, Room 2302.
Answer: "
"Where is the President's Graduates Toast for bachelor's students going to be held?
","['Commencement.txt', 'Commencement.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Where is the President's Graduates Toast for bachelor's students going to be held?

Context: Main Commencement Ceremony
Bachelor’s, master’s and doctoral degree candidates and their guests are invited to join the main commencement ceremony on Sunday, May 12, for the conferral of all degrees.
The main ceremony will include remarks from the president, keynote speaker, student speaker and academic deans, in addition to recognition of the honorary degree recipients.
There is no limit on number of guests who can attend the main commencement ceremony and tickets are not needed.
The ceremony will take place on CMU’s campus beginning at 10 a.m. and will be approximately 1.5 hours long. All guests must be seated by 9:15 a.m. for the start of the student procession. Access to guest seating will be restricted once the student procession begins.

Diploma Ceremonies
In addition to the main commencement ceremony, each college/school/department will host a diploma ceremony to recognize all graduating students. Diploma ceremonies will be held over the course of the weekend (Friday, May 10–Sunday, May 12).
Diploma ceremonies will include the presentation of diplomas to graduates, hooding of doctoral candidates and remarks from their college/school/department leadership. Each ceremony is organized and customized by their college/school/department.
Diploma ceremonies will take place both on and off campus. Locations and times for diploma ceremonies will be provided soon.
Diploma ceremonies typically include a reception and the length of time for each ceremony varies based on the number of graduates.
More details on the weekend schedule, including a diploma ceremony schedule, will be provided in the coming weeks. 

Thursday, May 9
Phi Beta Kappa Initiation Ceremony
Ceremony: 2–3 p.m.
McConomy Auditorium, Cohon University Center
Reception: 3–4 p.m.
Connan Room, Cohon University Center

Contact:
Joseph Devine
jd0x@andrew.cmu.edu 

Joanne Ursenbach
joanneu@andrew.cmu.edu 

President's Graduates Toast (bachelor's students)
3:30–4:30 p.m.
Location TBD
Registration required. Invitation, along with registration details, will be sent in late April.

First Gen Graduation Recognition
Reception: 5-5:30 p.m.
Alumni Concert Hall, College of Fine Arts
Ceremony: 5:30-6:30p.m.
Invitation, along with registration details, will be sent in late April.

First Gen Graduation Recognition
Reception: 5-5:30 p.m.
Alumni Concert Hall, College of Fine Arts
Ceremony: 5:30-6:30p.m.
Kresge Theater, College of Fine Arts

Contact:
M. Shernell Smith
mssmith@andrew.cmu.edu 

Sam Colavecchio
scolavec@andrew.cmu.edu 
412-268-7733


Friday, May 10
Diploma Ceremonies
Various times

Senior Leadership Recognition Ceremony
4–5:30 p.m.
Wiegand Gym, Cohon University Center
Undergraduate students and their guests by invitation only. This ceremony recognizes nominated seniors who have reflected upon their specific leadership contributions during their time at CMU.


Saturday, May 11
Diploma Ceremonies
Various times

Center for Student Diversity and Inclusion Ceremony
Reception: 12:30 p.m.
Program: 1-2:15 p.m.
Simmons Auditorium, Tepper Building

Contact:
M. Shernell Smith
mssmith@andrew.cmu.edu 

Rowshan Lang
rowshan@andrew.cmu.edu
Naval ROTC Commissioning
Ceremony: 1:30-2:30 p.m.
Auditorium, Soldiers & Sailors Memorial Hall & Museum *
4141 Fifth Avenue, Pittsburgh, PA 15213
Contact:
Mike Danko
mdanko@andrew.cmu.edu 

The President’s Reception in honor of CMU’s Doctoral Candidates
4–6 p.m.
Tepper Building Atrium


Sunday, May 12
Gesling Stadium opens to guests
8 a.m.

Robing and procession for graduates
9–10 a.m.
Various locations across campus

Student procession begins
9:15 a.m.
All guests in stadium must be seated. Access to guest seating will be restricted once the procession begins.

Commencement Ceremony
10–11:30 a.m.
Gesling Stadium, CMU’s campus

Diploma Ceremonies
Various times
Answer: "
"What does CLIP stand for?
","['eric xing_1250676d646a9b48cf3bab66f13dc3c628ff68af_metadata.txt', 'daniel fried_19f59c14b3d79e3203c696128a135d33eb35e468_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What does CLIP stand for?

Context: Faculty Name: eric xing
Paperid: 1250676d646a9b48cf3bab66f13dc3c628ff68af
Title: 3D Open-vocabulary Segmentation with Foundation Models
Year: 2023
Abstract: Open-vocabulary segmentation of 3D scenes is a fundamental function of human perception and thus a crucial objective in computer vision research. However, this task is heavily impeded by the lack of large-scale and diverse 3D open-vocabulary segmentation datasets for training robust and generalizable models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation models helps but it compromises the open-vocabulary feature significantly as the 2D models are mostly finetuned with close-vocabulary datasets. We tackle the challenges in 3D open-vocabulary segmentation by exploiting the open-vocabulary multimodal knowledge and object reasoning capability of pre-trained foundation models CLIP and DINO, without necessitating any fine-tuning. Specifically, we distill open-vocabulary visual and textual knowledge from CLIP into a neural radiance field (NeRF) which effectively lifts 2D features into view-consistent 3D segmentation. Furthermore, we introduce the Relevancy-Distribution Alignment loss and Feature-Distribution Alignment loss to respectively mitigate the ambiguities of CLIP features and distill precise object boundaries from DINO features, eliminating the need for segmentation annotations during training. Extensive experiments show that our method even outperforms fully supervised models trained with segmentation annotations, suggesting that 3D open-vocabulary segmentation can be effectively learned from 2D images and text-image pairs.
Authors: Kunhao Liu, Fangneng Zhan, Jiahui Zhang, Muyu Xu, Yingchen Yu, Abdulmotaleb El-Saddik, Christian Theobalt, Eric P. Xing, Shijian Lu
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work distill open-vocabulary visual and textual knowledge from CLIP into a neural radiance field (NeRF) which effectively lifts 2D features into view-consistent 3D segmentation, suggesting that 3D open- Vocabulary segmentation can be effectively learned from 2D images and text-image pairs.'}
Url: https://arxiv.org/pdf/2305.14093
Title: Pragmatic Inference with a CLIP Listener for Contrastive Captioning
Authors: Jiefu Ou, Benno Krojer, Daniel Fried
Section: 2 Related Work
of image captions requires multifaceted evaluation. Prior work on contrastive/discriminative captioning investigates the tradeoff of model performance between discriminativeness and accuracy/fluency (Wang et al., 2021; Liu et al., 2019; Honda et al., 2022; Cho et al., 2022; Vedantam et al., 2017; Andreas and Klein, 2016). In this paper, we also perform an extensive study on the tradeoff between informativeness and fluency. Specifically, we focus on analyzing the robustness of the proposed and baseline methods in the tradeoff according to the selection of hyperparameters.
Answer: "
"Which independent organization set a course record of 2:06.20 in 1988 buggy?
","['cmubuggy_d403.txt', 'cmubuggy_d403.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which independent organization set a course record of 2:06.20 in 1988 buggy?

Context: This year's buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It's one of the few times spectators can see buggies up close. Dave Singh explains how Fringe builds its buggies. Each team keeps its actual processes a secret, but each buggy has certain features such as a body, pushbar, wheels and driving and braking mechanisms. Behind Singh are other buggies built by Fringe. Behind the Wheel Drivers, have the closest connection to the vehicles, aside from the mechanics who spent countless hours building and maintaining individual buggies. While races are brief, each driver must log a number of practice runs to qualify to race. Teams practice nights and weekends throughout the fall and spring semesters as weather permits. Buggies are often built around drivers, so the fit can be snug, and drivers are often less than 5 feet, 3 inches in height. Most — but not all — are women. Tishya Girdhar, a junior in neuroscience, is social chair for Fringe. She came to CMU wanting to drive, but first she had to get over being claustrophobic. She started as a mechanic and got behind the wheel for last year's Sweepstakes. ""Our team's philosophy is to teach everyone how to do everything,"" Girdhar said. ""But I came to CMU wanting to drive. I wanted to drive so badly."" Amy Chen demonstrates how a driver is positioned inside of Boson, a Fringe buggy built in 2016. Safety is a top priority for teams. All drivers are required to have five pieces of safety gear: mouth guard, goggles, a harness that includes three points of contact to the body of the buggy, gloves and a helmet. And during practice runs and races, flaggers man the course to let drivers know if it's safe to proceed. The streets of the race course haven't changed, but the condition of the course varies due to potholes, which can make or break a fast run. The last course records were set in 2017, but this year potholes are making for some challenging conditions, said Amy Chen, a senior in psychology who has been driving for four years.
The last course records were set in 2017, but this year potholes are making for some challenging conditions, said Amy Chen, a senior in psychology who has been driving for four years. With the vehicles just inches from the ground, ""even poorly filled potholes makes it dangerous to drive,"" she added. Depending on Friday and Saturday's conditions, most heats may run just two lanes instead of three. But still, despite some of the challenges, Chen said she wouldn't miss it. ""Being a driver is really fun,"" Chen said. ""I love going fast and going around the course."" — #CMUcarnival — Powered by Curator.io — Related Content — Media Advisory: Carnegie Mellon Celebrates Spring Carnival Buggy Races Keep Rolling at Carnegie Mellon Enjoy a World of Fun at Spring Carnival Student Architects Design Carnival Archway Take a Ride on The Old Mill at CMU MoBot Turns 25 Witchner, Wood Roll Into Buggy Royalty The Piper: Campus & Community News Official Events Calendar Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA 15213 412-268-2900 Legal Info www.cmu.edu © 2021 Carnegie Mellon University CMU on Facebook CMU on Twitter CMU on LinkedIn CMU YouTube Channel CMU RSS Feed CMU on Instagram CMU Social Media Directory Stories College of Engineering College of Fine Arts Dietrich College of Humanities & Social Sciences Heinz College of Information Systems and Public Policy Mellon College of Science School of Computer Science Tepper School of Business Archives 2021 March February January 2020 December November October September August July June May April March February January 2019 December November October September August July June May April March February January 2018 December November October September August July June May April March February January 2017 January February March April May June July August September October November December 2016 January February March April May June July August September October November December 2015 January February March April May June July August September October November December 2014 January February March April May June July August September October November December 2013 January February March April May June July August September October November December 2012 January February March April May June July August September October November December 2011 January February March April May June July August September October November December Media Highlights Media Resources Experts (Alphabetical) Experts (by Topic) Contact Us The Piper: Campus & Community News
Answer: "
"Does LTI offer a course on large language models?
","['mlt-student-handbook-2023-2024.txt', 'handbook-msaii-2022-2023.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Does LTI offer a course on large language models?

Context: International Students: Immigration status for students in F-1 and J-1 non-immigrant 
status is tied to making normal progress toward completing degree requirements. 
Therefore, F-1 and J-1 students who are considering completing their degree 
requirements early, anticipating longer-than-standard completion, or moving from an 
undergraduate to a graduate student classification (integrated undergraduate-graduate 
study) should consult with their designated  advisor in the Office of International 
Education (OIE) to ensure compliance with  immigration regulations. 
 
MLT Graduate Student Handbook 
Page 13 
 
4 MLT Degree Attainment 
4.1 Course Requirements 
In order to complete the Master of Language Technologies degree, the student must pass 120 
or more course units of senior-to-graduate courses, and meet the following criteria: 
 within those 120 units, at least 72 units of LTI courses and 24 units of SCS courses, 
 within those 72 units, 11-711, 11-791 (or an equivalent, see below), and one ``Task 
Orientation Focus'' class, and 
 within those 72 units, at least one of the following: 
o an LTI lab course, 
o 11-792, or 
o project-oriented Masters thesis; 
 Of the remaining 24 units, 12 must be 11-910 Directed Research; 
 The final 12 units are an Open Elective. 
The student must also complete two summers of full-time directed research, attend the LTI 
Colloquium (11-700) each semester, and satisfy the Research Speaking Requirement described 
elsewhere. 
Since 11-791 is not being offered currently, the faculty have defined a list of acceptable 
substitute courses: 
 11-727: Computational Semantics for NLP (only if the course project was done as a 
group project)  
 11-731: Machine Translation  
 11-747: Neural Networks for NLP  
 11-751: Speech Recognition  
 11-775: Large-Scale Multimedia  
 11-776: Multimodal Affective Computing 
 11-777: Multimodal Machine Learning  
 11-785: Deep Learning  
 11-797: Question Answering  
 
Students may request to have other LTI courses with a group engineering project component to 
be added to this list.  
For definitions of quoted terms, see the section on Definitions of LTI Terminology.
First Fall semester.   Only open to MSAII.  
 
17-762 – Law of Computer Technology (12 units). First Fall semester 
 
11-695 – AI Engineering (12 units). First Spring semester 
  
11-935 – LTI Practicum (3 units).  Summer Internship 
 
11-654 – AI Innovation (12 units). Second Fall semester.  Only open to MSAII. 
 
11-699 – Capstone Project (36 units). Second Spring semester.  Only open to MSAII.     
 
3) The Knowledge Area Courses (72 units):  
 
11-601, Coding Bootcamp (12 units).  First fall semester. 
 
10-601, Machine Learning (12 units), First fall semester. (Normally    
      11-691, Math for Machine Learning, which is not being offered in Fall 2022.) 
 
10-605, Machine Learning with Large Datasets (12 units).  First spring semester. 
 
11-611, Natural Language Processing (12 units).  Second fall semester. 
 
11-785, Deep Learning (12 units).  Second spring semester. 
 
One more 12-unit AI, ML or NLP course of the student’s choice (with approval of the 
Program Director) 
In the event that a course is not available, a course covering equivalent material with a 
similar degree of difficulty may be substituted with the permission of the Director. If a 
15 
 
student has already taken an equivalent course and shows proficiency in the subject 
area, a more advanced course in the area may be taken in place of these required 
courses with permission of the Director.  Graduate students cannot receive credit for 
undergraduate courses.       
 
4) Electives:  
A minimum of 36 units of SCS courses must be taken.  It is recommended to take one 
elective in the first fall semester, one or two in the first spring semester, one or two in 
the second fall semester and zero or one in the second spring semester.  The courses 
below are automatically approved.  A student who wants to take a course not listed 
must obtain approval of the Director prior to registering.  In general, the Director will 
approve any graduate SCS course of not more than 12 units.  It is also possible to seek 
approval for courses in other Colleges at CMU.
Answer: "
"What's the title for 11700?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What's the title for 11700?

Context: In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'LTI Colloquium' with Course ID 11700 and Section P offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Lisbon, Portugal location,led by experienced instructor Instructor TBA located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Advanced Natural Language Processing:' with Course ID 11711 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Advanced Natural Language Processing' with Course ID 11711 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Neubig located in Building TEP, Room 1403.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Lab in Natural Language Processing: Self-Paced' with Course ID 11712 and Section A offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Frederking located in Building TBA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Grammar Formalisms' with Course ID 11722 and Section A offers 12.0 units. The Class meets Monday Wednesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin located in Building WEH, Room 4707.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Linguistics Lab: Self-Paced' with Course ID 11723 and Section A offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin located in Building TBA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'LTI Colloquium' with Course ID 11700 and Section P offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Lisbon, Portugal location,led by experienced instructor Instructor TBA located in Building DNM, Room DNM.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Advanced Natural Language Processing:' with Course ID 11711 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Advanced Natural Language Processing' with Course ID 11711 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Neubig located in Building TEP, Room 1403.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Lab in Natural Language Processing: Self-Paced' with Course ID 11712 and Section A offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Frederking located in Building TBA, Room None.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Grammar Formalisms' with Course ID 11722 and Section A offers 12.0 units. The Class meets Monday Wednesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin located in Building WEH, Room 4707.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Linguistics Lab: Self-Paced' with Course ID 11723 and Section A offers 6.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin located in Building TBA, Room None.
Answer: "
"In the study ""On the Interactions of Structural Constraints and Data Resources for Structured Prediction"", what are the three structured prediction tasks evaluated?
","['emma strubell_71debf888acd57bb1baa4c146f31e58c66ea51af_content_0.txt', 'emma strubell_71debf888acd57bb1baa4c146f31e58c66ea51af_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the study ""On the Interactions of Structural Constraints and Data Resources for Structured Prediction"", what are the three structured prediction tasks evaluated?

Context: Title: On the Interactions of Structural Constraints and Data Resources for Structured Prediction
Authors: Zhisong Zhang, Emma Strubell, Eduard Hovy
Section: 5 Conclusion
In this work, we explore the interactions of constraint-based decoding algorithms and the amounts of training data for typical structured prediction tasks in NLP. Specifically, we train local models with different amounts of training data and analyze the influence of whether to adopt constrained decoding or not. The results show that when the model is trained with less data, the predictions contain more structural violations with greedy decoding and there are more benefits on model performance by further applying constrained decoding. Such patterns also generally hold with more efficient models and when transferring across text genres, where there are further interesting patterns with regard to model sizes and genre distances. Limitations This work has several limitations. First, we only experiment on English datasets. It would be interesting to explore whether the general patterns hold for non-English languages with different structural properties. Moreover, we only explore incorporating hard constraints for decoding with local models at testing time. Exploring more applications of structural constraints, such as learning with constraints, or incorporating other types of constraints, such as soft ones, would be promising future directions. Finally, we only explore three simple sentence-level structured prediction tasks, while extentions can be made to more complex tasks with larger output space, such as text generation or document-level information extraction, where constraints may play more interesting roles.
Title: On the Interactions of Structural Constraints and Data Resources for Structured Prediction
Authors: Zhisong Zhang, Emma Strubell, Eduard Hovy
Section: 1 Introduction
influences the outputs and how such influences change with dif- 147 ferent amounts of training data. RQ2: What is the influence of constraints when using more efficient models? Although neural models can obtain impressive results, one shortcoming is that they are usually computationally expensive. Recently, there have been many works on improving model efficiency. Knowledge distillation is one of the most widelyutilized methods, learning a smaller student model from a larger teacher model (Kim and Rush, 2016; Sanh et al., 2019; Jiao et al., 2020). An interesting question to explore is how these more efficient models interact with the explicit incorporation of structural constraints. RQ3: What is the influence of constraints for out-of-domain generalization? We usually expect the model to be able to generalize to scenarios that can be different from those represented by the training data, for example, to different domains or text genres. It will be interesting to explore how the constraints influence predictions for these cases and especially whether there are specific patterns with regard to the discrepancies between the source and the target. To answer these questions, we conduct extensive experiments on three typical structured prediction tasks, including named entity recognition (NER), dependency parsing (DPAR) and an information extraction task of event argument extraction (EAE). We find that models trained with less training data tend to produce outputs that contain more structural violations when using constraint-agnostic greedy decoding. Further applying constrained decoding brings consistent performance improvements and the benefits are more prominent in lower data scenarios (§3.2). A similar trend can be found with regard to model size: Smaller models tend to output more violations with greedy decoding and benefit more from constrained decoding (§3.3). Finally, in cross-genre settings, we find a weak pattern with regard to genre discrepancies: More structural violations tend to be made with greedy decoding when transferring to more distant genres (§3.4).
Answer: "
"In Justine Cassell's recent SIGDIAL paper, what feature does the study find to have a significant impact on hedge prediction?
","['justine cassell_24bff26f19051b1413d1e343322c1ae4bba05428_metadata.txt', 'justine cassell_74fedee9d809ec766a2089a89435fa7dd1346693_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In Justine Cassell's recent SIGDIAL paper, what feature does the study find to have a significant impact on hedge prediction?

Context: Faculty Name: justine cassell
Paperid: 24bff26f19051b1413d1e343322c1ae4bba05428
Title: When to generate hedges in peer-tutoring interactions
Year: 2023
Abstract: This paper explores the application of machine learning techniques to predict where hedging occurs in peer-tutoring interactions. The study uses a naturalistic face-to-face dataset annotated for natural language turns, conversational strategies, tutoring strategies, and nonverbal behaviors. These elements are processed into a vector representation of the previous turns, which serves as input to several machine learning models, including MLP and LSTM. The results show that embedding layers, capturing the semantic information of the previous turns, significantly improves the model’s performance. Additionally, the study provides insights into the importance of various features, such as interpersonal rapport and nonverbal behaviors, in predicting hedges by using Shapley values for feature explanation. We discover that the eye gaze of both the tutor and the tutee has a significant impact on hedge prediction. We further validate this observation through a follow-up ablation study.
Authors: Alafate Abulimiti, C. Clavel, Justine Cassell
Venue: SIGDIAL Conferences
Tldr: {'model': 'tldr@v2.0.0', 'text': 'The results show that embedding layers, capturing the semantic information of the previous turns, significantly improves the model’s performance and provides insights into the importance of various features, such as interpersonal rapport and nonverbal behaviors, in predicting hedges by using Shapley values for feature explanation.'}
Url: https://arxiv.org/pdf/2307.15582
Faculty Name: justine cassell
Paperid: 74fedee9d809ec766a2089a89435fa7dd1346693
Title: How About Kind of Generating Hedges using End-to-End Neural Models?
Year: 2023
Abstract: Hedging is a strategy for softening the impact of a statement in conversation. In reducing the strength of an expression, it may help to avoid embarrassment (more technically, “face threat”) to one’s listener. For this reason, it is often found in contexts of instruction, such as tutoring. In this work, we develop a model of hedge generation based on i) fine-tuning state-of-the-art language models trained on human-human tutoring data, followed by ii) reranking to select the candidate that best matches the expected hedging strategy within a candidate pool using a hedge classifier. We apply this method to a natural peer-tutoring corpus containing a significant number of disfluencies, repetitions, and repairs. The results show that generation in this noisy environment is feasible with reranking. By conducting an error analysis for both approaches, we reveal the challenges faced by systems attempting to accomplish both social and task-oriented goals in conversation.
Authors: Alafate Abulimiti, C. Clavel, Justine Cassell
Venue: Annual Meeting of the Association for Computational Linguistics
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work develops a model of hedge generation based on fine-tuning state-of-the-art language models trained on human-human tutoring data, followed by reranking to select the candidate that best matches the expected hedging strategy within a candidate pool using a hedge classifier.'}
Url: http://arxiv.org/pdf/2306.14696
Answer: "
"What site can I visit for more information about CMU's COVID policies?
","['handbook-msaii-2022-2023.txt', 'handbook-msaii-2022-2023.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What site can I visit for more information about CMU's COVID policies?

Context: The program stresses both 
10 
 
intrapreneurship and entrepreneurship and encourages students to develop ideas for startups 
and for introduction into established companies. 
The curriculum is designed for students who have a strong background in computer science 
with interest in in-depth study of AI to enable them to develop real-world applications, 
especially in areas to which AI has not yet been applied. The integrated curriculum includes 
coursework and team projects leading to a full-semester capstone project to produce a working 
prototype.     
8.2 Program Contacts 
Michael I. Shamos  
 
 
 
 
 
 
Director, MSAII 
 
 
 
 
 
Distinguished Career Professor 
 
 
 
 
GHC 6707 
 
 
 
 
 
 
 
 
 
 
shamos@cs.cmu.edu 
 
Amber Vivis 
 
Academic Program Manager, MSAII 
 
TCS 357 
 
412-268-9998 
 
avivis@cs.cmu.edu 
 
 
Carolyn Penstein Rosé 
 
 
 
 
 
Interim Director, Language Technologies Institute 
 
Professor 
 
GHC 5415 
 
412-268-7130 
 
cprose@cs.cmu.edu 
8.3 The Reasonable Person Principle (RPP) 
It is a long-standing and revered principle of the School of Computer Science (SCS) that 
members of our community are expected to act reasonably, and therefore we try to keep formal, 
written SCS policies to a minimum.  The faculty do not burden the students with numerous 
rules, and, in return, we expect the students to not try to find technical loopholes that violate 
the clear intent of program policies.  In any situation not covered by an explicit rule, you should 
ask yourself how reasonable people would behave in that situation.  The RPP does not alter 
University-wide policies. 
8.4 Academic Calendar 
11 
 
The Academic Calendar can be found at https://www.cmu.edu/hub/calendar/index.html and 
provides information on all deadlines including registration dates, class start dates, add/drop 
deadlines, exam dates and more. 
9 The Language Technologies Institute 
9.1 Working Space for MS Students 
Except for restrictions due to COVID, full-time students in the LTI’s MS degree programs on the 
Pittsburgh campus have access to a shared working space to create a sense of community and 
provide space for working when on campus.
Information about the following is included in The WORD (not an 
exhaustive list) and graduate students are encouraged to bookmark this site and refer to it 
often. University policies can also be found in full text at:  http://www.cmu.edu/policies/.  
 
Carnegie Mellon Vision, Mission 
Statement of Assurance 
Carnegie Code 
 
Academic Standards, Policies and Procedures 
Educational Goals 
Academic and Individual Freedom  
Statement on Academic Integrity Standards for Academic & Creative Life 
Assistance for Individuals with Disabilities  
Master’s Student Statute of Limitations  
Conduct of Classes 
Copyright Policy 
Cross-college & University Registration 
Doctoral Student Status Policy 
Evaluation & Certification of English Fluency for Instructors 
Final Exams for Graduate Courses 
Grading Policies 
Intellectual Property Policy  
Privacy Rights of Students  
 
  Student’s Rights 
 
 
Research 
Human Subjects in Research 
Office of Research Integrity & Compliance 
Office of Sponsored Programs 
Policy for Handling Alleged Misconduct of Research 
Policy on Restricted Research 
 
Tax Status of Graduate Student Awards 
 
Campus Resources & Opportunities 
Alumni Relations 
Assistance for Individuals with Disabilities  
52 
 
Athletics, Physical Fitness & Recreation  
Carnegie Mellon ID Cards and Services  
Cohon University Center 
Copying, Printing & Mailing  
Division of Student Affairs  
Domestic Partner Registration  
Emergency Student Loan Program  
Gender Programs & Resources  
Health Services 
Dining Services 
The HUB Student Services Center 
ID Card Services 
Leonard Gelfand Center 
LGBTQ Resources 
Multicultural and Diversity Initiatives  
Opportunities for Involvement  
Parking and Transportation Services 
Shuttle and Escort Services  
Spiritual Development  
University Police 
Student Activities 
University Stores 
 
Community Standards, Policies and Procedures 
Alcohol and Drugs Policy 
AIDS Policy 
Bicycle/Wheeled Transportation Policy  
Damage to Carnegie Mellon Property  
Deadly Weapons 
Discriminatory Harassment 
Disorderly Conduct 
Equal Opportunity/Affirmative Action Policy 
Freedom of Expression Policy  
Health Insurance Policy Immunization Policy 
Missing Student Protocol  
Non-Discrimination Policy  
On-Campus Emergencies  
Pets 
Political Activities 
Recycling Policy 
Riotous and Disorderly Behavior 
Safety Hazards 
Scheduling and Use of University Facilities  
Sexual Harassment and Sexual Assault Policy  
Smoking Policy 
53 
 
Student Accounts Receivable and Collection Policy and Procedures 
Student Activities Fee 
Student Enterprises 
Workplace Threats and Violence Policy
Answer: "
"What is the name of the author of the paper The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Linkbetween Phonemes and Facial Features who is from Max Planck Institute?
","['bhiksha raj_a6e3a10a6286967413e3406374bbeea533640030_content_1.txt', 'rita singh_a6e3a10a6286967413e3406374bbeea533640030_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the name of the author of the paper The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Linkbetween Phonemes and Facial Features who is from Max Planck Institute?

Context: Title: The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features
Authors: Liao Qu, Xianwei Zou, Xiang Li, Yandong Wen, Rita Singh, Bhiksha Raj
Section: 6. References
Z.-Q. Wang and I. Tashev, “Learning utterance-level representations for speech emotion and age/gender recognition using deep neural networks,” 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5150–5154, 2017. [18] T. Riede, E. Bronson, H. Hatzikirou, and K. Zuberbühler, “Vocal production mechanisms in a non-human primate: morphological data and a model,” Journal of Human Evolution, vol. 48, no. 1, pp. 85–96, 2005. [19] R. Singh, B. Raj, and D. Gençaga, “Forensic anthropometry from voice: An articulatory-phonetic approach,” 2016 39th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO), pp. 1375–1380, 2016. [20] M. T. Ghiselin, P. Ekman, and H. E. Gruber, “Darwin and facial expression: A century of research in review.@@@darwin on man: A psychological study of scientific creativity.” Systematic Biology, vol. 23, p. 562, 1974. [21] M. Tan, B. Chen, R. Pang, V. Vasudevan, and Q. V. Le, “Mnasnet: Platform-aware neural architecture search for mobile,” 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2815–2823, 2018. [22] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778, 2015.
Title: The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features
Authors: Liao Qu, Xianwei Zou, Xiang Li, Yandong Wen, Rita Singh, Bhiksha Raj
Section: 6. References
Z.-Q. Wang and I. Tashev, “Learning utterance-level representations for speech emotion and age/gender recognition using deep neural networks,” 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5150–5154, 2017. [18] T. Riede, E. Bronson, H. Hatzikirou, and K. Zuberbühler, “Vocal production mechanisms in a non-human primate: morphological data and a model,” Journal of Human Evolution, vol. 48, no. 1, pp. 85–96, 2005. [19] R. Singh, B. Raj, and D. Gençaga, “Forensic anthropometry from voice: An articulatory-phonetic approach,” 2016 39th International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO), pp. 1375–1380, 2016. [20] M. T. Ghiselin, P. Ekman, and H. E. Gruber, “Darwin and facial expression: A century of research in review.@@@darwin on man: A psychological study of scientific creativity.” Systematic Biology, vol. 23, p. 562, 1974. [21] M. Tan, B. Chen, R. Pang, V. Vasudevan, and Q. V. Le, “Mnasnet: Platform-aware neural architecture search for mobile,” 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2815–2823, 2018. [22] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778, 2015.
Answer: "
"In which semester do the Buggy Races happen?
","['cmubuggy_d403.txt', 'cmubuggy_d403.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In which semester do the Buggy Races happen?

Context: This year's buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It's one of the few times spectators can see buggies up close. Dave Singh explains how Fringe builds its buggies. Each team keeps its actual processes a secret, but each buggy has certain features such as a body, pushbar, wheels and driving and braking mechanisms. Behind Singh are other buggies built by Fringe. Behind the Wheel Drivers, have the closest connection to the vehicles, aside from the mechanics who spent countless hours building and maintaining individual buggies. While races are brief, each driver must log a number of practice runs to qualify to race. Teams practice nights and weekends throughout the fall and spring semesters as weather permits. Buggies are often built around drivers, so the fit can be snug, and drivers are often less than 5 feet, 3 inches in height. Most — but not all — are women. Tishya Girdhar, a junior in neuroscience, is social chair for Fringe. She came to CMU wanting to drive, but first she had to get over being claustrophobic. She started as a mechanic and got behind the wheel for last year's Sweepstakes. ""Our team's philosophy is to teach everyone how to do everything,"" Girdhar said. ""But I came to CMU wanting to drive. I wanted to drive so badly."" Amy Chen demonstrates how a driver is positioned inside of Boson, a Fringe buggy built in 2016. Safety is a top priority for teams. All drivers are required to have five pieces of safety gear: mouth guard, goggles, a harness that includes three points of contact to the body of the buggy, gloves and a helmet. And during practice runs and races, flaggers man the course to let drivers know if it's safe to proceed. The streets of the race course haven't changed, but the condition of the course varies due to potholes, which can make or break a fast run. The last course records were set in 2017, but this year potholes are making for some challenging conditions, said Amy Chen, a senior in psychology who has been driving for four years.
Buggy Races Keep Rolling at Carnegie Mellon - News - Carnegie Mellon University Carnegie Mellon University ——— Search Search Search this site only News News › Stories › Archives › 2019 › April › Buggy Races Keep Rolling at Carnegie Mellon April 10, 2019 Buggy Races Keep Rolling at Carnegie Mellon In its 99th year, the tradition is a Spring Carnival treat By Heidi Opdyke opdyke(through)andrew.cmu.edu Media Inquiries Julie Mattera Marketing and Communications jmattera(through)cmu.edu 412-268-2902 Sweepstakes, also known as the Buggy Races, has come a long way at Carnegie Mellon University. The slick, torpedo-like vessels carrying drivers with nerves of steel are a far cry from the two-man teams that once changed places halfway through a race and rode in everything from rain barrels with bicycle wheels to three-wheeled ash cans 99 years ago. Today, it takes six people to maneuver the .84 -mile course around Schenley Park's Flagstaff Hill. But while five pushers and a driver navigate the course's hills, dozens of people are needed to make a successful race happen. A year of planning goes into just over two minutes of racing. The Machine The basics of a buggy are straightforward, but teams are often secretive in how they build the machines, in particular the way they brake, steer and what types of wheels are used. Each has a body, pushbar for runners to move the machine up the hills, wheels, a safety harness and driving and braking mechanisms. Some also include fairings, a type of housing around the wheels that help reduce drag, make the vehicle quieter and just looks cool. Fairings have been a key feature for the Fringe team in recent years, which is celebrating its 50th anniversary of Buggy. ""We have a reputation of being the quietest on the course,"" said Diya Nuxoll, who wrapped up her bachelor's degree in mechanical engineering in December and is working on an advanced studies master's degree in design and manufacturing. Nuxoll leads the Fringe team as one of two head mechanics. An mural celebrating Buggy can be found in CMU's Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions. Sweepstakes Slang Buggy: Vehicle being raced and also a nickname for the competition.
Answer: "
"When did Andrew Carnegie emigrate from Scotland to Pittsburgh?
","['cmuhistory_d402.txt', 'tartanfacts_d404.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When did Andrew Carnegie emigrate from Scotland to Pittsburgh?

Context: History - CMU - Carnegie Mellon University Carnegie Mellon University ——— Search Search CMU › About › History Andrew Carnegie A self-educated ""working boy"" who loved books, Andrew Carnegie emigrated from Scotland in 1848 and settled in Pittsburgh, Pa. Attending night school and borrowing books, Carnegie went from factory worker in a textile mill to successful entrepreneur and industrialist. He rose to prominence by founding what became the world's largest steel producing company by the end of the 19th century. Carnegie Technical Schools At one point the richest man in the world, Carnegie believed that ""to die rich is to die disgraced."" He turned his attention to writing, social activism and philanthropy, determined to establish educational opportunities for the general public where few existed. In 1900, he donated $1 million for the creation of a technical institute for the city of Pittsburgh, envisioning a school where working-class men and women of Pittsburgh could learn practical skills, trades and crafts that would enhance their careers, lives and communities. ""My heart is in the work,"" he stated, which would become part of the school's official motto. The Carnegie Technical Schools offered two- and three-year certificates in the arts as well as in engineering disciplines and included a college for women, Margaret Morrison Carnegie College. Carnegie Tech – Early Years Soon faced with the demand for baccalaureate programs, Carnegie Technical Schools began offering bachelor's degrees through its College of Engineering and College of Fine Arts, becoming the Carnegie Institute of Technology, or ""Carnegie Tech."" During the first half of the 20th century, with support from Andrew Carnegie and other funders, Carnegie Tech laid the foundation for a school on the cutting edge. Some key developments were: It expanded from two buildings into an elegant 20th century campus designed in the beaux arts architectural style, housing a wealth of machine shops, studios and laboratories — the hands-on center of learning that persists today. It pioneered conservatory degree programs in music and drama, in addition to visual art and design programs. The first U.S. drama degree was awarded in 1914 at Carnegie Tech. It began offering graduate degrees. In 1919, the first doctorate (in civil engineering) was awarded to Mao Yisheng, a student from China.
Tartan Facts
Who founded Carnegie Mellon University?
Carnegie Technical Schools was founded in 1900 by Andrew Carnegie. Twelve years later it became known as the Carnegie Institute of Technology. In 1967, the school merged with Mellon Institute and became what is known today as Carnegie Mellon University.
What is a Tartan?
The Carnegie Mellon athletic teams are nicknamed the ""Tartans"" as a nod to Andrew Carnegie's Scottish heritage. A tartan is often misrepresented as a fierce warrior from either the Asian tundra or Scottish highlands. In actuality, a Tartan is a twilled woolen fabric with a plaid design. It is of Scottish origin and consists of stripes of various colors and widths against a solid ground, denoting a particular family lineage. The school's founder, Andrew Carnegie, was born in Dunfermline, Scotland, in 1835. Carnegie came to the United States in 1848 and founded Carnegie Technical Schools in Pittsburgh in 1900.
The Scottish terrier mascot performer sports Carnegie tartan attire, while the graphic mascot is wearing a plaid scarf around its neck. So what's the difference between tartan and plaid?
You'll know it's a tartan if...
• ""It's a check or pattern in a variety of colours in woven fabric in which bands of colour are repeated in equal proportion in warp (running lengthwise) and weft (running across).""
• ""Each stripe of the warp crosses every stripe of the weft, so when vertical and horizontal stripes of the same color cross, the result is solid color at the point of intersection.""
• ""The arrangement of colored threads is the same in the warp as in the weft.""
You can find our official tartan on various items in the
University Store
.
Source: ""Tartan: Romancing the Plaid,"" by Jeffrey Banks and Doria De La Chapelle
Official Mascot?
More than a century after Carnegie Mellon University opened its doors, an official mascot finally made its mark. Although students have dressed as a Scottish terrier — typically referred to as Scotty — for 50 years, it wasn't until 2007 that Carnegie Mellon officially welcomed the Scottish terrier as the university's first mascot.
In keeping with tradition, students, faculty, staff and alumni voted to name the new official mascot Scotty. But it's not just the costumed mascot that voters named.
Answer: "
"What issue is reciprocal rank found to have?
","['fernando diaz_55704caaf3d31e1795a1ca0c3bed9e77ae3a3c95_metadata.txt', 'fernando diaz_55704caaf3d31e1795a1ca0c3bed9e77ae3a3c95_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What issue is reciprocal rank found to have?

Context: Faculty Name: fernando diaz
Paperid: 55704caaf3d31e1795a1ca0c3bed9e77ae3a3c95
Title: Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision
Year: 2023
Abstract: Across a variety of ranking tasks, researchers use reciprocal rank to measure the effectiveness for users interested in exactly one relevant item. Despite its widespread use, evidence suggests that reciprocal rank is brittle when discriminating between systems. This brittleness, in turn, is compounded in modern evaluation settings where current, high-precision systems may be difficult to distinguish. We address the lack of sensitivity of reciprocal rank by introducing and connecting it to the concept of best-case retrieval, an evaluation method focusing on assessing the quality of a ranking for the most satisfied possible user across possible recall requirements. This perspective allows us to generalize reciprocal rank and define a new preference-based evaluation we call lexicographic precision or lexiprecision. By mathematical construction, we ensure that lexiprecision preserves differences detected by reciprocal rank, while empirically improving sensitivity and robustness across a broad set of retrieval and recommendation tasks.
Authors: Fernando Diaz
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work addresses the lack of sensitivity of reciprocal rank by introducing and connecting it to the concept of best-case retrieval, an evaluation method focusing on assessing the quality of a ranking for the most satisfied possible user across possible recall requirements.'}
Url: http://arxiv.org/pdf/2306.07908
Title: Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision
Authors: Fernando Diaz
Section: 7 CONCLUSION
Motivated by ceiling effects in evaluation with reciprocal rank, we have attempted to increase our understanding of the metric and designed a well-grounded mitigation to conducting best-case retrieval evaluation. We have shown that lexiprecision can effectively address the limitations of reciprocal rank in retrieval evaluation. Our results highlight the importance of considering the effects of tie-breaking in the evaluation process and provide a method for conducting more reliable best-case retrieval evaluation. Given the use of retrieval metrics—including reciprocal rank—outside of information retrieval contexts, we believe these contributions will be relevant to a researchers in the broader research community.
Answer: "
"What is the proposed method for grounding pre-trained text-only language models to the visual domain?
","['daniel fried_2b2d9ee18507aa89a7f1ecba0bf68844c7d9aa75_metadata.txt', 'daniel fried_6fb5c0eff3696ef252aca9638e10176ecce7cecb_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the proposed method for grounding pre-trained text-only language models to the visual domain?

Context: Faculty Name: daniel fried
Paperid: 2b2d9ee18507aa89a7f1ecba0bf68844c7d9aa75
Title: Grounding Language Models to Images for Multimodal Generation
Year: 2023
Abstract: We propose an efﬁcient method to ground pre-trained text-only language models to the visual domain, enabling them to process and generate arbitrarily interleaved image-and-text data. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and ﬁnetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text inter-leaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.
Authors: Jing Yu Koh, R. Salakhutdinov, Daniel Fried
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': None}
Url: http://arxiv.org/pdf/2301.13823
Faculty Name: daniel fried
Paperid: 6fb5c0eff3696ef252aca9638e10176ecce7cecb
Title: Generating Images with Multimodal Language Models
Year: 2023
Abstract: We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text -- outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence.
Authors: Jing Yu Koh, Daniel Fried, R. Salakhutdinov
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work proposes a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces, and exhibits a wider range of capabilities compared to prior multimodal language models.'}
Url: https://arxiv.org/pdf/2305.17216
Answer: "
"In fall 2023, What is the course title for unit 02090?
","['metadata_course_fall_23.txt', 'combined_metadata_final.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, What is the course title for unit 02090?

Context: In Semester Fall 2023, from the department of Business Administration, the subject titled 'Financial Computing I' with Course ID 46901 and Section I1 offers 6.0 units. The Class meets Tuesday Thursday between 01:00PM and 02:30PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ostlund located in Building TCS, Room 250.
In Semester Fall 2023, from the department of Business Administration, the subject titled 'Financial Computing I:' with Course ID 46901 and Section NA offers 6.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Business Administration, the subject titled 'Financial Computing I' with Course ID 46901 and Section M1 offers 6.0 units. The Class meets Tuesday Thursday between 01:00PM and 02:30PM ET. Students attend lectures at the New York, New York location,led by experienced instructor Ostlund located in Building BRD, Room 507.
In Semester Fall 2023, from the department of Business Administration, the subject titled 'Financial Computing IV' with Course ID 46904 and Section I1 offers 6.0 units. The Class meets Monday Wednesday between 01:00PM and 02:30PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kramkov located in Building TCS, Room 250.
In Semester Fall 2023, from the department of Business Administration, the subject titled 'Financial Computing IV' with Course ID 46904 and Section M1 offers 6.0 units. The Class meets Monday Wednesday between 01:00PM and 02:30PM ET. Students attend lectures at the New York, New York location,led by experienced instructor Kramkov located in Building BRD, Room 507.
In Semester Fall 2023, from the department of Business Administration, the subject titled 'MSCF Business Communication I' with Course ID 46906 and Section A offers 3.0 units. The Class meets Monday between 09:00AM and 09:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Barr located in Building TCS, Room 251.
In Semester Fall 2023, from the department of Business Administration, the subject titled 'Managing Teams and Organizations' with Course ID 46890 and Section A1 offers 6.0 units. The Class meets Monday Wednesday between 02:00PM and 03:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Chow located in Building TEP, Room 2110.
In Semester Fall 2023, from the department of Business Administration, the subject titled 'Managing Teams and Organizations' with Course ID 46890 and Section M1 offers 6.0 units. The Class meets Wednesday between 08:00PM and 09:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Chow located in Building TBA, Room None.
In Semester Fall 2023, from the department of Business Administration, the subject titled 'Business Communication for Analytical Decision Making:' with Course ID 46897 and Section NA offers 6.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Business Administration, the subject titled 'Business Communication for Analytical Decision Making' with Course ID 46897 and Section A2 offers 6.0 units. The Class meets Monday Wednesday between 08:00AM and 09:45AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Walter located in Building TEP, Room 2118.
In Semester Fall 2023, from the department of Business Administration, the subject titled 'Business Communication for Analytical Decision Making:' with Course ID 46897 and Section NA offers 6.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Business Administration, the subject titled 'Business Communication for Analytical Decision Making' with Course ID 46897 and Section B2 offers 6.0 units. The Class meets Monday Wednesday between 02:00PM and 03:45PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Walter located in Building TEP, Room 2118.
Answer: "
"How many months is the shorter track of the MIIS program?
","['miis-handbook_2023-2024.txt', 'program_info_MasterofScienceinIntelligentInformationSystems.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many months is the shorter track of the MIIS program?

Context: Under U.S. Federal Title IV regulations, 
student eligibility for federal financial aid is contingent upon enrollment in and successful 
completion of courses that are counted as credit toward their current degree program. To 
receive the maximum amount of federal financial aid for which they may be eligible, students 
must enroll each semester in at least 36 units that count toward their current degree level. (See 
separate guidance regarding integrated degree completion.) Students should consult with their 
designated college liaison in The HUB regarding billing and financial aid, particularly for early 
completion, longer-than standard completion, or integrated undergraduate and master’s degree 
programs.   
 
International Students 
Immigration status for students in F-1 and J-1 nonimmigrant status is tied to making normal 
progress toward completing degree requirements. Therefore, F-1 and J-1 students who are 
considering completing their degree requirements early, anticipating longer-than-standard 
completion, or moving from an undergraduate to a graduate student classification (integrated 
undergraduate-graduate study) should consult with their designated advisor in the Office of 
International Education (OIE) to ensure compliance with immigration regulations. 
MIIS Graduate Student Handbook 
Page 13 
 
4 MIIS Degree Requirements and Related Policies/Protocols 
4.1 Program Options 
The MIIS degree is offered in two options: 
Option 1. Standard MIIS degree (MIIS-16) - A 16-month track that is completed in three 
academic semesters (fall, spring, fall) and a summer internship.  
Option 2. MIIS: Advanced Study degree (MIIS-21) - A 21-month track that is completed in 
four academic semesters (fall, spring, fall, spring) and a summer internship. 
MIIS: Advanced Study track offers in depth degree in one of the following areas of 
concentration: 
• Human Language for Language Technologies 
• Language Technology Application 
• Machine Learning for Language Technologies 
Part-time options are available in some cases. 
4.2 Required Units for Degree Attainment 
To complete the Master of Science in Intelligent Information Systems, a student must satisfy 
three types of requirements. Curricular requirements ensure that MIIS students receive 
instruction in core intelligent information systems technologies while also allowing an 
opportunity to specialize in areas of personal interest. Practice requirements are opportunities 
to apply and hone new skills while building state-of-the-art systems.
Academic Program Name:
Master of Science in Intelligent Information Systems

Website:
https://lti.cs.cmu.edu/academics/masters-programs/miis.html

Overview:
The Master's in Intelligent Information Systems degree focuses on recognizing and extracting meaning from text, spoken language and video. As an MIIS student, you’ll receive the department’s deepest exposure to content analysis and machine learning. In addition to completing the program’s coursework, you’ll work on directed study projects with your faculty advisor for two semesters; participate in a summer internship; and collaborate with your peers on a semester-long, group-oriented capstone project. This combination of classroom instruction, professional experience, and using new skills in significant projects with world-class colleagues will help prepare you for a successful career in industry or government. Our alumni have gone on to exciting careers at places like Apple, IBM and Google, and most have job offers within six weeks of graduation.

Requirements:
The Intelligent Information Systems degree offers students the flexibility to create their own course of study in consultation with their advisor.
MIIS students gain three types of practical experience: software development supervised by their advisor (24 units equivalent to two courses); a summer internship (which can be waived for students that have sufficient prior professional experience); and a capstone project executed in a group of peers (42 units equivalent to three 12-unit courses and one 6-unit course). This combination is proven to help IIS students to broaden their skills quickly. The MIIS degree is offered in two options:
Option 1. Standard MIIS degree (MIIS-16) - A 16-month track that is completed in three academic semesters (fall, spring, fall) and a summer internship.
Option 2. MIIS: Advanced Study degree (MIIS-21) - A 21-month track that is completed in four academic semesters (fall, spring, fall, spring) and a summer internship.
MIIS: Advanced Study track offers an in-depth degree in one of the following areas of concentration:
Human Language for Language Technologies
Language Technology Application
Machine Learning for Language Technologies
Part-time education option is available in some cases.
MIIS-16 students must take at least 84 units (typically 7 courses) of qualifying and elective courses that satisfy human language, machine learning, and language technology applications breadth requirements.
Answer: "
"Is the university open on January 15th 2024?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Is the university open on January 15th 2024?

Context: On Monday, 11 December, 2024, during the Fall 2024 (F24) semester, Reading Day is observed.
From 12 December, 2024,Thursday to 13 December, 2023,Friday marks the Final Exams for Fall 2024 (F24) semester.
On Saturday, 14 December, 2024, during the Fall 2024 (F24) semester, Reading Day is observed.
On Sunday, 15 December, 2024, during the Fall 2024 (F24) semester, Final Exams is observed.
On Sunday, 15 December, 2024, during the Fall 2024 (F24) semester, Semester & Mini-2 Faculty Course Evaluations close is observed.
On Monday, 16 December, 2024, during the Fall 2024 (F24) semester, Make-Up Final Exams is observed.
On Wednesday, 18 December, 2024, during the Fall 2024 (F24) semester, Final Grades Due by 4 pm is observed.
From 24 December, 2024,Tuesday to 25 December, 2023,Wednesday marks the University Closed for Fall 2024 (F24) semester.
From 31 December, 2024,Tuesday to 01 January, 2023,Wednesday marks the University Closed for Fall 2024 (F24) semester.
On Monday, 06 January, 2025, during the Fall 2024 (F24) semester, Fall Deans' Lists Posted is observed.
On Monday, 13 January, 2025, during the Spring 2025 (S25) semester, First Day of Classes is observed.
On Friday, 17 January, 2025, during the Spring 2025 (S25) semester, Mini-3 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 20 January, 2025, during the Spring 2025 (S25) semester, Martin Luther King Day is observed, leading to No Classes & University Closed.
On Monday, 27 January, 2025, during the Spring 2025 (S25) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Wednesday, 05 February, 2025, during the Spring 2025 (S25) semester, Mini-3 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 07 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 pass/no pass & withdrawal deadline (3) is observed.
On Friday, 14 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Faculty Course Evaluations open is observed.
On Tuesday, 18 June, 2024, during the Summer One 2024 (M24) semester, Semester course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Wednesday, 19 June, 2022, during the Summer One 2024 (M24) semester, Juneteenth is observed, leading to University Closed & No Classes.
On Thursday, 20 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Last Day of Classes is observed.
On Thursday, 20 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 voucher deadline (4) is observed.
On Friday, 21 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Final Exams is observed.
On Friday, 21 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Faculty Course Evaluations close is observed.
On Monday, 24 June, 2024, during the Summer One 2024 (M24) semester, Mini-6 First Day of Classes is observed.
On Tuesday, 25 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 Final Grades Due by 4 pm is observed.
On Friday, 28 June, 2024, during the Summer One 2024 (M24) semester, Mini-6 add, audit, & tuition adjustment drop deadline (1) is observed.
On Thursday, 04 July, 2024, during the Summer One 2024 (M24) semester, Independence Day is observed, leading to University Closed & No Classes.
On Friday, 05 July, 2024, during the Summer One 2024 (M24) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
Answer: "
"What are some diffusion models mentioned in the document, ""Extracting Training Data from Diffusion Models""?
","['daphne ippolito_2e965b5d97c2d6fb4af284307735be39283792ba_content_0.txt', 'daphne ippolito_2e965b5d97c2d6fb4af284307735be39283792ba_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are some diffusion models mentioned in the document, ""Extracting Training Data from Diffusion Models""?

Context: Title: Extracting Training Data from Diffusion Models
Authors: Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramèr, Borja Balle, Daphne Ippolito, Eric Wallace, Ann Graham Lotz
Section: F Additional GAN Extraction Results
Figure 24 and Figure 25 contain additional examples extracted from GANs trained on CIFAR-10. 10https://github.com/POSTECH-CVLab/PyTorch-StudioGAN
Title: Extracting Training Data from Diffusion Models
Authors: Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramèr, Borja Balle, Daphne Ippolito, Eric Wallace, Ann Graham Lotz
Section: 9 Discussion and Conclusion
each wrote the corresponding sections of the paper. • Jamie performed the membership inference attacks and inpainting attacks on CIFAR-10 diffusion models, and Nicholas performed the diffusion extraction experiments; each wrote the corresponding sections of the paper. • Matthew ran experiments for canary memorization and wrote the corresponding section of the paper. • Florian and Vikash performed preliminary experiments on memorization in GANs, and Milad and Vikash ran the experiments included in the paper. • Milad ran the membership inference experiments on GANs. • Vikash ran extraction experiments on pretrained GANs. • Daphne and Florian improved figure clarity and presentation. • Daphne, Borja, and Eric edited the paper and contributed to paper framing. • Nicholas organized the project and wrote the initial paper draft. Acknowledgements and Conflicts of Interest The authors are grateful to Tom Goldstein, Olivia Wiles, Katherine Lee, Austin Tarango, Ian Wilbur, Jeff Dean, Andreas Terzis, Robin Rombach, and Andreas Blattmann for comments on early drafts of this paper. Nicholas, Milad, Matthew, and Daphne are employed at Google, and Jamie and Borja are employed at DeepMind, companies that both train large machine learning models (including diffusion models) on both public and private datasets. Eric Wallace is supported by the Apple Scholars in AI/ML Fellowship.
Answer: "
"In fall 2023, Who are the instructors for course 05430?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, Who are the instructors for course 05430?

Context: In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Programming Usable Interfaces' with Course ID 05430 and Section NA offers 15.0 units. The Class meets Monday Wednesday between 08:00AM and 09:20AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ion, Levin-Decanini located in Building TEP, Room 1403.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Programming Usable Interfaces:' with Course ID 05430 and Section NA offers 15.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Programming Usable Interfaces' with Course ID 05430 and Section A offers 15.0 units. The Class meets Monday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin-Decanini, Ion located in Building WEH, Room 5310.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Programming Usable Interfaces:' with Course ID 05430 and Section NA offers 15.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Programming Usable Interfaces' with Course ID 05430 and Section B offers 15.0 units. The Class meets Monday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ion, Levin-Decanini located in Building PH, Room 125B.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Programming Usable Interfaces:' with Course ID 05430 and Section NA offers 15.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Programming Usable Interfaces' with Course ID 05430 and Section NA offers 15.0 units. The Class meets Monday Wednesday between 08:00AM and 09:20AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ion, Levin-Decanini located in Building TEP, Room 1403.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Programming Usable Interfaces:' with Course ID 05430 and Section NA offers 15.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Programming Usable Interfaces' with Course ID 05430 and Section A offers 15.0 units. The Class meets Monday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin-Decanini, Ion located in Building WEH, Room 5310.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Programming Usable Interfaces:' with Course ID 05430 and Section NA offers 15.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Programming Usable Interfaces' with Course ID 05430 and Section B offers 15.0 units. The Class meets Monday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ion, Levin-Decanini located in Building PH, Room 125B.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Programming Usable Interfaces:' with Course ID 05430 and Section NA offers 15.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
Answer: "
"In fall 2024, When are Fall Deans' Lists Posted?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2024, When are Fall Deans' Lists Posted?

Context: On Monday, 11 December, 2024, during the Fall 2024 (F24) semester, Reading Day is observed.
From 12 December, 2024,Thursday to 13 December, 2023,Friday marks the Final Exams for Fall 2024 (F24) semester.
On Saturday, 14 December, 2024, during the Fall 2024 (F24) semester, Reading Day is observed.
On Sunday, 15 December, 2024, during the Fall 2024 (F24) semester, Final Exams is observed.
On Sunday, 15 December, 2024, during the Fall 2024 (F24) semester, Semester & Mini-2 Faculty Course Evaluations close is observed.
On Monday, 16 December, 2024, during the Fall 2024 (F24) semester, Make-Up Final Exams is observed.
On Wednesday, 18 December, 2024, during the Fall 2024 (F24) semester, Final Grades Due by 4 pm is observed.
From 24 December, 2024,Tuesday to 25 December, 2023,Wednesday marks the University Closed for Fall 2024 (F24) semester.
From 31 December, 2024,Tuesday to 01 January, 2023,Wednesday marks the University Closed for Fall 2024 (F24) semester.
On Monday, 06 January, 2025, during the Fall 2024 (F24) semester, Fall Deans' Lists Posted is observed.
On Monday, 13 January, 2025, during the Spring 2025 (S25) semester, First Day of Classes is observed.
On Friday, 17 January, 2025, during the Spring 2025 (S25) semester, Mini-3 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 20 January, 2025, during the Spring 2025 (S25) semester, Martin Luther King Day is observed, leading to No Classes & University Closed.
On Monday, 27 January, 2025, during the Spring 2025 (S25) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Wednesday, 05 February, 2025, during the Spring 2025 (S25) semester, Mini-3 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 08 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Last Day of Classes is observed.
On Friday, 08 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 voucher deadline (4) is observed.
From 11 December, 2023,Monday to 12 December, 2023,Tuesday marks the Final Exams for Fall 2023 (F23) semester.
On Wednesday, 13 December, 2023, during the Fall 2023 (F23) semester, Reading Day is observed.
From 14 December, 2023,Thursday to 15 December, 2023,Friday marks the Final Exams for Fall 2023 (F23) semester.
On Saturday, 16 December, 2023, during the Fall 2023 (F23) semester, Reading Day is observed.
On Sunday, 17 December, 2023, during the Fall 2023 (F23) semester, Final Exams is observed.
On Monday, 18 December, 2023, during the Fall 2023 (F23) semester, Make-Up Final Exams is observed.
On Monday, 18 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Faculty Course Evaluations close is observed.
On Wednesday, 20 December, 2023, during the Fall 2023 (F23) semester, Final Grades Due by 4 pm is observed.
From 23 December, 2023,Saturday to 02 January, 2024,Tuesday marks the Winter Break for Fall 2023 (F23) semester which leads to University Closed.
On Monday, 08 January, 2023, during the Fall 2023 (F23) semester, Fall Deans' Lists Posted is observed.
On Monday, 15 January, 2024, during the Spring 2024 (S24) semester, Martin Luther King Day is observed, leading to No Classes.
On Tuesday, 16 January, 2024, during the Spring 2024 (S24) semester, First Day of Class is observed.
On Monday, 22 January, 2024, during the Spring 2024 (S24) semester, Mini-3 add, audit & tuition adjustment drop deadline (1) is observed.
Answer: "
"In the ICTIR paper, what does KALE stand for?
","['jamie callan_1e0a4ff0c5d2d850ff5907e310ffcedc9cad9718_metadata.txt', 'jamie callan_1e0a4ff0c5d2d850ff5907e310ffcedc9cad9718_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the ICTIR paper, what does KALE stand for?

Context: Faculty Name: jamie callan
Paperid: 1e0a4ff0c5d2d850ff5907e310ffcedc9cad9718
Title: KALE: Using a K-Sparse Projector for Lexical Expansion
Year: 2023
Abstract: Recent research has proposed retrieval approaches based on sparse representations and inverted indexes, with terms produced by neural language models and leveraging the advantages from both neural retrieval and lexical matching. This paper proposes KALE, a new lightweight method of this family that uses a small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary. The KALE vocabulary captures semantic concepts than perform well when used in isolation, and perform better when extending the original lexical vocabulary, this way improving first-stage retrieval accuracy. Experiments with the MSMARCOv1 passage retrieval dataset, the TREC Deep Learning dataset, and BEIR datasets, examined the effectiveness of KALE under varying conditions. Results show that the KALE terms can replace the original lexical vocabulary, with gains in accuracy and efficiency. Combining KALE with the original lexical vocabulary, or with other learned terms, can further improve retrieval accuracy with only a modest increase in computational cost.
Authors: Luís Borges, Bruno Martins, Jamie Callan
Venue: International Conference on the Theory of Information Retrieval
Tldr: {'model': 'tldr@v2.0.0', 'text': 'KALE is a new lightweight method that uses a small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary, which can replace the original lexical vocabulary with gains in accuracy and efficiency.'}
Url: https://dl.acm.org/doi/pdf/10.1145/3578337.3605131
Title: KALE: Using a K-Sparse Projector for Lexical Expansion
Authors: Luís Borges, Bruno Martins, Jamie Callan
Section: ACKNOWLEDGMENTS
This research was supported by the Portuguese Recovery and Resilience Plan through project C645008882-00000055, through Fundação para a Ciência e Tecnologia (FCT) with the Ph.D. scholarship SFRH/BD/150497/2019 under the CMU-PT Program, and through the INESC-ID multi-annual funding from the PIDDAC programme, corresponding to reference UIDB/50021/2020.
Answer: "
"What time in eastern time was the final application deadline for the PhD program in language and information technology? Give your answer in 12h time format with either an am or pm label.
","['handbook_phd_2023-2024.txt', 'combined_metadata_final.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What time in eastern time was the final application deadline for the PhD program in language and information technology? Give your answer in 12h time format with either an am or pm label.

Context: CS ids are being phased out very slowly, so it is likely that you 
will need both types of user id. 
LTI Ph.D. Graduate Student Handbook 
Page 14 
 
The School of Computer Science has a Help Center in GHC 4201. It can be contacted at 
help@cs.cmu.edu , extension 8-4231 from a campus phone, or 412-268-4231 from an outside line 
(M-F, 9am-5pm). 
3 
Standard Degree Requirements & Degree Certification  
                                      
3.1 
LTI Ph.D. Degree Requirements 
To complete the Ph.D. in Language and Information Technologies degree, the student must satisfy 
the following requirements:  
 Pass at least 96 units of graduate level courses, with additional requirements detailed 
below; 
 Satisfy proficiencies in Writing, Presentation, Programming, and Teaching; 
 Propose, write, and defend a Ph.D. dissertation (thesis); 
 Attend the LTI Colloquium (11-700) each semester; and 
 Satisfy the Research Speaking Requirement.  
The sections below provide more detail about each of these requirements. 
3.1.1 Course Requirements 
To complete the course requirements for the Ph.D. in Language and Information Technologies 
degree, the student must pass 96 or more course units of graduate courses, and meet the following 
criteria: 
 At least 72 units of LTI courses and 24 units of SCS courses, 
 At least one class in each LTI Focus Area, and  
 At least two labs, in two different research areas. 
For definitions of quoted terms, see the section on Definitions of LTI Terminology. 
Unless approved by the Program Director in advance, the course requirements must be satisfied 
by actual classroom courses, not credit given for research or independent study. 
An LTI course is any 12-unit course with a number of 11-XXX; a 6-unit course with 11-XXX 
counts as one-half of an LTI course. Unless otherwise specified, ""course"" means an actual 
classroom course, not credit given for research or independent study. Note that the LTI allows 
any one MLD (10-XXX) graduate course to count as an ""LTI course"".
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Natural Language Processing' with Course ID 11411 and Section W offers 12.0 units. The Class meets Monday Wednesday between 02:30PM and 03:45PM ET. Students attend lectures at the Doha, Qatar location,led by experienced instructor Oflazer located in Building CMB, Room 2052.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Grammar Formalisims' with Course ID 11422 and Section A offers 12.0 units. The Class meets Monday Wednesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin located in Building WEH, Room 4707.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'ConLanging: Lrng. Ling. & Lang Tech via Constru Artif. Lang.' with Course ID 11423 and Section A offers 12.0 units. The Class meets Monday Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Levin located in Building GHC, Room 4301.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Subword Modeling' with Course ID 11424 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Mortensen located in Building PH, Room 100.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Machine Learning with Graphs' with Course ID 11441 and Section A offers 9.0 units. The Class meets Tuesday Thursday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Yang located in Building NSH, Room 1305.
In Semester Spring 2024, from the department of Language Technologies Institute, the subject titled 'Search Engines' with Course ID 11442 and Section A offers 9.0 units.
Answer: "
"Which LTI faculty was a contributor on the HomeRobot paper?
","['yonatan bisk_3b0c02955e88f5862e61b560c7f70ba8cf235b1d_metadata.txt', 'shinji watanabe_f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which LTI faculty was a contributor on the HomeRobot paper?

Context: Faculty Name: yonatan bisk
Paperid: 3b0c02955e88f5862e61b560c7f70ba8cf235b1d
Title: HomeRobot: Open-Vocabulary Mobile Manipulation
Year: 2023
Abstract: HomeRobot (noun): An affordable compliant robot that navigates homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future research work improve performance. See videos on our website: https://ovmm.github.io/.
Faculty Name: shinji watanabe
Paperid: f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b
Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN
Year: 2023
Abstract: This paper reports on the shared tasks organized by the 20th IWSLT Conference. The shared tasks address 9 scientific challenges in spoken language translation: simultaneous and offline translation, automatic subtitling and dubbing, speech-to-speech translation, multilingual, dialect and low-resource speech translation, and formality control. The shared tasks attracted a total of 38 submissions by 31 teams. The growing interest towards spoken language translation is also witnessed by the constantly increasing number of shared task organizers and contributors to the overview paper, almost evenly distributed across industry and academia.
Authors: Sweta Agrawal, Antonios Anastasopoulos, L. Bentivogli, Ondrej Bojar, Claudia Borg, Marine Carpuat, R. Cattoni, Mauro Cettolo, Mingda Chen, William Chen, K. Choukri, Alexandra Chronopoulou, Anna Currey, T. Declerck, Qianqian Dong, Kevin Duh, Y. Estève, Marcello Federico, Souhir Gahbiche, B. Haddow, B. Hsu, Phu Mon Htut, H. Inaguma, Dávid Javorský, J. Judge, Yasumasa Kano, Tom Ko, Rishu Kumar, Peng Li, Xutai Ma, Prashant Mathur, E. Matusov, Paul McNamee, John P. McCrae, Kenton Murray, Maria Nadejde, Satoshi Nakamura, Matteo Negri, H. Nguyen, J. Niehues, Xing Niu, Atul Kr.
Answer: "
"When did CMU get its first IBM 650 computer?
","['history_d401.txt', 'history_d401.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When did CMU get its first IBM 650 computer?

Context: A history of SCS | SCS25 - Carnegie Mellon University School of Computer Science Skip to main content Legal Events Register 25 Things History Video A history of SCS For an expanded history of the School of Computer Science and its predecessors at CMU, read ""Institutional Memories"" in the Summer 2014 issue of The Link magazine.In 2014, the School of Computer Science celebrated its 25th year as a stand-alone college within Carnegie Mellon University. It was arguably the first college devoted solely to computer science in the United States, and a model for others that followed. But CMU’s computer science era begins much earlier—in 1956, with the arrival of an IBM 650 computer on the campus of what was then known as Carnegie Institute of Technology. The IBM 650 had magnetic-drum memory and a processing speed of approximately 60 instructions per second. Herb Simon (H’90), associate dean of the Graduate School of Industrial Administration—now known as CMU’s Tepper School of Business—established Carnegie Tech’s first Computation Center with the help of its first director, Alan Perlis (S’42).First freshman-level computer science courseIn 1956 and 1957, Simon, Allen Newell (IA’57) and Cliff Shaw of RAND designed the Logic Theorist, a computer program that could develop proofs for theorems in much the same way a human would work. They also developed linked-list data structures, the foundation of computer programming. Perlis, Simon and Newell are credited with defining the term “computer science” as “the theory and design of computers,” as well as (in Newell’s words) “the study of the phenomena arising from them.” In 1958, Perlis began teaching the first freshman-level computer programming course in the United States at Carnegie Tech.Computer science Ph.D. program createdIn 1961, the Computation Center and its newest computer, a Bendix G-20, were moved to recently completed Scaife Hall. That same year, Carnegie Tech created an interdisciplinary Ph.D. program called Systems and Communications Sciences, combining elements of computer science, mathematics, psychology, business and electrical engineering. The university’s first computer science Ph.D.s were graduates of this program.Computer Science Department establishedIn 1965, Carnegie Tech established its Computer Science Department, or CSD, with a $5 million grant from the R.K. Mellon Foundation.
As the 1970s progressed, Newell became interested in human-computer interaction, and began a long relationship with Xerox’s Palo Alto Research Center, or PARC, which released the Xerox Alto in 1973. Considered a forerunner to many of the computing environments that followed, Alto featured a graphical-user interface and was among the first commercially available workstations controlled with a mouse. Inspired by Alto, Reddy launched a drive for development of CMU’s own “three-M” machine—a personal workstation with a megabyte of memory, a megapixel display and at least one million instructions per second of processing power.Launching a Robotics InstituteIn 1979, an executive at Pittsburgh’s Westinghouse Electric Corp., Tom Murrin, collaborated with Jordan and Reddy to create the Robotics Institute, with Reddy as its first director. By 1982, the Computer Science Department included more than 30 faculty members and 100 graduate students.The best-wired campus in the worldWorking with IBM in the early 1980s, the university and the Computer Science Department established another new research frontier: Development of a high-speed computer network that would reach virtually every room on campus, along with a GUI-based computing environment, and providing networked PCs or workstations for 7,000 students, faculty members and employees. Called the Andrew Project, it turned Carnegie Mellon into the best-connected, most-wired university in the world—a process Newell called “greening up the campus with computer science.” CMU also became home to a new Software Engineering Institute, funded by the Defense Department, to study computer security and develop best practices in the design of operating systems. Between 1982 and 1985, the amount of sponsored research in the Computer Science Department doubled, from $7.2 million to $15.3 million—more than the other four departments in the Mellon College of Science combined.A “school of computer science” is proposedFeeling that CSD’s needs were inadequately represented in MCS, CSD head A. Nico Habermann and then-CMU provost Angel Jordan in 1986 wrote a white paper proposing the creation of “a School of Computer Science.” Responding to concerns from the faculty that the change might be taking place too quickly, the university first established a free-floating Department of Computer Science. The experiment, which lasted two years, was an unqualified success.
Answer: "
"In the Plan, Eliminate and Track paper, what percentage gain did the proposed framework achieve over the state-of-the-art?
","['yonatan bisk_5ce2f1dff23a5620f77f9b11f1e534422ab8ff3f_metadata.txt', 'eric xing_1262758538525835d918007d15726794e19a07b7_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the Plan, Eliminate and Track paper, what percentage gain did the proposed framework achieve over the state-of-the-art?

Context: Faculty Name: yonatan bisk
Paperid: 5ce2f1dff23a5620f77f9b11f1e534422ab8ff3f
Title: Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents
Year: 2023
Abstract: Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accomplished each sub-task. On the AlfWorld instruction following benchmark, the PET framework leads to a significant 15% improvement over SOTA for generalization to human goal specifications.
Authors: Yue Wu, So Yeon Min, Yonatan Bisk, R. Salakhutdinov, A. Azaria, Yuan-Fang Li, Tom M. Mitchell, Shrimai Prabhumoye
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A framework to use the knowledge in LLMs to simplify the control problem, rather than solving it is proposed, which leads to a significant 15% improvement over SOTA for generalization to human goal specifications.'}
Url: http://arxiv.org/pdf/2305.02412
Faculty Name: eric xing
Paperid: 1262758538525835d918007d15726794e19a07b7
Title: Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective
Year: 2023
Abstract: We present a new dataset condensation framework termed Squeeze, Recover and Relabel (SRe$^2$L) that decouples the bilevel optimization of model and synthetic data during training, to handle varying scales of datasets, model architectures and image resolutions for efficient dataset condensation. The proposed method demonstrates flexibility across diverse dataset scales and exhibits multiple advantages in terms of arbitrary resolutions of synthesized images, low training cost and memory consumption with high-resolution synthesis, and the ability to scale up to arbitrary evaluation network architectures. Extensive experiments are conducted on Tiny-ImageNet and full ImageNet-1K datasets. Under 50 IPC, our approach achieves the highest 42.5% and 60.8% validation accuracy on Tiny-ImageNet and ImageNet-1K, outperforming all previous state-of-the-art methods by margins of 14.5% and 32.9%, respectively. Our approach also surpasses MTT in terms of speed by approximately 52$\times$ (ConvNet-4) and 16$\times$ (ResNet-18) faster with less memory consumption of 11.6$\times$ and 6.4$\times$ during data synthesis. Our code and condensed datasets of 50, 200 IPC with 4K recovery budget are available at https://github.com/VILA-Lab/SRe2L.
Authors: Zeyuan Yin, Eric P. Xing, Zhiqiang Shen
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'The proposed dataset condensation framework demonstrates flexibility across diverse dataset scales and exhibits multiple advantages in terms of arbitrary resolutions of synthesized images, low training cost and memory consumption with high-resolution synthesis, and the ability to scale up to arbitrary evaluation network architectures.'}
Url: http://arxiv.org/pdf/2306.13092
Answer: "
"How many scenes were included in the HomeRobot OVMM benchmark?
","['yonatan bisk_3b0c02955e88f5862e61b560c7f70ba8cf235b1d_metadata.txt', 'yonatan bisk_3b0c02955e88f5862e61b560c7f70ba8cf235b1d_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many scenes were included in the HomeRobot OVMM benchmark?

Context: We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future research work improve performance. See videos on our website: https://ovmm.github.io/.
Authors: Sriram Yenamandra, A. Ramachandran, Karmesh Yadav, Austin S. Wang, Mukul Khanna, Théophile Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander Clegg, John Turner, Z. Kira, M. Savva, Angel X. Chang, Devendra Singh Chaplot, Dhruv Batra, Roozbeh Mottaghi, Yonatan Bisk, Chris Paxton
Venue: Conference on Robot Learning
Tldr: {'model': 'tldr@v2.0.0', 'text': 'The HomeRobot OVMM benchmark is introduced, where an agent navigates household environments to grasp novel objects and place them on target receptacles, and baselines achieve a 20% success rate in the real world; the experiments identify ways future research work improve performance.'}
Url: http://arxiv.org/pdf/2306.11565
Faculty Name: yonatan bisk
Paperid: 3b0c02955e88f5862e61b560c7f70ba8cf235b1d
Title: HomeRobot: Open-Vocabulary Mobile Manipulation
Year: 2023
Abstract: HomeRobot (noun): An affordable compliant robot that navigates homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future research work improve performance. See videos on our website: https://ovmm.github.io/.
Answer: "
"In the paper ""Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization, what are the three unseen tasks investigated for Whisper model?
","['shinji watanabe_10e8dc07ea256c6a88d7043cf135417402ed38f4_metadata.txt', 'shinji watanabe_10e8dc07ea256c6a88d7043cf135417402ed38f4_content_6.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the paper ""Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization, what are the three unseen tasks investigated for Whisper model?

Context: Faculty Name: shinji watanabe
Paperid: 10e8dc07ea256c6a88d7043cf135417402ed38f4
Title: Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization
Year: 2023
Abstract: We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper
Authors: Puyuan Peng, Brian Yan, Shinji Watanabe, David F. Harwath
Venue: Interspeech
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work investigates the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering, and designs task-specific prompts that improve performance on the three zero-shot tasks and even outperform SotA supervised models on some datasets.'}
Url: https://arxiv.org/pdf/2305.11095
Title: Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization
Authors: Puyuan Peng, Brian Yan, Shinji Watanabe, David Harwath
Section: 2. The Whisper model
We note that the comparison in this table 5Whisper Large for En→De; LargeV2 for En→Ru and En→Fr. should only be treated as a reference. This is because even for the unsupervised and zero-shot approaches, they are particularly designed for ST, and they either leverage machine translation systems [30, 33] or multilingual sentence embedding models [34]. For Whisper, however, we simply adjust its prompt, and the goal is to probe the multilingual understanding of the model. Remarks. Although Whisper is trained with massive multilingual data, performing En→X might be harder than one expects. Because for the <|st|> task token, the model is never trained to generate non-English text; for the <|asr|> task token the model is never trained to generate text belonging to a different language than the input speech. The fact that Whisper is able to do En→X ST with a simple modification on its prompt reveals that semantically related words and phrases from different languages might be close in the model’s latent space. We also expect that we could fine-tune Whisper to boost the performance of ST on new language pairs.
Answer: "
"When was the last day of classes for the Fall 2023 semester?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When was the last day of classes for the Fall 2023 semester?

Context: From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mini-2 Classes Begin is observed.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mid-Semester & Mini-1 grades due by 4 pm is observed.
On Friday, 27 October, 2023, during the Fall 2023 (F23) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
On Tuesday, 07 November, 2023, during the Fall 2023 (F23) semester, Democracy Day is observed, leading to No Classes, except Evening classes after 5 pm will still meet.
On Saturday, 11 November, 2023, during the Fall 2023 (F23) semester, Homecoming is observed.
From 13 November, 2023,Monday to 17 November, 2023,Friday marks the Spring 2024 Registration Week for Fall 2023 (F23) semester.
On Monday, 13 November, 2023, during the Fall 2023 (F23) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Wednesday, 15 November, 2023, during the Fall 2023 (F23) semester, Mini-2 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 22 November, 2023,Wednesday to 24 November, 2023,Friday marks the Thanksgiving Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Mini-2 pass/no pass & withdrawal deadline (2) is observed.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Faculty Course Evaluations open is observed.
On Friday, 08 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Last Day of Classes is observed.
On Friday, 08 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Last Day of Classes is observed.
On Friday, 08 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 voucher deadline (4) is observed.
From 11 December, 2023,Monday to 12 December, 2023,Tuesday marks the Final Exams for Fall 2023 (F23) semester.
On Wednesday, 13 December, 2023, during the Fall 2023 (F23) semester, Reading Day is observed.
From 14 December, 2023,Thursday to 15 December, 2023,Friday marks the Final Exams for Fall 2023 (F23) semester.
On Saturday, 16 December, 2023, during the Fall 2023 (F23) semester, Reading Day is observed.
On Sunday, 17 December, 2023, during the Fall 2023 (F23) semester, Final Exams is observed.
On Monday, 18 December, 2023, during the Fall 2023 (F23) semester, Make-Up Final Exams is observed.
On Monday, 18 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Faculty Course Evaluations close is observed.
On Wednesday, 20 December, 2023, during the Fall 2023 (F23) semester, Final Grades Due by 4 pm is observed.
From 23 December, 2023,Saturday to 02 January, 2024,Tuesday marks the Winter Break for Fall 2023 (F23) semester which leads to University Closed.
On Monday, 08 January, 2023, during the Fall 2023 (F23) semester, Fall Deans' Lists Posted is observed.
On Monday, 15 January, 2024, during the Spring 2024 (S24) semester, Martin Luther King Day is observed, leading to No Classes.
On Tuesday, 16 January, 2024, during the Spring 2024 (S24) semester, First Day of Class is observed.
On Monday, 22 January, 2024, during the Spring 2024 (S24) semester, Mini-3 add, audit & tuition adjustment drop deadline (1) is observed.
Answer: "
"What is reciprocal rank used to measure?
","['fernando diaz_55704caaf3d31e1795a1ca0c3bed9e77ae3a3c95_metadata.txt', 'fernando diaz_55704caaf3d31e1795a1ca0c3bed9e77ae3a3c95_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is reciprocal rank used to measure?

Context: Faculty Name: fernando diaz
Paperid: 55704caaf3d31e1795a1ca0c3bed9e77ae3a3c95
Title: Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision
Year: 2023
Abstract: Across a variety of ranking tasks, researchers use reciprocal rank to measure the effectiveness for users interested in exactly one relevant item. Despite its widespread use, evidence suggests that reciprocal rank is brittle when discriminating between systems. This brittleness, in turn, is compounded in modern evaluation settings where current, high-precision systems may be difficult to distinguish. We address the lack of sensitivity of reciprocal rank by introducing and connecting it to the concept of best-case retrieval, an evaluation method focusing on assessing the quality of a ranking for the most satisfied possible user across possible recall requirements. This perspective allows us to generalize reciprocal rank and define a new preference-based evaluation we call lexicographic precision or lexiprecision. By mathematical construction, we ensure that lexiprecision preserves differences detected by reciprocal rank, while empirically improving sensitivity and robustness across a broad set of retrieval and recommendation tasks.
Authors: Fernando Diaz
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work addresses the lack of sensitivity of reciprocal rank by introducing and connecting it to the concept of best-case retrieval, an evaluation method focusing on assessing the quality of a ranking for the most satisfied possible user across possible recall requirements.'}
Url: http://arxiv.org/pdf/2306.07908
Title: Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision
Authors: Fernando Diaz
Section: 6 DISCUSSION
there may be version of lexiprecision that can indeed be represented as an interval measure.
Answer: "
"How many authors are on SantaCoder paper?
","['daniel fried_1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a_content_1.txt', 'daniel fried_1bf21dabbdfc81fd4f9e92b1201ecce744cabb6a_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many authors are on SantaCoder paper?

Context: Title: SANTACODER: DON’T REACH FOR THE STARS!
Authors: Loubna Ben Allal, Raymond Li, Chenghao Mou, Joel Lamy Poirier, Ian Yu PIISA, Paulo Villegas, Leandro von Werra
Section: 6.1 ABLATIONS
Note that the stars filter removes the most data (over 60%) and, therefore, raises the question whether the performance difference is due to the smaller dataset. However, as can be seen in Figure 3, HumanEval pass@100 diverged early on in training, indicating that the drop in performance is not only due to data size but also data quality.
Title: SANTACODER: DON’T REACH FOR THE STARS!
Authors: Loubna Ben Allal, Raymond Li, Chenghao Mou, Joel Lamy Poirier, Ian Yu PIISA, Paulo Villegas, Leandro von Werra
Section: C PII
C.1 REGULAR EXPRESSIONS Email addresses We used the following regular expression to detect emails. email_pattern = r’’’ (?<= ˆ | [\b\s@,?!;:)(’"".\p{Han}<] ) ( [ˆ\b\s@?!;,:)(’""<]+ @ [ˆ\b\s@!?;,/]* [ˆ\b\s@?!;,/:)(’"">.] \. \p{L} \w{1,} ) (?= $ | [\b\s@,?!;:)(’"".\p{Han}>] ) ’’’ We replace detected emails with [random 5 character string]@example.com. IP addresses We used the following regular expressions to detect IPv4 and IPv6 addresses. ipv4_pattern = r""(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?) (?:\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}"" ipv6_pattern = r""(?:[0-9a-fA-F]{1,4}:){7,7}[0-9a-fA-F ]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,7}:|(?:[0-9a-fA-F]{1,4}:) {1,6}:[0-9a-fA-F]{1,4}|(?:[0-9a-fA-F]{1,4}:){1,5}(?::[0-9a-fAF]{1,4}){1,2}|(?:[0-9a-fA-F]{1,4}:){1,4}(?::[0-9a-fA-F]{1,4}) {1,3}|(?:[0-9a-fA-F]{1,4}:){1,3}(?
Answer: "
"In fall 2024, What is the deadline for Mini-1 Pass/no pass & withdrawal?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-2425.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2024, What is the deadline for Mini-1 Pass/no pass & withdrawal?

Context: On Monday, 28 August, 2023, during the Fall 2023 (F23) semester, Semester & Mini-1 Classes Begin is observed.
On Friday, 01 September, 2023, during the Fall 2023 (F23) semester, Mini-1 add, audit & tuition adjustment drop deadline  (1) is observed.
On Monday, 04 September, 2023, during the Fall 2023 (F23) semester, Labor Day is observed, leading to No Classes & University Closed.
On Monday, 11 September, 2023, during the Fall 2023 (F23) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Wednesday, 20 September, 2023, during the Fall 2023 (F23) semester, Mini-1 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 02 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Pass/no pass & withdrawal deadline (3) is observed.
On Monday, 02 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Faculty Course Evaluations open is observed.
On Monday, 09 October, 2023, during the Fall 2023 (F23) semester, Semester drop deadline is observed, leading to withdrawal grade assigned after this date.
On Friday, 13 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Last Day of Classes is observed.
On Friday, 13 October, 2023, during the Fall 2023 (F23) semester, Mini-1 voucher election deadline (4) is observed.
From 13 October, 2023,Friday to 14 October, 2023,Saturday marks the Family Weekend for Fall 2023 (F23) semester.
On Saturday, 14 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Exams is observed.
On Saturday, 14 October, 2023, during the Fall 2023 (F23) semester, Mini-1 Faculty Course Evaluations close is observed.
From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
Day: 'Friday', Event: 'Mini-1 voucher election deadline (4)', Semester: 'Fall 2024 (F24)'
Date: '2024-10-12', Day: 'Saturday', Event: 'Mini-1 Exams', Semester: 'Fall 2024 (F24)'
Date: '2024-10-12', Day: 'Saturday', Event: 'Mini-1 Faculty Course Evaluations close', Semester: 'Fall 2024 (F24)'
Start Date: '2024-10-14', End Date: '2023-10-18', Days: 'Monday to Friday', Event: 'Fall Break; No Classes', Semester: 'Fall 2024 (F24)'
Date: '2024-10-21', Day: 'Monday', Event: 'Mini-2 Classes Begin ', Semester: 'Fall 2024 (F24)'
Date: '2024-10-23', Day: 'Wednesday', Event: 'Mid-Semester & Mini-1 grades due by 4 pm', Semester: 'Fall 2024 (F24)'
Date: '2024-10-25', Day: 'Friday', Event: 'Mini-2 add, audit & tuition adjustment drop deadline (1)', Semester: 'Fall 2024 (F24)'
Date: '2024-11-05', Day: 'Tuesday', Event: 'Democracy Day; No Classes, except Evening classes after 5 pm will still meet', Semester: 'Fall 2024 (F24)'
Date: '2024-11-11', Day: 'Monday', Event: 'Semester pass/no pass & withdrawal deadline (3)', Semester: 'Fall 2024 (F24)'
Date: '2024-11-13', Day: 'Wednesday', Event: 'Mini-2 drop deadline; withdrawal grade assigned after this date (2)', Semester: 'Fall 2024 (F24)'
Start Date: '2024-11-18', End Date: '2024-11-22', Days: 'Monday to Friday', Event: 'Spring 2025 Registration Week', Semester: 'Fall 2024 (F24)'
Date: '2024-11-25', Day: 'Monday', Event: 'Mini-2 pass/no pass & withdrawal deadline (3)', Semester: 'Fall 2024 (F24)'
Date: '2024-11-25', Day: 'Monday',
Answer: "
"What number do all of the Integrated Innovation Institute classes start with in Summer 2024?
","['metadata_course_summer_one_all_24.txt', 'combined_metadata_final.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What number do all of the Integrated Innovation Institute classes start with in Summer 2024?

Context: The Class meets Thursday between 08:00PM and 09:30PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Zlotnikov located in Building TBA, Room None.
In Semester Summer One(All) 2024, from the department of Integrated Innovation Institute, the subject titled 'Designing for Production and Sustainability' with Course ID 49602 and Section A offers 10.0 units. The Class meets Monday between 08:00PM and 09:30PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building TBA, Room None.
In Semester Summer One(All) 2024, from the department of Integrated Innovation Institute, the subject titled 'Cyber-Physical Product Design' with Course ID 49603 and Section A offers 10.0 units. The Class meets Tuesday between 08:00PM and 09:30PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building TBA, Room None.
In Semester Summer One(All) 2024, from the department of Integrated Innovation Institute, the subject titled 'Software Management Independent Study' with Course ID 49790 and Section A offers 361224 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the San Jose, California location,led by experienced instructor Mercier located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Integrated Innovation Institute, the subject titled 'Software Management Capstone Project' with Course ID 49791 and Section A offers 361224 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the San Jose, California location,led by experienced instructor Mercier located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Language Technologies Institute, the subject titled 'MCDS Independent Study' with Course ID 11633 and Section A offers 3-36 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rose located in Building DNM, Room DNM.
The Class meets Monday between 08:00PM and 09:30PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building TBA, Room None.
In Semester Summer One(All) 2024, from the department of Integrated Innovation Institute, the subject titled 'Cyber-Physical Product Design' with Course ID 49603 and Section A offers 10.0 units. The Class meets Tuesday between 08:00PM and 09:30PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Instructor TBA located in Building TBA, Room None.
In Semester Summer One(All) 2024, from the department of Integrated Innovation Institute, the subject titled 'Software Management Independent Study' with Course ID 49790 and Section A offers 361224 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the San Jose, California location,led by experienced instructor Mercier located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Integrated Innovation Institute, the subject titled 'Software Management Capstone Project' with Course ID 49791 and Section A offers 361224 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the San Jose, California location,led by experienced instructor Mercier located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Language Technologies Institute, the subject titled 'MCDS Independent Study' with Course ID 11633 and Section A offers 3-36 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rose located in Building DNM, Room DNM.
In Semester Summer One(All) 2024, from the department of Language Technologies Institute, the subject titled 'Foundations of Computational Data Science' with Course ID 11637 and Section A offers 12.0 units. The Class meets Tuesday between 08:00AM and 09:20AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rose, Han located in Building DNM, Room DNM.
Answer: "
"What number do all of the Civil & Environmental Engineering classes start with?
","['combined_metadata_final.txt', 'metadata_course_spring_24.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What number do all of the Civil & Environmental Engineering classes start with?

Context: In Semester Spring 2024, from the department of Civil & Environmental Engineering, the subject titled 'Urban Systems Modeling' with Course ID 12735 and Section A offers 12.0 units. The Class meets Monday Wednesday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Pozzi located in Building GHC, Room 4301.
In Semester Spring 2024, from the department of Civil & Environmental Engineering, the subject titled 'Climate Change Adaptation:' with Course ID 12749 and Section NA offers 6.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Civil & Environmental Engineering, the subject titled 'TBA' with Course ID 12749 and Section A4 offers 6.0 units. The Class meets Monday Wednesday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ozis located in Building WEH, Room 5421.
In Semester Spring 2024, from the department of Civil & Environmental Engineering, the subject titled 'Infrastructure Management' with Course ID 12750 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Coffelt located in Building BH, Room A53.
In Semester Spring 2024, from the department of Civil & Environmental Engineering, the subject titled 'Finite Elements in Mechanics I:' with Course ID 12755 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Civil & Environmental Engineering, the subject titled 'Finite Elements in Mechanics I' with Course ID 12755 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 10:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Dayal located in Building MI, Room 348.
In Semester Spring 2024, from the department of Civil & Environmental Engineering, the subject titled 'Urban Systems Modeling' with Course ID 12735 and Section A offers 12.0 units. The Class meets Monday Wednesday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Pozzi located in Building GHC, Room 4301.
In Semester Spring 2024, from the department of Civil & Environmental Engineering, the subject titled 'Climate Change Adaptation:' with Course ID 12749 and Section NA offers 6.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Civil & Environmental Engineering, the subject titled 'TBA' with Course ID 12749 and Section A4 offers 6.0 units. The Class meets Monday Wednesday between 12:30PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ozis located in Building WEH, Room 5421.
In Semester Spring 2024, from the department of Civil & Environmental Engineering, the subject titled 'Infrastructure Management' with Course ID 12750 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Coffelt located in Building BH, Room A53.
In Semester Spring 2024, from the department of Civil & Environmental Engineering, the subject titled 'Finite Elements in Mechanics I:' with Course ID 12755 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Spring 2024, from the department of Civil & Environmental Engineering, the subject titled 'Finite Elements in Mechanics I' with Course ID 12755 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 10:00AM and 11:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Dayal located in Building MI, Room 348.
Answer: "
"In spring 2025, When do the first day of classes for the winter semester take place?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In spring 2025, When do the first day of classes for the winter semester take place?

Context: On Friday, 08 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Last Day of Classes is observed.
On Friday, 08 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 voucher deadline (4) is observed.
From 11 December, 2023,Monday to 12 December, 2023,Tuesday marks the Final Exams for Fall 2023 (F23) semester.
On Wednesday, 13 December, 2023, during the Fall 2023 (F23) semester, Reading Day is observed.
From 14 December, 2023,Thursday to 15 December, 2023,Friday marks the Final Exams for Fall 2023 (F23) semester.
On Saturday, 16 December, 2023, during the Fall 2023 (F23) semester, Reading Day is observed.
On Sunday, 17 December, 2023, during the Fall 2023 (F23) semester, Final Exams is observed.
On Monday, 18 December, 2023, during the Fall 2023 (F23) semester, Make-Up Final Exams is observed.
On Monday, 18 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Faculty Course Evaluations close is observed.
On Wednesday, 20 December, 2023, during the Fall 2023 (F23) semester, Final Grades Due by 4 pm is observed.
From 23 December, 2023,Saturday to 02 January, 2024,Tuesday marks the Winter Break for Fall 2023 (F23) semester which leads to University Closed.
On Monday, 08 January, 2023, during the Fall 2023 (F23) semester, Fall Deans' Lists Posted is observed.
On Monday, 15 January, 2024, during the Spring 2024 (S24) semester, Martin Luther King Day is observed, leading to No Classes.
On Tuesday, 16 January, 2024, during the Spring 2024 (S24) semester, First Day of Class is observed.
On Monday, 22 January, 2024, during the Spring 2024 (S24) semester, Mini-3 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 11 December, 2024, during the Fall 2024 (F24) semester, Reading Day is observed.
From 12 December, 2024,Thursday to 13 December, 2023,Friday marks the Final Exams for Fall 2024 (F24) semester.
On Saturday, 14 December, 2024, during the Fall 2024 (F24) semester, Reading Day is observed.
On Sunday, 15 December, 2024, during the Fall 2024 (F24) semester, Final Exams is observed.
On Sunday, 15 December, 2024, during the Fall 2024 (F24) semester, Semester & Mini-2 Faculty Course Evaluations close is observed.
On Monday, 16 December, 2024, during the Fall 2024 (F24) semester, Make-Up Final Exams is observed.
On Wednesday, 18 December, 2024, during the Fall 2024 (F24) semester, Final Grades Due by 4 pm is observed.
From 24 December, 2024,Tuesday to 25 December, 2023,Wednesday marks the University Closed for Fall 2024 (F24) semester.
From 31 December, 2024,Tuesday to 01 January, 2023,Wednesday marks the University Closed for Fall 2024 (F24) semester.
On Monday, 06 January, 2025, during the Fall 2024 (F24) semester, Fall Deans' Lists Posted is observed.
On Monday, 13 January, 2025, during the Spring 2025 (S25) semester, First Day of Classes is observed.
On Friday, 17 January, 2025, during the Spring 2025 (S25) semester, Mini-3 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 20 January, 2025, during the Spring 2025 (S25) semester, Martin Luther King Day is observed, leading to No Classes & University Closed.
On Monday, 27 January, 2025, during the Spring 2025 (S25) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Wednesday, 05 February, 2025, during the Spring 2025 (S25) semester, Mini-3 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
Answer: "
"How many authors contributed to the paper CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code?
","['graham neubig_31366ff634fc905affd78dbd8ddc9a872c006a87_content_2.txt', 'graham neubig_31366ff634fc905affd78dbd8ddc9a872c006a87_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many authors contributed to the paper CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code?

Context: Title: CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code
Authors: Shuyan Zhou, Uri Alon, Sumit Agarwal, Graham Neubig
Section: 1 Introduction
BLEU, CodeBLEU, and CrystalBLEU. (c) We pretrain and release five language-specific CodeBERT models to use with our publicly available code, for Java, Python, C, C++, and JavaScript. As of the time of this submission, our models have been downloaded from the Huggingface Hub more than 1,000,000 times.
Title: CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code
Authors: Shuyan Zhou, Uri Alon, Sumit Agarwal, Graham Neubig
Section: G Additional Examples
In this section, we provide additional examples in which CodeBERTScore prefers the functionally correct prediction, while the best baseline metric in each language ranks higher a functionally incorrect prediction, which is inequivalent to the reference. Figure 9 shows an example in Java, and Figure 10 shows a C++ example.
Answer: "
"Which section of the freeroll portion of the buggy course do buggies make a sharp right-hand turn?
","['cmubuggy_d403.txt', 'cmubuggy_d403.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Which section of the freeroll portion of the buggy course do buggies make a sharp right-hand turn?

Context: An mural celebrating Buggy can be found in CMU's Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions. Sweepstakes Slang Buggy: Vehicle being raced and also a nickname for the competition. Chute: A section of the freeroll portion of the buggy course (near the southwestern end of Frew Street at its intersection with Schenley Drive) where buggies make the sharp righthand turn from Schenley Drive onto Frew Street. Chute Flagger: Team member who provides a signal for buggy drivers to know when to start the right-hand turn from Schenley Drive onto Frew Street. Driver: Person who travels with a buggy and controls the vehicles via steering and braking systems. Pushbar: Structure attached to a buggy that a person pushes to propel that buggy forward. Pusher: Person who propels a buggy via a pushbar along one of the five hills of the buggy course. Shell: Entire outer structure or covering of a buggy that determines that buggy's aerodynamic characteristics. Transition: Procedure whereby one pusher finishes pushing a buggy and the next pusher in sequence starts to push that same buggy. Members of Fringe, celebrating their 50th year of Buggy racing, allowed a rare visit into its shop for a behind-the-scenes tour. This year Fringe is planning to roll four different vehicles, built and maintained in the Fringe workshop in the basement of the East Campus Garage, known as the ""Froom."" ""We're used to saying everything with 'fr' in front of it but when we say something in front of other people, it gets them confused,"" said Fringe head mechanic Dave Singh, who will graduate in May with a bachelor's degree in mechanical engineering and biomedical engineering. Fringe vehicles often are named with the letter ""B,"" like Boson, Blueshift, Bissa and Bumper. Other teams, such as Apex, often use names that connotate fire, while the SDC (Student Dormitory Committee) team, uses names such as Vice, Bane, Avarice and Malice. This year's buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It's one of the few times spectators can see buggies up close.
This year's buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It's one of the few times spectators can see buggies up close. Dave Singh explains how Fringe builds its buggies. Each team keeps its actual processes a secret, but each buggy has certain features such as a body, pushbar, wheels and driving and braking mechanisms. Behind Singh are other buggies built by Fringe. Behind the Wheel Drivers, have the closest connection to the vehicles, aside from the mechanics who spent countless hours building and maintaining individual buggies. While races are brief, each driver must log a number of practice runs to qualify to race. Teams practice nights and weekends throughout the fall and spring semesters as weather permits. Buggies are often built around drivers, so the fit can be snug, and drivers are often less than 5 feet, 3 inches in height. Most — but not all — are women. Tishya Girdhar, a junior in neuroscience, is social chair for Fringe. She came to CMU wanting to drive, but first she had to get over being claustrophobic. She started as a mechanic and got behind the wheel for last year's Sweepstakes. ""Our team's philosophy is to teach everyone how to do everything,"" Girdhar said. ""But I came to CMU wanting to drive. I wanted to drive so badly."" Amy Chen demonstrates how a driver is positioned inside of Boson, a Fringe buggy built in 2016. Safety is a top priority for teams. All drivers are required to have five pieces of safety gear: mouth guard, goggles, a harness that includes three points of contact to the body of the buggy, gloves and a helmet. And during practice runs and races, flaggers man the course to let drivers know if it's safe to proceed. The streets of the race course haven't changed, but the condition of the course varies due to potholes, which can make or break a fast run. The last course records were set in 2017, but this year potholes are making for some challenging conditions, said Amy Chen, a senior in psychology who has been driving for four years.
Answer: "
"What was  H. John Heinz III College previously called?
","['cmuhistory_d402.txt', 'combined_metadata_final.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What was  H. John Heinz III College previously called?

Context: The first U.S. drama degree was awarded in 1914 at Carnegie Tech. It began offering graduate degrees. In 1919, the first doctorate (in civil engineering) was awarded to Mao Yisheng, a student from China. It laid the groundwork for a research institution, recruiting leading scientists, offering sponsored fellowships with government and industry leaders and pioneering nontraditional interdisciplinary research, which brought together physicists, chemists and metallurgists, for example. Interdisciplinary research would become the hallmark of Carnegie Mellon research. It initiated the 'Carnegie Plan' in 1938, a new curriculum that required science and engineer students to take courses in humanities and social sciences in order to better understand the needs of society. Carnegie died in 1919, but his vision for an educated public lived on after him. Carnegie Tech - Post-war Years With the end of World War II, the latter half of the 20th century brought unprecedented growth to Carnegie Tech. In 1956, the arrival of the first IBM computer to campus was revolutionary, initiating a university culture where information technology pervaded virtually all areas of study. University culture also changed in 1973 when Margaret Morrison closed and women joined their male peers in classrooms and dorms. The times were changing, and Tech positioned itself at the forefront, opening three new schools: 1948: The Graduate School of Industrial Administration, later renamed the David A. Tepper School of Business, focusing on quantitative analysis and pioneering the field of management science. 1968: School of Urban and Public Affairs, later renamed the H. John Heinz III College, providing graduate training for work in the public sector. 1986: School of Computer Science, pioneering computing and artificial intelligence, led by interdisciplinary efforts of Allen Newell and Herbert Simon. Carnegie Mellon University In 1967, Carnegie Tech merged with the Mellon Institute, a science research center founded by the Mellon family of Pittsburgh, to become known as Carnegie Mellon University. The merger built upon a long history of support from the Mellons. It allowed Carnegie Mellon to establish the last of its current pillars: the Mellon College of Science and the College of Humanities and Social Sciences, now known as Marianna Brown Dietrich College of Humanities and Social Sciences.
In Semester Spring 2024, from the department of Heinz College Wide Courses, the subject titled 'Management Consulting' with Course ID 94808 and Section A offers 12.0 units. The Class meets Wednesday between 06:30PM and 09:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Brussalis located in Building HBH, Room 1005.
In Semester Spring 2024, from the department of Heinz College Wide Courses, the subject titled 'Strategy Development' with Course ID 94811 and Section A3 offers 6.0 units. The Class meets Monday Wednesday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Synnott located in Building HBH, Room 2008.
In Semester Spring 2024, from the department of Heinz College Wide Courses, the subject titled 'Applications of NL(X) and LLM' with Course ID 94812 and Section A3 offers 6.0 units. The Class meets Tuesday Thursday Friday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rao located in Building HBH, Room 1007.
In Semester Spring 2024, from the department of Heinz College Wide Courses, the subject titled 'Project Management' with Course ID 94813 and Section A3 offers 6.0 units. The Class meets Monday Wednesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Synnott located in Building HBH, Room 2009.
In Semester Spring 2024, from the department of Heinz College Wide Courses, the subject titled 'Project Management' with Course ID 94813 and Section B3 offers 6.0 units. The Class meets Monday Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Synnott located in Building HBH, Room 2009.
In Semester Spring 2024, from the department of Heinz College Wide Courses, the subject titled 'Agent-Based Modeling & Digital Twins' with Course ID 94815 and Section A4 offers 6.0 units.
Answer: "
"What is the full name of the conference where the paper Transformed Protoform Reconstruction, got published?
","['david mortensen_c5c6d006e399386c99068daba138021a62d6cc17_metadata.txt', 'david mortensen_papers.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the full name of the conference where the paper Transformed Protoform Reconstruction, got published?

Context: Faculty Name: david mortensen
Paperid: c5c6d006e399386c99068daba138021a62d6cc17
Title: Transformed Protoform Reconstruction
Year: 2023
Abstract: Protoform reconstruction is the task of inferring what morphemes or words appeared like in the ancestral languages of a set of daughter languages. Meloni et al (2021) achieved the state-of-the-art on Latin protoform reconstruction with an RNN-based encoder-decoder with attention model. We update their model with the state-of-the-art seq2seq model: the Transformer. Our model outperforms their model on a suite of different metrics on two different datasets: their Romance data of 8,000 cognates spanning 5 languages and a Chinese dataset (Hou 2004) of 800+ cognates spanning 39 varieties. We also probe our model for potential phylogenetic signal contained in the model. Our code is publicly available at https://github.com/cmu-llab/acl-2023.
Authors: Young Min Kim, Kalvin Chang, Chenxuan Cui, David R. Mortensen
Venue: Annual Meeting of the Association for Computational Linguistics
Tldr: {'model': 'tldr@v2.0.0', 'text': 'The Meloni et al (2021) model is updated with the state-of-the-art seq2seq model: the Transformer, which outperforms their model on a suite of different metrics on two different datasets: their Romance data of 8,000 cognate spanning 5 languages and a Chinese dataset (Hou 2004) of 800+ cognates spanning 39 varieties.'}
Url: https://arxiv.org/pdf/2307.01896
List of 2023 Open Access papers by david mortensen are:
ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages
Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models
SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing
Construction Grammar Provides Unique Insight into Neural Language Models
Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation
Transformed Protoform Reconstruction
PWESuite: Phonetic Word Embeddings and Tasks They Facilitate
Kuki-Chin Phonology: An Overview
Answer: "
"When is the buggy bash at the spring carnival?
","['cmubuggy_d403.txt', 'cmubuggy_d403.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When is the buggy bash at the spring carnival?

Context: Buggy Races Keep Rolling at Carnegie Mellon - News - Carnegie Mellon University Carnegie Mellon University ——— Search Search Search this site only News News › Stories › Archives › 2019 › April › Buggy Races Keep Rolling at Carnegie Mellon April 10, 2019 Buggy Races Keep Rolling at Carnegie Mellon In its 99th year, the tradition is a Spring Carnival treat By Heidi Opdyke opdyke(through)andrew.cmu.edu Media Inquiries Julie Mattera Marketing and Communications jmattera(through)cmu.edu 412-268-2902 Sweepstakes, also known as the Buggy Races, has come a long way at Carnegie Mellon University. The slick, torpedo-like vessels carrying drivers with nerves of steel are a far cry from the two-man teams that once changed places halfway through a race and rode in everything from rain barrels with bicycle wheels to three-wheeled ash cans 99 years ago. Today, it takes six people to maneuver the .84 -mile course around Schenley Park's Flagstaff Hill. But while five pushers and a driver navigate the course's hills, dozens of people are needed to make a successful race happen. A year of planning goes into just over two minutes of racing. The Machine The basics of a buggy are straightforward, but teams are often secretive in how they build the machines, in particular the way they brake, steer and what types of wheels are used. Each has a body, pushbar for runners to move the machine up the hills, wheels, a safety harness and driving and braking mechanisms. Some also include fairings, a type of housing around the wheels that help reduce drag, make the vehicle quieter and just looks cool. Fairings have been a key feature for the Fringe team in recent years, which is celebrating its 50th anniversary of Buggy. ""We have a reputation of being the quietest on the course,"" said Diya Nuxoll, who wrapped up her bachelor's degree in mechanical engineering in December and is working on an advanced studies master's degree in design and manufacturing. Nuxoll leads the Fringe team as one of two head mechanics. An mural celebrating Buggy can be found in CMU's Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions. Sweepstakes Slang Buggy: Vehicle being raced and also a nickname for the competition.
The last course records were set in 2017, but this year potholes are making for some challenging conditions, said Amy Chen, a senior in psychology who has been driving for four years. With the vehicles just inches from the ground, ""even poorly filled potholes makes it dangerous to drive,"" she added. Depending on Friday and Saturday's conditions, most heats may run just two lanes instead of three. But still, despite some of the challenges, Chen said she wouldn't miss it. ""Being a driver is really fun,"" Chen said. ""I love going fast and going around the course."" — #CMUcarnival — Powered by Curator.io — Related Content — Media Advisory: Carnegie Mellon Celebrates Spring Carnival Buggy Races Keep Rolling at Carnegie Mellon Enjoy a World of Fun at Spring Carnival Student Architects Design Carnival Archway Take a Ride on The Old Mill at CMU MoBot Turns 25 Witchner, Wood Roll Into Buggy Royalty The Piper: Campus & Community News Official Events Calendar Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA 15213 412-268-2900 Legal Info www.cmu.edu © 2021 Carnegie Mellon University CMU on Facebook CMU on Twitter CMU on LinkedIn CMU YouTube Channel CMU RSS Feed CMU on Instagram CMU Social Media Directory Stories College of Engineering College of Fine Arts Dietrich College of Humanities & Social Sciences Heinz College of Information Systems and Public Policy Mellon College of Science School of Computer Science Tepper School of Business Archives 2021 March February January 2020 December November October September August July June May April March February January 2019 December November October September August July June May April March February January 2018 December November October September August July June May April March February January 2017 January February March April May June July August September October November December 2016 January February March April May June July August September October November December 2015 January February March April May June July August September October November December 2014 January February March April May June July August September October November December 2013 January February March April May June July August September October November December 2012 January February March April May June July August September October November December 2011 January February March April May June July August September October November December Media Highlights Media Resources Experts (Alphabetical) Experts (by Topic) Contact Us The Piper: Campus & Community News
Answer: "
"When will the Spring Break end in 2024?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When will the Spring Break end in 2024?

Context: From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mini-2 Classes Begin is observed.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mid-Semester & Mini-1 grades due by 4 pm is observed.
On Friday, 27 October, 2023, during the Fall 2023 (F23) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
On Tuesday, 07 November, 2023, during the Fall 2023 (F23) semester, Democracy Day is observed, leading to No Classes, except Evening classes after 5 pm will still meet.
On Saturday, 11 November, 2023, during the Fall 2023 (F23) semester, Homecoming is observed.
From 13 November, 2023,Monday to 17 November, 2023,Friday marks the Spring 2024 Registration Week for Fall 2023 (F23) semester.
On Monday, 13 November, 2023, during the Fall 2023 (F23) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Wednesday, 15 November, 2023, during the Fall 2023 (F23) semester, Mini-2 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 22 November, 2023,Wednesday to 24 November, 2023,Friday marks the Thanksgiving Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Mini-2 pass/no pass & withdrawal deadline (2) is observed.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Faculty Course Evaluations open is observed.
On Friday, 08 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Last Day of Classes is observed.
On Monday, 22 January, 2024, during the Spring 2024 (S24) semester, Mini-3 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 29 January, 2024, during the Spring 2024 (S24) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Wednesday, 07 February, 2024, during the Spring 2024 (S24) semester, Mini-3 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 19 February, 2024, during the Spring 2024 (S24) semester, Mini-3 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 19 February, 2024, during the Spring 2024 (S24) semester, Mini-3 Faculty Course Evaluations open is observed.
On Monday, 26 February, 2024, during the Spring 2024 (S24) semester, Semester course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 01 March, 2024, during the Spring 2024 (S24) semester, Mini-3 Last Day of Classes is observed.
On Friday, 01 March, 2024, during the Spring 2024 (S24) semester, Mini-3 voucher deadline (4) is observed.
On Saturday, 02 March, 2024, during the Spring 2024 (S24) semester, Mini-3 Exams is observed.
On Saturday, 02 March, 2024, during the Spring 2024 (S24) semester, Mini-3 Faculty Course Evaluations close is observed.
From 04 March, 2024,Monday to 08 March, 2024,Friday marks the Spring Break for Spring 2024 (S24) semester which leads to No Classes.
On Monday, 11 March, 2024, during the Spring 2024 (S24) semester, First day of Mini-4 Classes is observed.
On Monday, 11 March, 2024, during the Spring 2024 (S24) semester, Mid-Semester & Mini-3 grades due by 4 pm is observed.
On Tuesday, 12 March, 2024, during the Spring 2024 (S24) semester, Summer 2024 Registration Opens is observed.
Answer: "
"What are the two innovative designs of StyleRF?
","['eric xing_8cc1cd002bfc36a8cba8bcbe63d32eacc656097f_metadata.txt', 'eric xing_8cc1cd002bfc36a8cba8bcbe63d32eacc656097f_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the two innovative designs of StyleRF?

Context: Faculty Name: eric xing
Paperid: 8cc1cd002bfc36a8cba8bcbe63d32eacc656097f
Title: StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields
Year: 2023
Abstract: 3D style transfer aims to render stylized novel views of a 3D scene with multiview consistency. However, most existing work suffers from a three-way dilemma over accurate geometry reconstruction, high-quality stylization, and being generalizable to arbitrary new styles. We propose StyleRF (Style Radiance Fields), an innovative 3D style transfer technique that resolves the three-way dilemma by performing style transformation within the feature space of a radiance field. StyleRF employs an explicit grid of high-level features to represent 3D scenes, with which highfidelity geometry can be reliably restored via volume rendering. In addition, it transforms the grid features according to the reference style which directly leads to high-quality zero-shot style transfer. StyleRF consists of two innovative designs. The first is sampling-invariant content transformation that makes the transformation invariant to the holistic statistics of the sampled 3D points and accordingly ensures multi-view consistency. The second is deferred style transformation of 2D feature maps which is equivalent to the transformation of 3D points but greatly reduces memory footprint without degrading multi-view consistency. Extensive experiments show that StyleRF achieves superior 3D stylization quality with precise geometry reconstruction and it can generalize to various new styles in a zero-shot manner. Project website: https://kunhao-liu.github.io/StyleRF/
Authors: Kunhao Liu, Fangneng Zhan, Yiwen Chen, Jiahui Zhang, Yingchen Yu, Abdulmotaleb El Saddik, Shijian Lu, E. Xing
Venue: Computer Vision and Pattern Recognition
Tldr: {'model': 'tldr@v2.0.0', 'text': 'StyleRF (Style Radiance Fields), an innovative 3D style transfer technique that resolves the three-way dilemma by performing style transformation within the feature space of a radiance field, achieves superior 3D stylization quality with precise geometry reconstruction and it can generalize to various new styles in a zero-shot manner.'}
Url: https://arxiv.org/pdf/2303.10598
Title: StyleRF: Zero-shot 3D Style Transfer of Neural Radiance Fields
Authors: Kunhao Liu, Fangneng Zhan, Yiwen Chen, Jiahui Zhang, Yingchen Yu, Abdulmotaleb El Saddik, Shijian Lu, Eric Xing
Section: 2. Related Work
styles consistently across adjacent video frames. Several studies leverage optical flow [5,18,47,56,57] as temporal constraints to estimate the movement of video contents. They can produce smooth videos, but have little knowledge of the underlying 3D geometry and cannot render consistent frames in arbitrary views [19, 37]. Huang et al. first tackle stylizing complex 3D scenes [19]. They construct a 3D scene by back-projecting image features into the 3D space to form a point cloud and then perform style transformation on the features of 3D points. Their method can achieve zero-shot style transfer, but requires an error-prone pre-trained depth estimator to model scene geometry. [37] also constructs a point cloud for stylization but it mainly focuses on monocular images. Instead, [6, 8, 11,22, 39, 63] use NeRF [36] as the 3D representation which can reconstruct scene geometry more faithfully. [6] is a photorealistic style transfer method that can only transfer the color tone of style images. [39,63] achieve 3D style transfer via optimization and can produce visually high-quality stylization, but they require a time-consuming optimization procedure for every reference style. [11, 22] employ latent codes to represent a set of pre-defined styles, but cannot generalize to unseen styles. [8] can achieve arbitrary style transfer by implicitly instilling the style information into MLP parameters. However, it can only transfer the color tone of style images but cannot capture detailed style patterns. StyleRF can transfer arbitrary style in a zero-shot manner, and it can capture style details such as strokes and textures as well.
Answer: "
"What is the technique used by Pengi to leverage Transfer Learning?
","['rita singh_ad22af138fa1d1490cda0301abf8159a7c30c5a2_metadata.txt', 'rita singh_papers.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the technique used by Pengi to leverage Transfer Learning?

Context: Faculty Name: rita singh
Paperid: ad22af138fa1d1490cda0301abf8159a7c30c5a2
Title: Pengi: An Audio Language Model for Audio Tasks
Year: 2023
Abstract: In the domain of audio processing, Transfer Learning has facilitated the rise of Self-Supervised Learning and Zero-Shot Learning techniques. These approaches have led to the development of versatile models capable of tackling a wide array of tasks, while delivering state-of-the-art performance. However, current models inherently lack the capacity to produce the requisite language for open-ended tasks, such as Audio Captioning or Audio Question&Answering. We introduce Pengi, a novel Audio Language Model that leverages Transfer Learning by framing all audio tasks as text-generation tasks. It takes as input, an audio recording, and text, and generates free-form text as output. The input audio is represented as a sequence of continuous embeddings by an audio encoder. A text encoder does the same for the corresponding text input. Both sequences are combined as a prefix to prompt a pre-trained frozen language model. The unified architecture of Pengi enables open-ended tasks and close-ended tasks without any additional fine-tuning or task-specific extensions. When evaluated on 22 downstream tasks, our approach yields state-of-the-art performance in several of them. Our results show that connecting language models with audio models is a major step towards general-purpose audio understanding
Authors: Soham Deshmukh, Benjamin Elizalde, Rita Singh, Huaming Wang
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'Pengi is introduced, a novel Audio Language Model that leverages Transfer Learning by framing all audio tasks as text-generation tasks, and shows that connecting language models with audio models is a major step towards general-purpose audio understanding.'}
Url: http://arxiv.org/pdf/2305.11834
List of 2023 Open Access papers by rita singh are:
Implementing International Federation of Gynecology and Obstetrics Nutrition Checklist for Pregnant Women: Opportunities and Challenges in Low- and Middle-income Countries
Mean Platelet Volume in Type 2 Diabetes: Correlation with Poor Glycaemic Control
Gonadotropin Receptor Cross-Talk and Altered Functions in Gonadal and Non-Gonadal Tissues
BASS: Block-wise Adaptation for Speech Summarization
Rethinking Voice-Face Correlation: A Geometry View
A Gene-Based Algorithm for Identifying Factors That May Affect a Speaker’s Voice
Deriving Vocal Fold Oscillation Information from Recorded Voice Signals Using Models of Phonation
Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations
The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features
Pengi: An Audio Language Model for Audio Tasks
GPT-Sentinel: Distinguishing Human and ChatGPT Generated Content
Token Prediction as Implicit Classification to Identify LLM-Generated Text
Effect of myo-inositol and di-chiro inositol plus vitamin D supplementation during pregnancy on prevention of gestational diabetes: a multi-centric, prospective, randomized, double-blind clinical trial
Plant-Avian Frugivory in the Urban Ecosystem of Delhi
LoFT: Local Proxy Fine-tuning For Improving Transferability Of Adversarial Attacks Against Large Language Model
Evaluating Speech Synthesis by Training Recognizers on Synthetic Speech
Rethinking Audiovisual Segmentation with Semantic Quantization and Decomposition
Towards Noise-Tolerant Speech-Referring Video Object Segmentation: Bridging Speech and Text
Importance of negative sampling in weak label learning
Completing Visual Objects via Bridging Generation and Segmentation
Training Audio Captioning Models without Audio
Prompting Audios Using Acoustic Properties For Emotion Representation
Pairwise Similarity Learning is SimPLE
Comparison of freeze-thaw and sonication cycle-based methods for extracting AMR-associated metabolites from Staphylococcus aureus
APPLIED ASPECT OF SATVAVAJAYA CHIKITSA
Utilization of the Whole Cowpea Pod and Barley Husk in The Production of Nutritionally Enriched Composite Flour
Answer: "
"In the paper ""Quantifying & Modeling Feature Interactions: An Information Decomposition Framework"", in which areas are the real-world applicability of the proposed approach demonstrated?
","['louis philippe morency_0a425c0d87c674b142104a07e17c5084b3ad28ca_metadata.txt', 'louis philippe morency_papers.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the paper ""Quantifying & Modeling Feature Interactions: An Information Decomposition Framework"", in which areas are the real-world applicability of the proposed approach demonstrated?

Context: Faculty Name: louis philippe morency
Paperid: 0a425c0d87c674b142104a07e17c5084b3ad28ca
Title: Quantifying & Modeling Feature Interactions: An Information Decomposition Framework
Year: 2023
Abstract: The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different signals. Despite these empirical advances, there remain fundamental research questions: how can we quantify the nature of interactions that exist among input features? Subsequently, how can we capture these interactions using suitable data-driven methods? To answer this question, we propose an information-theoretic approach to quantify the degree of redundancy , uniqueness , and synergy across input features, which we term the PID statistics of a multimodal distribution. Using 2 newly proposed estimators that scale to high-dimensional distributions, we demonstrate their usefulness in quantifying the interactions within multimodal datasets, the nature of interactions captured by multimodal models, and principled approaches for model selection. We conduct extensive experiments on both synthetic datasets where the PID statistics are known and on large-scale multimodal benchmarks where PID estimation was previously impossible. Finally, to demonstrate the real-world applicability of our approach, we present three case studies in pathology, mood prediction, and robotic perception where our framework accurately recommends strong multimodal models for each application.
Authors: P. Liang, Yun Cheng, Xiang Fan, Chun Kai Ling, Suzanne Nie, Richard J. Chen, Zihao Deng, Faisal Mahmood, R. Salakhutdinov, Louis-Philippe Morency
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work proposes an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy across input features, which is term the PID statistics of a multimodal distribution.'}
Url: https://arxiv.org/pdf/2302.12247
List of 2023 Open Access papers by louis philippe morency are:
Quantifying & Modeling Feature Interactions: An Information Decomposition Framework
Reconstructing the neurodynamics of face perception during real world vision in humans using intracranial EEG recordings
MultiZoo & MultiBench: A Standardized Toolkit for Multimodal Deep Learning
SenteCon: Leveraging Lexicons to Learn Human-Interpretable Language Representations
Neural Mixed Effects for Nonlinear Personalized Predictions
SHAP-based Prediction of Mother's History of Depression to Understand the Influence on Child Behavior
Counterfactual Augmentation for Multimodal Learning Under Presentation Bias
Difference-Masking: Choosing What to Mask in Continued Pretraining
Expanding the Role of Affective Phenomena in Multimodal Interaction Research
Multimodal Fusion Interactions: A Study of Human and Automatic Quantification
Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications
Representation Learning for Interpersonal and Multimodal Behavior Dynamics: A Multiview Extension of Latent Change Score Models
MultiViz: Towards User-Centric Visualizations and Interpretations of Multimodal Models
Understanding Masked Autoencoders via Hierarchical Latent Variable Models
Factorized Contrastive Learning: Going Beyond Multi-view Redundancy
Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions
Answer: "
"In the paper ""Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation"", what is the performance degradation of the progressively distilled model on the TSP-50 dataset?
","['yang yiming_papers.txt', 'yang yiming_papers.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In the paper ""Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation"", what is the performance degradation of the progressively distilled model on the TSP-50 dataset?

Context: List of 2023 Open Access papers by yang yiming are:
Functional targeted therapy for glioma based on platelet membrane-coated nanogels
Dual Responsive Magnetic Drug Delivery Nanomicelles with Tumor Targeting for Enhanced Cancer Chemo/Magnetothermal Synergistic Therapy
Expression of ALCAM in Clinical Colon Cancer and Relationship With Patients’ Treatment Responses
Claudin-10 in the Blood Brain Barrier Function of Cerebral Endothelial Cells and Transendothelial Invasion of Breast Cancer Cells
Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation
Automatic synchrotron tomographic alignment schemes based on genetic algorithms and human-in-the-loop software
High CD8+tumor-infiltrating lymphocytes indicate severe exhaustion and poor prognosis in angioimmunoblastic T-cell lymphoma
Numerical Investigation of Fatigue Crack Propagation Behaviour of 550E High-Performance Steel
Association between albumin-to-globulin ratio and the risk of overall survival in advanced non-small cell lung cancer patients with anlotinib treatment: a retrospective cohort study
Analysis of Volatile Components in Dried Fruits and Branch Exudates of Schisandra chinensis with Different Fruit Colors Using GC-IMS Technology
Secreted endogenous macrosomes reduce Aβ burden and ameliorate Alzheimer’s disease
DIFUSCO: Graph-based Diffusion Solvers for Combinatorial Optimization
Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs
Aligning Large Multimodal Models with Factually Augmented RLHF
An Experimental Study on Secondary Transfer Performances of Prestress after Anchoring Failure of Steel Wire Strands
MRI Features for Predicting Microvascular Invasion and Postoperative Recurrence in Hepatocellular Carcinoma Without Peritumoral Hypointensity
Impact of local governments’ construction land allocation strategies on innovation-driven development of China
16p11.2 CNV gene Doc2α functions in neurodevelopment and social behaviors through interaction with Secretagogin.
Chinese EFL learners different from English natives in cataphora resolution: Evidence from eye-tracking studies
Strain-driven Kovacs-like memory effect in glasses
The Relationship between the Serum NLRP1 Level and Coronary Lesions in Patients with Coronary Artery Disease
Matrine induces ferroptosis in cervical cancer through activation of piezo1 channel.
gives insight into Vitis divergence and sex determination
Inference of single cell profiles from histology stains with the Single-Cell omics from Histology Analysis Framework (SCHAF)
Recent Advances in the Ecology of Bloom-Forming Raphidiopsis (Cylindrospermopsis) raciborskii: Expansion in China, Intraspecific Heterogeneity and Critical Factors for Invasion
An Ultra-Low-Power Analog Multiplier–Divider Compatible with Digital Code for RRAM-Based Computing-in-Memory Macros
Extension of Pt–Ag cluster units by incorporating silver salts
Optimization of Critical Factors Affecting Dynamic Membrane Formation in a Gravity-Driven Self-Forming Dynamic Membrane Bioreactor towards Low-Cost and Low-Maintenance Wastewater Treatment
PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification
Learning Performance-Improving Code Edits
Generation-driven Contrastive Self-training for Zero-shot Text Classification with Instruction-tuned GPT
Self-Refine: Iterative Refinement with Self-Feedback
Long-tailed Extreme Multi-label Text Classification by the Retrieval of Generated Pseudo Label Descriptions
Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers
Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation
Active Retrieval Augmented Generation
Retrieval-Enhanced Generative Model for Large-Scale Knowledge Graph Completion
Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs
Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision
Policy Representation via Diffusion Probability Model for Reinforcement Learning
A Neural PDE Solver with Temporal Stencil Modeling
Research on Comprehensive Performance Optimization Method of Explosives and Propellants Oriented to the Whole Process
Answer: "
"In fall 2023, Who is the instructor for course 05432?
","['combined_metadata_final.txt', 'metadata_course_fall_23.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, Who is the instructor for course 05432?

Context: In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Software Structures for User Interfaces' with Course ID 05431 and Section NA offers 12.0 units. The Class meets Monday Wednesday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hudson located in Building 4SC, Room 104.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Software Structures for User Interfaces:' with Course ID 05431 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Software Structures for User Interfaces' with Course ID 05431 and Section A offers 12.0 units. The Class meets Friday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hudson located in Building 4SC, Room 104.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Personalized Online Learning' with Course ID 05432 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 03:00PM and 04:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Aleven located in Building WEH, Room 5409.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Machine Learning in Practice' with Course ID 05434 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rose located in Building SH, Room 105.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Service Design' with Course ID 05452 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 08:00AM and 09:50AM ET.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Software Structures for User Interfaces' with Course ID 05431 and Section NA offers 12.0 units. The Class meets Monday Wednesday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hudson located in Building 4SC, Room 104.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Software Structures for User Interfaces:' with Course ID 05431 and Section NA offers 12.0 units. The Class meets Day not scheduled between NA and NA ET. Students attend lectures at the NA location,led by experienced instructor NA located in Building NA, Room None.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Software Structures for User Interfaces' with Course ID 05431 and Section A offers 12.0 units. The Class meets Friday between 11:00AM and 12:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Hudson located in Building 4SC, Room 104.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Personalized Online Learning' with Course ID 05432 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 03:00PM and 04:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Aleven located in Building WEH, Room 5409.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Machine Learning in Practice' with Course ID 05434 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Rose located in Building SH, Room 105.
In Semester Fall 2023, from the department of Human-Computer Interaction, the subject titled 'Service Design' with Course ID 05452 and Section A offers 12.0 units. The Class meets Tuesday Thursday between 08:00AM and 09:50AM ET.
Answer: "
"When will the Spring Break start in 2024?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When will the Spring Break start in 2024?

Context: From 16 October, 2023,Monday to 20 October, 2023,Friday marks the Fall Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mini-2 Classes Begin is observed.
On Monday, 23 October, 2023, during the Fall 2023 (F23) semester, Mid-Semester & Mini-1 grades due by 4 pm is observed.
On Friday, 27 October, 2023, during the Fall 2023 (F23) semester, Mini-2 add, audit & tuition adjustment drop deadline (1) is observed.
On Tuesday, 07 November, 2023, during the Fall 2023 (F23) semester, Democracy Day is observed, leading to No Classes, except Evening classes after 5 pm will still meet.
On Saturday, 11 November, 2023, during the Fall 2023 (F23) semester, Homecoming is observed.
From 13 November, 2023,Monday to 17 November, 2023,Friday marks the Spring 2024 Registration Week for Fall 2023 (F23) semester.
On Monday, 13 November, 2023, during the Fall 2023 (F23) semester, Semester pass/no pass & withdrawal deadline (3) is observed.
On Wednesday, 15 November, 2023, during the Fall 2023 (F23) semester, Mini-2 drop deadline is observed, leading to withdrawal grade assigned after this date (2).
From 22 November, 2023,Wednesday to 24 November, 2023,Friday marks the Thanksgiving Break for Fall 2023 (F23) semester which leads to No Classes.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Mini-2 pass/no pass & withdrawal deadline (2) is observed.
On Monday, 27 November, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Faculty Course Evaluations open is observed.
On Friday, 08 December, 2023, during the Fall 2023 (F23) semester, Semester & Mini-2 Last Day of Classes is observed.
On Monday, 22 January, 2024, during the Spring 2024 (S24) semester, Mini-3 add, audit & tuition adjustment drop deadline (1) is observed.
On Monday, 29 January, 2024, during the Spring 2024 (S24) semester, Semester add, audit & tuition adjustment drop deadline (1) is observed.
On Wednesday, 07 February, 2024, during the Spring 2024 (S24) semester, Mini-3 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 19 February, 2024, during the Spring 2024 (S24) semester, Mini-3 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 19 February, 2024, during the Spring 2024 (S24) semester, Mini-3 Faculty Course Evaluations open is observed.
On Monday, 26 February, 2024, during the Spring 2024 (S24) semester, Semester course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 01 March, 2024, during the Spring 2024 (S24) semester, Mini-3 Last Day of Classes is observed.
On Friday, 01 March, 2024, during the Spring 2024 (S24) semester, Mini-3 voucher deadline (4) is observed.
On Saturday, 02 March, 2024, during the Spring 2024 (S24) semester, Mini-3 Exams is observed.
On Saturday, 02 March, 2024, during the Spring 2024 (S24) semester, Mini-3 Faculty Course Evaluations close is observed.
From 04 March, 2024,Monday to 08 March, 2024,Friday marks the Spring Break for Spring 2024 (S24) semester which leads to No Classes.
On Monday, 11 March, 2024, during the Spring 2024 (S24) semester, First day of Mini-4 Classes is observed.
On Monday, 11 March, 2024, during the Spring 2024 (S24) semester, Mid-Semester & Mini-3 grades due by 4 pm is observed.
On Tuesday, 12 March, 2024, during the Spring 2024 (S24) semester, Summer 2024 Registration Opens is observed.
Answer: "
"In fall 2023, What is the course title for unit 02801?
","['metadata_course_fall_23.txt', 'combined_metadata_final.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: In fall 2023, What is the course title for unit 02801?

Context: In Semester Fall 2023, from the department of Business Administration, the subject titled 'Microeconomics II' with Course ID 47801 and Section A2 offers 6.0 units. The Class meets Tuesday Thursday between 04:00PM and 05:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kushnir located in Building TEP, Room 5219.
In Semester Fall 2023, from the department of Business Administration, the subject titled 'Macroeconomics I' with Course ID 47803 and Section A2 offers 6.0 units. The Class meets Monday Wednesday between 04:00PM and 05:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Shourideh located in Building TEP, Room 5219.
In Semester Fall 2023, from the department of Business Administration, the subject titled 'Dynamic Competitive Analysis' with Course ID 47808 and Section A1 offers 6.0 units. The Class meets Monday Wednesday between 02:00PM and 03:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Spear located in Building TEP, Room 4219.
In Semester Fall 2023, from the department of Business Administration, the subject titled 'Econometrics I' with Course ID 47811 and Section A1 offers 6.0 units. The Class meets Monday Wednesday between 12:00PM and 01:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Sowell located in Building TEP, Room 5219.
In Semester Fall 2023, from the department of Business Administration, the subject titled 'Econometrics II' with Course ID 47812 and Section A2 offers 6.0 units. The Class meets Monday Wednesday between 02:00PM and 03:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Miller located in Building TEP, Room 5219.
In Semester Fall 2023, from the department of Business Administration, the subject titled 'Linear Programming' with Course ID 47834 and Section A1 offers 6.0 units. The Class meets Monday Wednesday between 04:00PM and 05:50PM ET.
In Semester Fall 2023, from the department of Heinz College Wide Courses, the subject titled 'Negotiation' with Course ID 94800 and Section E2 offers 6.0 units. The Class meets Monday between 06:30PM and 09:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Ciccone located in Building HBH, Room 2009.
In Semester Fall 2023, from the department of Heinz College Wide Courses, the subject titled 'Acting for Management' with Course ID 94801 and Section A1 offers 6.0 units. The Class meets Monday Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Murphy located in Building HBH, Room 2008.
In Semester Fall 2023, from the department of Heinz College Wide Courses, the subject titled 'Acting for Management' with Course ID 94801 and Section B2 offers 6.0 units. The Class meets Monday Wednesday between 03:30PM and 04:50PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Murphy located in Building HBH, Room 2009.
In Semester Fall 2023, from the department of Heinz College Wide Courses, the subject titled 'Acting for Management' with Course ID 94801 and Section C2 offers 6.0 units. The Class meets Monday Wednesday between 02:00PM and 03:20PM ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Murphy located in Building HBH, Room 2009.
In Semester Fall 2023, from the department of Heinz College Wide Courses, the subject titled 'Geographic Information Systems' with Course ID 94802 and Section A offers 12.0 units. The Class meets Schedule will be added between NA and NA ET. Students attend lectures at the Pittsburgh, Pennsylvania location,led by experienced instructor Kurland located in Building TBA, Room None.
In Semester Fall 2023, from the department of Heinz College Wide Courses, the subject titled 'Geographic Information Systems' with Course ID 94802 and Section B offers 12.0 units. The Class meets Tuesday Thursday between 09:30AM and 10:50AM ET.
Answer: "
"What does the MLT program prepare students for?
","['program_info_MasterofLanguageTechnologies.txt', 'mlt-student-handbook-2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What does the MLT program prepare students for?

Context: Academic Program Name:
Master of Language Technologies

Website:
https://lti.cs.cmu.edu/academics/masters-programs/mlt.html

Overview:
The MLT program prepares students for a research career in academia or industry. In this program, you’ll be immersed in research for two full years. During the academic year, your time will be evenly split between taking courses and doing research with your faculty advisor. Your summer will be devoted entirely to research. Many MLT grads continue on to Ph.D. programs at CMU and other top institutions, while others pursue careers at companies emphasizing research and rapid innovation.

Requirements:
The MLT program lasts two years (24 months), and students must complete two summers of research. Students should usually expect to graduate in August of their second year.
MLT students take 120 or more course units (about 10 courses), at least 72 of which are LTI courses, and 24 of which are School of Computer Science (SCS) courses. Most of these are 12-unit courses, although lab courses are typically 6 units. Our courses generally assume knowledge of programming and data structures. The remaining units may also be taken from the LTI, or with approval from the faculty advisor, any other senior- or graduate-level course offered at CMU or Pitt.
Directed research is another integral part of the MLT program; MLT students carry out directed research during their studies, with guidance from their faculty advisors.
Students may also choose to complete an optional MLT thesis. Guidelines can be found in the MLT Handbook.

Curriculum:
Here's an example of how your two years in the MLT program may break down.
It is to the student's advantage to avoid switching advisors, especially late in 
their graduate studies, because forging a strong student-advisor relationship takes time. 
4.3 Optional Masters Thesis 
MLT students may optionally elect to complete a Masters Thesis; it is not required. Students 
who intend to apply for the PhD program should consider the MLT Thesis as one of several 
ways to demonstrate their research potential. Other possibilities include innovative research in 
a research project or class project work. 
The MLT Thesis must be completed according to the following guidelines. Note in particular the 
October 31 proposal deadline! 
 There are two course components required for students who elect to complete the M.S. 
Thesis: 11-928  Masters Thesis I (with the chosen thesis advisor, typically for 12 units, 
typically in the Fall of their second year) and 11-929  Masters Thesis II in the following 
Spring. 
 Each of these two courses counts towards 6 units of MLT-level LTI course credit; if the 
student goes on to an LTI PhD, the second course (11-929) counts towards one PhD lab 
requirement.   
 The MLT thesis may focus on either a significant research result or a significant 
implementation of a new technology. The student should work with their advisor to 
define an interesting but bounded thesis research project. The MLT thesis culminates in 
MLT Graduate Student Handbook 
Page 16 
 
a publication-quality paper that clearly describes the work in terms of its research 
significance. 
 The MLT thesis committee will be composed of a thesis advisor and two additional 
readers, to be chosen by the student in consultation with the advisor, during the Fall 
Masters Thesis I course. At least 2 of the 3 committee members must be SCS faculty 
working in Language Technologies. 
 To receive a final grade for the first semester's course, the student must have prepared 
a written proposal of at least five pages, describing the scientific thesis to be 
investigated, any relevant research already completed by the student, a comparison 
with related work by others, and a detailed description of the work to be done in the 
next semester. The proposal MUST be accepted by the committee by the end of 
October, or the student will not be allowed to enroll in 11-929  Masters Thesis II!
Answer: "
"How many authors does the WebArena paper have?
","['daniel fried_e41482f4ee984f17382f6cdd900df094d928be06_content_2.txt', 'graham neubig_e41482f4ee984f17382f6cdd900df094d928be06_content_2.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many authors does the WebArena paper have?

Context: Title: WEBARENA: A REALISTIC WEB ENVIRONMENT FOR BUILDING AUTONOMOUS AGENTS
Authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig
Section: 3.2 EVALUATION ANNOTATION
remaining examples were contributed by three of the authors who are proficient in JavaScript programming. Difficult tasks were often discussed collectively to ensure the correctness of the annotation. The annotation required the annotator to undertake the full execution and scrutinize the intermediate states. Avg. Time 110s Success Rateinfo 74.68% Success Rateothers 81.32% Success Rateall 78.24% Human Performance We sample one task from each of the 170 templates and ask five computer science graduate students to perform these tasks. The human performance is on the right. Overall, the human annotators complete 78.24% of the tasks, with lower performance on information-seeking tasks. Through examining the recorded trajectories, we found that 50% of the failures are due to misinterpreting the intent (e.g., providing travel distance when asked for travel time), incomplete answers (e.g., providing only name when asked for name and email), and incomplete executions (e.g., partially filling the product information), while the remaining instances have more severe failures, where the executions are off-target.
Title: WEBARENA: A REALISTIC WEB ENVIRONMENT FOR BUILDING AUTONOMOUS AGENTS
Authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig
Section: 3.2 EVALUATION ANNOTATION
remaining examples were contributed by three of the authors who are proficient in JavaScript programming. Difficult tasks were often discussed collectively to ensure the correctness of the annotation. The annotation required the annotator to undertake the full execution and scrutinize the intermediate states. Avg. Time 110s Success Rateinfo 74.68% Success Rateothers 81.32% Success Rateall 78.24% Human Performance We sample one task from each of the 170 templates and ask five computer science graduate students to perform these tasks. The human performance is on the right. Overall, the human annotators complete 78.24% of the tasks, with lower performance on information-seeking tasks. Through examining the recorded trajectories, we found that 50% of the failures are due to misinterpreting the intent (e.g., providing travel distance when asked for travel time), incomplete answers (e.g., providing only name when asked for name and email), and incomplete executions (e.g., partially filling the product information), while the remaining instances have more severe failures, where the executions are off-target.
Answer: "
"How many modalities does the MultiBench benchmark include?
","['louis philippe morency_40fb36ee67fdde99b196b4d1772de114aa821698_metadata.txt', 'shinji watanabe_ef567580e167c3e7c546345df93d644be5d4f66f_content_2.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many modalities does the MultiBench benchmark include?

Context: Faculty Name: louis philippe morency
Paperid: 40fb36ee67fdde99b196b4d1772de114aa821698
Title: MultiZoo & MultiBench: A Standardized Toolkit for Multimodal Deep Learning
Year: 2023
Abstract: Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MultiZoo, a public toolkit consisting of standardized implementations of>20 core multimodal algorithms and MultiBench, a large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. Together, these provide an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, we offer a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MultiBench paves the way towards a better understanding of the capabilities and limitations of multimodal models, while ensuring ease of use, accessibility, and reproducibility. Our toolkits are publicly available, will be regularly updated, and welcome inputs from the community.
Authors: P. Liang, Yiwei Lyu, Xiang Fan, Arav Agarwal, Yun Cheng, Louis-Philippe Morency, R. Salakhutdinov
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'MultiZoo is released, a public toolkit consisting of standardized implementations of>20 core multimodal algorithms and MultiBench, a large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas that provide an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation.'}
Url: http://arxiv.org/pdf/2306.16413
[31] Paul Pu Liang et al., “Multibench: Multiscale benchmarks for multimodal representation learning,” in NeurIPS Track on Datasets and Benchmarks, 2021. [32] G. Potamianos et al., “Recent advances in the automatic recognition of audiovisual speech,” Proc. IEEE, 2003. [33] Arsha Nagrani et al., “Disentangled speech embeddings using cross-modal self-supervision,” in ICASSP, 2020. [34] Egils Avots et al., “Audiovisual emotion recognition in wild,” Machine Vision and Applications, 2019. [35] Bowen
Answer: "
"How many datasets does the MultiBench benchmark include?
","['shinji watanabe_ef567580e167c3e7c546345df93d644be5d4f66f_content_2.txt', 'louis philippe morency_40fb36ee67fdde99b196b4d1772de114aa821698_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: How many datasets does the MultiBench benchmark include?

Context: [19] Will Kay et al., “The kinetics human action video dataset,” arXiv preprint arXiv:1705.06950, 2017. [20] Khurram Soomro et al., “Ucf101: A dataset of 101 human actions classes from videos in the wild,” arXiv preprint arXiv:1212.0402, 2012. [21] Dima Damen et al., “Rescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100,” IJCV, 2022. [22] Martin Cooke et al., “An audio-visual corpus for speech perception and automatic speech recognition,” The Journal of the Acoustical Society of America, 2006. [23] Joon Son Chung et al., “Lip reading sentences in the wild,” in CVPR, 2017. [24] Triantafyllos Afouras et al., “Lrs3-ted: a large-scale dataset for visual speech recognition,” arXiv preprint arXiv:1809.00496, 2018. [25] A. Nagrani et al., “Voxceleb: a large-scale speaker identification dataset,” in Interspeech, 2017. [26] Joon Son Chung et al., “Voxceleb2: Deep speaker recognition,” in Interspeech, 2018. [27] Christoph Feichtenhofer et al., “A large-scale study on unsupervised spatiotemporal representation learning,” in CVPR, 2021. [28] Kristen Grauman et al., “Ego4d: Around the world in 3,000 hours of egocentric video,” in CVPR, 2022. [29] Wangchunshu Zhou et al., “VLUE: A multi-task multidimension benchmark for evaluating vision-language pretraining,” in ICML, 2022. [30] Linjie Li et al., “Value: A multi-task benchmark for videoand-language understanding evaluation,” in NeurIPS Track on Datasets and Benchmarks, 2021. [31] Paul Pu Liang et al., “Multibench: Multiscale benchmarks for multimodal representation learning,” in NeurIPS Track on Datasets and Benchmarks, 2021.
Faculty Name: louis philippe morency
Paperid: 40fb36ee67fdde99b196b4d1772de114aa821698
Title: MultiZoo & MultiBench: A Standardized Toolkit for Multimodal Deep Learning
Year: 2023
Abstract: Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MultiZoo, a public toolkit consisting of standardized implementations of>20 core multimodal algorithms and MultiBench, a large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. Together, these provide an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, we offer a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MultiBench paves the way towards a better understanding of the capabilities and limitations of multimodal models, while ensuring ease of use, accessibility, and reproducibility. Our toolkits are publicly available, will be regularly updated, and welcome inputs from the community.
Authors: P. Liang, Yiwei Lyu, Xiang Fan, Arav Agarwal, Yun Cheng, Louis-Philippe Morency, R. Salakhutdinov
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'MultiZoo is released, a public toolkit consisting of standardized implementations of>20 core multimodal algorithms and MultiBench, a large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas that provide an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation.'}
Url: http://arxiv.org/pdf/2306.16413
Answer: "
"What computation is offloaded to a k-nearest-neighbor (kNN) index in the Unlimiformer approach?
","['graham neubig_dbc368bc8b49347dd27679894524fa62f88492c9_metadata.txt', 'graham neubig_c432aff446d55e72a28394a1508e760cc9a25c08_content_2.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What computation is offloaded to a k-nearest-neighbor (kNN) index in the Unlimiformer approach?

Context: Faculty Name: graham neubig
Paperid: dbc368bc8b49347dd27679894524fa62f88492c9
Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input
Year: 2023
Abstract: Since the proposal of transformers, these models have been limited to bounded input lengths, because of their need to attend to every token in the input. In this work, we propose Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores. This kNN index can be kept on either the GPU or CPU memory and queried in sub-linear time; this way, we can index practically unlimited input sequences, while every attention head in every decoder layer retrieves its top-k keys, instead of attending to every key. We evaluate Unlimiformer on several long-document and book-summarization benchmarks, showing that it can process even 500k token-long inputs from the BookSum dataset, without any input truncation at test time. We demonstrate that Unlimiformer improves pretrained models such as BART and Longformer by extending them to unlimited inputs without additional learned weights and without modifying their code. We make our code and models publicly available at https://github.com/abertsch72/unlimiformer .
Authors: Amanda Bertsch, Uri Alon, Graham Neubig, Matthew R. Gormley
Venue: Neural Information Processing Systems
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This work proposes Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores.'}
Url: http://arxiv.org/pdf/2305.01625
Title: Why do Nearest Neighbor Language Models Work?
Authors: Frank F. Xu, Uri Alon, Graham Neubig
Section: 4.2 Increasing the Softmax Capacity
make the multiple embeddings not converge to the same vector, rendering over-parameterization useless. Besides simply increasing the number of embedding vectors equally for each word type, we also propose other alternatives to increase softmax capacity. First, we hypothesize that different word types have different difficulties for the language model to predict. For those words that appear very frequently, they may appear in many different contexts. As a result, instead of adding an equal number of additional embeddings to each word type, we propose to adaptively increase the number of embeddings for word types based on word frequency, or total training loss for the word. Second, we try to break the softmax bottleneck with a Mixture of Softmax. Yang et al. (2017) proposes a solution to the problem using a Mixture of Softmax (MoS) to produce more linearly independent probability distributions of words given different contexts. Last, opposite to training the word embeddings of increased size, we also consider ways to compress the datastore down to a similar-sized embedding matrix for softmax computation by clustering the whole datastore and allowing for further finetuning of the embedding matrix consisting of cluster centroids. However, none of these alternative methods provided additional benefits over the simple multi-embedding approach. More details on these attempts can be found in Appendix C. 5 Approximate kNN Search & Softmax Temperature 5.1 Comparing Approximate kNN Search To calculate PkNN of the non-parametric component in Equation 5, it is usually prohibitive to use exhaustive kNN search, and thus Khandelwal et al. (2020a) use approximate kNN search using the FAISS library (Johnson et al., 2019). The use of FAISS (similarly to other approximate search libraries) results in two varieties of approximation. • Approximate Neighbors: Because the search for nearest neighbors is not exact, the set of nearest neighbors might not be equivalent to the actual nearest neighbors. Recall the function mask-to-k(·) in Equation 5, it is the function where we select the kNN entries from the datastore Wds.
Answer: "
"Is chalk permitted in the Fitness Centre at the Jared L. Cohon University Center?
","['handbook_phd_2023-2024.txt', 'handbook_phd_2023-2024.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: Is chalk permitted in the Fitness Centre at the Jared L. Cohon University Center?

Context: More information regarding these services, business hours, locations and 
contact 
information 
can 
be 
found 
on 
the 
Office 
of 
Tartan 
Ink 
website 
at:  
https://www.cmu.edu/tartanink/aboutus/index.html. 
6.16 University Center 
www.cmu.edu/university-center 
The Jared L. Cohon University Center is a centerpiece of the campus that provides a space for 
special events, physical fitness, student organizations and various activities, as well as 
LTI Ph.D. Graduate Student Handbook 
Page 39 
 
accommodating retail and dining services. As the campus crossroads, the University Center 
functions as a place for students to interact, get involved and enjoy new experiences. Visit the 
University Center website for information about campus eateries, ATMs and PNC Bank, fitness 
rooms and schedules, retail stores, scheduling University Center space, the public prayer room, 
student organizations, and the Wright-Rogal Chapel. 
The University Center Information Desk (first floor of the Cohon Center next to Wean Commons 
and Kirr Commons) is the location if you want to know about upcoming campus events or have 
questions about Carnegie Mellon in general, call the Information Desk at 412-268-2107. The 
Information Desk not only provides information about campus events, but also sells postage 
stamps, makes copies, sends faxes, distributes campus maps, manages a lost & found, and has 
information brochures about Pittsburgh and the campus. 
6.17 Athletic/Fitness Facilities 
 www.cmu.edu/athletics 
For the participant or the spectator, Carnegie Mellon offers intercollegiate athletics, intramural 
sports, physical education classes and club sports. The Athletics Department also offers aerobics 
classes in the University Center as well as occasional workshops and instruction related to fitness 
and health. The administrative offices are located in the Cohon University Fitness Center. 
The University Centers recreational facilities include an eight-lane pool, racquetball and squash 
courts, aerobics room, fitness center and gym for basketball and volleyball.
The administrative offices are located in the Cohon University Fitness Center. 
The University Centers recreational facilities include an eight-lane pool, racquetball and squash 
courts, aerobics room, fitness center and gym for basketball and volleyball. With renovations to 
Skibo Gym and the new Highmark Center for Health, Wellness, and Athletics scheduled for 
completion in 2024, the strength and conditioning facility has been temporarily placed on the 
lawn next to the outdoor basketball court close to the Donner locker rooms, Gesling Stadium, and 
Weigand Gymnasium. All users must present a valid CMU ID to use these facilities. 
6.18 CMU Alert 
www.cmu.edu/alert  
CMU Alert sends voice and/or text messages to registered phones in the event of a campus 
emergency that threatens public safety or during tests of the system in the spring and fall 
semesters. Students are automatically registered for CMU-Alert using the current contact 
information that has been entered into the Student Information Online (SIO): 
https://www.cmu.edu/hub/sio/about.html. 
 
 
 
 
LTI Ph.D. Graduate Student Handbook 
Page 40 
 
 
Appendix A 
2023-2024 
Highlighted University Resources for Graduate Students 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Note: The following pages are meant to broadly include only some of the resources available to 
graduate students.  It is not an exhaustive appendix of resources, and students are strongly 
encouraged to visit the various websites linked below for the most up-to-date information.   
LTI Ph.D. Graduate Student Handbook 
Page 41 
 
A.1 Key Resources for Graduate Student Support  
A.1.1 Office of Graduate and Postdoctoral Affairs  
https://www.cmu.edu/graduate  graded@cmu.edu  
The Office of Graduate and Postdoctoral Affairs provides university-wide support for all graduate 
students and academic programs, with a focus on supporting graduate student success at 
Carnegie Mellon.
Answer: "
"What is the novel architecture introduced in the paper ""Efficient Sequence Transduction by Jointly Predicting Tokens and Durations""?
","['shinji watanabe_16fbcf340648b302ad8d4e6ed34c7ab5ad346db9_metadata.txt', 'shinji watanabe_16fbcf340648b302ad8d4e6ed34c7ab5ad346db9_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the novel architecture introduced in the paper ""Efficient Sequence Transduction by Jointly Predicting Tokens and Durations""?

Context: Faculty Name: shinji watanabe
Paperid: 16fbcf340648b302ad8d4e6ed34c7ab5ad346db9
Title: Efficient Sequence Transduction by Jointly Predicting Tokens and Durations
Year: 2023
Abstract: This paper introduces a novel Token-and-Duration Transducer (TDT) architecture for sequence-to-sequence tasks. TDT extends conventional RNN-Transducer architectures by jointly predicting both a token and its duration, i.e. the number of input frames covered by the emitted token. This is achieved by using a joint network with two outputs which are independently normalized to generate distributions over tokens and durations. During inference, TDT models can skip input frames guided by the predicted duration output, which makes them significantly faster than conventional Transducers which process the encoder output frame by frame. TDT models achieve both better accuracy and significantly faster inference than conventional Transducers on different sequence transduction tasks. TDT models for Speech Recognition achieve better accuracy and up to 2.82X faster inference than conventional Transducers. TDT models for Speech Translation achieve an absolute gain of over 1 BLEU on the MUST-C test compared with conventional Transducers, and its inference is 2.27X faster. In Speech Intent Classification and Slot Filling tasks, TDT models improve the intent accuracy by up to over 1% (absolute) over conventional Transducers, while running up to 1.28X faster. Our implementation of the TDT model will be open-sourced with the NeMo (https://github.com/NVIDIA/NeMo) toolkit.
Authors: Hainan Xu, Fei Jia, Somshubra Majumdar, Hengguan Huang, Shinji Watanabe, Boris Ginsburg
Venue: International Conference on Machine Learning
Tldr: {'model': 'tldr@v2.0.0', 'text': 'A novel Token-and-Duration Transducer architecture for sequence-to-sequence tasks by jointly predicting both a token and its duration, i.e. the number of input frames covered by the emitted token.'}
Url: http://arxiv.org/pdf/2304.06795
Title: Efficient Sequence Transduction by Jointly Predicting Tokens and Durations
Authors: Hainan Xu, Fei Jia, Somshubra Majumdar, He Huang, Shinji Watanabe, Boris Ginsburg
Section: F. Robustness to Token Repetitions
usually the case. The equations above are not meant to be rigorous proof but only serve to explain the issue. Therefore in this next decoding step, it is likely that, two = argmax ( join(enc[t], dec(<bos>, two, two, two, two, two, two︸ ︷︷ ︸ 6 twos ) ) (56) i.e. the model emits two for a second time at frame t. This will likely keep happening for 3 twos, 4 twos, etc, causing an infinite loop and won’t terminate unless some max-symbol-per-decoding-step is implemented in the decoding algorithm. In this case, we will end up having a lot of insertion errors in the output in the form of the same token repeating too many times. Option 2. if the model keeps emitting all blanks until somewhere after frame 41, and then it emits a five. Then we would have deletion errors in the decoding output. TDT is less prone to such repetition issues because the duration output of the model makes it not likely to stay on the same frame at different decoding steps (refer back to Fig. 4, there are very rare cases when duration 0 is emitted). Due to a lack of datasets specifically made with text repetitions, we use NeMo-TTS to generate 100 utterances, which randomly pick three digits from 1 to 9, and repeat each digit 3 - 5 times. We run ASR with different models on this dataset and results are reported in Table 10. We see that TDT models are much more robust than RNN-Ts with repeated speech.
Answer: "
"What is the effect of training speakers with a highly weighted ToM listener component?
","['graham neubig_e7b3b692b0816821aafc0d354749bc3802cbf6ac_metadata.txt', 'yonatan bisk_e7b3b692b0816821aafc0d354749bc3802cbf6ac_metadata.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the effect of training speakers with a highly weighted ToM listener component?

Context: Faculty Name: graham neubig
Paperid: e7b3b692b0816821aafc0d354749bc3802cbf6ac
Title: Computational Language Acquisition with Theory of Mind
Year: 2023
Abstract: Unlike current state-of-the-art language models, young children actively acquire language through interactions with their surrounding environment and caretakers. One mechanism that has been argued to be critical to language learning is the ability to infer the mental states of other agents in social environments, coined Theory of Mind (ToM) by Premack&Woodruff (1978). Drawing inspiration from the modern operationalized versions of ToM implemented in Rabinowitz et al. (2018) and Zhu et al. (2021), we build language-learning agents equipped with ToM, and measure its effects on the learning process. We model ToM by giving the speaker agent an internal listener model that is trained alongside the speaker and used to rerank potential utterances. We experiment with varying task difficulty, hypothesizing that models will acquire more complex language to adapt to stronger environmental pressures. We find that training speakers with a highly weighted ToM listener component leads to performance gains in our image referential game setting. We also find some evidence that increasing task difficulty in the training process results in more fluent and precise utterances in evaluation. This suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition.
Authors: Andy T. Liu, Hao Zhu, Emmy Liu, Yonatan Bisk, Graham Neubig
Venue: International Conference on Learning Representations
Tldr: {'model': 'tldr@v2.0.0', 'text': ""It is found that training speakers with a highly weighted ToM listener component leads to performance gains in the authors' image referential game setting, and suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition.""}
Url: http://arxiv.org/pdf/2303.01502
Faculty Name: yonatan bisk
Paperid: e7b3b692b0816821aafc0d354749bc3802cbf6ac
Title: Computational Language Acquisition with Theory of Mind
Year: 2023
Abstract: Unlike current state-of-the-art language models, young children actively acquire language through interactions with their surrounding environment and caretakers. One mechanism that has been argued to be critical to language learning is the ability to infer the mental states of other agents in social environments, coined Theory of Mind (ToM) by Premack&Woodruff (1978). Drawing inspiration from the modern operationalized versions of ToM implemented in Rabinowitz et al. (2018) and Zhu et al. (2021), we build language-learning agents equipped with ToM, and measure its effects on the learning process. We model ToM by giving the speaker agent an internal listener model that is trained alongside the speaker and used to rerank potential utterances. We experiment with varying task difficulty, hypothesizing that models will acquire more complex language to adapt to stronger environmental pressures. We find that training speakers with a highly weighted ToM listener component leads to performance gains in our image referential game setting. We also find some evidence that increasing task difficulty in the training process results in more fluent and precise utterances in evaluation. This suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition.
Authors: Andy T. Liu, Hao Zhu, Emmy Liu, Yonatan Bisk, Graham Neubig
Venue: International Conference on Learning Representations
Tldr: {'model': 'tldr@v2.0.0', 'text': ""It is found that training speakers with a highly weighted ToM listener component leads to performance gains in the authors' image referential game setting, and suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition.""}
Url: http://arxiv.org/pdf/2303.01502
Answer: "
"When are the Spring 2024 grades due for graduating students?
","['Meta-Data-Calendar-Sentences.txt', 'Meta-Data-Calendar-Sentences.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When are the Spring 2024 grades due for graduating students?

Context: From 04 May, 2024,Saturday to 05 May, 2024,Sunday marks the Reading Days for Spring 2024 (S24) semester.
On Monday, 06 May, 2024, during the Spring 2024 (S24) semester, Final Examinations is observed.
On Monday, 06 May, 2024, during the Spring 2024 (S24) semester, Semester & Mini-4 Faculty Course Evaluations close is observed.
On Tuesday, 07 May, 2024, during the Spring 2024 (S24) semester, Make-Up Final Examinations is observed.
On Wednesday, 08 May, 2024, during the Spring 2024 (S24) semester, Graduating Final Grades Due by 4 pm is observed.
On Sunday, 12 May, 2024, during the Spring 2024 (S24) semester, Commencement is observed.
On Tuesday, 14 May, 2024, during the Spring 2024 (S24) semester, Final Grades Due by 4 pm is observed.
On Tuesday, 21 May, 2023, during the Spring 2024 (S24) semester, Spring Deans' Lists Posted is observed.
On Monday, 13 May, 2024, during the Summer One 2024 (M24) semester, Semester & Mini-5 Classes Begin is observed.
On Friday, 17 May, 2024, during the Summer One 2024 (M24) semester, Mini-5 add, audit, & tuition adjustment drop deadline (1) is observed.
On Friday, 24 May, 2024, during the Summer One 2024 (M24) semester, Semester add, audit, & tuition adjustment drop deadline (1) is observed.
On Monday, 27 May, 2024, during the Summer One 2024 (M24) semester, Memorial Day is observed, leading to University Closed & No Classes.
On Tuesday, 28 May, 2024, during the Summer One 2024 (M24) semester, Mini-5 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Friday, 07 June, 2024, during the Summer One 2024 (M24) semester, Mini-5 pass/no pass & withdrawal deadline (3) is observed.
On Monday, 05 May, 2025, during the Spring 2025 (S25) semester, Semester & Mini-4 Faculty Course Evaluations close is observed.
On Tuesday, 06 May, 2025, during the Spring 2025 (S25) semester, Make-Up Final Examinations is observed.
On Wednesday, 08 May, 2025, during the Spring 2025 (S25) semester, Graduating Students' Final Grades Due by 4 pm is observed.
On Sunday, 11 May, 2025, during the Spring 2025 (S25) semester, Commencement is observed.
On Tuesday, 13 May, 2025, during the Spring 2025 (S25) semester, Final Grades Due by 4 pm is observed.
On Tuesday, 20 May, 2025, during the Spring 2025 (S25) semester, Spring Deans' Lists Posted is observed.
On Monday, 12 May, 2025, during the Summer 2025 (M25) semester, Summer Session All, One & Mini-5 Classes Begin is observed.
On Friday, 16 May, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 add, audit, & tuition adjustment drop deadline (1) is observed.
On Friday, 23 May, 2025, during the Summer 2025 (M25) semester, Summer All add, audit, & tuition adjustment drop deadline (1) is observed.
On Friday, 26 May, 2025, during the Summer 2025 (M25) semester, Memorial Day is observed, leading to University Closed & No Classes.
On Tuesday, 27 May, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 course drop deadline is observed, leading to withdrawal grade assigned after this date (2).
On Monday, 09 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 pass/no pass & withdrawal deadline (3) is observed.
On Friday, 13 June, 2025, during the Summer 2025 (M25) semester, Summer One & Mini-5 Faculty Course Evaluations open is observed.
Answer: "
"What dataset does the BASS paper by Bhiksha Raj's group evaluate on?
","['bhiksha raj_3bd320ddb25886417ae90011b00f13f5d558097b_metadata.txt', 'bhiksha raj_papers.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What dataset does the BASS paper by Bhiksha Raj's group evaluate on?

Context: Faculty Name: bhiksha raj
Paperid: 3bd320ddb25886417ae90011b00f13f5d558097b
Title: BASS: Block-wise Adaptation for Speech Summarization
Year: 2023
Abstract: End-to-end speech summarization has been shown to improve performance over cascade baselines. However, such models are difficult to train on very large inputs (dozens of minutes or hours) owing to compute restrictions and are hence trained with truncated model inputs. Truncation leads to poorer models, and a solution to this problem rests in block-wise modeling, i.e., processing a portion of the input frames at a time. In this paper, we develop a method that allows one to train summarization models on very long sequences in an incremental manner. Speech summarization is realized as a streaming process, where hypothesis summaries are updated every block based on new acoustic information. We devise and test strategies to pass semantic context across the blocks. Experiments on the How2 dataset demonstrate that the proposed block-wise training method improves by 3 points absolute on ROUGE-L over a truncated input baseline.
Authors: Roshan Sharma, Kenneth Zheng, Siddhant Arora, Shinji Watanabe, Rita Singh, B. Raj
Venue: Interspeech
Tldr: {'model': 'tldr@v2.0.0', 'text': 'This paper develops a method that allows one to train summarization models on very long sequences in an incremental manner and devise and test strategies to pass semantic context across the blocks.'}
Url: https://arxiv.org/pdf/2307.08217
List of 2023 Open Access papers by bhiksha raj are:
FREDOM: Fairness Domain Adaptation Approach to Semantic Scene Understanding
UTOPIA: Unconstrained Tracking Objects without Preliminary Examination via Cross-Domain Adaptation
SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning
Fixed Inter-Neuron Covariability Induces Adversarial Robustness
Understanding political polarization using language models: A dataset and method
Prolonged school closure during the pandemic time in successive waves of COVID-19- vulnerability of children to sexual abuses – A case study in Tamil Nadu, India
BASS: Block-wise Adaptation for Speech Summarization
Understanding Political Polarisation using Language Models: A dataset and method
An Approach to Ontological Learning from Weak Labels
Rethinking Voice-Face Correlation: A Geometry View
Paaploss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement
Improving Perceptual Quality, Intelligibility,
Answer: "
"What are the three aspects assessed by the holistic evaluation in MultiZoo & MultiBench?
","['louis philippe morency_40fb36ee67fdde99b196b4d1772de114aa821698_metadata.txt', 'louis philippe morency_40fb36ee67fdde99b196b4d1772de114aa821698_content_0.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What are the three aspects assessed by the holistic evaluation in MultiZoo & MultiBench?

Context: Faculty Name: louis philippe morency
Paperid: 40fb36ee67fdde99b196b4d1772de114aa821698
Title: MultiZoo & MultiBench: A Standardized Toolkit for Multimodal Deep Learning
Year: 2023
Abstract: Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MultiZoo, a public toolkit consisting of standardized implementations of>20 core multimodal algorithms and MultiBench, a large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. Together, these provide an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, we offer a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MultiBench paves the way towards a better understanding of the capabilities and limitations of multimodal models, while ensuring ease of use, accessibility, and reproducibility. Our toolkits are publicly available, will be regularly updated, and welcome inputs from the community.
Authors: P. Liang, Yiwei Lyu, Xiang Fan, Arav Agarwal, Yun Cheng, Louis-Philippe Morency, R. Salakhutdinov
Venue: arXiv.org
Tldr: {'model': 'tldr@v2.0.0', 'text': 'MultiZoo is released, a public toolkit consisting of standardized implementations of>20 core multimodal algorithms and MultiBench, a large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas that provide an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation.'}
Url: http://arxiv.org/pdf/2306.16413
Title: MULTIZOO & MULTIBENCH: A Standardized Toolkit for Multimodal Deep Learning
Authors: Paul Pu Liang, Yiwei Lyu, Xiang Fan, Arav Agarwal, Yun Cheng, Louis-Philippe Morency, Ruslan Salakhutdinov, Antti Honkela
Section: 4 Conclusion
In conclusion, we present MULTIZOO and MULTIBENCH, a large-scale open-source toolkit unifying previously disjoint efforts in multimodal research with a focus on ease of use, accessibility, and reproducibility, thereby enabling a deeper understanding of multimodal models. Through its unprecedented range of research areas, datasets, modalities, tasks, and evaluation metrics, our toolkit paves the way toward building more generalizable, lightweight, and robust multimodal models. These tools have already been used for new directions for visualizing trained models (Liang et al., 2023a), large-scale multimodal foundation models (Liang et al., 2023b), multimodal fusion methods (Huang et al., 2022; Xue and Marculescu, 2023), and other theoretical and empirical studies of multimodal learning in applications ranging from robotics (Li et al., 2022) and HCI (Wu et al., 2023) to IoT (Hou et al., 2023), remote sensing (Xiong et al., 2022), and healthcare (Suvon et al., 2022). Our toolkits are publicly available, regularly updated with new tasks and modeling paradigms, and welcome inputs from the community.
Answer: "
"What LTI professor was on ""SYNTACC : Synthesizing Multi-Accent Speech By Weight Factorization""?
","['alexander waibel_papers.txt', 'shinji watanabe_8402d64fde12cafaf8a1daa60de0acd1abedbffb_content_1.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What LTI professor was on ""SYNTACC : Synthesizing Multi-Accent Speech By Weight Factorization""?

Context: List of 2023 Open Access papers by alexander waibel are:
AdapITN: A Fast, Reliable, and Dynamic Adaptive Inverse Text Normalization
Train Global, Tailor Local: Minimalist Multilingual Translation into Endangered Languages
Towards Efficient Simultaneous Speech Translation: CUNI-KIT System for Simultaneous Track at IWSLT 2023
SYNTACC : Synthesizing Multi-Accent Speech By Weight Factorization
KIT’s Multilingual Speech Translation System for IWSLT 2023
Convoifilter: A case study of doing cocktail party speech recognition
Continually learning new languages
Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff
End-to-End Evaluation for Low-Latency Simultaneous Speech Translation
FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN
Incremental Learning of Humanoid Robot Behavior from Natural Interaction and Large Language Models
[30] Jungil Kong, Jaehyeon Kim, and Jaekyoung Bae, “HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis,” NeurIPS, 2020. [31] Ahmed Mustafa, Nicola Pia, and Guillaume Fuchs, “Stylemelgan: An efficient high-fidelity adversarial vocoder with temporal adaptive normalization,” in ICASSP, 2021. [32] Keith Ito and Linda Johnson, “The LJ speech dataset,” 2017. [33] Yinhan Liu, Jiatao Gu, Naman Goyal, et al., “Multilingual denoising pre-training for neural machine translation,” TACL, 2020. [34] Tomoki Hayashi, Ryuichi Yamamoto, Katsuki Inoue, et al., “ESPnet-TTS: Unified, reproducible, and integratable open source end-to-end text-to-speech toolkit,” in ICASSP, 2020. [35] Tomoki Hayashi, Ryuichi Yamamoto, Takenori Yoshimura, et al., “ESPnet2-TTS: Extending the edge of TTS research,” arXiv preprint arXiv:2110.07840, 2021. [36] Ilya Loshchilov and Frank Hutter, “Decoupled weight decay regularization,” in ICLR, 2018. [37] Matt Post, “A call for clarity in reporting BLEU scores,” in Conference on Machine Translation, 2018. [38] Nicholas A Nystrom, Michael J Levine, Ralph Z Roskies, et al., “Bridges: a uniquely flexible hpc resource for new communities and data analytics,” in XSEDE, 2015.
Answer: "
"What is the contact number of the Director of Sports Medicine?
","['tartanfacts_d404.txt', 'handbook-msaii-2022-2023.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: What is the contact number of the Director of Sports Medicine?

Context: In keeping with tradition, students, faculty, staff and alumni voted to name the new official mascot Scotty. But it's not just the costumed mascot that voters named. The live dog is also known as Scotty to the Carnegie Mellon community.
About Scotty
|
Celebrating Scotty
|
Performer Requests
Department Quick Facts
Name of School: Carnegie Mellon University
City/Zip: Pittsburgh, PA 15213
Founded: 1900
Enrollment (undergrad): 6,982- Fall 2021
Enrollment (graduate and doctoral): 7,062 - Fall 2021
Alumni: 102,577
Nickname: Tartans
Mascot: Scottie Dog
School Colors: Cardinal and Gray
Football Stadium and Track and Field: Gesling Stadium
Capacity: 3,500
Surface: FieldTurf
Basketball/Volleyball Gym: Wiegand Gymnasium
Capacity: 500
Soccer Stadium: CMU Soccer Field
Capacity: 250
Surface: FieldTurf
Affiliation: NCAA Division III
Conference: University Athletic Association | Presidents' Athletic Conference (Football Only)
President: Dr. Farnam Jahanian
Alma Mater, Year: University of Texas at Austin
Athletic Director: Dr. Josh Centor
Alma Mater, Year: Brandeis, 2004
Athletic Department Phone: 412-268-8054
Mailing Address:  5000 Forbes Avenue / Pittsburgh, PA 15213
It is the responsibility of each student to make arrangements with Student Health Services to 
either pay for their insurance at the beginning of the semester, or elect a payment plan over the 
course of the academic year. More information is available at the Student Health Services Web 
site www.cmu.edu/health-services/student-insurance/.  
12.4 Emergency Loans  
https://www.cmu.edu/student-affairs/dean/loans/ 
All students regardless of their program are eligible for the Emergency Student Loan, which is an 
interest-free and emergency-based loan repayable within 30 days. It is available through the 
Office of the Dean of Student Affairs; students may apply for the loan by stopping in to the 
Student Affairs Office, Warner Hall 321, or by calling (412) 268-2075 for an appointment.  
35 
 
13 Additional University Resources 
13.1 The HUB Student Services Center  
thehub@andrew.cmu.edu and www.cmu.edu/hub/  
The HUB is located in Warner Hall, Lower Level.  The HUB staff delivers comprehensive service 
and counsel to students and families regarding financial aid, billing and payment, registration 
and academic records. The Assistant Directors in The HUB serve as contacts for specific colleges 
and assist enrolled students with key aspects of the enrollment process.  A student can find their 
assigned HUB Assistant Director on their Student Information Online (SIO) Resource 
page.  Questions that need specialized, in-depth attention can be directed to the student's 
assigned Assistant Director.  For general questions and information, students may email The 
HUB or call 412-268-8186. 
13.2 Student Information Online (SIO)  
Student Information Online (SIO) is a secure site where students can find important, 
personalized information, including E-Bills and student account information, financial aid status 
and eligibility, grades and QPA, and course schedules. Students can update their and their 
spouse’s or domestic partner's contact information, sign up for E-Check & E-Refund, authorize 
their spouses, domestic partners or another individual to receive a copy of their E-Bill, request 
verifications, view their housing and meal plan assignments, and much more. Students can log 
on to SIO by going to www.cmu.edu/hub/sio and entering their Andrew User ID and password.
Answer: "
"When were Simon and Newell of CMU awarded the Turing award?
","['history_d401.txt', '25things_d400.txt']","You are a helpful assistant. Answer the question concisely in 1-10 words using the context.
Question: When were Simon and Newell of CMU awarded the Turing award?

Context: The university’s first computer science Ph.D.s were graduates of this program.Computer Science Department establishedIn 1965, Carnegie Tech established its Computer Science Department, or CSD, with a $5 million grant from the R.K. Mellon Foundation. Perlis was the first department head. There were no undergraduates; only Ph.D. students were admitted, and the department’s focus was on research, much of it funded by the federal government through the Defense Department’s Advanced Research Projects Agency, or DARPA. In 1967, Carnegie Institute of Technology merged with the nearby Mellon Institute of Industrial Research to form Carnegie-Mellon University. The Department of Computer Science moved into the newly created Mellon Institute of Science, later renamed Mellon College of Science, or MCS. Future SCS dean Raj Reddy joined the CSD in 1969 after three years as an assistant professor at Stanford. He brought with him research in speech, language and computer vision. But in 1970 and 1971, the new Computer Science Department faced its first crisis, as half of its tenured faculty members—including department head Perlis—left for other universities. Joe Traub was recruited from Bell Labs to CMU to become the new department head.Multi-processor machines emergeCSD emerged from the brief crisis as a highly agile, interdisciplinary entity, with many new faculty members taking joint appointments with other CMU departments. Several large projects emerged in the Computer Science Department, including C.mmp, the first shared-memory multiprocessor computer, with 16 processing units, and Cm*, a 50-processor computer. These computers were the forerunners of today’s ubiquitous multi-core desktops and laptops.Turing Awards and a Nobel PrizeIn 1975, Simon and Newell were awarded the A.M. Turing Award for their work in artificial intelligence. (As of 2014, 12 CMU alumni or faculty have been awarded Turings, sometimes considered the Nobel Prize of computing.) Three years later, Simon received the Nobel Prize in Economics for his work on decision-making theory. As the 1970s progressed, Newell became interested in human-computer interaction, and began a long relationship with Xerox’s Palo Alto Research Center, or PARC, which released the Xerox Alto in 1973.
25 Great Things About SCS | SCS25 - Carnegie Mellon University School of Computer Science Skip to main content Legal Events Register 25 Things History Video 25 Great Things About SCS What’s so great about computer science at Carnegie Mellon?We're glad you asked! Here are 25 great ideas from CMU computer scientists to think about as we celebrate the birthday of the School of Computer Science.1. Artificial intelligence, 1955-56 Can you write a working computer program without a computer? Herb Simon (H’90), at left, Allen Newell (IA’57), at right, and Cliff Shaw did. The team created the first artificial intelligence program, Logic Theorist, which could solve logic puzzles in the same way that a human might solve them. Newell demonstrated that it worked by writing the instructions on 3-by-5 index cards that were manipulated on the kitchen table by Newell, his wife, and a group of Carnegie Tech grad students. 2. Multi-core processors, 1971 Multi-core processors are common in today’s computers, but they were still science fiction in the early 1970s. But when CMU researchers found their existing machines too slow to keep pace with the advance of speech and graphics programs, they knew they had to do something. They solved the problem by ganging together 16 processors to build a pioneering computer called C.mmp—then topped the feat by linking 50 processors into Cm*. 3. Tutoring machines, 1973Games and drills, such as flash cards, have long been used to help students learn tough subjects. But the cognitive tutoring programs developed at Carnegie Mellon, beginning in the 1970s, did more than simply drill students on math problems. Cognitive tutors were able to adapt, presenting harder or easier problems as students learned or stumbled. Today, cognitive tutors teach subjects such as algebra to hundreds of thousands of students every year. 4. Speech recognition, 1976If you have an iPhone, ask Siri to look up “Hearsay I,” the first computer system capable of continuous speech recognition. It was developed by future Turing Award winner and future SCS dean Raj Reddy along with his students. Their work on subsequent systems established many of the principles that still underlie speech recognition software. 5.
Answer: "
