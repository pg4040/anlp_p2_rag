Title: TOWARDS PRACTICAL AND EFFICIENT IMAGE-TO-SPEECH CAPTIONING WITH VISION-LANGUAGE PRE-TRAINING AND MULTI-MODAL TOKENS
Authors: Minsu Kim, Jeongsoo Choi, Soumi Maiti, Jeong Hun Yeo, Shinji Watanabe, Yong Man Ro
Section: 3.3. Experimental Results
Effectiveness of vision-language pre-training. To confirm the effectiveness of vision-language pre-training strategies, we train three variants of the Im2Sp model. 1) The baseline that does not utilize the strategy and just initializes the image encoder with a pretrained image classifier [32], similar to [28]. 2) The model whose image encoder is initialized with the vision-language pre-trained model, CLIP [20]. 3) The proposed model that both image encoder and text decoder are initialized from the vision-language pre-trained model, GiT [21]. The speech decoders for the first two models are randomly initialized and trained on Im2Sp datasets. Table 2 shows the ablation results on Flickr8k and COCO. Without utilizing the vision-language pre-training, we achieve 12.9 and 17.4 BLEU scores on each database. By initializing the image encoder with pretrained CLIP using image-text association, we can greatly improve the performances on all metrics from the baseline. Therefore, we can confirm that by utilizing a vision-language pre-trained image encoder instead of a simple image classifier, the model can better capture the language-associated semantics from input images. Next, when we additionally initialize the speech decoder with a pre-trained text decoder, we can further improve the performance. This is due to the fact that speech units mainly hold linguistic information and can be regarded as unified representations of speech and text [9], enabling the speech decoder to inherit the language model knowledge of the large-scale pre-trained text decoder. Comparisons with the state-of-the-art methods. Table 3 shows the evaluation results on Flickr8k and COCO databases. For analysis purposes, we also report the performance of image captioning and cascaded (i.e., image captioning & text-to-speech) systems. Note that our text-based systems (i.e., image captioning and cascaded) are the models before transferred to Im2Sp, which are trained on over 3M image-text pairs [21]. As the Im2Sp model is trained on 89K image-audio pairs, a direct comparison cannot be made between different modal systems. We highlight that even though the performance of the Im2Sp model is lower than the cascaded system, we still need to develop an end-to-end Im2Sp model for the following reasons. 1) More than 40% of languages have no writing systems [49], so the text-based model is not feasible for them. 2) We can reduce the inference time and maintenance costs compared to using two systems of image captioning and textto-speech. Through continuous research efforts, we may achieve performance comparable to cascaded systems, much like what has been accomplished with end-to-end speech translation [50]. By comparing the performance of the proposed Im2Sp method with the previous state-of-the-art methods