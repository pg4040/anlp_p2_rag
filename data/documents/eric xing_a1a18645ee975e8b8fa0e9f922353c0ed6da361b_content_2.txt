Title: DOES COMPRESSING ACTIVATIONS HELP MODEL PARALLEL TRAINING?
Authors: Song Bian, Dacheng Li, Hongyi Wang, Eric P. Xing, Shivaram Venkataraman, MBZUAI Petuum
Section: 4.7 Performance Analysis
Bse w , where w is the bandwidth. Thus the overall speedup can be written as: (m−1n + 1)× LT + (n− 1)× Bsh w (m−1n + 1)× LTAE + (n− 1)× Bse w (3) From the Table 10, we see that we can maintain a ∼1.5x speedup as we scale the hidden size to 25600. This shows that if we increase the number of nodes when we increase in hidden size, AE compression retains its benefits. However, it is possible to avoid the diminishing speedup by properly scaling up the number of nodes n, where the speedup will asymptotically converge to he . In summary, compression in model parallelism has diminishing returns if we only scale up the model on a fixed cluster. To gain benefits from compression methods, one needs to also properly manage other parameters in the cost model, e.g. also scaling up the number of nodes and use the pipeline parallelism.