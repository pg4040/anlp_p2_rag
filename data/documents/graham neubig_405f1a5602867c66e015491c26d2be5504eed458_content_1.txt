Title: Neural Machine Translation for the Indigenous Languages of the Americas: An Introduction
Authors: Manuel Mager, Rajat Bhatnagar, Graham Neubig, Ngoc Thang Vu, Katharina Kann
Section: A.5 Transfer learning
for MT or used in an unsupervised fashion. Improvements to BART can be obtained by augmenting the maximum likelihood objective with an additional objective, which is a data-dependent Gaussian prior distribution (Li et al., 2020). Huge LMs can improve zero-shot and few-shot learning even further (Brown et al., 2020), but at a high computational cost. Pursuing another direction, Wang et al. (2019a) develops a hybrid architecture between a transformer and a pointer-generator network. At training time, the authors jointly train the encoder and the decoder in a denoising auto-encoding fashion. One crucial problem for transfer-learning is minimizing catastrophic forgetting (Serra et al., 2018). Chen et al. (2021) show that it is possible to combine a pre-trained multilingual model, with fine-tuining it with one single language pair, to improve zero-shot machine translation. Another way to handle this problem is reducing the number of parameter to be updated. Gheini et al. (2021) propose to only update the cross attention parameters.