Title: ADVANTAGE-BASED OFFLINE POLICY GRADIENTS
Authors: Ashutosh Baheti, Ximing Lu, Faeze Brahman, Ronan Le Bras, Maarten Sap, Mark Riedl
Section: C ADDITIONAL EXPERIMENTS AND RESULTS
hallucinated responses, to construct a smaller and more faithful training set called FaithDial (Dziri et al., 2022a). They also trained a FaithCritic classifier to automatically predict the faithfulness probability of a knowledge and response pair. Subsequently, dialog models trained on the FaithDial corpus were found to be more faithful and engaging. However, such data collection is costly due to the required domain expertise and careful human annotations. Models trained on FaithDial training set πref (DialoGPT) 2.89 .99 .90 .75 .25 .34 2.31 15.8 .147/.408/.565 + NLL 2.89 .99 .91 .75 .25 .32 2.01 15.6 .146/.408/.568 + wBC 2.90 .98 .91 .75 .25 .34 2.36 15.4 .154/.435/.605 + R GOLD 2.90 .99 .91 .74 .26 .40 3.00 15.5 .150/.411/.562 + R-LOL 2.91 .98 .91 .76 .26 .36 2.35 14.4 .159/.440/.608 + A-LOL (ref. free) 2.93 .98 .92 .76 .26 .33 2.21 15.0 .155/.437/.606 + A-LOL 2.94 .98 .93 .77 .26 .33 2.15 14.1 .159/.430/.592 + A-LOL seq 2.94 .97 .93 .78 .26 .32 1.82 14.5 .160/.450/.630 + A-LOL KL 2.92 .98 .92 .77 .26 .33 2.02 14.8 .159/.447/.623 Models trained on both WoW and FaithDial training set πref (DialoGPT) 2.80 .90 .91 .73 .25 .43 3.78 15.7 .160/.449/.622 + wBC 2.86 .95 .91 .75 .25 .41 3.48 15.7 .157/.447/.618 + R GOLD 2.87 .97 .91 .73 .25 .50 5.27 16.1 .158/.422/.569 + R-LOL 2.88 .94 .92 .75 .26 .43 3.66 15.0 .168/.465/.639 + A-LOL (ref. free) 2.92 .98 .92 .76 .26 .43 3.51 14.8 .164/.453/.624 + A-LOL 2.92 .97 .93 .75 .27 .40 2.92 14.1 .164/.452/.625 + A-LOL seq 2.93 .97 .93 .77 .27 .38 2.53 14.1 .164/.462/.650 + A-LOL KL 2.91 .97 .92 .76 .26 .41 3.39 15.0 .164/.454/.626 FaithDial Dtest 2.76 .95 .81 .76 .24 .23 .97 17.6 .166/.555/.792 In this experiment, we test whether A-LOL methods can improve LMs faithfulness even from suboptimal WoW data. Consequently, we select Dtr, Dv from WoW, containing 69K and 3.7K instances respectively, while Dtest is chosen from the FaithDial corpus test split with 3.6K high-quality faithful gold responses. While keeping Dtest fixed, we also create two additional Dtr with 18.3K instances from FaithDial and 87.3K instances from merged FaithDial and WoW. Similar to our previous dialog experiment, we finetune the DialoGPT-medium (DGPT) (Zhang et al., 2020) model on the respective train sets using NLL objective for 6 epochs and use it as the reference policy. Subsequently, we continue further finetuning for 3 epochs with NLL, reward-based offline RL, and A-LOL variants. In knowledge-grounded dialogs, responses should not only be faithful but also