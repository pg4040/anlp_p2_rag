Title: Methods for Measuring, Updating, and Visualizing Factual Beliefs in Language Models
Authors: Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, Srinivasan Iyer
Section: E Additional Results
Ablation across num. sequential steps. Fig. 5 shows the results for an ablation across rtest using two kinds of learned optimizers: SLAG1, where rtrain = 1, and a SLAG condition where rtrain = rtest. It is critical to the success of learned optimizers to train them to update points sequentially when this is a desired application. Further, sequential updating with sequence prediction tasks is the only setting where we see learned optimizers outperform baselines across all relevant metrics. Choosing training labels for learned optimizers. In early experiments, we found that it is beneficial to use all data points (including correctly predicted points) as Main Inputs during training, rather than restricting training to only incorrectly predicted points. We still focus on correcting wrong outputs at test time. But so we must select what label to use during optimizer training. To get a Hard Label, we use the correct label for incorrectly predicted points, and for correctly predicted points, we simply draw a label randomly from the labels in the training data. The alternative Beam Label condition uses a sample from the model’s beam search for a data point, as done in past work (De Cao et al., 2021; Mitchell et al., 2021). We show update metrics for zsRE split by the desired label in Table 16. If one’s goal is to fix wrong model outputs, then it is much better to use either the correct label or a random label as the desired model output during training rather than a sample from the model’s beam search. Update success improves by 3.27 (±0.65; p<1e−4) points for the Main Input and 2.38 (±1.05; p<1e−4) for Paraphrases, while ∆-Acc rises by 0.15 (±0.18; p=.09). Which beliefs are hard to update? We hypothesize that beliefs will be easier to update when they are more belief-like to begin with. We principally measure this via the correlation between update success rate and a belief’s consistency on paraphrases before the update, for our learned optimizer in a single-update setting (r = 1). Surprisingly, we observe no relationship between update success and the belief consistency. The correlation between consistency and update success is near 0 for both zsRE (ρ = −.027) and Wikidata5m (ρ = .013); see Fig. 6 for a plot of the relationship. So it appears that the learned optimizer can update model beliefs independently of how belief-like they are to begin with. We would also be interested in considering consistency under entailment, but the update success rate on LeapOfThought is already 100%, so there is no