Title: Completing Visual Objects via Bridging Generation and Segmentation
Authors: Xiang Li, Yinpeng Chen, Chung-Ching Lin, Hao Chen, Kai Hu, Rita Singh, Bhiksha Raj, Lijuan Wang, Zicheng Liu
Section: B. More Discussion
the centroid. The width and height of the rectangle are determined by the width and height of the bounding box of the ground truth object. We uniformly sample a ratio within [0.2, 0.9] and apply it to the ground truth width and height to occlude the object. Second, we randomly occlude the object by shifting its mask. Specifically, we randomly shift its mask by a range of [0.17, 0.25] and occluded the region within the shifted mask. We equally leverage these two occlusion strategies during training. For the object encoder to extract partial token cp in the CompNet, we utilize a Swin-Transformer (Liu et al., 2021) pre-trained on ImageNet (Deng et al., 2009) with an additional convolution layer to accept the concatenation of mask and image as input. We initialize the CompNet with the pre-trained weight of ControlNet with additional mask conditions. To segment objects in the segmentation stage, we give a mix of box and point prompts to the Segment Anything Model (SAM). Specifically, we uniformly sample three points from the partial object as the point prompts and we leverage an extended bounding box of the partial object as the box prompts. We also add negative point prompts at the corners of the box to further improve the segmentation quality. More visualization. As shown in Fig. 13, we provide more qualitative comparisons with Stable Diffusion (Rombach et al., 2022). We notice that Stable Diffusion tends to complete irrelevant objects to the complete parts and thus leads to an unrealism of objects. Instead, MaskComp is guided by a mask shape and successfully captures the correct object shape thus achieving superior results. Details of user study. There are 16 participants in the user study. All participants have relevant knowledge to understand the task. During the assessment, each participant is provided with instructions and an example to understand the task. We show an example of the images presented during the user study as Fig. 16 and Fig. 17. We list the instructions as follows. Task: Given the partial object (lower left), generate the complete object (upper left). Instruction: • Ranking images 1-5, put the best on the left and the worst on the right. • Please focus on the foreground object and ignore the difference presented in the background. • Original image is provided as a good example. • The criteria for ranking are founded on object quality, encompassing aspects such as completeness, realism, sharpness, and more. • It must be strictly ordered (no tie). • Please rank the image in the