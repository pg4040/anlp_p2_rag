Title: Overview of Robust and Multilingual Automatic Evaluation Metrics for Open-Domain Dialogue Systems at DSTC 11 Track 4
Authors: Mario Rodríguez-Cantelar, Chen Zhang, Chengguang Tang, Ke Shi, Sarik Ghazarian, João Sedoc, Luis Fernando D’Haro, Alexander Rudnicky
Section: B Appendix: Existing Benchmark Datasets
Descriptions of the datasets that constitute the DSTC10 benchmark can be found at Zhang et al. (2022c). Details of the remaining evaluation datasets are as follows: ECM-Eval - The test instances in ECM-Eval test set are sampled from the Emotional Short-Text Conversation (ESTC) dialogue corpus (Zhou et al., 2018b), which is built on top of the Short-Text Conversation (STC) dataset (Shang et al., 2015). ESTC is designed to build Chinese empathetic dialogue systems. The dialogues are crawled from Weibo and post-processing, such as the removal of trivial responses and filtering out potential advertisements, has been conducted by Shang et al. (2015). The dialogues are automatically annotated by pre-trained emotion classifiers along six different emotion categories, such as angry, happy, sad, etc. The dialogues in ESTC are much shorter. Most contain only a single post-response pair. LCCC-Eval - Data in LCCC-Eval are sampled from the Large-scale Cleaned Chinese Conversation dialogue corpus (LCCC) (Wang et al., 2020b). The LCCC corpus is designed for pretraining the Chinese dialogue model. The dialogues are mainly collected from Weibo, a Chinese microblogging website17 and other open-source Chinese dialogue corpora, such as the Douban Conversation (Wu et al., 2017) and the E-Commerce Conversation Corpus (Zhang et al., 2018b). All the dialogues belong to the general domain and a rigorous cleaning process, which is based on a series of heuristic rules and several classifiers, is conducted to filter out dialogues with noise, such as dirty words, special characters, facial expressions, ungrammatical sentences, etc. Both ESTC and LCCC are released by the THU-COAI group for research purposes at https://www.luge.ai/#/ KdConv-Eval - KdConv-Eval is constructed based 17https://en.wikipedia.org/wiki/Sina_W eibo on the KdConv corpus (Zhou et al., 2020a), a multi-domain Chinese dialogue dataset towards multi-turn knowledge-driven conversation. The corpus links the subjects of multi-turn discussions to knowledge graphs. It encompasses conversations from three categories (movies, music, and travel). These conversations involve detailed exchanges about relevant subjects and seamlessly move between a variety of topics. We sampled 354 dialogues from the original corpus to form the KdConv-Eval test dataset. HCChinese - Dialogues in HCChinese are collected by interacting with three state-of-the-art Chinese chatbots, Baidu Plato-XL (Bao et al., 2022), Microsoft XiaoIce (Zhou et al., 2020b), and a Chinese DialoGPT model that is trained in a similar manner to DialoGPT (Zhang et al., 2020). We chat with the chatbots on a diverse set of topics, such as entertainment, relationship, arts, travel, food, etc. The discussion of sensitive topics, such as politics and race, was avoided. A manual check is performed on each dialogue, and