Title: Why do Nearest Neighbor Language Models Work?
Authors: Frank F. Xu, Uri Alon, Graham Neubig
Section: 4.2 Increasing the Softmax Capacity
5a and Figure 5b respectively. We can see that the default temperature t = 1 does not always result in the best-interpolated perplexity and tuning softmax temperature is desirable for all sizes of datastore. The lesson learned here is that tuning the softmax temperature for the kNN distribution is crucial for getting optimal results from each setting. Only coincidentally, a temperature of 1.0 was close to optimal in the original settings of Khandelwal et al. (2020b), which hid the importance of this hyperparameter. In both the 5% subsampled datastore and the full datastore scenarios, temperature t = 1 is close to optimal when using “FAISS mask, FAISS score”. When using either “real mask” or “real score”, this is not true anymore. Even at the optimal temperature for each setting, “real mask, real score” somewhat underperforms “FAISS mask, real score”. It is consistent with the counter-intuitive phenomenon discussed in Section 5.1. There are also differences between the two scenarios of different datastore sizes. With the full datastore, using “real score” outperforms “FAISS score” given the same “FAISS mask”. However, the opposite is true when using the 5% datastore. This suggests that as the datastore size grows, using accurate distance values are better than the approximate ones. The relatively small gap between using “real score” and “FAISS score” in both datastore settings shows that the main contributor to the improvements is using approximate nearest neighbors (“FAISS mask”) rather than using approximate distance values (“FAISS score”). We hypothesize that this is related to regularization for preventing overfitting, and approximate search provides fuzziness that functions as a regularizer. We can think of the non-parametric part in kNN-LM, the kNN component as a model, where the datastore size is its model capacity, and the datastore is its training data. Considering that the kNN component uses the exact same training data as the base parametric LM, having ground truth, accurate kNN search may cause the kNN component to overfit the training data. Comparing the small datastore with only 5% with the original datastore, we see that a small datastore means a small training set for the kNN “model” and it thus it benefits more from this regularization, both both through using the FAISS mask and FAISS score (at optimal temperature settings). From these experiments, we can see that, surprisingly, one of the important ingredients in kNN-LM seems to be approximate kNN search, which likely prevents overfitting to the datastore created from the same training set. We further analyze this unexpected result in Appendix D, where we