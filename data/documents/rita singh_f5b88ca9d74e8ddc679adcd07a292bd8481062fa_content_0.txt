Title: PROMPTING AUDIOS USING ACOUSTIC PROPERTIES FOR EMOTION REPRESENTATION
Authors: Hira Dhamyal, Benjamin Elizalde, Soham Deshmukh, Huaming Wang, Bhiksha Raj, Rita Singh
Section: 6. REFERENCES
[1] Robert Plutchik, The emotions, University Press of America, 1991. [2] Paul Ekman, Are there basic emotions?, American Psychological Association, 1992. [3] Shahan Ali Memon, Hira Dhamyal, Oren Wright, Daniel Justice, Vijaykumar Palat, William Boler, Bhiksha Raj, and Rita Singh, “Detecting gender differences in perception of emotion in crowdsourced data,” arXiv preprint arXiv:1910.11386, 2019. [4] Hira Dhamyal, Shahan Ali Memon, Bhiksha Raj, and Rita Singh, “The phonetic bases of vocal expressed emotion: Natural versus acted,” Proc. Interspeech 2020, pp. 3451–3455, 2020. [5] Robert W Frick, “Communicating emotion: The role of prosodic features.,” Psychological bulletin, vol. 97, no. 3, pp. 412, 1985. [6] Klaus R Scherer, “Acoustic concomitants of emotional dimensions: Judging affect from synthesized tone sequences.,” 1972. [7] Aneta Pavlenko, Emotions and multilingualism., Cambridge University Press, 2005. [8] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail, and Huaming Wang, “Clap: Learning audio concepts from natural language supervision,” arXiv preprint arXiv:2206.04769, 2022. [9] Soham Deshmukh, Benjamin Elizalde, and Huaming Wang, “Audio retrieval with wavtext5k and clap training,” arXiv preprint arXiv:2209.14275, 2022. [10] Hira Dhamyal, Bhiksha Raj, and Rita Singh, “Positional encoding for capturing modality specific cadence for emotion detection,” Proc. Interspeech 2022, pp. 166–170, 2022. [11] Weixing Wang, Qianqian Li, Jingwen Xie, Ningfeng Hu, Ziao Wang, and Ning Zhang, “Research on emotional semantic retrieval of attention mechanism oriented to audio-visual synesthesia,” Neurocomputing, vol. 519, pp. 194–204, 2023. [12] Ha Thi Phuong Thao, Gemma Roig, and Dorien Herremans, “Emomv: Affective music-video correspondence learning datasets for classification and retrieval,” Information Fusion, vol. 91, pp. 64–79, 2023. [13] Qiuqiang Kong, Yin Cao, Turab Iqbal, Yuxuan Wang, Wenwu Wang, and Mark D Plumbley, “Panns: Large-scale pretrained audio neural networks for audio pattern recognition,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 2880–2894, 2020. [14] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al., “Huggingface’s transformers: State-of-the-art natural language processing,” arXiv preprint arXiv:1910.03771, 2019. [15] Konstantinos Drossos, Samuel Lipping, and Tuomas Virtanen, “Clotho: an audio captioning dataset,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020. [16] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and Gunhee Kim, “AudioCaps: Generating Captions for Audios in The Wild,” in NAACL-HLT, 2019. [17] Irene Martı́n-Morató and Annamaria Mesaros, “What is the ground truth? reliability of multi-annotator data for audio tagging,” in 2021 29th European Signal Processing Conference (EUSIPCO). IEEE, 2021, pp. 76–80. [18] Eduardo Fonseca, Xavier Favory, Jordi Pons, Frederic Font, and Xavier Serra, “Fsd50k: An open dataset of human-labeled sound events,”