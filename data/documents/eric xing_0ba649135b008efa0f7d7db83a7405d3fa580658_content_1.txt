Title: Effective Multi-Agent Deep Reinforcement Learning Control with Relative Entropy Regularization
Authors: Chenyang Miao, Yunduan Cui, Huiyun Li, Xinyu Wu
Section: C. Cooperation in Traditional Control Task
quickly reached an average return over 0 within 70k steps and finally obtained 210%, 12.05%, 264.6% and 16.3% higher maximum average return than DDPG, MADDPG, SAC and TD3. 2) Evaluation of the Sample Efficiency: The sample efficiency of MACDPP was evaluated in Fig. 7. Compared with MADDPG which converged slower to a certain level of control performances than the signal-agent baselines, the proposed method demonstrated great advantage in sample efficiency with the regularization of relative entropy. It only spent 36.6% samples to reach the same performance. This result indicated the importance of properly restricting large policy updates in MARL for superior effectiveness. MCDPP successfully reduced 44.91%, 37.94% and 27.98% usage of interactions than DDPG, SAC and TD3. 3) Evaluation of the Computational Efficiency: The computational burden was evaluated in Table VII by summarizing the average calculation time of the first 1000 steps in four control scenarios. Although the proposed method required 132.85%, 213.5%%, 196.92% and 220.43% more computational times compared with MADDPG, DDPG, TD3 and SAC. It was observed that the computational burden of MACDPP was alleviated in traditional control scenarios where the system operation took more time. The proposed method additionally consumed 32.85% time than MADDPG. Compared with the faster singleagent methods, our method increased the computation time from 96% to 120% while employing about four times more agents. Furthermore, with the increasing system operation and communication times (i.e., from Mujoco to robo-gym based on ROS toolkit), the phenomenon above became more and more noticeable. It demonstrated the potential and effectiveness of MACDPP in jointly controlling large-scale systems. 4) Case Study: In this subsection, we investigated the superior control behaviors of MACDPP through the rollouts of learned policies in both 6-agent HalfCheetah and 5-agent UR5 control scenarios. In the first case study, we explored the learned policies of MADDPG and MACDPP under the same setting of parameter and random seed. The test rollouts with 20 steps and the corresponding trajectories of each joint were analyzed in Fig. 8. It is clearly observed that the learned control behavior of MACDPP is more effective than the one of MADDPG. Effectively coordinating six joints by six agents, the proposed method learned a superior control strategy. Each joint timely conducted proper torque according to the current system states, resulting in a faster movement. In comparison, MADDPG had significant disadvantages in terms of coordinating six joints by separate agents. Although the agent in charge of dimension 2 successfully learned a similar policy to the one of MACDPP, the whole multi-agent system struggled to generate