Title: DIFFERENCE-MASKING: Choosing What to Mask in Continued Pretraining
Authors: Alex Wilf, Syeda Nahida Akter, Leena Mathur, Paul Pu Liang, Sheryl Mathew, Mengrou Shou, Eric Nyberg, Louis-Philippe Morency
Section: C Masking Language Tokens
In Section 4.3 we describe the motivation for using a word-level strategy in our implementation of DIFFERENCE-MASKING. An alternative implementation could be to assign each token in a word the same masking likelihood, and mask tokens only by this probability. This could result in some tokens from the same word being masked where others are not. Our intuition is that for specialized domains such as chemistry, subword tokens may be trivial to predict from their neighbors, but whole words may not be trivial to predict given the context. For example, a word such as “phosphates” would be tokenized into “phos” and “-phates”. We expect that it may be trivial to predict “phos” given “-phates” or vice versa, but it may be hard (and may promote a better understanding of the task) to predict the word “phosphates” given the context. Empirically, we find that this decision improved performance substantially, as shown in the results in Table 7 below.