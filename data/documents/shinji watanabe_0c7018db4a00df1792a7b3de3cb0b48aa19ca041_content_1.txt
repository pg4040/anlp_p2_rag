Title: Integrating Pretrained ASR and LM to Perform Sequence Generation for Spoken Language Understanding
Authors: Siddhant Arora, Hayato Futami, Yosuke Kashiwagi, Emiru Tsunoo, Brian Yan, Shinji Watanabe
Section: 7. References
“A study on the integration of pre-trained ssl, asr, lm and slu models for spoken language understanding,” in Proc. SLT, 2023, pp. 406–413. [18] L. Lugosch et al., “Speech model pre-training for end-to-end spoken language understanding,” in Proc. Interspeech, G. Kubin and Z. Kacic, Eds., 2019, pp. 814–818. [19] P. Wang et al., “Large-scale unsupervised pre-training for endto-end spoken language understanding,” in Proc. ICASSP, 2020, pp. 7999–8003. [20] C.-I. Lai et al., “Semi-supervised spoken language understanding via self-supervised speech and language model pretraining,” in Proc. ICASSP, 2021, pp. 7468–7472. [21] A. Pasad et al., “On the use of external data for spoken named entity recognition,” in Proc. NAACL, M. Carpuat, M. de Marneffe, and I. V. M. Ruiz, Eds., 2022, pp. 724–737. [22] E. Morais et al., “End-to-end spoken language understanding using transformer networks and self-supervised pre-trained features,” in Proc. ICASSP, 2021, pp. 7483–7487. [23] B. Agrawal et al., “Tie your embeddings down: Cross-modal latent spaces for end-to-end spoken language understanding,” in Proc. ICASSP, 2022, pp. 7157–7161. [24] Y.-S. Chuang et al., “SpeechBERT: An Audio-and-text Jointly Learned Language Model for End-to-end Spoken Question Answering,” in Proc. Interspeech, 2021. [25] Y. Chung, C. Zhu, and M. Zeng, “SPLAT: speech-language joint pre-training for spoken language understanding,” in Proc. NAACL, K. Toutanova et al., Eds., 2021, pp. 1897–1907. [26] M. Kim et al., “St-bert: Cross-modal language model pretraining for end-to-end spoken language understanding,” in Proc. ICASSP, 2021, pp. 7478–7482. [27] Z. Zhang et al., “A joint learning framework with bert for spoken language understanding,” IEEE Access, vol. 7, pp. 168 849– 168 858, 2019. [28] C.-J. Hsu et al., T5lephone: Bridging speech and text selfsupervised models for spoken language understanding via phoneme level t5, 2022. [29] Y. Huang et al., “Leveraging unpaired text data for training end-to-end speech-to-intent systems,” in Proc. ICASSP, 2020, pp. 7984–7988. [30] S. Seo, D. Kwak, and B. Lee, “Integration of pre-trained networks with continuous token interface for end-to-end spoken language understanding,” in Proc. ICASSP, 2022, pp. 7152– 7156. [31] S. Arora et al., “Two-pass low latency end-to-end spoken language understanding,” in Proc. Interspeech, H. Ko and J. H. L. Hansen, Eds., 2022, pp. 3478–3482. [32] D. Le et al., “Deliberation model for on-device spoken language understanding,” in Proc. Interspeech, 2022, pp. 3468–3472. [33] Y. Xia et al., “Deliberation networks: Sequence generation beyond one-pass decoding,” Proc. NeurIPS, pp. 1784–1794, 2017. [34] K. Hu et al., “Deliberation model based two-pass end-to-end speech recognition,” in Proc. ICASSP, 2020, pp. 7799–7803. [35] S. Dalmia et al., “Searchable hidden intermediates for end-toend models