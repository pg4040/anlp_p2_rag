Title: The Troubling Emergence of Hallucination in Large Language Models – An Extensive Definition, Quantification, and Prescriptive Remediations
Authors: Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S.M Towhidul Islam Tonmoy, Aman Chadha, Amit Sheth, Amitava Das
Section: 
tweets (NYT) (factually correct – FM) and (ii) the Politifact dataset (Politifact) (factually incorrect – SL). We selected 15 LLMs, based on the criteria delineated in Section 3.1, and used them to generate a total of 75,000 text passages, with each LLM producing 5,000 text prose entries. These entries were categorized as 2,500 each for FM and SL. The text prompts provided to these LLMs consisted of tweets from NYTimes and headlines sourced from the Politifact dataset. Table 1 reports detailed statistics about . 3.1 Choice of LLMs: Rationale and Coverage We chose 15 contemporary LLMs that have exhibited exceptional results on a wide range of NLP tasks, including: (i) GPT-4 (OpenAI, 2023a), (ii) GPT-3.5 (OpenAI, 2022), (iii) GPT-3 (Brown et al., 2020), (iv) GPT-2 (Radford et al., 2019), (v) MPT (Wang et al., 2023), (vi) OPT (Zhang et al., 2022), (vii) LLaMA (Touvron et al., 2023), (viii) BLOOM (Scao et al., 2022), (ix) Alpaca (Taori et al., 2023), (x) Vicuna (Chiang et al., 2023), (xi) Dolly (databricks, 2023), (xii) StableLM (AI, 2023), (xiii) XLNet (Yang et al., 2019), (xiv) T5 (Raffel et al., 2020), and (xv) T0 (Deleu et al., 2022). Appendix C.1 discusses additional details behind our selection criteria. Given the ever-evolving nature of the field, and HVI benchmark leaderboards will remain accessible to the research community, fostering an environment of continuous updates and contributions. 3.2 Annotating Hallucination For the annotation task of the 75,000 text snippets, we utilized Amazon Mechanical Turk (Amazon). We obtain sentence-level annotations for hallucination orientations and categories. We record four annotations per sentence and adopt the MACE tool (Hovy et al., 2013) to assess inter-annotator agreement and aggregate data. MACE has been empirically demonstrated to outperform majority voting, exhibiting superior performance (cf. Appendix B). 4 Hallucination Vulnerability Index (HVI) Given the growing usage of LLMs and their likeliness to hallucinate, there exists no uniform evaluation metric to measure these LLMs’ hallucinations. To address this gap, we define HVI, a comparative spectrum that allows us to evaluate and rank LLMs based on their vulnerability to producing hallucinations. HVI is calculated as in Eq. (1): HV Ix = 100U∗2 [ ∑Ux=1(N(x)−N(EFM))∗ (1−P(EFM)+δ1)+ (N(x)−N(ESL))∗ (1−P(ESL)+δ2)] (1) When defining HVI, we take several factors into account. Firstly, not all sentences generated by an LLM are hallucinated, so it is important to determine the ratio of actual hallucinated sentences with the total number of sentences. In this context, we consider U as the total number of sentences and N(x) as the total number of hallucinated sentences produced by