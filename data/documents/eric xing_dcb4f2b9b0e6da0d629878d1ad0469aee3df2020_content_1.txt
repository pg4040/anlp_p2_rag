Title: Understanding Masked Autoencoders via Hierarchical Latent Variable Models
Authors: Lingjing Kong, Martin Q. Ma, Guangyi Chen, Eric P. Xing, Yuejie Chi, Louis-Philippe Morency, Kun Zhang
Section: C. Experimental Setup
for the Vision Transformer varies. C.2. Pretraining and Linear Probing For pretraining MAE under different masking ratios or patch sizes, we leverage the Tensor Processing Unit (TPU) from Google Cloud. We train separate MAE models for each (masking ratio, patch size) pair, and each pretrained MAE corresponds to a unique masking ratio and patch size. We train all MAEs for 800 epochs. Training time varies, with the shortest (patch size = 32) taking 18 hours on a TPU v3-128 Pod, and the longest (patch size = 8) taking 40 hours on a TPU v3-128 pod. The architecture follows the exact implementation from the original MAE paper [22], without any hyper-parameter tuning except masking ratio and patch size, which we study in this paper. Details of augmentation, initialization, and base learning rate scaling can be found in the Appendix section of [22], all of which we follow. After pretraining, we also follow the original MAE work to use linear probing to evaluate the representation quality. After pretraining, we remove the projection layers and add a supervised learning classifier on frozen features of MAE encoders. The decoders are discarded during linear probing. Other details of linear probing can be found in the Appendix section of [22]. We use the same hyper-parameters of linear probing as in [22]. C.3. Reconstructing high-level or low-level representations To perform reconstruction, we use both the encoder and the decoder from the pretrained MAEs. All samples from ImageNet1K are passed through the encoder without any masking, and the decoder reconstructs images in the original input space. Since no masking is applied, no masking token is applied to the input of the decoder. We use the reconstructed images and the original images to perform evaluations of four metrics: SSIM, FSIM, MSE, and PSNR. No training is performed, and the weights of the encoder and the decoder are frozen. In Fig. 9, we show the reconstruction analysis using the original patch size design in MAE. Similar to the result in the main text, higher patch sizes produce image reconstructions capturing highlevel similarities better, while low patch sizes have reconstructions better on low-level metrics. C.4. Attention Analysis We follow the attention heatmap visualization in DINO [10], where the chosen token is the [CLS] token or an object-related token. We visualize the self-attention module from the last block of the MAE encoder ViT. Brighter colors suggest larger attention weights. For easier visualization, attentions below a threshold of activation scores are not shown. We use the same threshold as [10]. For the self-attention