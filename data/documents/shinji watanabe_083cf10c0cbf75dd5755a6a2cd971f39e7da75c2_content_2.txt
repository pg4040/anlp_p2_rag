Title: UNIVERSLU: UNIVERSAL SPOKEN LANGUAGE UNDERSTANDING FOR DIVERSE CLASSIFICATION AND SEQUENCE GENERATION TASKS WITH A SINGLE NETWORK
Authors: Siddhant Arora, Hayato Futami, Jee-weon Jung, Yifan Peng, Roshan Sharma, Yosuke Kashiwagi, Emiru Tsunoo, Shinji Watanabe
Section: 7. REFERENCES
“Multi-task learning for speaker verification and voice trigger detection,” in Proc. ICASSP, 2020, pp. 6844–6848. [37] Z. Huang, M. Rao, A. Raju, Z. Zhang, B. Bui, and C. Lee, “MTLSLT: Multi-task learning for spoken language tasks,” in Proceedings of the 4th Workshop on NLP for Conversational AI, 2022, pp. 120– 130. [38] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, “Language models are unsupervised multitask learners,” 2019. [39] S. Zhang et al., “OPT: open pre-trained transformer language models,” CoRR, vol. abs/2205.01068, 2022. [40] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, “Pretrain, prompt, and predict: A systematic survey of prompting methods in natural language processing,” ACM Comput. Surv., vol. 55, no. 9, 195:1–195:35, 2023. [41] K. Zhu et al., “Promptbench: Towards evaluating the robustness of large language models on adversarial prompts,” CoRR, vol. abs/2306.04528, 2023. [42] Z. Jiang, F. F. Xu, J. Araki, and G. Neubig, “How can we know what language models know,” Trans. Assoc. Comput. Linguistics, vol. 8, pp. 423–438, 2020. [43] A. Haviv, J. Berant, and A. Globerson, “Bertese: Learning to speak to BERT,” in Proc. ACL, 2021, pp. 3618–3623. [44] X. L. Li and P. Liang, “Prefix-tuning: Optimizing continuous prompts for generation,” in Proc. ACL, 2021, pp. 4582–4597. [45] H. Gao, J. Ni, K. Qian, Y. Zhang, S. Chang, and M. HasegawaJohnson, “Wavprompt: Towards few-shot spoken language understanding with frozen language models,” in Proc. Interspeech, 2022, pp. 2738–2742. [46] C. H. Yang, Y. Tsai, and P. Chen, “Voice2series: Reprogramming acoustic models for time series classification,” in Proc. ICML, ser. Proceedings of Machine Learning Research, vol. 139, 2021, pp. 11 808–11 819. [47] H. Yen et al., “A study of low-resource speech commands recognition based on adversarial reprogramming,” CoRR, vol. abs/2110.03894, 2021. [48] K. Chang, W. Tseng, S. Li, and H. Lee, “SpeechPrompt: An exploration of prompt tuning on generative spoken language model for speech processing tasks,” in Proc. Interspeech, 2022, pp. 5005–5009. [49] K. Chang et al., “SpeechPrompt v2: Prompt tuning for speech classification tasks,” CoRR, vol. abs/2303.00733, 2023. [50] K. Lakhotia et al., “Generative spoken language modeling from raw audio,” CoRR, vol. abs/2102.01192, 2021. [51] B. Li et al., “Scaling end-to-end models for large-scale multilingual asr,” in Proc. ASRU, 2021, pp. 1011–1018. [52] S. Arora et al., “A study on the integration of pipeline and E2E SLU systems toward STOP quality challenge,” CoRR, 2023. [53] M. Wang et al., “WhiSLU: End-to-End Spoken Language Understanding with Whisper,” in Proc. INTERSPEECH 2023, 2023. [54] OpenAI ChatGPT description,