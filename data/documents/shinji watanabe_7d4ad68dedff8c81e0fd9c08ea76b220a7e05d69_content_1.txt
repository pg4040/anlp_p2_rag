Title: SPEAKER-INDEPENDENT ACOUSTIC-TO-ARTICULATORY SPEECH INVERSION
Authors: Peter Wu, Li-Wei Chen, Cheol Jun Cho, Shinji Watanabe, Louis Goldstein, Alan W Black, Gopala K. Anumanchipalli
Section: 5.3. Resynthesis Analysis without EMA Labels
in Speech production and speech modelling, pp. 151–186. Springer, 1990. [8] A. Wrench, “The mocha-timit articulatory database,” 1999. [9] A. A. Wrench, “A multi-channel/multi-speaker articulatory database for continuous speech recognition research.,” Phonus., 2000. [10] K. Richmond, P. Hoole, and S. King, “Announcing the electromagnetic articulography (day 1) subset of the mngu0 articulatory corpus,” in Interspeech, 08 2011, pp. 1505–1508. [11] M. K. Tiede et al., “Quantifying kinematic aspects of reduction in a contrasting rate production task,” Journal of the Acoustical Society of America, 2017. [12] H. Zen et al., “LibriTTS: A corpus derived from librispeech for text-to-speech,” in Interspeech, 2019. [13] N. Seneviratne et al., “Multi-corpus acoustic-to-articulatory speech inversion.,” in Interspeech, 2019. [14] T. Toda, A. Black, and K. Tokuda, “Acoustic-to-articulatory inversion mapping with gaussian mixture model,” in Eighth International Conference on Spoken Language Processing, 2004. [15] Y. M. Siriwardena et al., “Acoustic-to-articulatory speech inversion with multi-task learning,” Interspeech, 2022. [16] J. Wang et al., “Acoustic-to-articulatory inversion based on speech decomposition and auxiliary feature,” in ICASSP, 2022. [17] G. Sun, Z. Huang, L. Wang, and P. Zhang, “Temporal convolution network based joint optimization of acoustic-toarticulatory inversion,” Applied Sciences, 2021. [18] B. Uria, I. Murray, S. Renals, and K. Richmond, “Deep architectures for articulatory inversion,” in Interspeech, 2012. [19] A. S. Shahrebabaki, S. M. Siniscalchi, and T. Svendsen, “Raw speech-to-articulatory inversion by temporal filtering and decimation,” Interspeech, 2021. [20] N. Bozorg, M. T. Johnson, and M. Soleymanpour, “Autoregressive articulatory wavenet flow for speaker-independent acoustic-to-articulatory inversion,” in SpeD. IEEE, 2021. [21] S. K. Maharana, A. Illa, R. Mannem, et al., “Acoustic-toarticulatory inversion for dysarthric speech by using crosscorpus acoustic-articulatory data,” in ICASSP. IEEE, 2021. [22] S. Udupa, A. Roy, A. Singh, et al., “Estimating articulatory movements in speech production with transformer networks,” arXiv preprint arXiv:2104.05017, 2021. [23] J. Chartier, G. K. Anumanchipalli, K. Johnson, and E. F. Chang, “Encoding of articulatory kinematic trajectories in human speech sensorimotor cortex,” Neuron, 2018. [24] A. S. Shahrebabaki, G. Salvi, T. Svendsen, and S. M. Siniscalchi, “Acoustic-to-articulatory mapping with joint optimization of deep speech enhancement and articulatory inversion models,” TASLP, vol. 30, pp. 135–147, 2022. [25] A. S. Shahrebabaki, N. Olfati, A. S. Imran, et al., “A two-stage deep modeling approach to articulatory inversion,” in ICASSP, 2021, pp. 6453–6457. [26] J. Kominek and A. W. Black, “The CMU Arctic speech databases,” in Fifth ISCA workshop on speech synthesis, 2004. [27] A. Wilkinson, A. Parlikar, S. Sitaram, et al., “Open-source consumer-grade indic text to speech.,” in SSW, 2016. [28] P. Wu, S. Watanabe, L. Goldstein, et al.,