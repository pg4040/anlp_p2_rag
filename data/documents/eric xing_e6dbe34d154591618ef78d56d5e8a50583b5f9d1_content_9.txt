Title: Contextualized Networks Reveal Heterogeneous Transcriptomic Regulation in Tumors at Sample-Specific Resolution
Authors: Caleb N. Ellington, Benjamin J. Lengerich, Thomas B.K. Watkins, Jiekun Yang, Manolis Kellis, Eric P. Xing
Section: 
barriers that would typically prohibit240 sample-specific inference. Context-dependent models models naturally account for non-identically distributed data241 and provide a principled method for performing statistical inference on data that would traditionally be too small or242 too heterogeneous. While it is generally believed that biological observations are a product of latent cellular states and243 tumors exhibit extreme patient-to-patient heterogeneity, these ideas are orthogonal in traditional modeling regimes.244 Contextualized GRNs are the first to effectively unite the two: networks are a useful latent representation, relating245 biomarkers to pathology through systems of molecular interactions, and accounting for network heterogeneity allows246 us to explore both population-level and per-patient tumor pathology in terms of latent representations of molecular247 systems.248 Materials and methods249 Contextualized Networks250 We seek a context-specific density of network parameters P(θ ∣ C) such that251 P(X ∣ C) = ∫ θ dθPM(X ∣ θ)P(θ ∣ C) is maximized, where PN(X ∣ θ) is the probability of gene expression X ∈ Rp under network model class M with252 parameters θ ∈ Rp×p and context C, which can contain both multivariate and real features. To overcome θ being a253 11 high-dimensional, structured latent variable, we assume that all contextualized networks lie on a subspace spanned by254 a set of K network archetypes A ∶= span({Ak ∈ Rp×p ∶ A1, ...,AK}), i.e. θ ∈ A. Further, the space spanned by A is255 parameterized by a latent variable (“subtype”) Z ∈ RK such that Z is a deterministic function of context Z = f(C)256 and the context-specific network model θ (and subsequently the gene expression observations X) are independent of257 context given Z, i.e. C ⊥ (X,θ) ∣ Z. In this way, we constrain θ as a convex combination of network archetypes via258 latent mixing.259 P(X ∣ C) = ∫ θ,Z dθdZPM(X ∣ θ)P(θ∣Z)P(Z ∣ C) = ∫ θ,Z dθdZPM(X ∣ θ)δ(θ − K ∑ k=1 ZkAk)δ(Z − f(C)) = PM(X ∣ϕ(C; f,A)) ϕ(C; f,A) = K ∑ k=1 ZkAk = K ∑ k=1 f(C)kAk Where the context encoder ϕ(C; f,A) is parameterized by a learnable context-to-subtype mapping f and the set of260 archetypes A. This architecture is shown in Figure 1d, and is learned end-to-end with backpropagation. While the261 archetypal networks Ak could use prior knowledge for initialization or regularization, no prior knowledge is required.262 In all experiments reported here, we do not use any prior knowledge of network structure or parameters.263 This framework unites three different perspectives of GRNs: (1) Correlation networks, in which network edges264 are the pairwise Pearson’s correlation between nodes, (2) Markov networks,