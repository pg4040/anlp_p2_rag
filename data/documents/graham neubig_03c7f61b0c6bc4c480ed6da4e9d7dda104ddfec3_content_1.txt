Title: Cross-Modal Fine-Tuning: Align then Refine
Authors: Junhong Shen, Liam Li, Lucio M. Dery, Corey Staten, Mikhail Khodak, Graham Neubig, Ameet Talwalkar
Section: 4.1. A Breadth Perspective: Can Pretrained Models Transfer Across Modalities?
transfer, but aligning the feature distribution using ORCA can resolve this issue and benefit transfer. Indeed, recent work has shown that optimizing directly for the task loss may distort the pretrained weights and lead to suboptimal solutions (Kumar et al., 2022; Lee et al., 2022). By manipulating the target distribution to look like the source distribution, we lower the risk of weight distortion, thus obtaining better downstream performance. We also quantify the effect of data alignment by training the embedder for different number of epochs and see whether optimizing distribution distance to various levels of convergence affects downstream performance. Figure 4 (left) plots the fine-tuning accuracy and the final distribution distance for different embedder learning levels. We see that as the dataset distance decreases, the fine-tuning accuracy increases. In addition, learning the embedder separately from fine-tuning stabilizes training, as the performance variance of ORCA is constantly lower than that of naive fine-tuning. These results confirm that data alignment is the key to effective cross-modal fine-tuning. KEY 2: FINE-TUNING ALL MODEL PARAMETERS As discussed in Section 2, Frozen Pretrained Transformers (FPT) (Lu et al., 2022) is a related work that showed pretrained language models contain knowledge relevant to outof-modality tasks. While FPT presented a general pipeline for adapting GPT-2 to tasks like CIFAR-10, the resulting models were not as good as those trained from scratch. FPT differs from ORCA in that (1) it does not perform data alignment, and (2) it only fine-tunes the layer norms. We have verified the importance of (1). Now, we isolate the impact of (2) by fine-tuning only the layer norms for ORCA. The bottom rows of Table 3 show that ORCA with finetuning the layer norms outperforms FPT, so pretraining the embedder can boost the performance of FPT. However, this performance gain is smaller than that in the full fine-tuning setting, which implies that full fine-tuning can take better advantage of the learned embeddings. In terms of runtime, FPT yields less than a 2Ã— speedup compared with full fine-tuning (Appendix A.4.6), despite the fact that we are updating many fewer parameters. This is unsurprising since gradients are still back-propagated through the entire network. Therefore, when computation allows, we recommend using ORCA with full fine-tuning for better performance. KEY 3: ADAPTING FROM THE RIGHT MODALITY Finally, we study how the pretraining modality affects finetuning. In the results reported so far, we choose pretrained models for each task based on the input dimension, i.e., we use RoBERTa for all 1D tasks and Swin for all 2D tasks.