Title: IMPROVING AUDIO CAPTIONING MODELS WITH FINE-GRAINED AUDIO FEATURES, TEXT EMBEDDING SUPERVISION, AND LLM MIX-UP AUGMENTATION
Authors: Shih-Lun Wu, Xuankai Chang, Gordon Wichern, Jee-weon Jung, François Germain, Jonathan Le Roux, Shinji Watanabe
Section: 3.4. Ablation Study
and D. Lopez-Paz, “mixup: Beyond empirical risk minimization,” in Proc. ICLR, 2018. [24] T. Kouzelis, G. Bastas, A. Katsamanis and A. Potamianos, “Efficient audio captioning transformer with patchout and text guidance,” Tech. Rep., DCASE Challenge, 2022. [25] X. Mei, C. Meng, H. Liu et al., “WavCaps: A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research,” arXiv preprint arXiv:2303.17395, 2023. [26] A. Holtzman, J. Buys, L. Du et al., “The curious case of neural text degeneration,” in Proc. ICLR, 2020. [27] M. Lewis, Y. Liu, N. Goyal et al., “BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension,” in Proc. ACL, 2020. [28] J. Devlin, M.-W. Chang, K. Lee and K. Toutanova, “BERT: Pre-training of deep bidirectional Transformers for language understanding,” in Proc. NAACL-HLT, 2019. [29] C. Raffel, N. Shazeer, A. Roberts et al., “Exploring the limits of transfer learning with a unified text-to-text transformer,” Journal of Machine Learning Research, 2020. [30] N. Muennighoff, N. Tazi, L. Magne and N. Reimers, “MTEB: Massive text embedding benchmark,” arXiv preprint arXiv:2210.07316, 2022. [31] Y. Gong, Y.-A. Chung and J. Glass, “PSLA: Improving audio tagging with pretraining, sampling, labeling, and aggregation,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2021. [32] Z. Zhou, Z. Zhang, X. Xu et al., “Can audio captions be evaluated with image caption metrics?,” in Proc. ICASSP, 2022. [33] S. Chen, C. Wang, Z. Chen et al., “WavLM: Large-scale selfsupervised pre-training for full stack speech processing,” IEEE Journal of Selected Topics in Signal Processing, 2022. [34] S. J. Rennie, E. Marcheret, Y. Mroueh et al., “Self-critical sequence training for image captioning,” in Proc. CVPR, 2017. [35] T. Chen, S. Kornblith, M. Norouzi and G. Hinton, “A simple framework for contrastive learning of visual representations,” in Proc. ICML, 2020. [36] X. Mei, Q. Huang, X. Liu et al., “An encoder-decoder based audio captioning system with transfer and reinforcement learning,” in Proc. DCASE, 2021. [37] B. Elizalde, S. Deshmukh, M. Al Ismail and H. Wang, “CLAP: learning audio concepts from natural language supervision,” in Proc. ICASSP, 2023. [38] J. Schulman, B. Zoph, C. Kim et al., “ImageBind: holistic AI learning across six modalities,” Meta AI Blog, 2023. [39] G. O. dos Santos, E. L. Colombini and S. Avila, “CIDEr-R: Robust consensus-based image description evaluation,” in Proc. Workshop on Noisy User-generated Text, 2021. [40] J. Hessel, A. Holtzman, M. Forbes et al., “CLIPScore: A reference-free evaluation metric for image captioning,” in Proc. EMNLP, 2021.