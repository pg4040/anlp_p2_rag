Title: TORCHAUDIO 2.1: ADVANCING SPEECH RECOGNITION, SELF-SUPERVISED LEARNING, AND AUDIO PROCESSING COMPONENTS FOR PYTORCH
Authors: Jeff Hwang, Moto Hira, Caroline Chen, Xiaohui Zhang, Zhaoheng Ni, Guangzhi Sun, Pingchuan Ma, Ruizhe Huang, Vineel Pratap, Yuekai Zhang, Anurag Kumar, Chin-Yun Yu, Chuang Zhu, Chunxi Liu, Jacob Kahn, Mirco Ravanelli, Peng Sun, Shinji Watanabe, Yangyang Shi, Yumeng Tao, Robin Scheibler, Samuele Cornell, Sean Kim, Stavros Petridis
Section: 8. REFERENCES
Higuchi, Atsunori Ogawa, and Tomohiro Nakatani, “Spatial correlation model based observation vector clustering and mvdr beamforming for meeting recognition,” in ICASSP. IEEE, 2016, pp. 385–389. [32] Hakan Erdogan, John R Hershey, Shinji Watanabe, Michael I Mandel, and Jonathan Le Roux, “Improved mvdr beamforming using singlechannel mask prediction networks.,” in Interspeech, 2016, pp. 1981– 1985. [33] Zhuohuang Zhang, Yong Xu, Meng Yu, Shi-Xiong Zhang, Lianwu Chen, et al., “Adl-mvdr: All deep learning mvdr beamformer for target speech separation,” in ICASSP. IEEE, 2021, pp. 6089–6093. [34] Anurag Kumar, Ke Tan, Zhaoheng Ni, Pranay Manocha, Xiaohui Zhang, et al., “Torchaudio-squim: Reference-less speech quality and intelligibility measures in torchaudio,” in ICASSP. IEEE, 2023, pp. 1–5. [35] Minkyu Jung, Ohhyeok Kwon, Seunghyun Seo, and Soonshin Seo, “Blank collapse: Compressing ctc emission for the faster decoding,” arXiv preprint arXiv:2210.17017, 2022. [36] Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, et al., “SpecAugment: A simple data augmentation method for automatic speech recognition,” in Interspeech. sep 2019, ISCA. [37] David Snyder, Guoguo Chen, and Daniel Povey, “MUSAN: A Music, Speech, and Noise Corpus,” 2015, arXiv:1510.08484v1. [38] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, et al., “Attention is all you need,” in NeurIPS, 2017. [39] Triantafyllos Afouras, Joon Son Chung, and Andrew Zisserman, “LRS3TED: a large-scale dataset for visual speech recognition,” arXiv preprint arXiv:1809.00496, 2018. [40] Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson, et al., “Looking to listen at the cocktail party: a speaker-independent audiovisual model for speech separation,” ACM Transactions on Graphics, vol. 37, no. 4, pp. 112:1–112:11, 2018. [41] J Chung, A Nagrani, and A Zisserman, “Voxceleb2: Deep speaker recognition,” Interspeech, 2018. [42] Alexandros Haliassos, Pingchuan Ma, Rodrigo Mira, Stavros Petridis, and Maja Pantic, “Jointly learning visual and auditory speech representations from raw data,” in ICLR, 2023. [43] Bowen Shi, Wei-Ning Hsu, and Abdelrahman Mohamed, “Robust self-supervised audio-visual speech recognition,” in Proceedings of Interspeech, 2023, pp. 2118–2122. [44] Eric Guizzo, Christian Marinoni, Marco Pennese, Xinlei Ren, Xiguang Zheng, et al., “L3das22 challenge: Learning 3d audio sources in a real office environment,” in ICASSP. IEEE, 2022, pp. 9186–9190. [45] Christoph Boeddeker, Wangyou Zhang, Tomohiro Nakatani, Keisuke Kinoshita, Tsubasa Ochiai, et al., “Convolutive transfer function invariant sdr training criteria for multi-channel reverberant speech separation,” in ICASSP. IEEE, 2021, pp. 8428–8432. [46] Yen-Ju Lu, Samuele Cornell, Xuankai Chang, Wangyou Zhang, Chenda Li, et al., “Towards low-distortion multi-channel speech enhancement: The espnet-se submission to the l3das22 challenge,” in ICASSP. IEEE, 2022, pp. 9201–9205. [47] Yi Luo, Cong Han, Nima Mesgarani, Enea