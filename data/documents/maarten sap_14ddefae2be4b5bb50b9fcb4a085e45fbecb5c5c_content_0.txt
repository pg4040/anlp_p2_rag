Title: Modeling Empathic Similarity in Personal Narratives
Authors: Jocelyn Shen, Maarten Sap, Pedro Colon-Hernandez, Hae Won Park, Cynthia Breazeal
Section: H Using LLMs as a Proxy for Human Annotations
Recent works raise the question of whether LLMs can be used to proxy human annotations (Gilardi et al., 2023). The motivation behind this method is that obtaining human labels across many pairs of stories is costly, and this cost only compounds as the number of stories in the corpus increases. As such, we provide additional analyses as to whether or not these models can truly perform at the same level as human annotators for our task, which involves heavy empathy and emotion reasoning. H.1 Individual Story Annotation We prompt ChatGPT (gpt-3.5-turbo) to generate summaries of each story’s main event, emotion, and moral, in addition to a list of reasons why a narrator might empathize with the story. We compare these summaries against human-written summaries using BLEU, ROUGE, METEOR, and BertScore (Table 10), showing that ChatGPT has relatively low performance across all four metrics. H.2 Paired Story Annotation We feed the same prompt given to human annotators into ChatGPT, asking for a Likert score from 1-4 for the empathic similarity between two stories. The Spearman’s correlation between human and ChatGPT generated labels is 0.22 (p < 0.001), indicating weakly positive correlation between human annotations and ChatGPT annotations. In addition, we perform a one-sample t-test on the meansquared error between automatically generated labels and human annotations across all story pairs in the training data, obtaining a p-value < 0.001, indicating that the mean of all the errors is nonzero with statistical significance. Finally, we bin the ChatGPT annotations into agree/disagree categories, and compute the classification precision (0.59), recall (0.40), F1 score (0.48), and accuracy (0.59) as compared to human gold labels. These scores offer insight as to how well ChatGPT predicts the direction of the empathic similarity annotation, but we see that accuracy is low when comparing to human labels. In Figure 7, we see that ChatGPT similarity scores are skewed to the left, indicating that humans are more likely to find empathic similarities between experiences. These results are also supported by the higher number of false negatives when comparing ChatGPT classification to human gold labels.