Title: CUTTLEFISH: LOW-RANK MODEL TRAINING WITHOUT ALL THE TUNING
Authors: Hongyi Wang, Saurabh Agarwal, Pongsakorn U-chupala, Yoshiki Tanaka, Eric P. Xing, Dimitris Papailiopoulos
Section: C.2 CUTTLEFISH hyperparameters
closer to the reference line, it indicates that the model weights are more like full-rank, i.e., contain less redundancy. From Figure 9, we can see that to approximate the major information of the weight matrix, e.g., preserving 80% of the singular value information, relatively higherRs should be used, e.g., ρ = 1 2 . Additionally, the attention weights, such as Wq , Wk, Wv , as well as the projection layer after the query, key, and value layers (i.e., Wo in our notation) tend to have lower ranks (higher redundancy) compared to the FFN layers, i.e., FC1 and FC2 in Figure 9. In CUTTLEFISH, for DeiT and ResMLP models, we use a global rank ratio ρ = 1 2 for all factorized layers. It is worth noting that the linear projection layer after each Wq , Wk, Wv has dimensions of (768, 768). Using ρ for this layer, the factorized U,V> layers will have dimensions of (768, 384), (384, 768), which will not result in any model size reduction or computational complexity savings. Thus, we opt not to factorize the linear projection layers in each multi-head attention layer in DeiT-base. For ResMLP-S36, we factorize all layers except for the embedding layers with a fixed global rank ratio of ρ = 1 2 . A concern arises from the fact that our proposed stable rank selection heuristic for choosingR may not generally apply to both CNN and Transformer models, as Transformer model weights tend to have higher ranks. To address this issue, future work can adjust CUTTLEFISH’s rank selection heuristic to: max{scaled stable rank(Σ), accumulative rank(Σ, p)} Here, accumulative rank(Σ, p) measures the smallest rank value r such that (where σs represent the singular values of a model weight matrix W, and are also the elements on the diagonal of matrix Σ): r∑ i=1 σi ≥ p · rank(W)∑ j=1 σj . In the DeiT example mentioned earlier, we know that the accumulative rank(Σ, 80%) for most model layers is generally greater than 1 2 × rank(W) and the scaled stable rank for these layers is generally lower than those values. Consequently, the new metric defined above will consistently return 1 2 ×rank(W) for all factorized layers in the DeiT-base model. Another hyperparameter tuning we performed in our experiments is decaying the base learning rate by a certain fraction after switching from full-rank to low-rank training at epoch Ê. For CUTTLEFISH DeiT-base, we decay the base learning rate by 1 3 . For CUTTLEFISH ResMLP-S36, we decay the base learning rate