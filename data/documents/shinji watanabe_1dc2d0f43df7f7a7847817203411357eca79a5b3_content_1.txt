Title: Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute
Authors: William Chen, Xuankai Chang, Yifan Peng, Zhaoheng Ni, Soumi Maiti, Shinji Watanabe
Section: 7. References
of SelfSupervised Speech Models,” in Proc. INTERSPEECH, 2022. [19] C.-I. J. Lai et al., “PARP: Prune, Adjust and Re-Prune for SelfSupervised Speech Recognition,” in Proc. NeurIPS, 2021. [20] Y. Peng et al., “Structured Pruning of Self-Supervised Pretrained Models for Speech Recognition and Understanding,” in Proc. ICASSP, 2023. [21] A. Chowdhery et al., “PaLM: Scaling language modeling with pathways,” arXiv preprint arXiv:2204.02311, 2022. [22] T. Brown et al., “Language models are few-shot learners,” in Proc. NeurIPS, H. Larochelle et al., Eds., vol. 33, 2020, pp. 1877–1901. [23] S. Black et al., “GPT-neox-20b: An open-source autoregressive language model,” in Challenges & Perspectives in Creating Large Language Models, 2022. [24] T. L. Scao et al., “BLOOM: A 176b-parameter open-access multilingual language model,” arXiv preprint arXiv:2211.05100, 2022. [25] Y.-Y. Yang et al., “Torchaudio: Building blocks for audio and speech processing,” in Proc. ICASSP, 2022, pp. 6982–6986. [26] V. Panayotov et al., “Librispeech: An asr corpus based on public domain audio books,” in Proc. ICASSP, 2015, pp. 5206–5210. [27] K. Kim et al., “E-Branchformer: Branchformer with enhanced merging for speech recognition,” in Proc. SLT, 2023, pp. 84–91. [28] G. Chen et al., “GigaSpeech: An Evolving, Multi-Domain ASR Corpus with 10,000 Hours of Transcribed Audio,” in Proc. Interspeech, 2021, pp. 3670–3674. [29] M. McAuliffe et al., “Montreal Forced Aligner: Trainable TextSpeech Alignment Using Kaldi,” in Proc. Interspeech, 2017, pp. 498–502. [30] L. Lugosch et al., “Speech Model Pre-Training for End-to-End Spoken Language Understanding,” in Proc. Interspeech, 2019, pp. 814–818. [31] K. Shim, J. Choi, and W. Sung, “Understanding the role of self attention for efficient speech recognition,” in Proc. ICLR, 2022. [32] C. Wang et al., “Supervision-Guided Codebooks for Masked Prediction in Speech Pre-training,” in Proc. Interspeech, 2022, pp. 2643–2647. [33] H. Y. Kim et al., “ASBERT: Asr-specific self-supervised learning with self-training,” in Proc. SLT, 2023, pp. 9–14. [34] F. L. Kreyssig et al., “Biased self-supervised learning for ASR,” arXiv preprint arXiv:2211.02536, 2022. [35] M. Ott et al., “Fairseq: A fast, extensible toolkit for sequence modeling,” in Proceedings of NAACL-HLT 2019: Demonstrations, 2019. [36] C. R. Harris et al., “Array programming with NumPy,” Nature, vol. 585, no. 7825, pp. 357–362, 2020. [37] J. Kahn et al., “Libri-Light: A benchmark for asr with limited or no supervision,” in Proc. ICASSP, 2020, pp. 7669–7673. [38] S. Watanabe et al., “ESPnet: End-to-end speech processing toolkit,” in Proc. Interspeech, 2018, pp. 2207–2211. [39] S. Watanabe et al., “Hybrid CTC/attention architecture for endto-end speech recognition,” IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240–1253, 2017. [40]