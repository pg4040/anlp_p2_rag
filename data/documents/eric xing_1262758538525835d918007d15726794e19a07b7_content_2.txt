Title: Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective
Authors: Zeyuan Yin, Eric Xing, Zhiqiang Shen, Mohamed bin Zayed
Section: 2.1 Decoupling Outer-loop and Inner-loop Training
of the synthesized images. In practice, we implement it by randomly cropping on the entire image and subsequently resizing the cropped region to the target dimension of 224×224. Under these circumstances, only the cropped region is updated during each iteration. This approach aids in refining the recovered data when viewed from the perspective of cropped regions. Stage-3 Relabel ( ): To match our multi-crop optimization strategy, also to reflect the true soft label for the recovered data. We leverage the pre-generated soft label approach as FKD [22]. ỹi = ϕθT (x̃Ri) (8) where x̃Ri is the i-th crop in the synthetic image and ỹi is the corresponding soft label. Finally, we can train the model ϕθCsyn on the synthetic data using the following objective: Lsyn = − ∑ i ỹi log ϕθCsyn (x̃Ri) (9) We found that this stage is crucial to make synthetic data and labels more aligned and also significantly improve the performance of the trained models. Discussion: How does the proposed approach reduce compute and memory consumption? Existing solutions predominantly employ bilevel optimization [3, 23, 4] or long-range parameter matching strategies [1, 11], which necessitate the feeding of real data into the network to generate guiding variables (e.g., features, gradients, etc.) for target data updates, as well as for the backbone network training, through an iterative process. These approaches incur considerable computational and memory overhead due to the concurrent presence of real and synthetic data on computational hardware such as GPUs, thereby rendering this training strategy challenging to scale up for larger datasets and models. To this end, a natural idea is to decouple real and synthetic data during the training phase, thereby necessitating only minimal memory during each training session. This is achieved by bifurcating the bilevel training into a two-stage process: squeezing and recovering. Moreover, we can conveniently utilize off-the-shelf pre-trained models for the first squeezing stage.