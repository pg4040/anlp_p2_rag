Title: Efficient Sequence Transduction by Jointly Predicting Tokens and Durations
Authors: Hainan Xu, Fei Jia, Somshubra Majumdar, He Huang, Shinji Watanabe, Boris Ginsburg
Section: F. Robustness to Token Repetitions
usually the case. The equations above are not meant to be rigorous proof but only serve to explain the issue. Therefore in this next decoding step, it is likely that, two = argmax ( join(enc[t], dec(<bos>, two, two, two, two, two, two︸ ︷︷ ︸ 6 twos ) ) (56) i.e. the model emits two for a second time at frame t. This will likely keep happening for 3 twos, 4 twos, etc, causing an infinite loop and won’t terminate unless some max-symbol-per-decoding-step is implemented in the decoding algorithm. In this case, we will end up having a lot of insertion errors in the output in the form of the same token repeating too many times. Option 2. if the model keeps emitting all blanks until somewhere after frame 41, and then it emits a five. Then we would have deletion errors in the decoding output. TDT is less prone to such repetition issues because the duration output of the model makes it not likely to stay on the same frame at different decoding steps (refer back to Fig. 4, there are very rare cases when duration 0 is emitted). Due to a lack of datasets specifically made with text repetitions, we use NeMo-TTS to generate 100 utterances, which randomly pick three digits from 1 to 9, and repeat each digit 3 - 5 times. We run ASR with different models on this dataset and results are reported in Table 10. We see that TDT models are much more robust than RNN-Ts with repeated speech.