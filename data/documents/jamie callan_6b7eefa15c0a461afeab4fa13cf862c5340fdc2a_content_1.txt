Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms
Authors: Zhen Fan, Luyu Gao, Jamie Callan
Section: 5.2 Retrieval Efficiency
each dataset is labeled in bold. CSurF𝐻𝑁 (𝛾=0.0) denotes the original CSurF performance without interpolation. CSurF𝐻𝑁 -oracle performance is underlined when equal to or better than the best baseline. Model AA CF DB FE FQ HQ NF NQ QU SD SF T2 TC Avg. BM25 0.441 0.179 0.288 0.648 0.239 0.601 0.297 0.310 - 0.156 0.620 - 0.616 - Contriever 0.446 0.237 0.413 0.758 0.329 0.638 0.328 0.498 - 0.165 0.677 0.230 0.596 - ColBERTv2 0.463 0.176 0.446 0.785 0.356 0.667 0.338 0.562 0.854 0.154 0.693 0.263 0.738 0.500 SPLADE++ 0.518 0.237 0.436 0.796 0.349 0.693 0.345 0.533 0.849 0.161 0.710 0.242 0.725 0.507 CSurF𝐻𝑁 (𝛾 = 0.0) 0.521 0.186 0.453 0.720 0.361 0.693 0.351 0.543 0.861 0.158 0.708 0.262 0.726 0.503 CSurF𝐻𝑁 (𝛾 = 1.0) 0.352 0.159 0.420 0.692 0.329 0.690 0.310 0.505 0.833 0.154 0.690 0.232 0.696 0.466 CSurF𝐻𝑁 (oracle) 0.521 0.192 0.458 0.729 0.370 0.713 0.353 0.550 0.867 0.161 0.717 0.264 0.739 0.510 (Best 𝛾 ) 0.0 0.3 0.1 0.2 0.2 0.4 0.1 0.2 0.2 0.3 0.2 0.2 0.2 Figure 3: Lexical form frequency and weights for CSurF. The red dotted curve denotes weight distribution. Model trained with 𝜆𝑞=𝜆𝑑=1e-2. X axis denotes percentage of corpus CSFs. These experiments demonstrate that the CSurF retrieval process can be highly efficient due to the sparsity of CSF match signals. To look into the properties of the CSF generation and matching processes, we analyze and plot the distribution of corpus CSFs’ lexical form frequency and expansion weight in Figure 3. Compared to the lexical form frequency distribution of the original text, CSurF is trained to simultaneously expand meaningful lexical surface forms but also prune existing lexical terms with low term importance, and removes a significant proportion of tokens with the highest occurrence frequency, most of which do not carry important contextual meaning such as stop words. This leads to the aforementioned comparison where CSurF requires lower scoring operations per query than COIL-tok despite having "longer" queries or documents. Post-hoc index pruning with 𝛼 = 0.5 further removes redundant matching signals of lexical forms at all frequencies, resulting in a significant decrease of matching operations without major influence in retrieval performance. For instance, after training and post-hoc pruning, the five most frequent terms in the MSMARCO passage set ("the", "of", "and", "in", "to") are removed by over 98.5% compared to their original corpus term frequency. Finally, we note that CSurF utilizes vector term representations, and the storage and run cost of CSurF is affected by the representation dimension |v| and the computational