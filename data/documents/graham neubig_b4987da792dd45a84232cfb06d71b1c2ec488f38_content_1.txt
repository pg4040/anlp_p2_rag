Title: Crossing the Threshold: Idiomatic Machine Translation through Retrieval Augmentation and Loss Weighting
Authors: Emmy Liu, Aditi Chaudhary, Graham Neubig
Section: 4.1 Artificial Language Translation
them all correctly, which occurs when roughly 10% of training data contains a non-compositional pattern. A similar trend exists for larger models, but the threshold is less distinct. This corroborates the tendency for transformers to translate non-compositional phrases literally (Dankers et al., 2022b). Comparatively less data is required when the context is informative, but the trends remain similar to the non-informative case. As model size and corpus size increase, the rate of correct translations for non-compositional examples actually drops, contrary to expectation. It is unlikely that any individual idioms occur in 10% of sentences in natural language. Due to the highly regular translation rules in this synthetic language, there may be a stronger bias toward translating compositionally in this experiment. However, we gain the intuition that idioms can be translated effectively if they appear frequently, and that clear context clues reduce data required.