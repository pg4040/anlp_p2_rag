Title: Methods for Measuring, Updating, and Visualizing Factual Beliefs in Language Models
Authors: Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, Srinivasan Iyer
Section: 3 Updating Beliefs in Language Models
y∗i ) + λ2 1 |DP | ∑ xP∈DP LTask(fθ∗(xP ), y∗i ) + λ3 1 |DE | ∑ xE ,yE∈DE LTask(fθ∗(xE), yE) + λ4 1 |DLN | ∑ xLN∈DLN KL(fθ∗(xLN )||fθ(xLN )) + λ5 1 |DR| ∑ xR∈DR KL(fθ∗(xR)||fθ(xR)) (1) where LTask is the loss used to get gradients for fθ. We use the Cross Entropy loss for binary classification and sequence-to-sequence tasks. We optimize this objective w.r.t. ϕ using AdamW (Loshchilov and Hutter, 2019). To obtain update labels {y∗i }ni=1, we always use the opposite class in binary classification. For sequence-tosequence tasks, we use the correct label when ŷi is incorrect, and when ŷi is correct, we randomly select another label from the training data. This choice is in contrast to De Cao et al. (2021) and Mitchell et al. (2021), who use samples from the model beam search as update labels for all points. SLAG objective. To prepare the update method for a sequential-update setting, we consider training gϕ to update multiple datapoints in a row. Using the per-datapoint loss in Eq. 1, we obtain our Sequential, Local, and Generalizing (SLAG) loss for a set of r Main Inputs D = {xi, ŷi, y∗i }ri=1 as LSequential(ϕ;D, θt)= r∑ i=1 L(ϕ;xi, ŷi, y∗i , θt+i) (2) where θt+i = Update(xi, ŷi, y∗i , θt+i−1;ϕ,K) are the model parameters obtained from updating on the first i points in D (starting from θt). This objective allows us to train gϕ to update multiple beliefs in a row. To ensure training with this objective is still efficient, we limit how far back through the update history we backpropagate when computing the gradient w.r.t. ϕ for each term in the RHS sum of Eq. 2. Each parameter vector θt depends on ϕ and θt−1. We always apply the stop-gradient function to the most recent vector θt−1 to prevent backpropagating through it (visualized in Appendix Fig. 3). This choice allows our memory use to remain constant in r (see Appendix Fig. 4).