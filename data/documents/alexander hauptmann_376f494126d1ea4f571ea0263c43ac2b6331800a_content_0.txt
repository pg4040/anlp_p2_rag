Title: SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs
Authors: Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A. Ross, Irfan Essa, Yonatan Bisk, Ming-Hsuan Yang, Kevin Murphy, Alexander G. Hauptmann, Lu Jiang
Section: D Additional Qualitative Examples
Token pyramid visualization. Fig. 13 shows tokenization and reconstruction samples by a 6-layer SPAE from ImageNet validation set. Key concepts are captured in the first few layers, whereas the later layers focus on the visual appearance. In the coffee machine example, many keywords are present to describe various aspects from the stove to the thermometer. In the parrot case, a single unified concept is repeatedly highlighted. Coarse-to-fine reconstruction. Fig. 14 shows reconstruction samples by SPAE-8 from ImageNet validation set. We compare the reconstructed images from layer 5 to layer 8 to demonstrate the coarse-to-fine progress. Conditional image interpolation. To the best of our knowledge, there have been no successful attempts that demonstrate generic image generation capability using a frozen LLM. To this end, we define a very simple setup to explore the interpolation capability of LLM, where the conditions are integers from 1 to 9. The target images are created with different pixel-space transformations detailed in . As shown in Fig. 15, images 1-4 and 6-9 are fed as context to produce image 5, where the model interpolates the variable property. Fig. 16 shows generated samples at 256×256 resolution under the same setup. Conditional image denoising. We use PAR decoding to produce the first 5 token layers with taskspecific conditions, followed by task-agnostic PNAR decoding to fill in layer 6. Fig. 17 visualizes the input pairs for the image-to-image generation examples in Figs. 7 and 9, with more examples in Fig. 18. Under the in-context denoising setup, the LLM generates novel images based on the provided context, where multiple different generations can be obtained. Multimodal outputs. Fig. 19 shows a task requiring a single LLM to output both image and text, where it first inpaints the center region of an image using in-context denoising and then creates multiple captions for the output image. Image-to-video denoising. Fig. 20 shows an image-to-video example with the frame prediction task using progressive in-context denoising. The input is one frame tokenized by the image SPAE, while the output is a 16-frame clip tokenized by the video SPAE. We follow the same two-stage procedure as image-to-image generation, with more steps in each stage to account for the longer sequence. Due to the sequence length limit, only four samples can be fit into the context, which limits LLM’s performance for this task. Brightness Contrast Saturation Color 1 52 3 4 6 7 8 9Condition: Context ContextGeneration Figure 15. Examples of conditional image interpolation of different image transformations.