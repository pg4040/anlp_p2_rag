Title: ONE MODEL TO RULE THEM ALL ? TOWARDS END-TO-END JOINT SPEAKER DIARIZATION AND SPEECH RECOGNITION
Authors: Samuele Cornell, Jee-weon Jung, Shinji Watanabe, Stefano Squartini
Section: 6. REFERENCES
[1] J. S. Garofolo, J. G. Fiscus, A. F. Martin et al., “Nist rich transcription 2002 evaluation: A preview.,” in LREC, 2002. [2] T. J. Park, N. Kanda, D. Dimitriadis et al., “A review of speaker diarization: Recent advances with deep learning,” Computer Speech & Language, vol. 72, pp. 101317, 2022. [3] R. Prabhavalkar, T. Hori, T. N. Sainath et al., “End-to-end speech recognition: A survey,” arXiv preprint arXiv:2303.03329, 2023. [4] J. Barker, S. Watanabe, E. Vincent and J. Trmal, “The fifth CHiME speech separation and recognition challenge: Dataset, task and baselines,” in Proc. InterSpeech, 2018. [5] S. Watanabe, M. Mandel, J. Barker et al., “CHiME-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings,” in CHiME Workshop, 2020. [6] J. G. Fiscus, J. Ajot and J. S. Garofolo, “The rich transcription 2007 meeting recognition evaluation,” in International Evaluation Workshop on Rich Transcription. Springer, 2007, pp. 373–389. [7] S. Cornell, M. Wiesner, S. Watanabe et al., “The chime-7 dasr challenge: Distant meeting transcription with multiple devices in diverse scenarios,” CHiME Workshop, 2023. [8] F. Yu, S. Zhang, Y. Fu et al., “M2MeT: The ICASSP 2022 multichannel multi-party meeting transcription challenge,” in Proc. ICASSP, 2022. [9] I. Medennikov, M. Korenevsky, T. Prisyach et al., “The stc system for the chime-6 challenge,” in CHiME Workshop, 2020. [10] J. Du, Y.-H. Tu, L. Sun et al., “The ustc-nelslip systems for chime-6 challenge,” in CHiME Workshop, 2020. [11] D. Raj, P. Denisov, Z. Chen et al., “Integration of speech separation, diarization, and recognition for multi-speaker meetings: System description, comparison, and analysis,” in IEEE SLT, 2021. [12] F. Yu, S. Zhang, P. Guo et al., “Summary on the icassp 2022 multichannel multi-party meeting transcription grand challenge,” in Proc. ICASSP. IEEE, 2022, pp. 9156–9160. [13] C. Boeddeker, A. S. Subramanian, G. Wichern et al., “Ts-sep: Joint diarization and separation conditioned on estimated speaker embeddings,” arXiv preprint arXiv:2303.03849, 2023. [14] S. Maiti, Y. Ueda, S. Watanabe et al., “EEND-SS: Joint end-to-end neural speaker diarization and speech separation for flexible number of speakers,” in IEEE SLT. IEEE, 2023, pp. 480–487. [15] Z. Chen, T. Yoshioka, L. Lu et al., “Continuous speech separation: Dataset and analysis,” in Proc. ICASSP, 2020. [16] T. Yoshioka, X. Wang, D. Wang et al., “VarArray: Array-geometryagnostic continuous speech separation,” in Proc. ICASSP, 2022. [17] G. Morrone, S. Cornell, D. Raj et al., “Low-latency speech separation guided diarization for telephone conversations,” in 2022 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2023, pp. 641–646. [18] N. Kanda, J. Wu, X. Wang et al., “VarArray meets t-SOT: Advancing