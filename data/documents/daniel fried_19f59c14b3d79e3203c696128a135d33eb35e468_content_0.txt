Title: Pragmatic Inference with a CLIP Listener for Contrastive Captioning
Authors: Jiefu Ou, Benno Krojer, Daniel Fried
Section: Acknowledgments
We would like to thank Google for providing funding for this work through a gift on Action, Task, and User Journey modeling, and Samsung Electronics Co., Ltd. for providing funding for BK. Limitations We evaluate only on the “static” image partition of the ImageCoDe dataset. ImageCoDe contains another more challenging partition, containing frames from short temporal intervals in videos, which remains extremely difficult for all current discriminative captioning methods, including our PICL approach. (This partition, along with the static image partition that we use, has previously only been used in contrastive retrieval tasks, not in discriminative captioning.) While we made a substantial effort to explore the tradeoff between informativity and fluency, we were limited in the number of human evaluations that we were able to do and could only evaluate a few settings of the informativity parameter for each method. We complement these human evaluations with automated evaluations on a much wider range of parameters, and analyze the correlations between human performance and judgements and the automated metrics.