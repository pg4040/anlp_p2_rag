Title: Why do Nearest Neighbor Language Models Work?
Authors: Frank F. Xu, Uri Alon, Graham Neubig
Section: 4.2 Increasing the Softmax Capacity
make the multiple embeddings not converge to the same vector, rendering over-parameterization useless. Besides simply increasing the number of embedding vectors equally for each word type, we also propose other alternatives to increase softmax capacity. First, we hypothesize that different word types have different difficulties for the language model to predict. For those words that appear very frequently, they may appear in many different contexts. As a result, instead of adding an equal number of additional embeddings to each word type, we propose to adaptively increase the number of embeddings for word types based on word frequency, or total training loss for the word. Second, we try to break the softmax bottleneck with a Mixture of Softmax. Yang et al. (2017) proposes a solution to the problem using a Mixture of Softmax (MoS) to produce more linearly independent probability distributions of words given different contexts. Last, opposite to training the word embeddings of increased size, we also consider ways to compress the datastore down to a similar-sized embedding matrix for softmax computation by clustering the whole datastore and allowing for further finetuning of the embedding matrix consisting of cluster centroids. However, none of these alternative methods provided additional benefits over the simple multi-embedding approach. More details on these attempts can be found in Appendix C. 5 Approximate kNN Search & Softmax Temperature 5.1 Comparing Approximate kNN Search To calculate PkNN of the non-parametric component in Equation 5, it is usually prohibitive to use exhaustive kNN search, and thus Khandelwal et al. (2020a) use approximate kNN search using the FAISS library (Johnson et al., 2019). The use of FAISS (similarly to other approximate search libraries) results in two varieties of approximation. • Approximate Neighbors: Because the search for nearest neighbors is not exact, the set of nearest neighbors might not be equivalent to the actual nearest neighbors. Recall the function mask-to-k(·) in Equation 5, it is the function where we select the kNN entries from the datastore Wds. We denote “real mask” as the accurate nearest neighbors for mask-to-k(·) selection, and “FAISS mask” as the approximate nearest neighbors returned by the FAISS library.3 • Approximate Scores: In addition, FAISS makes some approximations in calculating the distances between the query and the retrieved neighbors for efficiency purposes. We denote “real score” as the scores calculated from ground truth distances between the embeddings, and “FAISS score” as the distances returned by FAISS approximate search. The comparison of the different approximation settings is shown in Table 3. Quite surprisingly, we actually find that the