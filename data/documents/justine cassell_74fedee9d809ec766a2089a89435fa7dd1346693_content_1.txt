Title: How About Kind of Generating Hedges using End-to-End Neural Models?
Authors: Alafate Abulimiti, Chlo√© Clavel, Justine Cassell
Section: B Metrics
allow the 2 grams to oc- 3github.com/huggingface/transformers 4github.com/Lightning-AI/lightning cur only once, and the repetition penalty = 1.2 is also applied. All models were fine-tuned on an Nvidia Quadro RTX 8000 GPU. A complete configuration of the hyperparameters used for each model is reported in the GitHub repository with the code of the paper: github.com/neuromaancer/ hedge_generation. Moreover, we apply beam search for the decoding strategy, as it reduces the risk of missing hidden high-probability word sequences by retaining the n most likely words in each generation output and ultimately selecting the utterances with the highest overall probability. To avoid repeating the same subsequences, we apply a penalty to the repeated 2-gram unit. In terms of the size of the candidate pool, logically, the more candidates generated, the more chances that one of them is the right hedge strategy (i.e., hedge or non-hedge), so we fix our candidate pool size to 50, as a compromise between the likelihood of obtaining a hedge and the speed of generation.