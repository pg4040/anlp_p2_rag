Title: From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models
Authors: Julia Mendelsohn, Ronan Le Bras, Yejin Choi, Maarten Sap
Section: A Appendix
A.1 Details for dogwhistle surfacing We create 51 total request formulations that ask for generic examples of dogwhistles (n=17), dogwhistles that target specific social groups (n=25), and dogwhistles that are used by certain personae/ingroups (n=9). For each prompt, we also consider three spelling variations of “dogwhistle”: dogwhistle, dog-whistle, and dog whistle. Exact prompt text can be found in our project repository. To encourage GPT-3 to generate a list, we conclude all prompts with a newline token followed by “1.”. All prompts were provided to a GPT-3 Instruct model (text-davinci-002) with de- fault hyperparameters except for max_tokens=256, temperature=0.7, and num_outputs=5 (5 generations per prompt). The resulting texts are strings that take the form of an enumerated list. To aggregate and compare surfaced dogwhistles across each text completion, we post-process by: splitting by newline characters, removing enumeration and other punctuation, converting all outputs to lowercase, lemmatizing each surfaced term with SpaCy, and removing definite articles that precede generated dogwhistles. We then aggregate over all generations to determine how often each dogwhistle is surfaced for each in-group. In calculating precision of dogwhistle surfacing, we mark each of the 154 candidate terms as true positives if they appear in the glossary. Some surfaced dogwhistles were marked as “correct” if they were closely related to a dogwhistle entry in our glossary, even if the exact term did not appear. Examples include national security, identity politics, the swamp, tax relief, and patriot. However, this is still a conservative estimate because our glossary is not exhaustive. GPT-3 surfaces a number of terms that potentially have dogwhistle usages but were not covered by our glossary, and thus not included in our precision estimates. Examples of these terms include names of Muslim political organizations (Hezbollah, Hamas, Muslim Brotherhood) and Second Amendment rights. Figure A.1 shows variation in precision of dogwhistle surfacing across prompt types (in-groups and generic prompting). A.2 Details for identifying covert meaning Variation across registers We identify variation in GPT-3’s ability to identify dogwhistles’ covert meanings based on prompt features, dog- https://www.merriam-webster.com/words-at-play/dog-whistle-political-meaning https://en.wikipedia.org/wiki/Dog_whistle_(politics) whistle register, and the interaction between the two. Figure A.2 shows that including the definition in prompts consistently improves GPT-3’s covert meaning identification for both formal and informal dogwhistles. However, including the secret cue has minimal effect for informal dogwhistles, and only leads to substantial improvement for identifying formal dogwhistles’ covert meanings. Variation across personae There is significant variation in GPT-3’s performance across personae, as can be seen in Table A.3. Variation across dogwhistle types GPT-3’s performance varies widely across dogwhistle types in our