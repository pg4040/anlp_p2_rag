Title: Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training
Authors: Zhisong Zhang, Emma Strubell, Eduard Hovy
Section: C Details of Algorithms
be calculated17 in a modified way (Tsuboi et al., 2008). Knowledge distillation. As described in the main context, we adopt the knowledge distillation objective for self-training with soft labels. For brevity, we denote the probabilities from the last model as p′(y|x) and keep using p(y|x) to denote the ones from the current model. Following Wang et al. (2021), the loss can be calculated by: L = − ∑ y∈Y p′(y|x) log p(y|x) = − ∑ y∈Y p′(y|x)s(y|x) + logZ(x) = − ∑ y∈Y p′(y|x) ∑ f ′∈y′ s(f ′|x) + logZ(x) = − ∑ f ′ s(f ′|x) ∑ y′∈Yf ′ p′(y′|x) + logZ(x) The loss function is broken down into two items whose gradients can be obtained by calculating marginals according to the last model or the current one, respectively. 17In our implementation, we adopt a simple method to enforce the constraints by adding negative-infinite to the scores of the impossible labels. In this case, the structures that violates the constraints will have a score of negative-infinite (and a probability of zero) and will thus be excluded.