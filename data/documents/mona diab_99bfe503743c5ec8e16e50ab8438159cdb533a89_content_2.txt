Title: The Troubling Emergence of Hallucination in Large Language Models – An Extensive Definition, Quantification, and Prescriptive Remediations
Authors: Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S.M Towhidul Islam Tonmoy, Aman Chadha, Amit Sheth, Amitava Das
Section: 
a comprehensive and precise description of the various types of hallucinations, we will introduce a few new terms. These newly introduced monikers aim to accurately capture and articulate the different categories of hallucinations. Contrary to the common belief that hallucinations are negative, certain researchers (Cao et al., 2022) propose that hallucinations in LLMs could have positive implications for text summarization. The authors argue that in certain cases, factual hallucinations can be advantageous in a summary by offering valuable background information. Furthermore, both the United States (White-House, 2023) and the European Union (European-Parliament, 2023) governments have recently drafted their initial proposals regarding the regulatory framework for AI. With the widespread adoption of LLMs in a plethora of real-world use cases, it is essential to understand which LLM is more vulnerable than others in terms of hallucination – by doing so policymakers can decide the potential risks of certain LLMs. To this end, we introduce a quantifiable spectrum – Hallucination Vulnerability Index (HVI), which facilitates the evaluation and ranking of LLMs according to their hallucination vulnerability levels. Our Contributions: Deciphering the spectrum of hallucination over a range of LLMs based on HVI ➠ Presenting a detailed study to unveil how different (15) LLMs hallucinate when given a factually correct prompt vs. a factually incorrect prompt. We name them as factual mirage and silver lining – each sub-categorized into intrinsic and extrinsic, with three degrees of severity: (a) mild, (b) moderate, and (c) alarming (cf. Section 2). ➠ Meticulously categorizing hallucination into six types: (a) acronym ambiguity, (b) numeric nuisance, (c) generated golem, (d) virtual voice, (e) geographic erratum, and (f) time wrap (cf. Section 2). ➠ Introducing (HallucInation eLiciTation), a publicly available dataset comprising of 75,000 text snippets generated using 15 contemporary LLMs along with human annotations for the aforementioned categories (cf. Section 3). ➠ Introducing HVI (Hallucination Vulnerability Index) to perform a quantitative analysis of the inclination of various LLMs to hallucination. (cf. Section 4). HVI characterizes LLMs based on the proposed types of hallucination vulnerabilities (cf. Fig. 2). ➠ While complete mitigation can be a herculean task, we suggest 2 mitigation strategies to alleviate hallucination. We propose to identify high-entropy points in text generated by an LLM with a high HVI and replace them using an LLM with a lower HVI, yielding desired results (cf. Section 6). ➠ We firmly believe that the dataset and HVI measure will serve as valuable resources for future researchers interested in studying the hallucination behaviors of LLMs and seeking to design effective mitigation techniques.