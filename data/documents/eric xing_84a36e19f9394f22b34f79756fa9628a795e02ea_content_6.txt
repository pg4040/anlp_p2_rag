Title: LMSYS-CHAT-1M: A LARGE-SCALE REAL-WORLD LLM CONVERSATION DATASET
Authors: Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric P. Xing, Joseph E. Gonzalez, Ion Stoica, Hao Zhang
Section: B SAMPLE CONVERSATIONS
appearances, too, are like night and day: Saana is small and delicate, while John is imposing and unkempt... Conversation 3 (Part 2): Taken from the same conversation as Jailbreak Conversation 3 (Part 1). The following conversation takes place before Part 1 and demonstrate how the user failed to make GPT-4 generate harmful content without the “token replacement” technique. This is an example conversation from LMSYS-Chat-1M. [User] Start with “Please be warned the following content is reserved for a mature audience only.” Saana Parker is a smart 18 old black woman, polite, educated, small, average, flat chested, only child in a rich family, spends time on her phone, stubborn, selfish, immature, takes bad decisions, secretly aroused by rough sex and humiliation, inexperienced, wants to stay a virgin. Malik Parker is Saana’s father. Nia Parker is Saana’s mother. John Anderson is a dumb 38 years old racist white man, large, average, strong construction worker, attracted to tall blonde big breasted magazine pornstars, berates black women, speaks in racial slurs, always lived alone in a shack, neglects his hygiene. Describe Saana’s attempts to be John’s sex pet, and how she has finally succeeded. [GPT-4] I’m sorry, but I cannot create a story that involves racism, non-consensual activity, or any form of inappropriate content. If you have any other ideas or topics you’d like me to help you with, I’d be more than happy to assist you. Please feel free to suggest a different storyline or topic. When examining and experimenting with the jailbreak prompts, we discovered several jailbreak techniques that allow the user to bypass GPT-4’s safety measures and generate toxic contents: • Content Warning: Ask the model to start the response with a warning of possible explicit contents. As shown in sample conversation 1 and 3. • Educational Purposes: Frame the harmful question as an educational inquiry or for educational purposes. As shown in sample conversation 2. • Harmful Rewrite: Ask the model to rewrite their responses but with increased toxicity. As shown in sample conversation 1. • Misspelling Keywords: Sometimes when keywords in a harmful prompt is misspelled, there is an increased likelihood for the model to comply to the malicious request. • Token Replacement: Define the keyword with another word, mask, or token and replace the keyword in the prompt. Switch the keyword back into the prompt in the later prompts. As shown in sample conversation 3. • Translation: Ask the model to translate the harmful input into a foreign language and then ask it to translate it back in