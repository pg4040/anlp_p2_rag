Title: FIXED INTER-NEURON COVARIABILITY INDUCES ADVERSARIAL ROBUSTNESS
Authors: Muhammad A. Shah, Bhiksha Raj
Section: 7. REFERENCES
[1] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus, “Intriguing properties of neural networks,” in ICLR, 2014. [2] Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry, “Adversarial examples are not bugs, they are features,” Advances in neural information processing systems, vol. 32, 2019. [3] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu, “Towards deep learning models resistant to adversarial attacks,” arXiv preprint arXiv:1706.06083, 2017. [4] Jeremy Cohen, Elan Rosenfeld, and Zico Kolter, “Certified adversarial robustness via randomized smoothing,” in International Conference on Machine Learning. PMLR, 2019. [5] Hadi Salman, Mingjie Sun, Greg Yang, Ashish Kapoor, and J Zico Kolter, “Denoised smoothing: A provable defense for pretrained classifiers,” NeurIPS, vol. 33, pp. 21945–21957, 2020. [6] Muhammad A Shah and Bhiksha Raj, “Less is more: Training on low-fidelity images improves robustness to adversarial attacks,” in Submitted to ICLR., 2023, under review. [7] Muhammad A Shah, Raphael Olivier, and Bhiksha Raj, “Towards adversarial robustness via compact feature representations,” in ICASSP. IEEE, 2021. [8] Raphael Olivier, Bhiksha Raj, and Muhammad Shah, “Highfrequency adversarial defense for speech and audio,” in ICASSP. IEEE, 2021, pp. 2995–2999. [9] Sander Joos, Tim Van hamme, Davy Preuveneers, and Wouter Joosen, “Adversarial robustness is not enough: Practical limitations for securing facial authentication,” in Proceedings of the 2022 ACM on International Workshop on Security and Privacy Analytics, 2022, pp. 2–12. [10] Yash Sharma and Pin-Yu Chen, “Attacking the madry defense model with l 1-based adversarial examples,” arXiv preprint arXiv:1710.10733, 2017. [11] Lukas Schott, Jonas Rauber, Matthias Bethge, and Wieland Brendel, “Towards the first adversarially robust neural network model on mnist,” arXiv preprint arXiv:1805.09190, 2018. [12] Dylan M Paiton, Charles G Frye, Sheng Y Lundquist, Joel D Bowen, Ryan Zarcone, and Bruno A Olshausen, “Selectivity and robustness of sparse coding networks,” Journal of vision, 2020. [13] Joel Dapello, Tiago Marques, Martin Schrimpf, Franziska Geiger, David Cox, and James J DiCarlo, “Simulating a primary visual cortex at the front of cnns improves robustness to image perturbations,” NeurIPS, vol. 33, pp. 13073–13087, 2020. [14] Aditya Jonnalagadda, William Yang Wang, B.S. Manjunath, and Miguel Eckstein, “Foveater: Foveated transformer for image classification,” 2022. [15] Manish Reddy Vuyyuru, Andrzej Banburski, Nishka Pant, and Tomaso Poggio, “Biologically inspired mechanisms for adversarial robustness,” NeurIPS, vol. 33, pp. 2135–2146, 2020. [16] Jay A Hennig, Emily R Oby, Darby M Losey, Aaron P Batista, M Yu Byron, and Steven M Chase, “How learning unfolds in the brain: toward an optimization view,” Neuron, 2021. [17] Patrick T