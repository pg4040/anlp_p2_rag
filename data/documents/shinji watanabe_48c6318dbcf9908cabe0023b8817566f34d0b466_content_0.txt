Title: I3D: TRANSFORMER ARCHITECTURES WITH INPUT-DEPENDENT DYNAMIC DEPTH FOR SPEECH RECOGNITION
Authors: Yifan Peng, Jaesong Lee, Shinji Watanabe
Section: 6. REFERENCES
[1] A. Graves, S. Fernández, et al., “Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,” in Proc. ICML, 2006. [2] K. Cho, B. Merrienboer, et al., “Learning phrase representations using RNN encoder-decoder for statistical machine translation,” in Proc. EMNLP, 2014. [3] D. Bahdanau, K. Cho, et al., “Neural machine translation by jointly learning to align and translate,” in Proc. ICLR, 2015. [4] W. Chan, N. Jaitly, et al., “Listen, attend and spell: A neural network for large vocabulary conversational speech recognition,” in Proc. ICASSP, 2016. [5] A. Graves, “Sequence transduction with recurrent neural networks,” arXiv:1211.3711, 2012. [6] A. Vaswani, N. Shazeer, N. Parmar, et al., “Attention is all you need,” in Proc. NeurIPS, 2017. [7] A. Gulati, J. Qin, C.-C. Chiu, et al., “Conformer: Convolutionaugmented Transformer for Speech Recognition,” in Proc. Interspeech, 2020. [8] Y. Peng, S. Dalmia, et al., “Branchformer: Parallel MLPattention architectures to capture local and global context for speech recognition and understanding,” in Proc. ICML, 2022. [9] K. Kim, F. Wu, Y. Peng, et al., “E-branchformer: Branchformer with enhanced merging for speech recognition,” arXiv:2210.00077, 2022. [10] S. Karita, N. Chen, T. Hayashi, et al., “A comparative study on transformer vs rnn in speech applications,” in Proc. ASRU, 2019. [11] G. Hinton, O. Vinyals, J. Dean, et al., “Distilling the knowledge in a neural network,” arXiv:1503.02531, 2015. [12] H. Chang, S. Yang, and H. Lee, “Distilhubert: Speech representation learning by layer-wise distillation of hidden-unit bert,” in Proc. ICASSP, 2022. [13] R. Wang, Q. Bai, et al., “LightHuBERT: Lightweight and Configurable Speech Representation Learning with Once-for-All Hidden-Unit BERT,” in Proc. Interspeech, 2022. [14] P. Dong, S. Wang, et al., “RTMobile: Beyond Real-Time Mobile Acceleration of RNNs for Speech Recognition,” in ACM/IEEE Design Automation Conference (DAC), 2020. [15] K. Tan and D.L. Wang, “Compressing deep neural networks for efficient speech enhancement,” in Proc. ICASSP, 2021. [16] C. J. Lai, Y. Zhang, et al., “Parp: Prune, adjust and re-prune for self-supervised speech recognition,” in Proc. NeurIPS, 2021. [17] Y. Han, G. Huang, S. Song, et al., “Dynamic neural networks: A survey,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 44, no. 11, pp. 7436–7456, 2022. [18] E. Bengio, P. Bacon, et al., “Conditional computation in neural networks for faster models,” arXiv:1511.06297, 2015. [19] A. Veit and S. Belongie, “Convolutional networks with adaptive inference graphs,” in Proc. ECCV, 2018. [20] X. Wang, F. Yu, et al., “Skipnet: Learning dynamic routing in convolutional networks,” in Proc. ECCV, 2018. [21] Z. Wu, T. Nagarajan, et al., “Blockdrop: Dynamic inference paths in residual