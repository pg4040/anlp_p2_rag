Title: Exploration on HuBERT with Multiple Resolutions
Authors: Jiatong Shi, Yun Tang, Hirofumi Inaguma, Hongyu Gong, Juan Pino, Shinji Watanabe
Section: 6. References
on sa-unet,” in 2019 4th International Conference on Mechanical, Control and Computer Engineering (ICMCCE), 2019, pp. 818–8183. [21] T. Zhao et al., “Unet++-based multi-channel speech dereverberation and distant speech recognition,” in 2021 12th International Symposium on Chinese Spoken Language Processing (ISCSLP), 2021, pp. 1–5. [22] R. Li et al., “Unet-tts: Improving unseen speaker and style transfer in one-shot voice cloning,” in Proc. ICASSP, 2022, pp. 8327– 8331. [23] X. Xiang, X. Zhang, and H. Chen, “A nested u-net with selfattention and dense connectivity for monaural speech enhancement,” IEEE Signal Processing Letters, vol. 29, pp. 105–109, 2021. [24] Y. Xian et al., “A multi-scale feature recalibration network for end-to-end single channel speech enhancement,” IEEE Journal of Selected Topics in Signal Processing, vol. 15, no. 1, pp. 143– 155, 2020. [25] G. Liu et al., “Cp-GAN: Context pyramid generative adversarial network for speech enhancement,” in Proc. ICASSP, 2020, pp. 6624–6628. [26] X. Xiang, X. Zhang, and H. Chen, “A convolutional network with multi-scale and attention mechanisms for end-to-end single-channel speech enhancement,” IEEE Signal Processing Letters, vol. 28, pp. 1455–1459, 2021. [27] J. Shi et al., “Bridging speech and textual pre-trained models with unsupervised ASR,” arXiv preprint arXiv:2211.03025, 2022. [28] V. Panayotov et al., “Librispeech: An asr corpus based on public domain audio books,” in Proc. ICASSP, 2015, pp. 5206–5210. [29] A. Baevski et al., “Wav2vec 2.0: A framework for selfsupervised learning of speech representations,” Advances in neural information processing systems, vol. 33, pp. 12 449–12 460, 2020. [30] M. Ott et al., “Fairseq: A fast, extensible toolkit for sequence modeling,” NAACL HLT 2019, p. 48, 2019. [31] J. D. Kahn et al., “Flashlight: Enabling innovation in tools for machine learning,” in Proc. ICML, 2022, pp. 10 557–10 574. [32] J. Kahn et al., “Libri-light: A benchmark for asr with limited or no supervision,” in Proc. ICASSP, 2020, pp. 7669–7673. [33] Y. Meng et al., “On compressing sequences for self-supervised speech models,” in Proc. SLT, 2023. [34] J. Kong, J. Kim, and J. Bae, “HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis,” Advances in Neural Information Processing Systems, vol. 33, pp. 17 022–17 033, 2020. [35] T. Hayashi et al., “ESPnet-TTS: Unified, reproducible, and integratable open source end-to-end text-to-speech toolkit,” in Proc. ICASSP, 2020, pp. 7654–7658. [36] T. Hayashi et al., “ESPnet2-TTS: Extending the edge of TTS research,” arXiv preprint arXiv:2110.07840, 2021.