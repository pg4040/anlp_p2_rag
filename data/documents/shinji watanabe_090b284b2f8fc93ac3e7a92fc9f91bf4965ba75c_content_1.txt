Title: ML-SUPERB: Multilingual Speech Universal PERformance Benchmark
Authors: Jiatong Shi, Dan Berrebbi, William Chen, Ho-Lam Chung, En-Pei Hu, Wei Ping Huang, Xuankai Chang, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Shinji Watanabe
Section: 5. References
in Proc. SLTU, 2020, pp. 21– 27. [22] F. He et al., “Open-source multi-speaker speech corpora for building gujarati, kannada, malayalam, marathi, tamil and telugu speech synthesis systems,” in Proc. LREC, 2020, pp. 6494– 6503. [23] G. Rehm and H. Uszkoreit, “Language technology support for norwegian,” in The Norwegian Language in the Digital Age: Bokmalsversjon, 2012, pp. 52–70. [24] A. Conneau et al., “Fleurs: Few-shot learning evaluation of universal representations of speech,” in Proc. SLT, 2023, pp. 798– 805. [25] E. Barnard et al., “The nchlt speech corpus of the south african languages,” 2014. [26] T. Baumann, A. Köhn, and F. Hennig, “The spoken wikipedia corpus collection: Harvesting, alignment and an application to hyperlistening,” LREC, vol. 53, pp. 303–329, 2019. [27] J. Shi et al., “Leveraging end-to-end asr for endangered language documentation: An empirical study on yolóxochitl mixtec,” in Proc. ACL, 2021, pp. 1134–1145. [28] J. Shi et al., “Highland puebla nahuatl speech translation corpus for endangered language documentation,” in Proc. AmericaNLP, 2021, pp. 53–63. [29] I. Solak, “M-ailab speech dataset,” Imdat Solak.[Online]. Available: https://www.caito.de/2019/01/03/the-m-ailabsspeech-dataset/.[Accessed by 2022], 2018. [30] D. A. Braude et al., “All together now: The living audio dataset.,” in INTERSPEECH, 2019, pp. 1521–1525. [31] N. J. De Vries et al., “A smartphone-based asr data collection tool for under-resourced languages,” Speech communication, vol. 56, pp. 119–131, 2014. [32] S. Watanabe, T. Hori, and J. R. Hershey, “Language independent end-to-end architecture for joint language identification and speech recognition,” in Proc. ASRU, 2017, pp. 265–271. [33] W. Hou et al., “Large-Scale End-to-End Multilingual Speech Recognition and Language Identification with Multi-Task Learning,” in Proc. Interspeech, 2020, pp. 1037–1041. [34] C. Zhang et al., “Streaming End-to-End Multilingual Speech Recognition with Joint Language Identification,” in Proc. Interspeech, 2022, pp. 3223–3227. [35] W. Chen et al., “Improving massively multilingual ASR with auxiliary CTC objectives,” Proc. ICASSP 2023, 2023. [36] T. Wolf et al., “Transformers: State-of-the-art natural language processing,” in Proc. EMNLP, 2020, pp. 38–45. [37] S. Watanabe et al., “ESPnet: End-to-end speech processing toolkit,” in Proc. Interspeech, 2018, pp. 2207–2211. [38] T.-h. Feng et al., “SUPERB@ SLT 2022: Challenge on generalization and efficiency of self-supervised speech representation learning,” in Proc. SLT, 2023, pp. 1096–1103. [39] X. Chang et al., “An exploration of self-supervised pretrained representations for end-to-end speech recognition,” in Proc. ASRU, 2021, pp. 228–235. [40] S. Chen et al., “WavLM: Large-scale self-supervised pretraining for full stack speech processing,” JSTSP, vol. 16, no. 6, pp. 1505–1518, 2022. [41] W.-N. Hsu et al., “Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training,” in Proc. Interspeech,