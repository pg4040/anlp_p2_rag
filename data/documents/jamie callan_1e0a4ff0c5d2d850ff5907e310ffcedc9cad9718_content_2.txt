Title: KALE: Using a K-Sparse Projector for Lexical Expansion
Authors: LuÃ­s Borges, Bruno Martins, Jamie Callan
Section: 3 THE KALE APPROACH
it was beneficial to connect the dense representation output by the projector into the ranking loss, instead of directly connecting the sparsified vector. Since TopK only extracts a small number of dimensions, the sparse vector likely had insufficient information to provide stable learning compared to the dense vector, and thus this dense vector was kept as the student. The regularization term promoting equipartitioning takes as input the same vectors that go into the M3SE loss. Given the dense vectors, the sum of the weights in each dimension is computed, as well as the sum of all the weights in the batch. Dividing these values returns the current weight distribution across the dimensions. Ideally, the distribution should be uniform, which translates to: dims = |ğµ |âˆ‘ï¸ ğ‘– Bğ‘– , ğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡ = |ğ‘‰ |âˆ‘ï¸ ğ‘£ ğ‘‘ğ‘–ğ‘šğ‘ ğ‘£, (2) Equipartition(B) = KL ( dims ğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡ , 1 ğ· ) + KL ( 1 ğ· , dims ğ‘¤ğ‘’ğ‘–ğ‘”â„ğ‘¡ ) . (3) In the previous equations, KL is the Kullback-Leibler divergence, |ğµ | is the batch size, and |ğ‘‰ | is the vocabulary size, which matches the dimensionality of the input dense vectors. Vectors and matrices are boldfaced, while scalars are italicized. The term 1 ğ· denotes a uniform distribution over the ğ· dimensions. With equipartitioning, the KALE loss is the sum of the two individual components: Loss = M3SE + Equipartition. (4)