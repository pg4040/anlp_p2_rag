Title: Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning
Authors: Xuankai Chang, Brian Yan, Yuya Fujita, Takashi Maekaku, Shinji Watanabe
Section: 5. Acknowledgements
Some experiments of this work used the Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. We also gratefully acknowledge the support of NVIDIA Corporation with the donation of the A6000 GPUs used for this research. 4To clarify, we didn’t implement the data storing part in bit in our implementation. Instead, we used the normal int32 for convenience and compatibility with neural network toolkits. However, in that case, the data is still less than 1 GB for 960 hours of speech. 6. References [1] A. Graves et al., “Connectionist temporal classification: La- belling unsegmented sequence data with recurrent neural networks,” in Proceedings of the 23rd international conference on Machine learning, 2006, pp. 369–376. [2] O. Abdel-Hamid et al., “Applying convolutional neural networks concepts to hybrid NN-HMM model for speech recognition,” in Proc. ICASSP, 2012, pp. 4277–4280. [3] A. Graves, “Sequence transduction with recurrent neural networks,” arXiv preprint arXiv:1211.3711, 2012. [4] A. Graves, A.-r. Mohamed, and G. Hinton, “Speech recognition with deep recurrent neural networks,” in Proc. ICASSP, 2013, pp. 6645–6649. [5] W. Chan et al., “Listen, attend and spell,” in Proc. ICASSP, 2016, pp. 4960–4964. [6] A. Vaswani et al., “Attention is all you need,” in Proc. NeurIPS, 2017, pp. 5998–6008. [7] L. Dong, S. Xu, and B. Xu, “Speech-Transformer: A norecurrence sequence-to-sequence model for speech recognition,” in Proc. ICASSP, 2018, pp. 5884–5888. [8] A. Gulati et al., “Conformer: Convolution-augmented Transformer for speech recognition,” in Proc. Interspeech, 2020, pp. 5036–5040. [9] V. Panayotov et al., “Librispeech: An ASR corpus based on public domain audio books,” in Proc. ICASSP, 2015, pp. 5206– 5210. [10] G. Chen et al., “GigaSpeech: An evolving, multi-domain ASR corpus with 10,000 hours of transcribed audio,” in Proc. Interspeech, 2021. [11] W. Chan et al., “SpeechStew: Simply mix all available speech recognition data to train one large neural network,” in Proc. Interspeech, 2021. [12] D.-H. Lee et al., “Pseudo-label: The simple and efficient semisupervised learning method for deep neural networks,” in Proc. ICML, 2013, p. 896. [13] G. Synnaeve et al., “End-to-end ASR: From supervised to semisupervised learning with modern architectures,” in Proc. ICML, 2020. [14] J. Kahn, A. Lee, and A. Hannun, “Self-training for end-to-end speech recognition,” in Proc. ICASSP, 2020, pp. 7084–7088. [15] A. Baevski et al., “Wav2vec 2.0: A framework for selfsupervised learning of speech representations,” Advances in neural information processing systems, vol. 33, pp. 12