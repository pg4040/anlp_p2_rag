Title: SEGMENT-LEVEL VECTORIZED BEAM SEARCH BASED ON PARTIALLY AUTOREGRESSIVE INFERENCE
Authors: Masao Someki, Nicholas Eng, Yosuke Higuchi, Shinji Watanabe
Section: 5.3.3. Segment-level Vectorized Beam Search
Oriol Vinyals, and Jeffrey Dean, “Distilling the knowledge in a neural network,” in NIPS Deep Learning and Representation Learning Workshop, 2015. [22] Heng-Jui Chang, Shu-wen Yang, and Hung-yi Lee, “DistilHuBERT: Speech representation learning by layer-wise distillation of hidden-unit bert,” in Proc. ICASSP, 2022, pp. 7087– 7091. [23] Hiroshi Seki, Takaaki Hori, Shinji Watanabe, Niko Moritz, and Jonathan Le Roux, “Vectorized beam search for CTCattention-based speech recognition,” in Proc. Interspeech, 2019, pp. 3825–3829. [24] Jiahui Yu, Chung-Cheng Chiu, Bo Li, Shuo yiin Chang, Tara N Sainath, Yanzhang (Ryan) He, Arun Narayanan, Wei Han, Anmol Gulati, Yonghui Wu, and Ruoming Pang, “FastEmit: Lowlatency streaming ASR with sequence-level emission regularization,” in Proc. ICASSP, 2021. [25] Zhuoyuan Yao, Di Wu, Xiong Wang, Binbin Zhang, Fan Yu, Chao Yang, Zhendong Peng, Xiaoyu Chen, Lei Xie, and Xin Lei, “WeNet: Production oriented streaming and nonstreaming end-to-end speech recognition toolkit,” in Proc. Interspeech, 2021, pp. 4054–4058. [26] Binbin Zhang, Di Wu, Zhendong Peng, Xingchen Song, Zhuoyuan Yao, Hang Lv, Lei Xie, Chao Yang, Fuping Pan, and Jianwei Niu, “WeNet 2.0: More productive end-to-end speech recognition toolkit,” in Proc. Interspeech, 2022, pp. 1661–1665. [27] Masao Someki, Yosuke Higuchi, Tomoki Hayashi, and Shinji Watanabe, “Espnet-ONNX: Bridging a gap between research and production,” in Proc. APSIPA ASC, 2022, pp. 420–427. [28] Yosuke Higuchi, Hirofumi Inaguma, Shinji Watanabe, Tetsuji Ogawa, and Tetsunori Kobayashi, “Improved Mask-CTC for non-autoregressive end-to-end ASR,” in Proc. ICASSP, 2021, pp. 8363–8367. [29] Hayato Futami, Hirofumi Inaguma, Sei Ueno, Masato Mimura, Shinsuke Sakai, and Tatsuya Kawahara, “Non-autoregressive error correction for CTC-based ASR with phone-conditioned masked LM,” in Proc. Interspeech, 2022, pp. 3889–3893. [30] Yosuke Higuchi, Brian Yan, Siddhant Arora, Tetsuji Ogawa, Tetsunori Kobayashi, and Shinji Watanabe, “BERT meets CTC: New formulation of end-to-end speech recognition with pre-trained masked language model,” in Proc. Findings of EMNLP, 2022, pp. 5486–5503. [31] Jay Mahadeokar, Yangyang Shi, Ke Li, Duc Le, Jiedan Zhu, Vikas Chandra, Ozlem Kalinli, and Michael L. Seltzer, “Streaming parallel transducer beam search with fast-slow cascaded encoders,” in Proc. Interspeech, 2022. [32] Zhengkun Tian, Jiangyan Yi, Jianhua Tao, Shuai Zhang, and Zhengqi Wen, “Hybrid autoregressive and non-autoregressive transformer models for speech recognition,” IEEE Signal Process. Lett., vol. 29, pp. 762–766, 2022. [33] Shinji Watanabe, Takaaki Hori, Suyoun Kim, John R Hershey, and Tomoki Hayashi, “Hybrid CTC/attention architecture for end-to-end speech recognition,” IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240–1253, 2017. [34] Liu Xiaoyu, Lin Eric, and Ning Emma, “Journey to optimize large scale transformer model inference with ONNX runtime,” https://cloudblogs.microsoft. com/opensource/2021/06/30/journey-tooptimize-large-scale-transformer-modelinference-with-onnx-runtime, 2021, [Online;