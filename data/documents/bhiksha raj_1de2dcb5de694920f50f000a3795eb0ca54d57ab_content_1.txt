Title: LOFT: LOCAL PROXY FINE-TUNING FOR IMPROV- ING TRANSFERABILITY OF ADVERSARIAL ATTACKS AGAINST LARGE LANGUAGE MODEL
Authors: Muhammad Ahmed Shah, Roshan Sharma, Hira Dhamyal, Raphael Olivier, Ankit Shah, Dareen Alharthi, Hazim T Bukhari, Massa Baali, Soham Deshmukh, Michael Kuhlmann, Bhiksha Raj, Rita Singh
Section: 6 CONCLUSION
on how to reproduce our experiments. We will also take efforts into including detailed descriptions of our experimental setup, including the hyperparameters used for training and fine-tuning the LLMs. We have conducted extensive experiments to evaluate the effectiveness of our approach and have included detailed results and analysis in the paper. We have also provided additional results and analysis in the supplementary material. Overall, we believe that our research is reproducible and can be used as a basis for further research in this area. We will actively engage with researchers who would like further questions regarding our work. We welcome feedback and suggestions from the community on how we can improve the reproducibility of our work and make it more accessible to others.