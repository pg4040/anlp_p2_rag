Title: The Troubling Emergence of Hallucination in Large Language Models – An Extensive Definition, Quantification, and Prescriptive Remediations
Authors: Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S.M Towhidul Islam Tonmoy, Aman Chadha, Amit Sheth, Amitava Das
Section: 
Eiffel, it was inaugurated in 1889 to celebrate the 100th anniversary of the European Civil War, while construction began a decade prior to its inauguration.... Fact 1: Eiffel tower was built to celebrate the 100th anniversary of the French Revolution. Fact 2: Eiffel Tower construction was started in 1887, not in 1879. Limitation 2: While we have meticulously defined the categories of hallucination, we recognize the potential for new categories to emerge in the future with the advancement of LLMs. An instance of this is the introduction of name-nationality hallucination by (Ladhak et al., 2023b), where a person named Jung Lee is falsely attributed with French nationality. Although one could argue that Mr Lee may indeed be a French national, considering his birth and upbringing there, the authors confirm that no such individual exists. We posit that namenationality hallucination falls under the sub-class of generated golems. It is plausible that a combination of our defined categories may exist, although we did not extensively studied these possibilities. Limitation 3: For this study, we have chosen 15 contemporary LLMs. In the dynamic landscape of LLM development, new models are constantly emerging, and we acknowledge that our selection may not encompass all available options. Keeping this in mind, we will make the benchmark and the HVI publicly accessible for collaborative updates and contributions. Limitation 4: The FACTUALITYGB technique operates based on entailment, allowing it to distinguish between sentences containing different entities such as Barack Obama and Joe Biden. However, it is unable to differentiate sentences that involve similar entities like AI and AAAI. In contrast, the ENTROPYBB technique operates at the token level and is capable of handling cases like 1789 vs. 1889. These distinctions become evident in the observed results. 9 Ethical Considerations Through our experiments, we have uncovered the susceptibility of LLMs to hallucination. In developing HVI, we intend to provide a framework that can inform future research and policies in this domain. However, we must address the potential misuse of our findings by malicious entities who may exploit AI-generated text, such as creating indistinguishable fake news from human-written content. We vehemently discourage such misuse and strongly advise against it. References 2023. The future of computational linguistics. Abien Fred Agarap. 2019. Deep learning using rectified linear units (relu). Ayush Agrawal, Lester Mackey, and Adam Tauman Kalai. 2023. Do language models know when they’re hallucinating references? Stability AI. 2023. Stability ai launches the first of its stable lm suite of language models. Amazon. Amazon mechanical turk. Rishi Bommasani, Kevin Klyman, Daniel Zhang,