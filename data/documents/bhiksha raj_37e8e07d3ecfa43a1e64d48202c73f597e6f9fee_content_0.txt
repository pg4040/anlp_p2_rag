Title: Towards Noise-Tolerant Speech-Referring Video Object Segmentation: Bridging Speech and Text
Authors: Xiang Li, Jinglu Wang, Xiaohao Xu, Muqiao Yang, Fan Yang, Yizhou Zhao, Rita Singh, Bhiksha Raj, Mohamed bin Zayed
Section: E More Visualization.
As shown in Fig. B, we demonstrate more visualizations of the proposed method. We notice that our method can correctly refer to the target object and help the R-VOS model segment temporally consistent object masks across frames. 𝑒! Object Query 𝑞 Transformer Decoder TrD Transformer Decoder TrD Transformer Decoder TrD 𝑓! 𝑒" 𝑒#⋯ ⋯ ⋯ 𝑃#𝑃"𝑃! 𝑀! 𝑀" 𝑀# Video Feature 𝑓# #$!% {𝑃#}#$!%Prototype Masks Dot ProductRepeat FC FC FC 𝑓" 𝑓# FPN Figure A: Illustration of mask decoder, which derives mask predictions {Mt}Tt=1 from the visual features {ft}Tt=1 the object query q. Given the query feature q from STBridge, we first repeat it N times to form the input to the transformer decoder TrD where N is the object candidate number (the final output is selected from object candidates based on confidence score). After that, we generate instance embedding {et}Tt=1 for each time step separately using a shared transformer decoder TrD with visual feature {ft}Tt=1 from visual encoder. The mask prediction Mt for each time step t is derived by a dynamic convolution between prototype masks Pt and dynamic weights which are learned from instance embedding et by two fully connected layers. The prototype masks {Pt}Tt=1 is generated by feature pyramid network (FPN) (Lin et al., 2017a) with visual feature {ft}Tt=1. a person grabbing a crocodile a white and brown owl standing next to a white cat a lizard in a small enclosure with two others outside and eat the giraffe walking around a zebra is standing to the left of view Sp ee ch G T O ur s Sp ee ch G T O ur s Sp ee ch G T O ur s Sp ee ch G T O ur s Sp ee ch G T O ur s Figure B: More visualization of our method on AVOS test set.