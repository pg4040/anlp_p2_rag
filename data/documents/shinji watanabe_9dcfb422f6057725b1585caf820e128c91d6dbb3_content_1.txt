Title: IMPROVING MASSIVELY MULTILINGUAL ASR WITH AUXILIARY CTC OBJECTIVES
Authors: William Chen, Brian Yan, Jiatong Shi, Yifan Peng, Soumi Maiti, Shinji Watanabe
Section: 8. REFERENCES
voice: A massivelymultilingual speech corpus,” English, in Proceedings of the Twelfth Language Resources and Evaluation Conference, Marseille, France: European Language Resources Association, May 2020. [19] A. Babu, C. Wang, A. Tjandra, et al., “XLS-R: Self-supervised cross-lingual speech representation learning at scale,” arXiv preprint arXiv:2111.09296, 2021. [20] Y.-A. Chung, Y. Zhang, W. Han, et al., “W2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training,” in Proc. ASRU, 2021, pp. 244–250. [21] A. Conneau, M. Ma, S. Khanuja, et al., “Fleurs: Few-shot learning evaluation of universal representations of speech,” arXiv preprint arXiv:2205.12446, 2022. [22] M. P. Lewis, Ethnologue: Languages of the world. SIL international, 2009. [23] S. Watanabe, T. Hori, S. Kim, et al., “Hybrid ctc/attention architecture for end-toend speech recognition,” IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240–1253, 2017. [24] J. Lee and S. Watanabe, “Intermediate loss regularization for ctc-based speech recognition,” in Proc. ICASSP 2021, 2021, pp. 6224–6228. [25] A. Tjandra, C. Liu, F. Zhang, et al., “Deja-vu: Double feature presentation and iterated loss in deep transformer networks,” in Proc. ICASSP, 2020, pp. 6899– 6903. [26] R. Sanabria and F. Metze, “Hierarchical multitask learning with ctc,” in Proc. SLT, 2018, pp. 485–490. [27] J. Nozaki and T. Komatsu, “Relaxing the conditional independence assumption of CTC-based ASR by conditioning on intermediate predictions,” in Proc. Interspeech, 2021, pp. 3735–3739. [28] J. Zhang, Y. Peng, H. Xu, et al., “Intermediate-layer output regularization for attention-based speech recognition with shared decoder,” arXiv preprint arXiv:2207.04177, 2022. [29] Y. Higuchi, K. Karube, T. Ogawa, et al., “Hierarchical conditional end-to-end asr with ctc and multi-granular subword units,” in Proc. ICASSP 2022, 2022, pp. 7797–7801. [30] Y. Higuchi, N. Chen, Y. Fujita, et al., “A comparative study on non-autoregressive modelings for speech-to-text generation,” in Proc. ASRU, 2021, pp. 47–54. [31] Y. Yang, Y. Li, and B. Du, “Improving ctc-based asr models with gated interlayer collaboration,” arXiv preprint arXiv:2205.12462, 2022. [32] Y. Fujita, T. Komatsu, and Y. Kida, “Multi-sequence intermediate conditioning for ctc-based asr,” arXiv preprint arXiv:2204.00175, 2022. [33] A. Graves, S. Fernández, F. Gomez, et al., “Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks,” in Proc. International Conference on Machine Learning, 2006, pp. 369–376. [34] B. Yan, C. Zhang, M. Yu, et al., “Joint modeling of code-switched and monolingual asr via conditional factorization,” in Proc. ICASSP, 2022, pp. 6412–6416. [35] B. Yan, S. Dalmia, Y. Higuchi, et al., “CTC alignments improve autoregressive translation,” arXiv preprint arXiv:2210.05200, 2022. [36] S.-w. Yang, P.-H. Chi, Y.-S. Chuang, et al., “SUPERB: