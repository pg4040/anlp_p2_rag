Title: UNIVERSLU: UNIVERSAL SPOKEN LANGUAGE UNDERSTANDING FOR DIVERSE CLASSIFICATION AND SEQUENCE GENERATION TASKS WITH A SINGLE NETWORK
Authors: Siddhant Arora, Hayato Futami, Jee-weon Jung, Yifan Peng, Roshan Sharma, Yosuke Kashiwagi, Emiru Tsunoo, Shinji Watanabe
Section: 7. REFERENCES
B. Andrassy, “Table filling multi-task recurrent neural network for joint entity and relation extraction,” in Proc. COLING, 2016, pp. 2537–2547. [19] M. Lan, J. Wang, Y. Wu, Z. Niu, and H. Wang, “Multi-task attentionbased neural networks for implicit discourse relationship representation and identification,” in Proc. EMNLP, 2017, pp. 1299–1308. [20] Y. Yang and T. M. Hospedales, “A unified perspective on multidomain and multi-task learning,” in Proc. ICLR, 2015. [21] R. Prabhavalkar, T. Hori, T. N. Sainath, R. Schlüter, and S. Watanabe, “End-to-end speech recognition: A survey,” CoRR, vol. abs/2303.03329, 2023. [22] S. Kim, T. Hori, and S. Watanabe, “Joint CTC-attention based end-toend speech recognition using multi-task learning,” in Proc. ICASSP, 2017, pp. 4835–4839. [23] Y. Shinohara, “Adversarial multi-task learning of deep neural networks for robust speech recognition,” in Proc. Interspeech, 2016, pp. 2369–2372. [24] A. Tripathi, A. Mohan, S. Anand, and M. Singh, “Adversarial learning of raw speech features for domain invariant speech recognition,” in Proc. ICASSP, 2018, pp. 5959–5963. [25] G. Saon et al., “English conversational telephone speech recognition by humans and machines,” in Proc. Interspeech, 2017, pp. 132–136. [26] Z. Meng et al., “Speaker-invariant training via adversarial learning,” in Proc. ICASSP, 2018, pp. 5969–5973. [27] S. Sun, C. Yeh, M. Hwang, M. Ostendorf, and L. Xie, “Domain adversarial training for accented speech recognition,” in Proc. ICASSP, 2018, pp. 4854–4858. [28] Y. Tang, J. M. Pino, C. Wang, X. Ma, and D. Genzel, “A general multi-task learning framework to leverage text data for speech to text tasks,” in Proc. ICASSP, 2021, pp. 6209–6213. [29] G. Pironkov, S. Dupont, and T. Dutoit, “Multi-task learning for speech recognition: An overview,” in ESANN, 2016. [30] A. Babu et al., “XLS-R: self-supervised cross-lingual speech representation learning at scale,” in Proc. Interspeech, 2022, pp. 2278– 2282. [31] S. Arora et al., “ESPnet-SLU: Advancing spoken language understanding through espnet,” in Proc. ICASSP, 2022, pp. 7167–7171. [32] S. Arora, S. Dalmia, B. Yan, F. Metze, A. W. Black, and S. Watanabe, “Token-level sequence labeling for slu using compositional end-toend models,” in EMNLP 2022, 2022. [33] A. Anastasopoulos and D. Chiang, “Tied multitask learning for neural speech translation,” in Proc. NAACL, 2018, pp. 82–91. [34] X. Zheng, C. Zhang, and P. C. Woodland, “Tandem multitask training of speaker diarisation and speech recognition for meeting transcription,” in Proc. Interspeech, 2022, pp. 3844–3848. [35] X. Cai, J. Yuan, R. Zheng, L. Huang, and K. Church, “Speech emotion recognition with multi-task learning,” in Proc. Interspeech, 2021, pp. 4508–4512. [36] S. Sigtia, E. Marchi, S. Kajarekar, D. Naik, and J. Bridle,