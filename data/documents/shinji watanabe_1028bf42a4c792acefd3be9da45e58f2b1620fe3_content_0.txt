Title: STRUCTURED PRUNING OF SELF-SUPERVISED PRE-TRAINED MODELS FOR SPEECH RECOGNITION AND UNDERSTANDING
Authors: Yifan Peng, Kwangyoun Kim, Felix Wu, Prashant Sridhar, Shinji Watanabe
Section: 4.5. Comparison with other compression methods
As introduced in Sec. 2.1, HJ-Pruning can be directly applied to other SSL models. In Fig. 6, we prune the HuBERT-base model based on the overall MACs for ASR. The performance is similar to that of the wav2vec2. We also include other compressed models for comparison, including DistilHuBERT [15] and LightHuBERT [16]. Note that these results are not really comparable due to: (1) Their WERs are from SUPERB [1], which combines a frozen SSL model with another learnable RNN. We also tried to replace the RNN with a single linear layer and fine-tune the entire model (same as our setting), but the performance was clearly worse. (2) Their compressed models are initially distilled using the 960h unlabeled LibriSpeech data and then fine-tuned on the 100h labeled data, but our taskspecific pruning only utilizes the 100h data. This comparison shows that our task-specific pruning method is highly effective. 5. CONCLUSION In this paper, we propose HJ-Pruning to jointly prune heterogeneous components of SSL speech models, which achieves strong performance-efficiency tradeoffs compared to several baselines. At a small sparsity (0.1 to 0.3), HJ-Pruning improves the wav2vec2 baseline while being faster. Depending on the task, HJ-Pruning saves 40% or 50% MACs while maintaining the performance of wav2vec2. HJ-Pruning is a general method that can be applied to most of speech SSL models such as HuBERT. In the future, we plan to explore the application of HJ-Pruning on encoder-decoder SSL models [43] and other SLU tasks [44, 5]. 6. REFERENCES [1] S. w. Yang, P.-H. Chi, Y.-S. Chuang, et al., “SUPERB: Speech Processing Universal PERformance Benchmark,” in Proc. Interspeech, 2021. [2] A. Mohamed, H.-y. Lee, L. Borgholt, et al., “Self- Supervised Speech Representation Learning: A Review,” arXiv:2205.10643, 2022. [3] X. Chang, T. Maekaku, P. Guo, et al., “An exploration of self- supervised pretrained representations for end-to-end speech recognition,” in Proc. ASRU, 2021. [4] Z. Huang, S. Watanabe, S.-w. Yang, et al., “Investigating Self-Supervised Learning for Speech Enhancement and Separation,” in Proc. ICASSP, 2022. [5] S. Shon, A. Pasad, F. Wu, et al., “SLUE: New Benchmark Tasks For Spoken Language Understanding Evaluation on Natural Speech,” in Proc. ICASSP, 2022. [6] S. Arora, S. Dalmia, P. Denisov, et al., “ESPnet-SLU: Ad- vancing Spoken Language Understanding Through ESPnet,” in Proc. ICASSP, 2022. [7] Y. Peng, S. Arora, Y. Higuchi, et al., “A Study on the Integra- tion of Pre-trained SSL, ASR, LM and SLU Models for Spoken Language Understanding,” in Proc. SLT, 2022. [8] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, “wav2vec 2.0: A framework