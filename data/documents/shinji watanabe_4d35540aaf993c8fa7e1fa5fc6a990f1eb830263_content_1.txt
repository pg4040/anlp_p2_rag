Title: A New Benchmark of Aphasia Speech Recognition and Detection Based on E-Branchformer and Multi-task Learning
Authors: Jiyang Tang, William Chen, Xuankai Chang, Shinji Watanabe, Brian MacWhinney
Section: 6. References
Journal of Selected Topics in Signal Processing, vol. 16, pp. 1505–1518, 2021. [20] J. Becker et al., “The Natural History of Alzheimer’s Disease: Description of Study Cohort and Accuracy of Diagnosis,” Archives of Neurology, vol. 51, no. 6, pp. 585–594, 1994. [21] S. Yang et al., “SUPERB: Speech Processing Universal PERformance Benchmark,” in Proc. Interspeech, 2021, pp. 1194–1198. [22] W. Chen et al., “Improving massively multilingual asr with auxiliary ctc objectives,” in Proc. ICASSP (in press), 2023. [23] A. Graves et al., “Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks,” in Proc. ICML, 2006, pp. 369–376. [24] D. Povey et al., “Purely sequence-trained neural networks for ASR based on lattice-free MMI,” in Proc. Interspeech, 2016, pp. 2751–2755. [25] J. Lee and S. Watanabe, “Intermediate loss regularization for ctcbased speech recognition,” in Proc. ICASSP, 2021, pp. 6224– 6228. [26] J. Nozaki and T. Komatsu, “Relaxing the conditional independence assumption of ctc-based ASR by conditioning on intermediate predictions,” in Proc. Interspeech, 2021, pp. 3735–3739. [27] A. Gulati et al., “Conformer: Convolution-augmented transformer for speech recognition,” in Proc. Interspeech, 2020, pp. 5036–5040. [28] Y. Peng et al., “Branchformer: Parallel mlp-attention architectures to capture local and global context for speech recognition and understanding,” in Proc. ICML, 2022, pp. 17 627–17 643. [29] W. Hsu et al., “Hubert: Self-supervised speech representation learning by masked prediction of hidden units,” IEEE/ACM Trans. Audio, Speech and Lang. Proc., vol. 29, pp. 3451–3460, 2021. [30] Y. Masuyama et al., “End-to-end integration of speech recognition, dereverberation, beamforming, and self-supervised learning representation,” in Proc. SLT, 2023, pp. 260–265. [31] S. Toshniwal et al., “Multilingual speech recognition with a single end-to-end model,” in Proc. ICASSP, 2018, pp. 4904–4908. [32] S. Watanabe, T. Hori, and J. R. Hershey, “Language independent end-to-end architecture for joint language identification and speech recognition,” in Proc. ASRU, 2017, pp. 265–271. [33] S. Zhou, S. Xu, and B. Xu, Multilingual end-to-end speech recognition with a single transformer on low-resource lan- guages, 2018. [34] S. Watanabe et al., “Espnet: End-to-end speech processing toolkit,” in Proc. Interspeech, 2018, pp. 2207–2211. [35] B. MacWhinney, “The childes project: Tools for analyzing talk,” Child Language Teaching and Therapy, vol. 8, no. 2, pp. 217– 218, 1992. [36] B. MacWhinney et al., “Aphasiabank: Methods for studying discourse,” Aphasiology, vol. 25, no. 11, pp. 1286–1307, 2011, PMID: 22923879. [37] T. Wang et al., “Conformer based elderly speech recognition system for alzheimer’s disease detection,” in Proc. Interspeech, 2022. [38] S. Hu et al., “Exploiting cross-domain and cross-lingual ultrasound tongue imaging features for elderly and