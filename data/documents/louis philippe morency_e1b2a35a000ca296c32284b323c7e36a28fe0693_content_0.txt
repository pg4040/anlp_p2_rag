Title: FACTORIZED CONTRASTIVE LEARNING: Going Beyond Multi-view Redundancy
Authors: Paul Pu Liang, Zihao Deng, Martin Q. Ma, James Zou, Louis-Philippe Morency, Ruslan Salakhutdinov
Section: D.3 Additional analysis and results
Fusion experiments: In Table 6 and 7 we present more detailed results on the Multibench [44] and IRFL [87] datasets computed from 5 independent runs. FACTORCL significantly outperforms the baselines that do not capture both shared and unique information in both supervised and selfsupervised settings, particularly on MUSTARD (where unique information expresses sarcasm, such as sardonic facial expressions or ironic tone of voice), and on MIMIC (with unique health indicators and sensor readings). There are also big improvements on the two sentiment analysis datasets MOSEI and MOSI, with 6.8% and 21.9% increases respectively when compared to SupCon [40]. In Table 7, we also see that FACTORCL substantially improves the state-of-the-art in classifying images and figurative captions which are not literally descriptive of the image on IRFL, outperforming zero-shot and fine-tuned CLIP [60] as well as continued pre-training baselines on top of CLIP. While the supervised version gives the best results overall, the self-supervised version with our proposed unique augmentations also performs better than independent augmentations, indicating that in the case without label information, we should always try to find and use unique augmentations when possible. In our experiments, we use word masking for text augmentations. For independent image augmentations, we use cropping, flipping, and color jittering. The unique augmentation simply removes the cropping operation, as illustrated in Figure 4 in the main text. Additional experiments on high shared information and low unique information: In Table 8 we include additional results using our method on the CIFAR10 [42] and MNIST [19] datasets. Our method outperforms the self-supervised contrastive learning on both datasets as expected, and roughly maintains the same performance as supervised contrastive learning. Therefore, in cases with abundant shared information (two modalities with high shared information or two different views generated from augmentations), our method recovers the performance of existing methods that do not capture unique information. Experiments on CMI estimator verification: In Table 9 and Table 10, we include experiment results which verify that computing the conditional MI lower and upper bounds via concatenation indeed yields reliable estimates. In particular, we aim to verify that the the Conditional InfoNCE objective gives a lower bound of the CMI, and the Conditional InfoNCE-CLUB objective gives an upper bound of the CMI. We follow the experiment setups in [54], presenting the true CMI and results of our estimators on synthetic data with fixing dimension of representation Z and varying samples n, and fixing samples n and varying dz . The specific implementations used for conditional InfoNCE and conditional InfoNCE-CLUB can be found