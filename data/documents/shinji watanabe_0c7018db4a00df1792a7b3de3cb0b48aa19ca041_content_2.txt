Title: Integrating Pretrained ASR and LM to Perform Sequence Generation for Spoken Language Understanding
Authors: Siddhant Arora, Hayato Futami, Yosuke Kashiwagi, Emiru Tsunoo, Brian Yan, Shinji Watanabe
Section: 7. References
of decomposable sequence tasks,” in Proc. NAACL, 2021, pp. 1882–1896. [36] A. Paszke et al., “Pytorch: An imperative style, highperformance deep learning library,” Proc. NeurIPS, vol. 32, 2019. [37] M. Lewis et al., “BART: denoising sequence-to-sequence pretraining for natural language generation, translation, and comprehension,” in Proc. ACL, D. Jurafsky et al., Eds., 2020, pp. 7871–7880. [38] T. Wolf et al., “Transformers: State-of-the-art natural language processing,” in Proc. EMNLP: System Demonstrations, 2020, pp. 38–45. [39] D. S. Park et al., “Specaugment: A simple data augmentation method for automatic speech recognition,” in Proc. Interspeech, 2019, pp. 2613–2617. [40] N. Srivastava et al., “Dropout: A simple way to prevent neural networks from overfitting,” The journal of machine learning research, vol. 15, no. 1, pp. 1929–1958, 2014. [41] R. Müller, S. Kornblith, and G. E. Hinton, “When does label smoothing help?” Advances in neural information processing systems, vol. 32, 2019.