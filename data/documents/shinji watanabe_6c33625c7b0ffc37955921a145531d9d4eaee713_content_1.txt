Title: EXPLORING THE INTEGRATION OF SPEECH SEPARATION AND RECOGNITION WITH SELF-SUPERVISED LEARNING REPRESENTATION
Authors: Yoshiki Masuyama, Xuankai Chang, Wangyou Zhang, Samuele Cornell, Zhong-Qiu Wang, Nobutaka Ono, Yanmin Qian, Shinji Watanabe
Section: 6. REFERENCES
et al., “Closing the gap between time-domain multi-channel speech enhancement on real and simulation conditions,” in Proc. WASPAA, 2021, pp. 146–150. [22] T. von Neumann et al., “Multi-talker ASR for an unknown number of sources: Joint training of source counting, separation and ASR,” Proc. Interspeech, pp. 3097–3101, 2020. [23] H. Seki et al., “An end-to-end language-tracking speech recognizer for mixed-language speech,” in Proc. ICASSP, 2018, pp. 4919–4923. [24] N. Kanda et al., “Serialized output training for end-to-end overlapped speech recognition,” in Proc. Interspeech, 2020, pp. 2797–2801. [25] I. Sklyar et al., “Streaming multi-speaker ASR with RNNT,” in Proc. ICASSP, 2021, pp. 6903–6907. [26] A. Baevski et al., “Wav2vec 2.0: A framework for selfsupervised learning of speech representations,” in Proc. NeurIPS, 2020. [27] W. N. Hsu et al., “HuBERT: Self-supervised speech representation learning by masked prediction of hidden units,” IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 29, pp. 3451–3460, 2021. [28] S. Chen et al., “WavLM: Large-scale self-supervised pretraining for full stack speech processing,” IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505–1518, 2022. [29] S. W. Yang and et al., “SUPERB: Speech processing universal performance benchmark,” in Proc. Interspeech, 2021, pp. 1194–1198. [30] H. S. Tsai and et al., “SUPERB-SG: Enhanced speech processing universal performance benchmark for semantic and generative capabilities,” in Proc. ACL, 2022, pp. 8479–8492. [31] X. Chang et al., “End-to-End Integration of Speech Recognition, Speech Enhancement, and Self-Supervised Learning Representation,” in Proc. Interspeech 2022, 2022, pp. 3819– 3823. [32] Y. Masuyama et al., “End-to-end integration of speech recognition, dereverberation, beamforming, and selfsupervised learning representation,” in Proc. SLT, 2023, pp. 260–265. [33] X. Chang et al., “An exploration of self-supervised pretrained representations for end-to-end speech recognition,” in Proc. ASRU, 2021, pp. 228–235. [34] J. Heymann et al., “Neural network based spectral mask estimation for acoustic beamforming,” in Proc. ICASSP, 2016, pp. 196–200. [35] H. Erdogan et al., “Improved MVDR beamforming using single-channel mask prediction networks,” Proc. Interspeech, pp. 1981–1985, 2016. [36] S. Gannot et al., “A consolidated perspective on multimicrophone speech enhancement and source separation,” IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 25, pp. 692–730, 2017. [37] T. Yoshioka et al., “Multi-microphone neural speech separation for far-field multi-talker speech recognition,” in Proc. ICASSP, 2018, pp. 5739–5743. [38] S. Kim et al., “Joint CTC-attention based end-to-end speech recognition using multi-task learning,” in Proc. ICASSP, 2017, pp. 4835–4839. [39] E. Vincent et al., “Performance measurement in blind audio source separation,” IEEE Trans. Audio, Speech, Lang. Process., vol. 14, no. 4, pp. 1462–1469, 2006. [40] W.