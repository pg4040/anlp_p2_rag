Title: Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation
Authors: Patrick Fernandes, Aman Madaan, Emmy Liu, António Farinhas, Pedro Henrique Martins, Amanda Bertsch, José G. C. de Souza, Shuyan Zhou, Tongshuang Wu, Graham Neubig, André F. T. Martins
Section: 4.1 Optimizing for Human Feedback
time, LaMDA is also used to filter out candidate responses for which its safety prediction is below a threshold. Finally, this can also be achieved by training the model to predict generation and conditioning on the feedback. This corresponds to minimizing the following loss: L(i)(θ) = − log pθ ( yi | f i, xi ) (6) Liu et al. (2023) proposed prompt-based finetuning, where they create prompts containing previous generations rated by humans, in the order of preference. They also suggest inserting languagebased feedback (e.g., “... is a worse answer than ...”) to the prompt, between the generations. Then, the model is fine-tuned to maximize the likelihood of generating the most preferred answer. Finally, reinforcement learning (RL) offers a more versatile approach, allowing for direct optimization of a model’s parameters based on human feedback, regardless of the feedback’s differentiability. A common RL algorithm used in this context is the REINFORCE algorithm (Williams, 1992), which updates the policy parameters using the following gradient: ∇θJ(θ) = Ex∼D,y∼pθ [h(x, y)∇θ log pθ(y | x)] (7) Here, D represents the set of inputs x, and pθ is the policy. This flexibility enables RL to handle various types of feedback and better align the generated output with human preferences. For instance, Kreutzer et al. (2018) proposed using task-based implicit feedback from user queries as a reward signal to train a machine translation model using a word-level variant of minimum risk training (Shen et al., 2016), while Jaques et al. (2019) used implicit human reactions in chat to improve open-domain dialog systems through off-policy Q-learning (Watkins and Dayan, 1992). Given that collecting human feedback can be expensive and time-consuming, learning is done offline from logged data, which is typically more favorable than on-policy settings that need feedback on the fly. Later in §5.2.1, we discuss several works that attempt to optimize feedback models using RL instead of directly optimizing human feedback. In conjuction, these aproaches are commonly known as Reinforcement Learning from Human Feedback (RLHF).