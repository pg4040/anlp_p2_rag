Title: JOINT MODELLING OF SPOKEN LANGUAGE UNDERSTANDING TASKS WITH INTEGRATED DIALOG HISTORY
Authors: Siddhant Arora, Hayato Futami, Emiru Tsunoo, Brian Yan, Shinji Watanabe
Section: 8. REFERENCES
Proc. ICASSP, 2022, pp. 7497–7501. [19] N. Tomashenko, C. Raymond, A. Caubrière, et al., “Dialogue history integration into end-to-end signal-to-concept spoken language understanding systems,” in Proc. ICASSP, 2020, pp. 8509–8513. [20] S. Arora, S. Dalmia, X. Chang, et al., “Two-pass low latency endto-end spoken language understanding,” in Proc. Interspeech, 2022, pp. 3478–3482. [21] T. O’Malley, A. Narayanan, Q. Wang, et al., “A conformer-based asr frontend for joint acoustic echo cancellation, speech enhancement and speech separation,” in Proc. ASRU, 2021, pp. 304–311. [22] W. Zhang, C. Boeddeker, S. Watanabe, et al., “End-to-end dereverberation, beamforming, and speech recognition with improved numerical stability and advanced frontend,” in Proc. ICASSP, 2021, pp. 6898– 6902. [23] S.-Y. Chang, R. Prabhavalkar, Y. He, et al., “Joint endpointing and decoding with end-to-end models,” in Proc. ICASSP, 2019, pp. 5626– 5630. [24] J. Lee and S. Watanabe, “Intermediate loss regularization for CTCbased speech recognition,” in Proc. ICASSP, 2021, pp. 6224–6228. [25] M. Wu, J. Nafziger, A. Scodary, et al., “Harpervalleybank: A domainspecific spoken dialog corpus,” arXiv preprint arXiv:2010.13929, 2020. [26] S. Arora, S. Dalmia, P. Denisov, et al., “Espnet-slu: Advancing spoken language understanding through espnet,” in Proc. ICASSP, 2022, pp. 7167–7171. [27] J. Devlin, M. Chang, K. Lee, et al., “BERT: pre-training of deep bidirectional transformers for language understanding,” in Proc. NAACLHLT, 2019, pp. 4171–4186. [28] S. Kim, T. Hori, and S. Watanabe, “Joint CTC-attention based end-toend speech recognition using multi-task learning,” in Proc. ICASSP, 2017, pp. 4835–4839. [29] C. Weng, D. Yu, M. L. Seltzer, et al., “Deep neural networks for single-channel multi-talker speech recognition,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 10, pp. 1670–1679, 2015. [30] J. R. Hershey, Z. Chen, J. Le Roux, et al., “Deep clustering: Discriminative embeddings for segmentation and separation,” in Proc. ICASSP, 2016, pp. 31–35. [31] D. Yu, M. Kolbæk, Z.-H. Tan, et al., “Permutation invariant training of deep models for speaker-independent multi-talker speech separation,” in Proc. ICASSP, 2017, pp. 241–245. [32] X. Chang, Y. Qian, K. Yu, et al., “End-to-end monaural multi-speaker asr system without pretraining,” in Proc. ICASSP, 2019, pp. 6256– 6260. [33] A. Deoras, R. Sarikaya, G. Tur, et al., “Joint decoding for speech recognition and semantic tagging,” in Proc. Interspeech, 2012, pp. 1067–1070. [34] A. Paszke, S. Gross, F. Massa, et al., “Pytorch: An imperative style, high-performance deep learning library,” Proc. NeurIPS, vol. 32, pp. 8024–8035, 2019. [35] S. Watanabe, T. Hori, S. Karita, et al., “ESPnet: End-to-end speech processing toolkit,” in Proc. Interspeech, 2018, pp. 2207–2211. [36] A. Gulati, J. Qin, C.