Title: BASS: Block-wise Adaptation for Speech Summarization
Authors: Roshan Sharma, Kenneth Zheng, Rita Singh, Bhiksha Raj
Section: 7. References
[1] R. Sharma and B. Raj, “Xnor-former: Learning accurate approximations in long speech transformers,” arXiv preprint arXiv:2210.16643, 2022. [2] S. Palaskar, R. Salakhutdinov, A. W. Black, and F. Metze, “Multimodal Speech Summarization Through Semantic Concept Learning,” in Proc. Interspeech 2021, 2021, pp. 791–795. [3] S.-H. Liu, K.-Y. Chen, B. Chen, H.-M. Wang, H.-C. Yen, and W.-L. Hsu, “Combining relevance language modeling and clarity measure for extractive speech summarization,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 6, pp. 957–969, 2015. [4] T. Kano, A. Ogawa, M. Delcroix, and S. Watanabe, “Attentionbased multi-hypothesis fusion for speech summarization,” in 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2021, pp. 487–494. [5] ——, “Integrating multiple asr systems into nlp backend with attention fusion,” in ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 6237–6241. [6] S. Shon, S. Arora, C.-J. Lin, A. Pasad, F. Wu, R. Sharma, W.-L. Wu, H.-Y. Lee, K. Livescu, and S. Watanabe, “Slue phase-2: A benchmark suite of diverse spoken language understanding tasks,” 2022. [Online]. Available: https://arxiv.org/abs/2212.10525 [7] R. Sharma, S. Palaskar, A. W. Black, and F. Metze, “End-to-end speech summarization using restricted self-attention,” in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 8072–8076. [8] K. Matsuura, T. Ashihara, T. Moriya, T. Tanaka, A. Ogawa, M. Delcroix, and R. Masumura, “Leveraging large text corpora for end-to-end speech summarization,” 2023. [Online]. Available: https://arxiv.org/abs/2303.00978 [9] I. Beltagy, M. E. Peters, and A. Cohan, “Longformer: The longdocument transformer,” arXiv preprint arXiv:2004.05150, 2020. [10] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang, “Conformer: Convolution-augmented Transformer for Speech Recognition,” in Proc. Interspeech 2020, 2020, pp. 5036–5040. [11] K. Deng, S. Watanabe, J. Shi, and S. Arora, “Blockwise Streaming Transformer for Spoken Language Understanding and Simultaneous Speech Translation,” in Proc. Interspeech 2022, 2022, pp. 1746–1750. [12] K. Rao, H. Sak, and R. Prabhavalkar, “Exploring architectures, data and units for streaming end-to-end speech recognition with rnn-transducer,” in 2017 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2017, pp. 193–199. [13] N. Moritz, T. Hori, and J. Le, “Streaming automatic speech recognition with the transformer model,” in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 6074–6078. [14] A. Narayanan, R. Prabhavalkar, C.-C. Chiu, D. Rybach, T. N. Sainath, and T. Strohman, “Recognizing long-form speech using streaming end-to-end models,” in 2019 IEEE Automatic Speech Recognition and Understanding