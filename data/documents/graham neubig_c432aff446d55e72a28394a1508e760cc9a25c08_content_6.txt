Title: Why do Nearest Neighbor Language Models Work?
Authors: Frank F. Xu, Uri Alon, Graham Neubig
Section: 4.2 Increasing the Softmax Capacity
tail of unlikely word types. However, we could not gain any benefits solely from sparsifying the output probability of a standard LM and interpolating it with the original LM. More details can be found in Appendix E.2. Stolen Probabilities The stolen probabilities effect (Demeter et al., 2020) refers to the situation where the output embeddings of an LM are learned such that some words are geometrically placed inside the convex hull that is formed by other word embeddings and can thus never be “selected” as the argmax word. We hypothesized that kNN-LM solves the stolen probabilities problem by allowing to assign the highest probability to any word, given a test context that is close enough to that word’s datastore key. However, we found that none of the vectors in our embedding matrix and in the original embedding matrix of Khandelwal et al. (2020b) is located in the convex hull of the others, which is consistent with the findings of Grivas et al. (2022). More details can be found in Appendix E.4. Memorization We hypothesized that the kNN component simply provides memorization of the training set. However, we could not improve a standard LM by interpolating its probability with another standard LM that was further trained to overfit the training set. More details can be found in Appendix E.6.1. Soft Labels We hypothesized that kNN-LM’s improvement lies in reducing the “over-correction” error when training with 1-hot labels, as hypothesized by Yang et al. (2022), and that retrieving neighbors is not important. If only “soft labels” are the key, we could hypothetically improve the performance of another fresh LM with the same model architecture but trained with the soft labels from the base LM, instead of from kNN-LM. This separates the effect of “soft labeling” from the additional guidance provided by kNN. However, this does not help with the interpolated perplexity at all. More details can be found in Appendix E.6.2. Optimizing Interpolated Loss We hypothesized that the standard LM cross-entropy training loss does not emphasize the examples where base LM performs badly which could benefit from kNN, and directly optimizing the interpolated loss of standard LM and a separate trainable softmax layer could be a better alternative. However, we could not gain any benefits by training an additional softmax layer together with a base LM using the interpolated loss. More details can be found in Appendix E.6.3.