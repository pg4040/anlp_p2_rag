Title: STRUCTURED PRUNING OF SELF-SUPERVISED PRE-TRAINED MODELS FOR SPEECH RECOGNITION AND UNDERSTANDING
Authors: Yifan Peng, Kwangyoun Kim, Felix Wu, Prashant Sridhar, Shinji Watanabe
Section: 4.5. Comparison with other compression methods
for self-supervised learning of speech representations,” in Proc. NeurIPS, 2020. [9] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, et al., “HuBERT: Self- supervised speech representation learning by masked prediction of hidden units,” IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 29, pp. 3451–3460, 2021. [10] S. Chen, C. Wang, Z. Chen, et al., “WavLM: Large-scale self- supervised pre-training for full stack speech processing,” IEEE Journal of Selected Topics in Signal Processing, 2022. [11] G. Hinton, O. Vinyals, J. Dean, et al., “Distilling the knowl- edge in a neural network,” arXiv:1503.02531, 2015. [12] V. Sanh, L. Debut, J. Chaumond, and T. Wolf, “DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter,” arXiv:1910.01108, 2019. [13] X. Jiao, Y. Yin, et al., “TinyBERT: Distilling BERT for natural language understanding,” in Findings of EMNLP, 2020. [14] Z. Peng, A. Budhkar, I. Tuil, et al., “Shrinking bigfoot: Reduc- ing wav2vec 2.0 footprint,” in SustaiNLP, 2021. [15] H.-J. Chang, S.-w. Yang, and H.-y. Lee, “DistilHuBERT: Speech representation learning by layer-wise distillation of hidden-unit BERT,” in Proc. ICASSP, 2022. [16] R. Wang, Q. Bai, J. Ao, et al., “LightHuBERT: Lightweight and Configurable Speech Representation Learning with Oncefor-All Hidden-Unit BERT,” in Proc. Interspeech, 2022. [17] Y. Lee, K. Jang, J. Goo, et al., “FitHuBERT: Going Thin- ner and Deeper for Knowledge Distillation of Speech SelfSupervised Models,” in Proc. Interspeech, 2022. [18] S. Han, J. Pool, et al., “Learning both weights and connections for efficient neural network,” in Proc. NeurIPS, 2015. [19] H. Li, A. Kadav, I. Durdanovic, et al., “Pruning Filters for Efficient ConvNets,” in Proc. ICLR, 2017. [20] Z. Liu, J. Li, Z. Shen, et al., “Learning Efficient Convolutional Networks Through Network Slimming,” in Proc. ICCV, 2017. [21] Q. Zhang, S. Zuo, C. Liang, et al., “PLATON: Pruning large transformer models with upper confidence bound of weight importance,” in Proc. ICML, 2022. [22] Z. Wang, J. Wohlwend, and T. Lei, “Structured Pruning of Large Language Models,” in Proc. EMNLP, 2020. [23] M. Xia, Z. Zhong, and D. Chen, “Structured Pruning Learns Compact and Accurate Models,” in Proc. ACL, 2022. [24] C. Liang, S. Zuo, M. Chen, et al., “Super tickets in pre-trained language models: From model compression to improving generalization,” in Proc. ACL, 2021. [25] P. Dong, S. Wang, W. Niu, et al., “Rtmobile: Beyond real- time mobile acceleration of rnns for speech recognition,” in ACM/IEEE Design Automation Conference (DAC), 2020. [26] S. Wang, P. Lin, R. Hu, et al., “Acceleration of LSTM With Structured Pruning Method on FPGA,” IEEE Access, 2019. [27] K. Tan and