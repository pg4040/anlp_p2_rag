Title: The Troubling Emergence of Hallucination in Large Language Models – An Extensive Definition, Quantification, and Prescriptive Remediations
Authors: Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S.M Towhidul Islam Tonmoy, Aman Chadha, Amit Sheth, Amitava Das
Section: 
propaganda and untruth?”. In fact, the majority of these falsehoods are widely recognized as hallucination, which can be defined as the generation of content that deviates from the real facts, resulting in unfaithful outputs (Maynez et al., 2020). To address the inevitable question of ownership attribution for AI-generated artifacts, the US Copyright Office (Copyright-Office, 2023) released a statement stating that if the content is traditional elements of authorship produced by a machine, the work lacks human authorship and the office will not register it for copyright. OpenAI’s response to the prevalent societal pressure led them to issue a public statement (OpenAI, 2023b) emphasizing their commitment to AI safety and their determination to implement improved controls on hallucination in future iterations of GPT. The recent roll-out of Google’s highly anticipated ChatGPT rival, Bard, led to a fiasco owing to it hallucinating a factually inaccurate answer in the company’s advertisement, which cost Google a $140 billion wipeout in terms of market value (Reuters, 2023). In the ad, Bard is prompted: What new discoveries from the James Webb Space Telescope (JWST)... Bard responds with a number of answers, including one suggesting the JWST was used to take the very first pictures of a planet outside the Earth’s solar system.... The first pictures of exoplanets were, however, taken by the European Southern Observatory’s VLT in 2004. In another incident, a lawyer used ChatGPT to help him prepare a filing in a lawsuit against a US airline. However, ChatGPT quoted a fabricated previous case precedent, which led the judge to consider imposing sanctions (Forbes, 2023). Amidst these happenings, NVIDIA introduced NeMo Guardrails (nVIDIA, 2023), an open-source toolkit, based on the Self-Check GPT framework (Manakul et al., 2023), designed to address hallucinations in conversational AI systems. The remarkable capabilities of generative AI have undeniably propelled it to a superpower status! Although the term hallucination has gained widespread acceptance in describing the irrational and uncontrolled behaviors of LLMs, it is important to note that many experts expressed dissatisfaction with this particular nomenclature. Within the AI community, efforts persist to find a more suitable alternative name to describe this phenomenon accurately. During an interview (You, 2023), Prof. Christopher Manning briefly expressed his discontent with the term “hallucination”, indicating a preference for an alternative term. In the ongoing conversation, Prof. Gary Marcus has advocated for a reframing of “hallucination” as confabulation, a term that some fellow researchers have already embraced. However, in this paper, we have decided to uphold the use of the term “hallucination”. In order to offer