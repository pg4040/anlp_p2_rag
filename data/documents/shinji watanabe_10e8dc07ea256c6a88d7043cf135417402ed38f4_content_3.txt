Title: Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization
Authors: Puyuan Peng, Brian Yan, Shinji Watanabe, David Harwath
Section: 2. The Whisper model
we cannot know for certain whether Whisper was trained on code-switched data, it is clear that the model’s language and task tokens can not explicitly direct the model to do CS-ASR - each language token only represents one 3We use 3 number of objects choices to reduce the tuning noise introduced by the mismatch between How2 and VisSpeech. Tiny.en Base.en Small.en Medium.en Tiny Base Small Medium Large LargeV26 8 10 12 14 16 18 W ER English Models Multilingual Models Audio Only Audio + Visual Prompt Figure 2: The effectiveness of visual prompt on VisSpeech across different models. Table 2: Comparison of model performance on VisSpeech. With visual prompt, Medium.en outperforms Large. Model Modality WER SotA [19] A+V 11.28 Whisper Medium.en A 8.35 Whisper Medium.en A+V 7.60 Whisper Large A 8.02 Whisper LargeV2 A 7.16 of the 99 training languages, and the task tokens do not convey any information on whether the model should output text in more than one language. Approach. To test Whisper’s CS-ASR capabilities, we use two Mandarin-English code-switched corpora. See table 1 for a quick summary of default and our proposed approach. The default approach (denoted as default) is to let Whisper to first run LID to detect the language between the two4, and then use the detected language in the prompt. While this default approach work to some degree, it relies heavily on Whisper’s LID capabilities, but our results show this can sometimes be inaccurate, especially on accented speech. In addition, this approach doesn’t explicitly instruct the model to output text in more than one language for intra-sentential code-switched utterances. We propose a simple approach called concat that handles the aforementioned issues. The concat approach replaces the single language token in the prompt with two language tokens i.e. <|zh|> and <|en|>, as shown in table 1. As will be shown later, despite the simplicity of this approach and the fact that Whisper has never be trained to take two language tokens in the prompt, our approach significantly improves performance. Datasets and implementational details. We use ASCEND [25] and SEAME [26], which are both Mandarin-English code-switched datasets. Both datasets are spontaneous conversational speech, but ASCEND was recorded from bilingual speakers with different Chinese dialects, while SEAME is recorded from Singaporean and Malaysian speakers. We’ll show that despite the fact that both datasets contains the same languages, Whisper performs very differently on them. Tuning is done on the validation sets of ASCEND and SEAME. For our proposed concat approach, the hyperparameters we tune are: 1. the order