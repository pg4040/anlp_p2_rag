Title: TOWARDS PRACTICAL AND EFFICIENT IMAGE-TO-SPEECH CAPTIONING WITH VISION-LANGUAGE PRE-TRAINING AND MULTI-MODAL TOKENS
Authors: Minsu Kim, Jeongsoo Choi, Soumi Maiti, Jeong Hun Yeo, Shinji Watanabe, Yong Man Ro
Section: 3.3. Experimental Results
[28, 29], we can confirm that the proposed method outperforms the previous methods with large gaps in all metrics. For example, the proposed Im2Sp achieves a 20.6 BLEU score on Flickr8k which outperforms the previous method [29] by 5.8 BLEU score. Furthermore, in contrast to previous methods that exhibited significantly lower performance than the popular image captioning system, SAT [2], the proposed Im2Sp model can now catch up with the performance of the textbased system (i.e., SAT). Please note that the works [28, 29] utilized ASR models trained on audio reconstructed from their audio features, hence some incorrect pronunciations are calibrated by the ASR model. In contrast, we achieve better performance by using an off-the-shelf ASR model [38]. We strongly recommend listening to the generated speech that is available on bit.ly/3Z9T6LJ. We also conduct Mean Opinion Score (MOS) tests, involving 15 participants who assessed 20 samples for each method. The subjects are asked to rate the naturalness of the generated speech and how correctly the generated speech describes the input image on a scale of 1 to 5. Moreover, we also report DNN-based MOS using MOSNet [26] and SpeechLMScore [27]. The MOS comparison results are shown in Table 4. The results on both human and DNN-based metrics clearly show that the proposed Im2Sp method generates more natural sound with better descriptiveness than the previous method. Performance of image unit-based system. The last row of Table 3 shows the Im2Sp performance of the image unit-based system. We can find that there is a trade-off between efficiency and performance, similar to [35]. However, we can achieve reasonable performances by achieving better performances than the previous state-ofthe-art [29] on Flickr8k data. From the MOS test in Table 4, we find that we lose some descriptiveness when we use image units, but we can maintain the speech quality. Please note that with the unit-based Im2Sp, we can reduce a great amount of data storage and computation costs. The required bit size is reduced to 0.8% and lower than 0.2% for input and output, compared to original signals (Sec. 2.3). 4. CONCLUSION In this paper, we proposed a practical and efficient Image-to-Speech captioning (Im2Sp) method. We showed that even if speech is not utilized in vision-language pre-training, the knowledge of image comprehension and language modeling can be transferred into the Im2Sp model. Finally, by employing image units instead of raw images as inputs for our system, we showed that we can greatly reduce the data size (bits) while still achieving reasonable performances. 5. REFERENCES [1]