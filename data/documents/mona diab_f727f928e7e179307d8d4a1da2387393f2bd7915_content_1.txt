Title: Methods for Measuring, Updating, and Visualizing Factual Beliefs in Language Models
Authors: Peter Hase, Mona Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, Srinivasan Iyer
Section: E Additional Results
variance to explain. Learning curve. In Fig. 7 we show the learning curve of a learned optimizer trained with SLAG on zsRE. The Main Input Update Success Rate steadily rises as a function of the training set size.‘ Ablation by objective term. We give objective ablation results in Table 17. Surprisingly, we do not always see that the objective terms help for the data they are intended to help with. First, we obtain mixed results for the paraphrase objective. On zsRE, the objective term seems to hinder performance, with update success dropping on Main Inputs by 0.71 (±0.60; p=.021) and ∆-Acc dropping by 0.18 (±0.19; p=.069), while the paraphrase Update Success Rate itself is unaffected. With Wiki- data5m, however, the paraphrase term improves paraphrase update success by a large margin of 16.94 (±1.03; p<1e−4) points. Adding the Local Neutral (LN) term with the paraphrase term greatly improves the LN Retain Rate for Wikidata5m, by 9.71 points (±1.44; p<1e−4), though both of these terms come at a cost to Main Input Update Success, similar to zsRE. Lastly, we do not find that the entailment objective improves Entailed Data Update Success; in fact, this metric falls by 4.56 (±7.22; p=.213) points with the objective. Ablation by num. update steps. Fig. 8 shows the results of an ablation across values of K using a learned optimizer trained using SLAG with r = 1 on zsRE. Main Input Update Success rises by over three points by increasing Ktest from 1 to at least 5. Using a value of Ktrain that matches Ktest gives a further increase of about 0.5 points.