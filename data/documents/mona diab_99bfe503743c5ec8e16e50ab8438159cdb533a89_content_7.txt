Title: The Troubling Emergence of Hallucination in Large Language Models – An Extensive Definition, Quantification, and Prescriptive Remediations
Authors: Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S.M Towhidul Islam Tonmoy, Aman Chadha, Amit Sheth, Amitava Das
Section: 
an LLM. Secondly, LLMs can exhibit different characteristics, such as higher EFM or ESL tendencies, or they can have varying levels of overall hallucination. This notion is captured by introducing the terms N(x)−N(EFM) and N(x)−N(ESL) in the equation. It is worth noting that we did not consider variations of intrinsic hallucinations in HVI calculation, as they are relatively minor and exhibit lower vulnerability overall. Lastly, comparative measures are needed to rank LLMs based on their vulnerability to hallucination. This is achieved using multiplicative damping factors, δ1 and δ2, which are calculated based on µ ± rankx ×σ . Initially, we calculate the HVI for all 15 LLMs, considering δ1 and δ2 as zero. With these initial HVIs, we obtain the mean (µ) and standard deviation (σ ), allowing us to recalculate the HVIs for all the LLMs. The resulting HVIs are then ranked and scaled providing a comparative spectrum as presented in Fig. 3, similar to z-score normalization (Wikipedia_zscore) and/or min-max normalization (Wikipedia_min_max). Having damping factors enables easy exponential smoothing with a handful of data points, 15 in this case. Finally, for ease of interpretability, HVI is scaled between 0−100. LLM Size HVI (0-100) GPT-3 175B 90 - StableLM 7B 82 - GPT-2 1.5B 70 - Vicuna 13B 62 - MPT 7B 59 - LLaMA 65B 57 - GPT-3.5 175B 53 - Dolly 12B 49 - OPT 175B 48 - GPT-4 1.7T 47 - Alpaca 65B 40 - BLOOM 176B 38 - T0 11B 36 - XLNet 340M 36 - T5 11B 32 - higher hallucination lower hallucination fragments, C - replaced text, D - highlighted text for no information found, and E - refuted text fragments by textual entailment. Appendix F contains more examples. 5 HVI vs. LLMs size for different LLMs: An insight from There is a general observation that LLMs may exhibit a higher tendency towards generating hallucinations or producing outputs that deviate from factual or coherent information. However, it is important to note that the relationship between LLM size and hallucination is not necessarily a direct correlation, but rather a consideration based on certain factors such as (a) training data quality, (b) lack of explicit training on facts, and (c) overconfi- dence in generated responses. A noteworthy pattern that emerges is that LLMs without RLHF (Reinforcement Learning from Human Feedback) (Ziegler et al., 2019) tend to exhibit a higher tendency for hallucination. Although we did not extensively evaluate this phenomenon, we have a keen interest in investigating it further in the near future. While we tried