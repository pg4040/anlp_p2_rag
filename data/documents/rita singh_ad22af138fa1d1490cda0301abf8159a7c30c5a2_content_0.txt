Title: Pengi: An Audio Language Model for Audio Tasks
Authors: Soham Deshmukh, Benjamin Elizalde, Rita Singh, Huaming Wang
Section: H Constrastive Learning and Generative Pretraining
We compare our model Pengi with CLAP [17], a state-of-the-art Zero-Shot model that has been evaluated on 16 downstream tasks. However, CLAP is trained on a smaller amount of audio-text data. This leads us to ask: “Is the improved performance due to the larger training data or the generative pretraining?”. We already know that generative pretraining allows us to perform open-ended tasks like Audio Captioning, AQA, which are not possible with contrastive models. But this does not tell us if: generative pretraining is beneficial for close-ended tasks like classification?. To answer this question, we train a CLAP model with the same data 4.1) that we use to train Pengi. We call this model CLAP*. Results. The results are shown in Table 15. We see generative pertaining (Pengi) outperforming contrastive learning (CLAP*) on average. Moreover, with generative pretraining, the model can perform open-ended tasks like Audio Captioning and Audio Question Answering. An interesting observation is Pengi outperforms human performance (81%) on ESC50. Humans have limitations inherent to how much information a participant can handle at once. In the case of ESC50, humans listen to the audio once, and have to remember the audio content, task description, and choose among 50 different classes. Moreover, listeners have different degrees of familiarity with prototypical content from different sound classes, whereas Pengi has been exposed to similar content during training. In a sense, Pengi is an expert listener, whereas the humans in the listening experiment were not.