Title: Pairwise Similarity Learning is SimPLE
Authors: Yandong Wen, Weiyang Liu, Yao Feng, Bhiksha Raj, Rita Singh, Adrian Weller, Michael J. Black, Bernhard Schölkopf
Section: B. Applying Generalized Inner Product to Proxy-based Methods
Since generalized inner product achieves significant improvement in SimPLE, we are interested in how it works in proxybased methods. Here we apply generalized inner product to two representative proxy-based methods: vanilla cross-entropy loss and CosFace. The formulations are given as follows. Note that LCosFace∗ is equivalent to LCE∗ when margin m is 0. LCE∗ = log ( 1 + ∑ i ̸=y exp(∥wi∥ · ∥x̃∥ · (cos(θw̃i,x̃)− bθ)− ∥wy∥ · ∥x̃∥ · (cos(θw̃y,x̃)− bθ) ) (9) LCosFace∗ = log ( 1 + ∑ i̸=y exp(∥wi∥ · ∥x̃∥ · (cos(θw̃i,x̃)− bθ −m)− ∥wy∥ · ∥x̃∥ · (cos(θ1w̃y, x̃)− bθ) ) (10) We adopt Setting A in this ablation. bθ is set from 0 to 0.9 for the vanilla cross-entropy loss. As shown in Table 10, we do not observe improved performance when applying generalized inner product to the proxy-based method. In contrast, our proxy-free SimPLE can enjoy accuracy gains from the generalized inner product, producing substantially better results. For CosFace, we also try different combinations of bθ and m. However, the models do not converge even if we use a very small m (i.e. 0.1), resulting in chance-level performance. This is also consistent with the observations in a large body of literature, where cosine similarity has become a de facto choice.