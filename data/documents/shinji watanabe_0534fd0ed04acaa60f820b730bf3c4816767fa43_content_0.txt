Title: TENSOR DECOMPOSITION FOR MINIMIZATION OF E2E SLU MODEL TOWARD ON-DEVICE PROCESSING
Authors: Yosuke Kashiwagi, Siddhant Arora, Hayato Futami, Jessica Huynh, Shih-Lun Wu, Yifan Peng, Brian Yan, Emiru Tsunoo, Shinji Watanabe
Section: 5. Conclusion
In this paper, we describe our investigation of the minimization of the E-Branchformer for on-device E2E SLU. We applied sequential distillation and tensor decomposition techniques to the E-Branchformer. Compared to the Conformer-based model, EBranchformer with Tucker decomposition achieved higher performance in both 15M and 30M limitations. In addition, it obtained better performance than the deliberation model in 15M limitation. The experiment with different compression ratios showed that compression around 0.3 points was efficient for E-Branchformer. Our comparison of the Tucker decomposition with the CP decomposition and Tensor-Train decomposition showed that the Tucker decomposition was a relatively efficient decomposition. Finally, our system, E-Branchformerbased decomposed E2E SLU model, achieved 70.9% EM with 15M parameter limitation on STOP data. 6. References [1] Dmitriy Serdyuk, Yongqiang Wang, Christian Fuegen, Anuj Ku- mar, Baiyang Liu, and Yoshua Bengio, “Towards end-to-end spoken language understanding,” in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 5754–5758. [2] Martin Radfar, Athanasios Mouchtaris, and Siegfried Kunzmann, “End-to-end neural transformer based spoken language understanding,” Proc. Interspeech 2020, pp. 866–870, 2020. [3] Jingjing Dong, Jiayi Fu, Peng Zhou, Hao Li, and Xiaorui Wang, “Improving spoken language understanding with cross-modal contrastive learning,” Proc. Interspeech 2022, pp. 2693–2697, 2022. [4] Ye Wang, Baishun Ling, Yanmeng Wang, Junhao Xue, Shaojun Wang, and Jing Xiao, “Adversarial knowledge distillation for robust spoken language understanding,” Proc. Interspeech 2022, pp. 2708–2712, 2022. [5] Siddhant Arora, Siddharth Dalmia, Xuankai Chang, Brian Yan, Alan Black, and Shinji Watanabe, “Two-pass low latency end-toend spoken language understanding,” Proc. Interspeech 2022, pp. 3478–3482, 2022. [6] Martin Radfar, Athanasios Mouchtaris, Siegfried Kunzmann, and Ariya Rastrow, “FANS: Fusing ASR and NLU for on-device SLU,” in Interspeech 2021, 2021. [7] Duc Le, Akshat Shrivastava, Paden D. Tomasello, Suyoun Kim, Aleksandr Livshits, Ozlem Kalinli, and Michael L. Seltzer, “Deliberation model for on-device spoken language understanding,” in Interspeech 2022, 2022, pp. 3468–3472. [8] Alaa Saade, Joseph Dureau, David Leroy, Francesco Caltagirone, Alice Coucke, Adrien Ball, Clément Doumouro, Thibaut Lavril, Alexandre Caulier, Théodore Bluche, et al., “Spoken language understanding on the edge,” in 2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing-NeurIPS Edition (EMC2-NIPS). IEEE, 2019, pp. 57–61. [9] Akshit Tyagi, Varun Sharma, Rahul Gupta, Lynn Samson, Nan Zhuang, Zihang Wang, and Bill Campbell, “Fast intent classification for spoken language understanding systems,” in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 8119–8123. [10] Thierry Desot, François Portet, and Michel Vacher, “End-to-end spoken language understanding: Performance analyses of a voice command task in a low resource