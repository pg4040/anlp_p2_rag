Title: JOINT PREDICTION AND DENOISING FOR LARGE-SCALE MULTILINGUAL SELF-SUPERVISED LEARNING
Authors: William Chen, Jiatong Shi, Brian Yan, Dan Berrebbi, Wangyou Zhang, Yifan Peng, Xuankai Chang, Soumi Maiti, Shinji Watanabe
Section: 8. REFERENCES
[1] A. Radford et al., “Robust speech recognition via largescale weak supervision,” arXiv preprint arXiv:2212.04356, 2022. [2] T. Brown et al., “Language models are few-shot learners,” in Proc. NeurIPS, vol. 33, 2020, pp. 1877–1901. [3] S. Chen et al., “WavLM: Large-scale self-supervised pre-training for full stack speech processing,” IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505–1518, 2022. [4] A. Mohamed et al., “Self-supervised speech representation learning: A review,” IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1179– 1210, 2022. [5] X. Chang et al., “An exploration of self-supervised pretrained representations for end-to-end speech recognition,” in Proc. ASRU, 2021, pp. 228–235. [6] A. Chowdhery et al., “PaLM: Scaling language modeling with pathways,” arXiv preprint arXiv:2204.02311, 2022. [7] T. L. Scao et al., “BLOOM: A 176b-parameter openaccess multilingual language model,” arXiv preprint arXiv:2211.05100, 2022. [8] S. Black et al., “GPT-neox-20b: An open-source autoregressive language model,” in Challenges & Perspectives in Creating Large Language Models, 2022. [9] A. Srivastava et al., “Beyond the imitation game: Quantifying and extrapolating the capabilities of language models,” arXiv preprint arXiv:2206.04615, 2022. [10] S. Schneider et al., “wav2vec: Unsupervised PreTraining for Speech Recognition,” in Proc. Interspeech, 2019, pp. 3465–3469. [11] A. Baevski et al., “wav2vec 2.0: A framework for selfsupervised learning of speech representations,” in Proc. NeurIPS, vol. 33, 2020, pp. 12 449–12 460. [12] W.-N. Hsu et al., “HuBERT: Self-supervised speech representation learning by masked prediction of hidden units,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451–3460, 2021. [13] C. Wang et al., “Improving self-supervised learning for speech recognition with intermediate layer supervision,” in Proc. ICASSP, 2022, pp. 7092–7096. [14] Y.-A. Chung et al., “W2v-BERT: Combining contrastive learning and masked language modeling for self-supervised speech pre-training,” in Proc. ASRU, 2021, pp. 244–250. [15] A. Baevski et al., “Data2vec: A general framework for self-supervised learning in speech, vision and language,” in Proc. ICML, K. Chaudhuri et al., Eds., ser. Proceedings of Machine Learning Research, vol. 162, 2022, pp. 1298–1312. [16] S.-w. Yang et al., “SUPERB: Speech Processing Universal PERformance Benchmark,” in Proc. Interspeech, 2021, pp. 1194–1198. [17] W. Chen et al., “Improving massively multilingual ASR with auxiliary CTC objectives,” in Proc. ICASSP, 2023, pp. 1–5. [18] A. Rouditchenko et al., “Comparison of multilingual self-supervised and weakly-supervised speech pretraining for adaptation to unseen languages,” in Proc. Interspeech, 2023. [19] S. Khurana, A. Laurent, and J. Glass, “SAMU-XLSR: Semantically-aligned multimodal utterance-level crosslingual speech representation,” IEEE Journal of Selected Topics in Signal Processing, vol. 16, no.