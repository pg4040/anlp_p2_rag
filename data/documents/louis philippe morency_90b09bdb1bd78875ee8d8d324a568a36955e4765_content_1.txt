Title: Multimodal Fusion Interactions: A Study of Human and Automatic sQuantification
Authors: Paul Pu Liang, Yun Cheng, Ruslan Salakhutdinov, Louis-Philippe Morency
Section: A HUMAN ANNOTATION DETAILS
predictions �1+2 and corresponding confdence ratings. We asked the second annotator to predict �2 similarly with only the second modality and then �2+1 with both modalities presented. A.3 Annotating information decomposition We asked 3 annotators to directly annotate the information decomposition values. Given both modalities, each annotator is asked to provide a rating on a scale of 0 (none at all) to 5 (large extent) for the following questions that correspond to �, �1, �2, and � respectively: (1) The extent to which both modalities enable them to make the same predictions about the task; (2) The extent to which modality 1 enables them to make a prediction that they would not if using modality 2; (3) The extent to which modality 2 enables them to make a prediction that they would not if using modality 1; (4) The extent to which both modalities enable them to make a prediction that they would not if using either modality individually. Finally, they are asked to rate their confdence for each rating they provided, on a scale of 0 (no confdence) to 5 (high confdence). ICMI ’23, October 09–13, 2023, Paris, France A.4 Video and audio presentation Annotators were provided with the full video link which opens up in a separate video player. They asked to either annotate based on all modalities in the video (i.e., video + audio), or asked to mute the videos themselves when annotating based on vision only, or are not provided the video at all when annotating based only on the transcript. We did not completely remove the audio from videos because in all tasks, annotators have to use video only (with mute), followed by audio+transcripts, and fnally with all modalities (video+audio+transcripts). In the counterfactual setting they may see video only (with mute), before playing the entire video with audio. Hence, we instructed the annotators to mute the videos for video-only prediction, and unmute the video for predictions that involve audio. We specifcally checked with the annotators and they strictly followed these guidelines. A.5 Label space for QA tasks For VQA and CLEVR datasets, annotators were requested to write the answer themselves. For CLEVR the answer is always yes/no. For VQA we let the users write their own answer, but we post-hoc modify these answers to defne a similarity with the fnal answer �: whether �1 or �2 are the same as multimodal �, or otherwise diferent. This binary distance function is sufcient to distinguish diferent interactions.