Title: Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications
Authors: Paul Pu Liang, Chun Kai Ling, Yun Cheng, Alex Obolenskiy, Yudong Liu, Rohan Pandey, Alex Wilf, Louis-Philippe Morency, Ruslan Salakhutdinov
Section: B Experimental Details
Implementation details: We first apply PCA to reduce the dimension of multimodal data. For the test split, we use unsupervised clustering to generate 20 clusters. We obtain a clustered version of the original dataset D = {(x1, x2, y)} as Dcluster = {(c1, c2, y)} where ci ∈ {1, . . . ,20} is the ID of the cluster that xi belongs to. In our experiments, where Y is typically a classification task, we set the unimodal classifiers f1 = p̂(y∣x1) and f2 = p̂(y∣x2) as the Bayes optimal classifiers for multiclass classification tasks. For classification, Y is the set of k-dimensional 1-hot vectors. Given two logits ŷ1, ŷ2 obtained from x1, x2 respectively, define d(ŷ1, ŷ2) = (ŷ1 − ŷ2)2. We have that cd = 1, and ϵ1 = ∣L(f1)−L(f∗1 )∣2 = 0 and ϵ2 = ∣L(f2) − L(f∗2 )∣2 = 0 for well-trained neural network unimodal classifiers f1 and f2 for Theorem 2. For datasets with 3 modalities, we perform the experiments separately for each of the 3 modality pairs, before taking an average over the 3 modality pairs. Extending the definitions of redundancy, uniqueness, and synergy, as well as our derived bounds on synergy for 3 or more modalities is an important open question for future work. B.2 Relationships between agreement, disagreement, and interactions 1. The relationship between redundancy and synergy: We give some example distributions to analyze when the lower bound based on redundancy Sagree is high or low. The bound is high for distributions where X1 and X2 are independent, but Y = 1 sets X1 ≠X2 to increase their dependence (i.e., AGREEMENT XOR distribution in Table 2b). Since X1 and X2 are independent but become dependent given Y , I(X1;X2;Y ) is negative, and the bound is tight Sagree = 1 ≤ 1 = S. Visual Question Answering 2.0 [37] falls under this category, with S = 4.92,R = 0.79, where the image and question are independent (some questions like ‘what is the color of the object’ or ‘how many people are there’ can be asked for many images), but the answer connects the two modalities, resulting in dependence given the label. As expected, the estimated lower bound for agreement synergy: Sagree = 4.03 ≤ 4.92 = S. Conversely, the bound is low for Table 2d with the probability mass distributed uniformly only when y = x1 = x2 and 0 elsewhere. As a result, X1 is always equal to X2 (perfect dependence), and yet Y perfectly explains away the dependence between X1 and