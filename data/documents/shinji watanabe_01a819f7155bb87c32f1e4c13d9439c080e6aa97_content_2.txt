Title: JOINT PREDICTION AND DENOISING FOR LARGE-SCALE MULTILINGUAL SELF-SUPERVISED LEARNING
Authors: William Chen, Jiatong Shi, Brian Yan, Dan Berrebbi, Wangyou Zhang, Yifan Peng, Xuankai Chang, Soumi Maiti, Shinji Watanabe
Section: 8. REFERENCES
resolutions,” arXiv preprint arXiv:2306.01084, 2023. [42] S. Kim et al., “Squeezeformer: An efficient transformer for automatic speech recognition,” in Proc. NeurIPS, S. Koyejo et al., Eds., vol. 35, 2022, pp. 9361–9373. [43] M. Burchi and V. Vielzeuf, “Efficient conformer: Progressive downsampling and grouped attention for automatic speech recognition,” in Proc. ASRU, 2021, pp. 8– 15. [44] T. Zhao et al., “Unet++-based multi-channel speech dereverberation and distant speech recognition,” in 2021 12th International Symposium on Chinese Spoken Language Processing (ISCSLP), 2021, pp. 1–5. [45] A. Graves et al., “Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks,” in Proc. ICML, 2006, pp. 369–376. [46] D. S. Park et al., “Specaugment: A simple data augmentation method for automatic speech recognition,” Proc. Interspeech, pp. 2613–2617, 2019. [47] K. Kim et al., “E-Branchformer: Branchformer with enhanced merging for speech recognition,” in Proc. SLT, 2023, pp. 84–91. [48] T. Dao et al., “FlashAttention: Fast and memoryefficient exact attention with IO-awareness,” in Proc. NeurIPS, 2022. [49] S. Rajbhandari et al., “Zero: Memory optimizations toward training trillion parameter models,” in SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, 2020, pp. 1– 16.