Title: Rethinking Voice-Face Correlation: A Geometry View
Authors: Xiang Li, Yandong Wen, Muqiao Yang, Jinglu Wang, Rita Singh, Bhiksha Raj, Mohamed bin Zayed
Section: 3.3 Facial AM Prediction
and ğœ–ğ¶ğ‘˜ on these splits. Optional phonatory module. Inspired by linear predictive coding (LPC) [25] which leverages voice producing to learn vocal tract geometry, we aim to facilitate face geometry capture by learning characteristics of voice. We enroll a phonatory module serving as an additional constraint when predicting facial AMs. In particular, we leverage a diffusion-based [18] voice generation method to model the time-domain speech signals. As shown in Fig. 2, the diffusion model converts the noise distribution to a speech ğ‘£ controlled by the voice code ğ‘’ extracted from speech ğ‘£ . During training speech ğ‘£ â€² which shares speaker identity with ğ‘£ is fed to the diffusion model as ground-truth. Please note that the phonatory module only serves as an additional training constraint and is not applied during inference. Let ğ‘¥0, Â· Â· Â· , ğ‘¥ğ‘‡ be a sequence of variables with the same dimension where ğ‘¡ is the index for diffusion time steps. Then the diffusion process transforms ğ‘¥0 into a Gaussian noise ğ‘¥ğ‘‡ through a chain of Markov transitions with a set of variance schedule ğ›½1, Â· Â· Â· , ğ›½ğ‘‡ . Specifically, each transformation is performed according to the Markov transition probability ğ‘(ğ‘¥ğ‘¡ |ğ‘¥ğ‘¡âˆ’1, ğ‘’) assumed to be independent of the style code ğ‘’ as ğ‘(ğ‘¥ğ‘¡ |ğ‘¥ğ‘¡âˆ’1, ğ‘’) = N(ğ‘¥ğ‘¡ ; âˆšï¸ 1 âˆ’ ğ›½ğ‘¡ğ‘¥ğ‘¡âˆ’1, ğ›½ğ‘¡ ğ¼ ). (5) Unlike the diffusion process, the denoising process aims to recover the speech signal from Gaussian noise which is defined as a conditional distribution ğ‘ğœƒ (ğ‘¥0:ğ‘‡âˆ’1 |ğ‘¥ğ‘‡ , ğ‘). Through the reverse transitions ğ‘ğœƒ (ğ‘¥0:ğ‘‡âˆ’1 |ğ‘¥ğ‘‡ , ğ‘), the variables are gradually restored to a speech signal with style code condition. The phonatory module actually models a distribution ğ‘(ğ‘¥0 |ğ‘). By applying the parameterization trick [21], we obtain the additional training constraint as {Eâˆ—, ğœƒâˆ—} = arg min E,ğœƒ = Eğ‘¥0,ğœ–,ğ‘¡ âˆ¥ğœ– âˆ’ ğœ–ğœƒ ( âˆš ğ›¼ğ‘¡ğ‘¥0 + âˆš 1 âˆ’ ğ›¼ğ‘¡ğœ–, ğ‘¡, ğ‘’)âˆ¥1 (6) where ğ›¼ğ‘¡ = 1 âˆ’ ğ›½ğ‘¡ and ğ›¼ğ‘¡ = âˆğ‘¡ ğ‘¡ â€²=1 ğ›¼ğ‘¡ â€² . As shown in Fig. 2, the ğœƒ is a Net [36] with cross-attention [35]. Since the phonatory model is only utilized as an auxiliary constraint during training, we omit the inference details to obtain ğ‘£ here.