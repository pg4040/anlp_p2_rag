Title: CUTTLEFISH: LOW-RANK MODEL TRAINING WITHOUT ALL THE TUNING
Authors: Hongyi Wang, Saurabh Agarwal, Pongsakorn U-chupala, Yoshiki Tanaka, Eric P. Xing, Dimitris Papailiopoulos
Section: 4.1 Experimental setup and implementation details
batch size for training. Consequently, we use a large batch size of 1,024 and measure the end-to-end training time for the experiments on CIFAR. The results, presented in Table 1, demonstrate that CUTTLEFISH consistently leads to faster end-to-end training time (including full-rank epochs and all other overhead computations, such as profiling and stable rank computing) compared to fullrank training. For instance, CUTTLEFISH achieves 1.2× end-to-end training speedups on both ResNet-18 and VGG19 trained on CIFAR-10. CUTTLEFISH yields comparable runtime to PUFFERFISH for ResNet-18 and faster runtime on VGG-19 because it finds smaller K for VGG-19, i.e., K = 4, while PUFFERFISH uses K = 9. SI&FD achieves faster runtime than CUTTLEFISH due to its use of K = 1 (and generally higher computational complexities for the initial convolution layers). However, employing such an aggressive value for K inevitably results in accuracy loss, as discussed earlier. Both FC compression and IMP require heavy computation to achieve small models, which are significantly slower than full-rank training. XNOR-Nets employ binary model weights and activations, which results in a reduction of final accuracy for tasks compared to dense networks. Ideally, the use of binarized weights and activations should greatly speed up model training and sub- stantially decrease memory consumption during the process. However, PyTorch lacks an efficient implementation of a binarized convolution operator. Consequently, our experiments utilized FP32 networks and activations to simulate binary networks, leading to a notably slower runtime compared to conventional FP32 training. This is because each layer’s output necessitates binarization, and model weights must be re-binarized for every iteration. For ImageNet experiments (presented in Table 2), the memory footprints are high, limiting us to a batch size of 256. CUTTLEFISH identifies factorized ResNet-50 and WideResNet-50 models that achieve 1.2× and 1.3× end-to-end speedups for ImageNet training, respectively. Although the factorized models found by CUTTLEFISH are comparable to PUFFERFISH, it eliminates the need for extensive hyperparameter tuning for such large-scale tasks.