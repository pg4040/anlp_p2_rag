Title: Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models
Authors: Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, Vered Shwartz
Section: 6 Summary of Findings and Insights
in the other direction, when analyzing how models work in order to learn about the human brain. Machine intelligence and Anthropomorphism Relatedly, our results also point to a need for caution when discussing the abilities of machines in relation to concepts referring to human cognition, such as Theory of Mind. While it is common in computer science to use human-related concepts and metaphors for AI systems, we caution readers to interpret “neural ToM” carefully and without aiming to make claims about “AI cognition,” especially since given our propensity for anthropomorphizing non-human animals and computers (Epley et al., 2007; Kim and Sundar, 2012); our measuring the performance on these benchmarks is not meant as an endorsement of the pursuit of a human-like social intelligence for AI systems.8 Instead, in light of the hype around AI and it’s “intelligence,” we sought out to provide a more sober look at the empirical performance of LLMs on tasks related to social intelligence and ToM. “Solving” a ToM benchmark is necessary but not sufficient Methodologically, if a model fails at least one ToM task, it does not have ToM in general. Success on one example or task is not a sound proof that a model has ToM. Future work will need to continue to develop benchmarks testing various ToM aspects, and these benchmarks will need to be designed to assess LLMs directly rather than using clinical tests designed for humans. Additionally, reporting the aggregated performance of LLMs on benchmarks obscures the performance differences across questions of different types and complexities. To overcome this, one approach is to pair a difficult question with an easy question, requiring model to answer both correctly. This methodology resembles the “joint score” employed in FauxPas-EAI, Adv-CSFB, and ToMi. In situations where pairing is challenging, a recommendation for future works is that dataset difficulty could be evaluated by calculating the final score across different splits of the dataset. The difficulty level of the dataset can then be determined based on the lowest score obtained among these splits. 8We leave the question of whether LLMs, AI, or even any non-biological entity could develop human-like cognition and Theory of Mind up to philosophers. Emergence vs. Supervised Learning vs Training on the Test set Prior work claimed that ToM abilities emerged as a byproduct of the LLM training (Kosinski, 2023). We argue that claims about emergence are (i) unfounded, and (ii) unfalsifiable without access to the LLMs’ training data. To make a statement regarding emergent ToM, a careful experiment is needed to ensure that