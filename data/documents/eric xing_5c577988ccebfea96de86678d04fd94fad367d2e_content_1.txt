Title: Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models
Authors: Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Satheesh Katipomu, Haonan Li, Fajri Koto, William Marshall, Gurpreet Gosal, Cynthia Liu, Zhiming Chen, Osama Mohammed Afzal, Samta Kamboj, Onkar Pandit, Rahul Pal, Lalit Pradhan, Zain Muhammad Mujahid, Massa Baali, Xudong Han, Sondos Mahmoud Bsharat, Alham Fikri Aji, Zhiqiang Shen, Zhengzhong Liu, Natalia Vassilieva, Joel Hestness, Andy Hock, Andrew Feldman, Jonathan Lee, Andrew Jackson, Hector Xuguang Ren, Preslav Nakov, Timothy Baldwin, Eric Xing
Section: 7 Related Work
In contrast, Jais and Jais-chat are open-source models, as elaborated in Section 3. Instruction-Tuning Fine-tuning language models using instruction–response pairs has enhanced the generalization capabilities of language models across various tasks [OWJ+22]. In terms of open-source models, BLOOMz [MWS+23] is a fine-tuned version of the foundation model BLOOM [SFA+23] based on large-scale instruction-tuning over a dataset created via templates, while LLaMA2 [TMS+23] uses a publicly available instruction–response pair dataset [CHL+22]. Moreover, instruction-tuning has been coupled with reinforcement learning with human feedback (RLHF). This combination aligns the generated responses to reward functions, optimizing the model’s factuality, and reducing its toxicity [OWJ+22]. The prompts used for instruction-tuning can have diverse origins. Some, as observed by [ZMH+23], are human-designed, while others can be autonomously generated. These prompts can be refined with follow-up instructions for more relevant or specific outputs, as studied by [GAS+23] and [MTG+23]. Recently, [WWS+22] introduced chain-of-thought prompting, directing models to clarify their reasoning over complex tasks, which was shown to enhance their accuracy. 44JASMINE [NAME+22] are Arabic GPT models ranging in sizes from 350M to 13B parameters, but these models have not been released to the public, and the arXiv paper describing them says the 6.7B and 13B models are still training. 45https://ai.google/static/documents/google-about-bard.pdf 46https://www.anthropic.com/index/introducing-claude Evaluating Large Language Models Large language models are proficient at generating coherent and fluent text, but have shortcomings in terms of factuality and reasoning skills. As a proxy to evaluate factuality, existing English large language models such as GPT-4 [Ope23] and LLaMA [TLI+23] use school exam questions [HBB+22] to understand how faithful the models are at providing knowledge. Evaluating commonsense reasoning abilities is also important, and is the target of datasets such as HellaSwag [ZHB+19], WinoGrande [SBBC21], ARC easy and challenge [CCE+18], and OpenBookQA [MCKS18]. Moreover, reasoning via programming is evaluated using HumanEval [CTJ+21] and MBPP [AON+21]. In Arabic NLP, existing benchmarks primarily focus on evaluating natural language understanding tasks. For instance, the ALUE benchmark [STG+21] encompasses semantic tasks such as irony detection [GKB+19], emotion classification [MBMSK18], sentiment classification [MBMSK18], offensive language [MDM+20] and hate speech identification [MDM+20]. Existing Arabic benchmarks, however, do not include knowledge and commonsense evaluation, posing a challenge for the assessment of Jais. In contrast, in other languages, researchers have effectively used methods such as machine translation or the construction of datasets in a similar manner to assess the knowledge proficiency and the commonsense understanding of language models [Ope23, LKW+23]. In this context, as detailed in Section 5, we used a combination of techniques, including crafting analogous datasets to those available for