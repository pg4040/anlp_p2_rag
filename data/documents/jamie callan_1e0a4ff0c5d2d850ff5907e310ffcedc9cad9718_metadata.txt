Faculty Name: jamie callan
Paperid: 1e0a4ff0c5d2d850ff5907e310ffcedc9cad9718
Title: KALE: Using a K-Sparse Projector for Lexical Expansion
Year: 2023
Abstract: Recent research has proposed retrieval approaches based on sparse representations and inverted indexes, with terms produced by neural language models and leveraging the advantages from both neural retrieval and lexical matching. This paper proposes KALE, a new lightweight method of this family that uses a small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary. The KALE vocabulary captures semantic concepts than perform well when used in isolation, and perform better when extending the original lexical vocabulary, this way improving first-stage retrieval accuracy. Experiments with the MSMARCOv1 passage retrieval dataset, the TREC Deep Learning dataset, and BEIR datasets, examined the effectiveness of KALE under varying conditions. Results show that the KALE terms can replace the original lexical vocabulary, with gains in accuracy and efficiency. Combining KALE with the original lexical vocabulary, or with other learned terms, can further improve retrieval accuracy with only a modest increase in computational cost.
Authors: Lu√≠s Borges, Bruno Martins, Jamie Callan
Venue: International Conference on the Theory of Information Retrieval
Tldr: {'model': 'tldr@v2.0.0', 'text': 'KALE is a new lightweight method that uses a small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary, which can replace the original lexical vocabulary with gains in accuracy and efficiency.'}
Url: https://dl.acm.org/doi/pdf/10.1145/3578337.3605131
