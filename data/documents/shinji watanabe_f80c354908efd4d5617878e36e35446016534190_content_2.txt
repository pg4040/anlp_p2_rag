Title: SEMI-AUTOREGRESSIVE STREAMING ASR WITH LABEL CONTEXT
Authors: Siddhant Arora, George Saon, Shinji Watanabe, Brian Kingsbury
Section: 4.4. Results and Discussion
attend and spell: A neural network for large vocabulary conversational speech recognition,” in Proc. ICASSP, 2016, pp. 4960–4964. [10] S. Kim, T. Hori, and S. Watanabe, “Joint CTC-attention based end-toend speech recognition using multi-task learning,” in Proc. ICASSP, 2017. [11] T. Hori, S. Watanabe, and J. R. Hershey, “Joint CTC/attention decoding for end-to-end speech recognition,” in Proc. ACL, 2017, pp. 518– 529. [12] Z. Tüske, K. Audhkhasi, and G. Saon, “Advancing sequenceto-sequence based speech recognition,” Proc. Interspeech 2019, pp. 3780–3784, 2019. [13] N. Chen et al., “Listen and fill in the missing letters: Nonautoregressive transformer for speech recognition,” CoRR, vol. abs/1911.04908, 2019. [14] Z. Tian et al., “Spike-triggered non-autoregressive transformer for end-to-end speech recognition,” in Proc. Interspeech, 2020. [15] Y. Higuchi et al., “Mask CTC: non-autoregressive end-to-end ASR with CTC and mask predict,” in Proc. Interspeech, 2020. [16] Z. Gao et al., “Paraformer: Fast and accurate parallel transformer for non-autoregressive end-to-end speech recognition,” in Proc. Interspeech, 2022, pp. 2063–2067. [17] J. Nozaki and T. Komatsu, “Relaxing the conditional independence assumption of ctc-based ASR by conditioning on intermediate predictions,” in Proc. Interspeech, 2021, pp. 3735–3739. [18] J. Lee and S. Watanabe, “Intermediate loss regularization for ctcbased speech recognition,” CoRR, vol. abs/2102.03216, 2021. [19] A. Vaswani et al., “Attention is all you need,” Proc. NeurIPS, vol. 30, pp. 5998–6008, 2017. [20] A. Gulati et al., “Conformer: Convolution-augmented transformer for speech recognition,” in Proc. Interspeech, 2020, pp. 5036–5040. [21] P. Guo et al., “Recent developments on espnet toolkit boosted by conformer,” in Proc. ICASSP, 2021, pp. 5874–5878. [22] J. Li et al., “On the comparison of popular end-to-end models for large scale speech recognition,” in Proc. Interspeech, H. Meng, B. Xu, and T. F. Zheng, Eds., 2020, pp. 1–5. [23] N. Moritz, T. Hori, and J. L. Roux, “Streaming automatic speech recognition with the transformer model,” in Proc. ICASSP, 2020, pp. 6074–6078. [24] D. Povey et al., “A time-restricted self-attention layer for ASR,” in Proc. ICASSP, 2018, pp. 5874–5878. [25] N. Jaitly et al., “An online sequence-to-sequence model using partial conditioning,” in Proc. NeurIPS, 2016, pp. 5067–5075. [26] L. Dong, F. Wang, and B. Xu, “Self-attention aligner: A latencycontrol end-to-end model for ASR using self-attention network and chunk-hopping,” in Proc. ICASSP, 2019, pp. 5656–5660. [27] E. Tsunoo et al., “Transformer ASR with contextual block processing,” in Proc. ASRU, 2019, pp. 427–433. [28] E. Tsunoo, Y. Kashiwagi, and S. Watanabe, “Streaming transformer ASR with blockwise synchronous inference,” CoRR, vol. abs/2006.14941, 2020. [29] E. Tsunoo, Y. Kashiwagi, and S. Watanabe, “Streaming transformer asr with blockwise