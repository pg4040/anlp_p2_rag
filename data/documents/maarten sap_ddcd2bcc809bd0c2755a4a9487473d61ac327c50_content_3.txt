Title: Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models
Authors: Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, Vered Shwartz
Section: 6 Summary of Findings and Insights
ToM did indeed appear spontaneously and not as a result of other factors such as training on related datasets, exposure to descriptions of clinical tests online, interactions with users, and more.9 However, since the data used to train the GPT models is not publicly available, it is impossible to quantify the degree of the potential data leakage.10 We echo calls by Dodge et al. (2021) for increased transparency and open-access to the training data of LLMs, which is crucial for scientifically valid and reproducible experiments (Rodgers, 2023). Improving neural ToM abilities (with CoT or other methods) Our objective in this study is not to measure benchmark performance or climb leaderboards. It is feasible that techniques such as chain-of-thought prompting (CoT; Wei et al., 2022) would enhance the performance of GPT-4 on tasks where it currently performs poorly. Nevertheless, we need to exercise caution to ensure that the utilization of methods like CoT or others does not excessively guide the models by essentially revealing the task structure to themâ€”just like Clever Hans who appeared proficient in math merely due to subtle hints given by the owner.