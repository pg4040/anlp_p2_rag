Title: Learning to Speak from Text: Zero-Shot Multilingual Text-to-Speech with Unsupervised Text Pretraining
Authors: Takaaki Saeki, Soumi Maiti, Xinjian Li, Shinji Watanabe, Shinnosuke Takamichi, Hiroshi Saruwatari
Section: D Observation of cross-attention map.
In this section, we show the cross-attention maps obtained during inference from both the byte-based proposed and the baseline methods defined in ยง 3.1. In both cases, an utterance sampled from an unseen language (Spanish) was used. Fig. 6 shows the cross-attention maps corresponding to the first attention head in the fifth and sixth layers of the decoder. It should be noted that we have cross-attention maps for other heads and layers. The results shown in Fig. 6 are derived from attention maps that have a diagonal shape in higher layers for both the baseline and the proposed methods. The top part of Fig. 6 represents results from the fifth layer, while the bottom part corresponds to results from the sixth layer. The left side of the figure shows the results of the baseline method, while the right side shows the results of the proposed method. We observe that the fifth layer of the baseline model shows a discontinuity in the attention map, which leads to instability of the linguistic content. Conversely, in the fifth layer of the proposed model, the attention map is significantly more continuous than in the baseline method. These results suggest that our unsupervised text pretraining can improve crossattention in the absence of paired speech-text data. The results are also reflected in the intelligibility difference between the baseline and the proposed methods presented in ยง 3.3.