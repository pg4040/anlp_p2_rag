Title: Long-tailed Extreme Multi-label Text Classification by the Retrieval of Generated Pseudo Label Descriptions
Authors: Ruohong Zhang, Yau-Shian Wang, Yiming Yang, Donghan Yu, Tom Vu, Likun Lei
Section: A Appendix
A.1 Experiments A.1.1 All-label Evaluation Metric We introduce the micro-averaged P@k as the metric for all-label prediction. Given a ranked list of the predicted labels for each test document, the micro-averaged P@k is: P@k = 1 k k∑ i=1 1y+i (pi) (7) where pi is the i-th label in the list p and 1y+i is the indicator function. A.1.2 More Baseline For the overall prediction of all labels, we also include the baselines of sparse classifiers: DisMEC (Babbar and Schölkopf, 2017), PfastreXML (Jain et al., 2016b), Parabel (Prabhu et al., 2018), Bonsai (Khandagale et al., 2019), and we use the published results for comparison. We provide an implementation of linear SVM model with our extracted tf-idf features as another sparse baseline, and a BERT-base classifier as another dense classifier (used to initialize DEPL). A.1.3 Implementation Details For the sparse model, since the public available BoW feature doesn’t have a vocabulary dictionary, we generate the tf-idf feature by ourselves. We tokenize and lemmatize the raw text with the spaCy (Honnibal and Montani, 2017) library and extract the tf-idf feature with the Sklearn (Pedregosa et al., 2011) library, with unigram whose df count is >= 2 and df frequency <= 70% of the total documents. We use the BERT model as the contextualize function for our retrieval model, which is initialized with a pretrained dense classifier. Specifically, we fine-tune a 12 layer BERT-base model with different learning rates for the BERT encoder, BERT pooler and the classifier. The learning rates are (1e − 5, 1e − 4, 1e − 3) for Wiki1031K and (5e − 5, 1e − 4, 2e − 3) for the rest datasets. For the negative sampling, we sample batch of 500 instances for Wiki10-31K, and 300 for EURLex-4K and AmazonCat-13K. For Wiki-500K dataset, we leverage the cluster-based algorithm in X-Transformer, and perform label re-ranking using our DEPL model to replace the linear model in XTransformer. We use a negative batch size of 500 for to train the re-ranker. We include 10 hard negatives predicted by the SVM model for each instances. We used learning rate 1e − 5 for fine-tuning the BERT of our retrieval model and 1e− 4 for the pooler and label embeddings. For the pseudo label descriptions, we concatenate the provided label description with the generated the top 20 keywords. The final length is truncated up to 32 tokens after BERT tokenization. We use length 16 of pseudo label description as the default setting for DEPL. A.1.4 Results in All-label Prediction The performance of our