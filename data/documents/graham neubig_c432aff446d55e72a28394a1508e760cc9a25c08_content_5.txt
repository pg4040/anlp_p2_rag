Title: Why do Nearest Neighbor Language Models Work?
Authors: Frank F. Xu, Uri Alon, Graham Neubig
Section: 4.2 Increasing the Softmax Capacity
find that longer words and words that appear in many different contexts have slightly better results with approximate nearest neighbors. Coincidentally, He et al. (2021) find that dimensionality reduction using PCA on datastore vectors (from 1024 to 512 dimensions) improves the perplexity of the original kNN-LM from 16.46 to 16.25, which can be explained by our findings as PCA may provide another source of “approximation” that contributes to regularization. Notably, similar effects, where an approximation component lead to better generalization, have been reported in other NLP tasks as well, and are sometimes referred to as “beneficial search bias”, when modeling errors cause the highest-scoring solution to not be the correct one: Meister et al. (2020b) suggest that “quite surprisingly, beam search often returns better results than exact inference due to beneficial search bias for NLP tasks.” Stahlberg and Byrne (2019) also conclude that “vanilla NMT in its current form requires just the right amount of beam search errors, which, from a modeling perspective, is a highly unsatisfactory conclusion indeed, as the model often prefers an empty translation”. 6 Probably Wrong Hypotheses for Why kNN-LMs Work The results in the previous sections are the result of extensive analysis and experimentation, in which we also tested a number of hypotheses that did not turn out to have a significant effect. Additional details of these hypotheses are detailed in Appendix E, and we hope that they may provide ideas for future improvements of retrieval-based LMs. Ensemble of Distance Metrics We hypothesized that the ensemble of two distance metrics: the standard inner product distance (which the LM uses) and the L2 distance (which the kNN component uses), is the key to the improvement. However, we found that similar gains can be achieved using the inner-product metric for the retrieved kNN. More details can be found in Appendix E.1. Ensembling of Two Models We hypothesized that the kNN component merely provides another model for ensembling. The improvement from kNN-LM is purely due to the ensembling effect of different models. However, we found that kNN-LM’s improvement is orthogonal to ensembling with a different base LM. More details can be found in Appendix E.5. Sparsification The mask-to-k(·) used by kNN retrieval induces sparsity in the distribution over the vocabulary, due to a small k (typically 1024) compared to the size of the vocabulary V (33K in our experiments and 260K in the original settings of Khandelwal et al. (2020b)). We hypothesized that kNN-LM increases the probability of the top-k entries while taking “probability mass” from the long