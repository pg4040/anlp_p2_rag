Title: EVALUATING SPEECH SYNTHESIS BY TRAINING RECOGNIZERS ON SYNTHETIC SPEECH
Authors: Dareen Alharthi, Roshan Sharma, Hira Dhamyal, Soumi Maiti, Bhiksha Raj, Rita Singh
Section: 4.4. Evaluation Metrics
MOS-Naturalness (MOS-N) : We conducted a crowdsourced Mean Opinion Score (MOS) evaluation to assess the naturalness of synthetic speech generated by each system, in comparison to real speech. We obtained 50 sentences from the LibriTTS test-clean dataset and another 50 from the LibriTTS test-other dataset, resulting in a total of 100 samples each for real speech, MQTTS, YourTTS and StyleTTS. Each sample was evaluated by 10 raters, who were instructed to rate the naturalness of the speech on a scale of 1 to 5, with 1 indicating poor and 5 indicating excellent quality. MOS-Intelligibility(MOS-I): We assessed intelligibility of spoken words by using nonsense sentences [26], effectively eliminating sentence structure and grammar from the evaluation. This absence of structure allowed listeners to only focus on the quality of the synthesized speech and not be distracted by the grammar. Participants were presented with a choice between the original sentence and a transcription generated by the Whisper-medium. We specifically selected 60 sentences with relatively high Word Error Rate (WER) from a pool of 200 random sentences generated by ChatGPT [27]. Among these, 30 sentences were short (less than 10 words), while the other 30 were long. This allowed us to evaluate the impact of sentence length variation on intelligibility. We generated synthetic speech using the three TTS systems for the 60 sentences using a test-clean set as a reference for the modelâ€™s speaker and style encoder. We used WebMushar [28] to create a test form along with Prolific for crowd-sourcing. Intelligibility of Synthetic Speech using WER from Pretrained ASR: We computed the WER for synthetic speech generated by three different systems using the Whisper medium multilingual. This model is pre-trained on real speech and evaluated on synthetic speech. This setting of training / testing demonstrates the traditional way that speech synthesis evaluation is performed. This evaluation was performed on both the test-clean and test-other datasets from LibriTTS. 5. EXPERIMENTAL RESULTS Table 1 reports the results of our experiments on Libri-TTS with the proposed evaluation method. We consider multiple metrics and report raw scores of the metric in the rows and rel- ative ranking scores in brackets next to the raw score. The first row, named WER shows the case when the model is trained on real data and evaluated on synthetic data. The last row shows our setting, where the model is trained on synthetic data and evaluated on real data. Based on the absolute raw numbers of the metric, we rank the TTS systems from 1 to 3 based on which one