Title: A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks
Authors: Yifan Peng, Kwangyoun Kim, Felix Wu, Brian Yan, Siddhant Arora, William Chen, Jiyang Tang, Suwon Shon, Prashant Sridhar, Shinji Watanabe
Section: 8. References
Learning Evaluation of Universal Representations of Speech,” in Proc. SLT, 2022. [43] G. Chen et al., “GigaSpeech: An Evolving, Multi-Domain ASR Corpus with 10,000 Hours of Transcribed Audio,” in Proc. Interspeech, 2021. [44] R. Sonobe, S. Takamichi, and H. Saruwatari, “Jsut corpus: free large-scale japanese speech corpus for end-to-end speech synthesis,” arXiv preprint arXiv:1711.00354, 2017. [45] R. Cattoni, M. A. Di Gangi, L. Bentivogli et al., “Must-c: A multilingual corpus for end-to-end speech translation,” Computer speech & language, vol. 66, p. 101155, 2021. [46] J. Godfrey et al., “SWITCHBOARD: telephone speech corpus for research and development,” in Proc. ICASSP, 1992. [47] A. Rousseau, P. Deléglise, Y. Esteve et al., “Enhancing the tedlium corpus with selected data for language modeling and more ted talks.” in LREC, 2014, pp. 3935–3939. [48] “VoxForge.” [Online]. Available: http://www.voxforge.org/ [49] D. B. Paul and J. Baker, “The design for the Wall Street Journalbased CSR corpus,” in Proc. Workshop on Speech and Natural Language, 1992. [50] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” Neural computation, vol. 9, no. 8, pp. 1735–1780, 1997. [51] G. Huang, Y. Sun, Z. Liu, D. Sedra, and K. Q. Weinberger, “Deep networks with stochastic depth,” in Proc. ECCV, 2016. [52] B. Yan et al., “CTC alignments improve autoregressive translation,” arXiv preprint arXiv:2210.05200, 2022. [53] E. Bastianelli, A. Vanzo et al., “SLURP: A Spoken Language Understanding Resource Package,” in Proc. EMNLP, 2020. [54] S. Shon, A. Pasad, F. Wu, P. Brusco, Y. Artzi, K. Livescu, and K. J. Han, “Slue: New benchmark tasks for spoken language understanding evaluation on natural speech,” in Proc. ICASSP, 2022. [55] P. Tomasello, A. Shrivastava, D. Lazar et al., “STOP: A Dataset for Spoken Task Oriented Semantic Parsing,” in Proc. SLT, 2022. [56] Y. Peng, S. Arora, Y. Higuchi et al., “A study on the integration of pre-trained ssl, asr, lm and slu models for spoken language understanding,” in Proc. SLT, 2022. [57] S. Chen et al., “Wavlm: Large-scale self-supervised pre-training for full stack speech processing,” IEEE J. Sel. Topics Signal Process., vol. 16, no. 6, pp. 1505–1518, 2022. [58] A. Baevski et al., “Efficient self-supervised learning with contextualized target representations for vision, speech and language,” arXiv preprint arXiv:2212.07525, 2022.