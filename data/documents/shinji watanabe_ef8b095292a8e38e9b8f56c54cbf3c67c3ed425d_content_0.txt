Title: Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation
Authors: Yu-Kuan Fu, Liang-Hsuan Tseng, Jiatong Shi, Chen-An Li, Tsu-Yuan Hsu, Shinji Watanabe, Hung-yi Lee
Section: 6 Conclusion
In this work, we build cascaded unsupervised speech-to-speech translation (US2ST) systems in several translation directions. To further improve the performance and mitigate the error propagation problems, we propose denoising back-translation (DBT), which is a novel method to improve the robustness of UNMT. DBT generally improves the performance of unsupervised speech translation (UST) across all the language pairs that we have experimented on. Without leveraging any paired data, our speech translation results are even better than some previous supervised methods. Additionally, we analyze the performance of each part in different settings individually; and we also attempt to integrate the TDN into the UNMT to reduce inference latency. In the future, we may investigate more techniques that can reduce the error propagation problems between different unsuper- vised cascaded modules; or conduct direct UST or US2ST. Limitations In this work, we have handled the problem of error propagation among UASR, TDN, and UNMT. Nevertheless, we didnâ€™t resolve that between UTTS and other modules, which may lead to a lower score of the S2ST result. Our methodology works for most languages, however, our US2ST is based on UNMT for unpaired text data. Therefore, it is limited to written languages. We believe that our denoise backtranslation brings new insights to US2ST and can extend to unwritten language setups. Ethics Statement Our works build an effective UST cascaded system and try to mitigate the error propagation and inference latency. The communities might be interested in how to build a direct US2TT or even US2ST system or how to further improve the performance of the UST system.