Title: ADVANTAGE-BASED OFFLINE POLICY GRADIENTS
Authors: Ashutosh Baheti, Ximing Lu, Faeze Brahman, Ronan Le Bras, Maarten Sap, Mark Riedl
Section: D LIMITATIONS AND SOCIETAL AND ETHICAL CONSIDERATIONS
We discuss some of the limitations of Advantage-Leftover Lunch RL. First, A-LOL requires that the evaluation metric aligns with the provided rewards. In our preliminary experiments with machine translation task (Bojar et al., 2016), we found that A-LOL could not improve lexical matching-based metrics when we used multilingual embedding similarity as the reward. Furthermore, a single sequence-level reward for each instance will obscure disagreement in how humans would label sequences (i.e., average out value pluralism). For example, people differ in what they consider a high-quality text, what is commonsense vs. domain-specific knowledge, etc. (de Marneffe et al., 2012; Plank, 2022). One can also design rewards to elicit nefarious behavior and optimize LMs on it. Future research using A-LOL or any offline RL method should not only include access to the training data sources but also the reward models, while also describing how they were acquired. Although less than other RL methods, A-LOL is also susceptible to reward hacking (Skalse et al., 2022; Pang et al., 2023) by learning bad sequences as “critical” actions (Kumar et al., 2022). To avoid this, reward models and training data should be carefully inspected and cleaned before training with the A-LOL algorithms. Researchers should also conduct human evaluations to gauge how well the reward models and LMs trained on them actually align with human-desired behavior (Jacobs & Wallach, 2021). On the positive side, LOL RL allows for both stable and sample-efficient training of models on existing language data. Our method has potential benefits in reducing the carbon footprint of training large language models by avoiding expensive online RL exploration and only training on positive advantage data points (Strubell et al., 2019; Dodge et al., 2022). Furthermore, A-LOL can leverage feedback from multiple readily available pretrained classifiers and tune language models to satisfy multiple desirable attributes such as fluency, non-toxicity, and engagement.18 18Our experiment of “human-like” Reddit response generation task (§5) is not intended towards making bots that post comments on Reddit or other social media. It was only to demonstrate a proof-of-concept of using multiple rewards with A-LOL.