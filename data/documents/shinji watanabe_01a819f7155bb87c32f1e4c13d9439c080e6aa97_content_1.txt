Title: JOINT PREDICTION AND DENOISING FOR LARGE-SCALE MULTILINGUAL SELF-SUPERVISED LEARNING
Authors: William Chen, Jiatong Shi, Brian Yan, Dan Berrebbi, Wangyou Zhang, Yifan Peng, Xuankai Chang, Soumi Maiti, Shinji Watanabe
Section: 8. REFERENCES
6, pp. 1493–1504, 2022. [20] S. Cahyawijaya et al., “Cross-lingual cross-age group adaptation for low-resource elderly speech emotion recognition,” in Proc. Interspeech, 2023. [21] T. Ashihara et al., “SpeechGLUE: How well can self-supervised speech models capture linguistic knowledge?” In Proc. Interspeech, 2023. [22] C. Sikasote et al., “Zambezi voice: A multilingual speech corpus for zambian languages,” in Proc. Interspeech, 2023. [23] A. Conneau et al., “Unsupervised cross-lingual representation learning for speech recognition,” arXiv preprint arXiv:2006.13979, 2020. [24] A. Babu et al., “XLS-R: Self-supervised cross-lingual speech representation learning at scale,” arXiv preprint arXiv:2111.09296, 2021. [25] V. Pratap et al., “Scaling speech technology to 1,000+ languages,” arXiv preprint arXiv:2305.13516, 2023. [26] A. Lee et al., “Textless speech-to-speech translation on real data,” in Proc. NAACL, 2022, pp. 860–872. [27] C.-C. Chiu et al., “Self-supervised learning with random-projection quantizer for speech recognition,” in Proc. ICML, vol. 162, 2022, pp. 3915–3924. [28] S. Zaiem et al., “Speech self-supervised representation benchmarking: Are we doing it right?” In Proc. Interspeech, 2023. [29] J. Shi et al., “ML-SUPERB: MultiLingual Speech Universal PERformance Benchmark,” in Proc. Interspeech, 2023. [30] Y. Zhang et al., “Google USM: Scaling automatic speech recognition beyond 100 languages,” arXiv preprint arXiv:2303.01037, 2023. [31] R. Ardila et al., “Common voice: A massively-multilingual speech corpus,” in Proceedings of the Twelfth Language Resources and Evaluation Conference, 2020, pp. 4218– 4222. [32] S. Watanabe et al., “ESPnet: End-to-end speech processing toolkit,” in Proc. Interspeech, 2018, pp. 2207– 2211. [33] C. K. Reddy et al., “The Interspeech 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results,” Proc. Interspeech, 2020. [34] C. Wang et al., “VoxPopuli: A large-scale multilingual speech corpus for representation learning, semisupervised learning and interpretation,” in Proc. ACL, 2021, pp. 993–1003. [35] V. Pratap et al., “MLS: A Large-Scale Multilingual Dataset for Speech Research,” in Proc. Interspeech, 2020, pp. 2757–2761. [36] M. J. Gales et al., “Speech recognition and keyword spotting for low-resource languages: Babel project research at cued,” in Fourth International workshop on spoken language technologies for under-resourced languages (SLTU-2014), 2014, pp. 16–23. [37] A. Conneau et al., “FLEURS: Few-shot learning evaluation of universal representations of speech,” in Proc. SLT, 2023, pp. 798–805. [38] A. Vaswani et al., “Attention is all you need,” Proc. NeurIPS, vol. 30, 2017. [39] W. Chen et al., “Reducing barriers to self-supervised learning: Hubert pre-training with academic compute,” in Proc. Interspeech, 2023. [40] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” in Proc. ICLR, 2015. [41] J. Shi et al., “Exploration on HuBERT with multiple