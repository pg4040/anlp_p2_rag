Title: TOWARD UNIVERSAL SPEECH ENHANCEMENT FOR DIVERSE INPUT CONDITIONS
Authors: Wangyou Zhang, Kohei Saijo, Zhong-Qiu Wang, Shinji Watanabe, Yanmin Qian
Section: 5. REFERENCES
text-to-speech synthesis system using deep recurrent neural networks,” in Interspeech, 2016, pp. 352–356. [23] C. K. Reddy et al., “The INTERSPEECH 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results,” in Interspeech, 2020, pp. 2492–2496. [24] E. Vincent et al., “An analysis of environment, microphone and data simulation mismatches in robust speech recognition,” Computer Speech & Language, vol. 46, pp. 535–557, 2017. [25] K. Kinoshita et al., “The REVERB challenge: A common evaluation framework for dereverberation and recognition of reverberant speech,” in Proc. IEEE WASPAA, 2013, pp. 1–4. [26] M. Maciejewski et al., “WHAMR!: Noisy and reverberant single-channel speech separation,” in ICASSP, 2019, pp. 696– 700. [27] C. Li et al., “ESPnet-SE: End-to-end speech enhancement and separation toolkit designed for ASR integration,” in Proc. IEEE SLT, 2021, pp. 785–792. [28] L. Yang et al., “TFPSNet: Time-frequency domain path scanning network for speech separation,” in ICASSP, 2022, pp. 6842–6846. [29] Z.-Q. Wang et al., “TF-GridNet: Making time-frequency domain models great again for monaural speaker separation,” in ICASSP, 2023, pp. 1–5. [30] J. Chen et al., “Dual-path transformer network: Direct contextaware modeling for end-to-end monaural speech separation,” in Interspeech, 2020, pp. 2642–2646. [31] S. Cornell et al., “Learning filterbanks for end-to-end acoustic beamforming,” in ICASSP, 2022, pp. 6507–6511. [32] M. S. Burtsev et al., “Memory transformer,” arXiv preprint arXiv:2006.11527, 2020. [33] X. L. Li and P. Liang, “Prefix-Tuning: Optimizing continuous prompts for generation,” in Proc. ACL/IJCNLP, 2021, pp. 4582–4597. [34] K.-W. Chang et al., “An exploration of prompt tuning on generative spoken language model for speech processing tasks,” in Interspeech, 2022, pp. 5005–5009. [35] J. R. Hershey et al., “Deep clustering: Discriminative embeddings for segmentation and separation,” in ICASSP, 2016, pp. 31–35. [36] Z.-Q. Wang et al., “Multi-channel deep clustering: Discriminative spectral and spatial embeddings for speaker-independent speech separation,” in ICASSP, 2018, pp. 1–5. [37] J. Le Roux et al., “SDR—half-baked or well done?” in ICASSP, 2019, pp. 626–630. [38] Y.-J. Lu et al., “Towards low-distortion multi-channel speech enhancement: The ESPnet-SE submission to the L3DAS22 challenge,” in ICASSP, 2022, pp. 9201–9205. [39] A. W. Rix et al., “Perceptual evaluation of speech quality (PESQ)—a new method for speech quality assessment of telephone networks and codecs,” in ICASSP, vol. 2, 2001, pp. 749– 752. [40] C. H. Taal et al., “An algorithm for intelligibility prediction of time–frequency weighted noisy speech,” IEEE Trans. ASLP., vol. 19, no. 7, pp. 2125–2136, 2011. [41] E. Vincent et al., “Performance measurement in blind audio source separation,” IEEE Trans. ASLP., vol. 14, no. 4, pp. 1462–1469,