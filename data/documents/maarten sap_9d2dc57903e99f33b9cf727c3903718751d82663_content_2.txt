Title: ADVANTAGE-BASED OFFLINE POLICY GRADIENTS
Authors: Ashutosh Baheti, Ximing Lu, Faeze Brahman, Ronan Le Bras, Maarten Sap, Mark Riedl
Section: C ADDITIONAL EXPERIMENTS AND RESULTS
fluent, engaging, and diverse. Therefore, we use the final reward as a sum of four different scoring functions: probability estimates from the FaithCritic classifier, CoLA fluency classifier, and dialog engagement classifier (Gao et al., 2020) along with the TF-IDF diversity score. We evaluate all LMs using the rewards obtained on Dtest. Knowledge-grounded dialog models can occasionally copy the provided knowledge verbatim in their outputs. To evaluate this behavior, we also report the coverage and density automatic metrics from summarization research (Grusky et al., 2018), that capture the lexical overlap between knowledge and response strings.17 Similar to our previous experiments, we also calculate the average response length and corpus-level distinct-n-gram diversity metrics (Li et al., 2016). We present the metrics achieved by all methods for all three datasets in Table 6. Results We again observe A-LOL models outperform reference LM and all other LMs trained with NLL and reward-based baselines in all three dataset settings. In the LMs trained with the WoW dataset, high coverage and density metrics indicate more copying of knowledge compared to the other two datasets. Interestingly, A-LOL models decrease the average density compared to models 17Coverage is the average lexical overlap between knowledge and response, whereas, Density is the average length of extractive spans in response that are copied from the knowledge. trained with NLL and reward-based objectives. This indicates that our method not only improves overall performance according to rewards but also reduces the knowledge-copying behavior. Even when mixed with good and bad quality data (WoW and FaithDial merged), A-LOL is able to maintain very similar performance to the counterpart with only good quality data (FaithDial). We find that A-LOL identified a negative advantage in 39% of WoW’s training data, 10% of FaithDial’s training data, and 55% of merged training instances. Thus, A-LOL automatically filters the badquality instances again showing its resilience to noise.