Title: Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning
Authors: Xuankai Chang, Brian Yan, Yuya Fujita, Takashi Maekaku, Shinji Watanabe
Section: 5. Acknowledgements
449–12 460, 2020. [16] W.-N. Hsu et al., “Hubert: Self-supervised speech representation learning by masked prediction of hidden units,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451–3460, 2021. [17] S. Chen et al., “Wavlm: Large-scale self-supervised pre-training for full stack speech processing,” IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505–1518, 2022. [18] A. Baevski et al., “Data2vec: A general framework for selfsupervised learning in speech, vision and language,” in Proc. ICML, 2022, pp. 1298–1312. [19] A. Mohamed et al., “Self-supervised speech representation learning: A review,” IEEE Journal of Selected Topics in Signal Processing, 2022. [20] S.-w. Yang et al., “Superb: Speech processing universal performance benchmark,” arXiv preprint arXiv:2105.01051, 2021. [21] X. Chang et al., “An exploration of self-supervised pretrained representations for end-to-end speech recognition,” in Proc. ASRU, 2021, pp. 228–235. [22] B. Thomas, S. Kessler, and S. Karout, “Efficient adapter transfer of self-supervised speech models for automatic speech recognition,” in Proc. ICASSP, 2022, pp. 7102–7106. [23] A. Nautsch et al., “Preserving privacy in speaker and speech characterisation,” Computer Speech & Language, vol. 58, pp. 441–480, 2019. [24] A. Van Den Oord, O. Vinyals, et al., “Neural discrete representation learning,” Advances in neural information processing systems, vol. 30, 2017. [25] A. Baevski, S. Schneider, and M. Auli, “Vq-wav2vec: Selfsupervised learning of discrete speech representations,” in Proc. ICLR. [26] A. Baevski and A. Mohamed, “Effectiveness of self-supervised pre-training for asr,” in Proc. ICASSP, 2020, pp. 7694–7698. [27] J. D. M.-W. C. Kenton and L. K. Toutanova, “Bert: Pre-training of deep bidirectional transformers for language understanding,” in Proceedings of NAACL-HLT, 2019, pp. 4171–4186. [28] S. Davis and P. Mermelstein, “Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences,” IEEE transactions on acoustics, speech, and signal processing, vol. 28, no. 4, pp. 357–366, 1980. [29] A. Lee et al., “Direct speech-to-speech translation with discrete units,” in Proc. ACL, 2022, pp. 3327–3339. [30] T. Hayashi and S. Watanabe, “Discretalk: Text-to-speech as a machine translation problem,” arXiv preprint arXiv:2005.05525, 2020. [31] N. Zeghidour et al., “Soundstream: An end-to-end neural audio codec,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, pp. 495–507, 2021. [32] C. Wang et al., “Neural codec language models are zero-shot text to speech synthesizers,” arXiv preprint arXiv:2301.02111, 2023. [33] A. Défossez et al., “High fidelity neural audio compression,” arXiv preprint arXiv:2210.13438, 2022. [34] Y. Masuyama et al., “End-to-end integration of speech recognition, dereverberation, beamforming, and self-supervised learning representation,” arXiv preprint arXiv:2210.10742, 2022. [35] P. Guo et al., “Recent