Title: Contextualized Networks Reveal Heterogeneous Transcriptomic Regulation in Tumors at Sample-Specific Resolution
Authors: Caleb N. Ellington, Benjamin J. Lengerich, Thomas B.K. Watkins, Jiekun Yang, Manolis Kellis, Eric P. Xing
Section: 
in which edges are the pairwise precision265 values representing conditional dependencies between nodes, and (3) Neighborhood regression networks, in which266 edges represent directed linear relationships between nodes. The key challenge for each network class is to define a267 differentiable loss function ℓM that is proportional to the negative log probability of our contextualized network model.268 f̂ , Â = argmax f,A N ∑ n=1 log (PM(Xn∣ϕ(Cn; f,A))) = argmin f,A N ∑ n=1 ℓM (ϕ(Cn; f,A),Xn) The loss objective can be used in the end-to-end optimization, solving for the context encoder and the network269 archetypes simultaneously, and subsequently inferring the context-specific parameters θ. Below, we outline a uni-270 fying linear parameterization of each network loss. Implementation details are discussed in Appendix S1.271 Contextualized Neighborhood Regression272 We first apply contextualization to the graph variable selection algorithm proposed by Meinhausen and Buhlmann273 [54]. The direct relationship of this model to lasso regression links contextualized neighborhood regression to original274 works on contextualized linear models [41], making it a convenient stepping stone toward the graphical models in the275 sequel. The model is a Gaussian graphical model where X ∼ N(0,Σ) and Σ has sparse off-diagonal entries. The276 algorithm, neighborhood regression, recovers edges between nodes with non-zero partial correlations by solving the277 lasso regression for every feature Xi, given every other feature X−i, where regression maximizes P (Xi∣X−i) via the278 loss279 θ̂i = argmin θ ∥Xi −X−iθ∥22 + λ∥θ∥1 resulting in edges with source Xj for every j ≠ i and sink Xi and strength θij , or no edge if θij = 0. Equivalently, we280 parameterize the neighborhood selection objective using the square matrix of network edge parameters θ ∈ Rp×p.281 12 θ̂ = argmin θ ∥X −Xθ∥2F + λ∑ i ∥θi∥1 s.t. diag(θ) = [0] Where the contextualized neighborhood network objective replaces θ for each sample with a context-specific θn =282 ϕ(Cn; f,A). Finally, we define a function ϕ′ to mask the diagonal of θ, presenting the loss function ℓNN for contex-283 tualized neighborhood regression networks284 ℓNN(ϕ(C; f,A),X) = ∥X −Xϕ ′ (C; f,A)∥22 + λ∑ i ∥ϕ′(C; f,A)i∥1 ϕ′(C; f,A) = (1 − I)⊗ ϕ(C; f,A) where ⊗ is the hadamard product.285 Contextualized Markov Networks286 Linear regression and Gaussian graphical models are intrinsically related, allowing us to extend work on contextualized287 linear models to various graphical representations of the Gaussian graphical model. To estimate sample-specific288 precision matrices representing the conditional dependency structure of an undirected graphical model or Markov289 network, we assume the data is drawn from X ∼ N(0,Ω−1) where Ω