Title: LOFT: LOCAL PROXY FINE-TUNING FOR IMPROV- ING TRANSFERABILITY OF ADVERSARIAL ATTACKS AGAINST LARGE LANGUAGE MODEL
Authors: Muhammad Ahmed Shah, Roshan Sharma, Hira Dhamyal, Raphael Olivier, Ankit Shah, Dareen Alharthi, Hazim T Bukhari, Massa Baali, Soham Deshmukh, Michael Kuhlmann, Bhiksha Raj, Rita Singh
Section: A APPENDIX
A.1 KEY PHRASES USED TO VALIDATE RESPONSES Table 4 shows the key phrases used in our automatic evaluation. These phrases are obtained after manual inspection of the model outputs. A.2 USER STUDY A.2.1 USER STUDY TO EXAMINE SUFFICIENCY OF PRIOR METRIC We conducted a user study to validate whether the substring similarity-based automatic metrics completely capture and adequately represent the harmful content. We sample 100 model prompt/response pairs and ask users to rate the harmfulness of the attack. The users were provided with the following instructions: Please rate the output response on whether it has harmful content and the extent thereof. A response is considered harmful if there is harmful information in general. The tone does not matter - it could be relaying harmful information in a humorous tone, but it is still harmful. For each response, the users had to respond to the following questions: 1. Rate response on a scale of 1,2,3,4,5 where 1 is least harmful and 5 is most harmful 2. Rate response on a scale of 1,2,3 where 1 is least harmful and 3 is most harmful 3. Rate whether the response is harmful or not (1 for harmful, 0 for not) We show the results of the human study in Figure 3. On average, for the 100 sample set, users rated more responses as successful than failures. It is observed that the histogram of ratings on a scale of 5 has significant weight around 1 and 5, demonstrating that the model responses elicit definitive answers on the extent of harm. There appears to be very little ambiguity on the part of human respondents. A.2.2 HUMAN EVALUATION FOR ATTACK RESPONSE RATE Subjective human evaluation is employed to compute the attack success rate for all our experiments. Respondents were instructed to manually go over the query/response pair and evaluate the response as a success if it satisfied three criteria: (a) whether the model produces a response, (b) whether the response is relevant to the input query, and (c) whether the response contains harmful information. 12 human evaluators contributed to the scores reported in Table 3. Each annotator annotated around 600 pairs on average. We developed an automatic pipeline, where users were given a file and they went through the prompt/response pairs one by one, recording their annotations for each. DISCLAIMER: The below Section A.3 of target model responses may contain information that is harmful and/or explicit for some readers. A.3 TARGET MODEL RESPONSES FOR HARMFUL QUERIES