Title: CROSS-MODAL MULTI-TASKING FOR SPEECH-TO-TEXT TRANSLATION VIA HARD PARAMETER SHARING
Authors: Brian Yan, Xuankai Chang, Antonios Anastasopoulos, Yuya Fujita, Shinji Watanabe
Section: 7. REFERENCES
[1] J. Pino et al., “Harnessing indirect training data for end-to-end automatic speech translation: Tricks of the trade,” Proc. IWSLT, 2019. [2] H. Inaguma, T. Kawahara, and S. Watanabe, “Source and target bidirectional knowledge distillation for end-to-end speech translation,” Proc. NAACL, 2021. [3] Y. Jia et al., “Leveraging unsupervised and weakly-supervised data to improve direct speech-to-speech translation,” Interspeech, 2022. [4] S. Ruder, “An overview of multi-task learning in deep neural networks,” arXiv preprint arXiv:1706.05098, 2017. [5] J. Ao et al., “Speecht5: Unified-modal encoder-decoder pre-training for spoken language processing,” Proc. ACL, 2021. [6] A. Bapna et al., “Mslam: Massively multilingual joint pre-training for speech and text,” arXiv preprint arXiv:2202.01374, 2022. [7] Y. Tang et al., “Unified speech-text pre-training for speech translation and recognition,” in Proc. ACL, 2022. [8] R. Ye, M. Wang, and L. Li, “Cross-modal contrastive learning for speech translation,” in Proc. NAACL, 2022. [9] Q. Fang et al., “STEMM: Self-learning with speech-text manifold mixup for speech translation,” in Proc. ACL, 2022. [10] Z. Chen et al., “Maestro: Matched speech text representations through modality matching,” Proc. Interspeech, 2022. [11] X. Cheng et al., “M 3 st: Mix at three levels for speech translation,” in Proc. ICASSP, 2023. [12] S. Ouyang, R. Ye, and L. Li, “WACO: Word-aligned contrastive learning for speech translation,” in Proc. ACL, 2023. [13] J. Wu et al., “On decoder-only architecture for speech-to-text and large language model integration,” arXiv:2307.03917, 2023. [14] X. Chang et al., “Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning,” in Proc. Interspeech, 2023. [15] P. K. Rubenstein et al., “Audiopalm: A large language model that can speak and listen,” arXiv preprint arXiv:2306.12925, 2023. [16] T. Hayashi and S. Watanabe, “Discretalk: Text-to-speech as a machine translation problem,” arXiv preprint arXiv:2005.05525, 2020. [17] K. Lakhotia et al., “On generative spoken language modeling from raw audio,” TACL, 2021. [18] C. Wang et al., “Neural codec language models are zero-shot text to speech synthesizers,” arXiv preprint arXiv:2301.02111, 2023. [19] M. Hassid et al., “Textually pretrained speech language models,” arXiv preprint arXiv:2305.13009, 2023. [20] A. Lee et al., “Direct speech-to-speech translation with discrete units,” in Proc. ACL, 2022. [21] X. Li, Y. Jia, and C.-C. Chiu, “Textless direct speech-to-speech translation with discrete speech representation,” in Proc. ICASSP, 2023. [22] S. Chen et al., “Wavlm: Large-scale self-supervised pre-training for full stack speech processing,” J-STSP, 2022. [23] M. A. Di Gangi et al., “Must-c: A multilingual speech translation corpus,” in Proc. NAACL, 2019. [24] O. Bojar et al., “Findings of the 2016 conference on machine translation,” in