Title: MULTI-CHANNEL TARGET SPEAKER EXTRACTION WITH REFINEMENT: THE WAVLAB SUBMISSION TO THE SECOND CLARITY ENHANCEMENT CHALLENGE
Authors: Samuele Cornell, Zhong-Qiu Wang, Yoshiki Masuyama, Shinji Watanabe, Manuel Pariente, Nobutaka Ono
Section: 4. REFERENCES
[1] Z.-Q. Wang, S. Cornell, S. Cho, Y. Lee, B.-Y. Kim, and S. Watanabe, “TF-GridNet: Making time-frequency domain models great again for monaural speaker separation,” in submission to ICASSP 2023, 2022. [2] Y.-J. Lu, S. Cornell, X. Chang, W. Zhang, C. Li, Z. Ni, Z.-Q. Wang, and S. Watanabe, “Towards low-distortion multi-channel speech enhancement: The ESPNET-SE submission to the L3DAS22 challenge,” in Proc. ICASSP, 2022, pp. 9201–9205. [3] Z.-Q. Wang, G. Wichern, S. Watanabe, and J. L. Roux, “STFT-domain neural speech enhancement with very low algorithmic latency,” arXiv preprint arXiv:2204.09911, 2022. [4] Z.-Q. Wang, P. Wang, and D. Wang, “Multi-microphone complex spectral mapping for utterance-wise and continuous speech separation,” IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 29, pp. 2001– 2014, 2021. [5] S. Gannot, E. Vincent, S. Markovich-Golan, and A. Ozerov, “A consolidated perspective on multi-microphone speech enhancement and source separation,” IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 25, pp. 692–730, 2017. [6] Z.-Q. Wang, G. Wichern, and J. Le Roux, “Leveraging low-distortion target estimates for improved speech enhancement,” arXiv preprint arXiv:2110.00570, 2021. [7] J. R. Hershey, Z. Chen, J. Le Roux, and S. Watanabe, “Deep clustering: Discriminative embeddings for segmentation and separation,” in Proc. ICASSP, 2016, pp. 31–35. [8] E. Perez, F. Strub, H. De Vries, V. Dumoulin, and A. Courville, “FiLM: Visual reasoning with a general conditioning layer,” in Proc. AAAI, vol. 32, no. 1, 2018. [9] D. Byrne and H. Dillon, “The National Acoustic Laboratories’(NAL) new procedure for selecting the gain and frequency response of a hearing aid,” Ear and hearing, vol. 7, no. 4, pp. 257–265, 1986. [10] L. McCormack, V. Välimäki et al., “FFT-based dynamic range compression,” in Proc. Sound and Music Computing, 2017, pp. 5–8. [11] X. Ren, L. Chen, X. Zheng et al., “A neural beamforming network for BFormat 3D speech enhancement and recognition,” in Proc. MLSP, 2021. [12] Y.-J. Lu, X. Chang, C. Li, W. Zhang, S. Cornell, Z. Ni, Y. Masuyama, B. Yan, R. Scheibler, Z.-Q. Wang et al., “ESPnet-SE++: Speech enhancement for robust speech recognition, translation, and understanding,” Proc. Interspeech, 2022. [13] J. Towns, T. Cockerill, M. Dahan et al., “XSEDE: Accelerating scientific discovery,” Computing in Science & Engineering, vol. 16, no. 5, pp. 62– 74, 2014. [14] N. A. Nystrom, M. J. Levine, R. Z. Roskies, and J. R. Scott, “Bridges: a uniquely flexible HPC resource for new communities and data analytics,” in Proc. XSEDE, 2015, pp. 1–8.