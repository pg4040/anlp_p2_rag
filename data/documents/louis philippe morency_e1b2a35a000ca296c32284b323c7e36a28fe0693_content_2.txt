Title: FACTORIZED CONTRASTIVE LEARNING: Going Beyond Multi-view Redundancy
Authors: Paul Pu Liang, Zihao Deng, Martin Q. Ma, James Zou, Louis-Philippe Morency, Ruslan Salakhutdinov
Section: D.2 Datasets
Sarcasmaholics Anonymous. It contains audiovisual utterances together with the textual context. We use the preprocessed features of the vision and text modalities for doing contrastive learning and performing sarcasm detection. In our experiments, we encode both the vision and text modalities using Transformer encoders with 5 heads and 5 layers, and map them to 40-dimensional embeddings. We train the model for 100 epochs using the Adam optimizer with a 1e-4 learning rate. 6. IRFL [87] is a dataset for understanding multimodal figurative languages. It contains 6,697 matching images and figurative captions (rather than literal captions) of three types of figurative languages: idiom, simile, and metaphor. The original data for the matching task is provided in the form of 1 caption, 3 distractor images, and 1 matching image. We convert it into a fusion task by only collecting the matching image and text pairs and assigning labels using the type of figurative language it belongs to. For this dataset, we do not train from scratch. Instead, we performed continued pretraining using our proposed objectives on pretrained CLIP [60] models. We used the CLIP-VIT-B/32 model and its pretrained image and text encoders. We performed training for 10 epochs using the Adam optimizer with a 1e-6 learning rate.