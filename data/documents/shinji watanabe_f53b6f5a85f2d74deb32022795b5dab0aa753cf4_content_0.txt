Title: Deep Speech Synthesis from MRI-Based Articulatory Representations
Authors: Peter Wu, Tingle Li, Yijing Lu, Yubin Zhang, Jiachen Lian, Alan W Black, Louis Goldstein, Shinji Watanabe, Gopala K. Anumanchipalli
Section: 9. References
[1] T. Hayashi, R. Yamamoto, T. Yoshimura, P. Wu, J. Shi, T. Saeki, Y. Ju, Y. Yasuda, S. Takamichi, and S. Watanabe, “Espnet2-tts: Extending the edge of tts research,” arXiv preprint arXiv:2110.07840, 2021. [2] P. Wu, J. Shi, Y. Zhong, S. Watanabe, and A. W. Black, “Crosslingual transfer for speech processing using acoustic language similarity,” in ASRU, 2021. [3] D. Lim, S. Jung, and E. Kim, “Jets: Jointly training fastspeech2 and hifi-gan for end to end text to speech,” 09 2022, pp. 21–25. [4] A. Polyak, Y. Adi, J. Copet, E. Kharitonov, K. Lakhotia, W.-N. Hsu, A. Mohamed, and E. Dupoux, “Speech Resynthesis from Discrete Disentangled Self-Supervised Representations,” in Interspeech, 2021. [5] K. Lakhotia, E. Kharitonov, W.-N. Hsu, Y. Adi, A. Polyak, B. Bolte, T.-A. Nguyen, J. Copet, A. Baevski, A. Mohamed et al., “On generative spoken language modeling from raw audio,” Transactions of the Association for Computational Linguistics, vol. 9, pp. 1336–1354, 2021. [6] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., “Wavlm: Large-scale selfsupervised pre-training for full stack speech processing,” IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505–1518, 2022. [7] K. Deng, S. Watanabe, J. Shi, and S. Arora, “Blockwise streaming transformer for spoken language understanding and simultaneous speech translation,” Interspeech, 2022. [8] G. K. Anumanchipalli, J. Chartier, and E. F. Chang, “Speech synthesis from neural decoding of spoken sentences,” Nature, vol. 568, no. 7753, pp. 493–498, 2019. [9] S. L. Metzger, J. R. Liu, D. A. Moses, M. E. Dougherty, M. P. Seaton, K. T. Littlejohn, J. Chartier, G. K. Anumanchipalli, A. TuChan, K. Ganguly et al., “Generalizable spelling using a speech neuroprosthesis in an individual with severe limb and vocal paralysis,” Nature Communications, vol. 13, no. 1, p. 6510, 2022. [10] G. Fant, “What can basic research contribute to speech synthesis?” Journal of Phonetics, vol. 19, no. 1, pp. 75–90, 1991. [11] P. Rubin, T. Baer, and P. Mermelstein, “An articulatory synthesizer for perceptual research,” The Journal of the Acoustical Society of America, vol. 70, no. 2, pp. 321–328, 1981. [12] C. Scully, “Articulatory synthesis,” in Speech production and speech modelling. Springer, 1990, pp. 151–186. [13] P. Wu, S. Watanabe, L. Goldstein, A. W. Black, and G. K. Anumanchipalli, “Deep speech synthesis from articulatory representations,” in Interspeech, 2022. [14] Y. Yu, A. H. Shandiz, and L. Tóth, “Reconstructing speech from real-time articulatory mri using neural vocoders,” in EUSIPCO, 2021, pp. 945–949. [15] G. Beguš, A. Zhou,