Title: MOSAIC: Learning Unified Multi-Sensory Object Property Representations for Robot Learning via Interactive Perception
Authors: Gyan Tatiya, Jonathan Francis, Ho-Hsiang Wu, Yonatan Bisk, Jivko Sinapov
Section: V. RESULTS
the effectiveness of the self-attention mechanism in enhancing the robot’s adaptability to diverse tasks. Fetch Object Results. The fetch object task, whose results are summarized in Table III, comprises five distinct levels designed to assess the robot’s ability to execute instructions. In L-1 (Level-1), the command specified the object category name. Our complete MOSAIC framework excelled in interactive behavior conditions, achieving an impressive target object selection rate of 99.0%, outperforming all baseline models. L-2 to L-5: These levels introduced object properties into the command instead of specifying the object category name. Generally, the interactive behaviors condition outperformed the non-interactive one, with our full MOSAIC model excelling in most cases. Interestingly, providing more object properties in the command led to better performance, exemplified by a higher target object selection rate in L-3 compared to L-2, across all conditions, except for “Look” without self-attention. This suggests that learning unified representations with self-attention prioritizes the most relevant object properties. L-4 presented greater challenges due to the inclusion of two distractor objects resembling the target object. Nevertheless, our complete MOSAIC framework with self-attention consistently outperformed all baselines. To evaluate the robot’s ability to fetch objects based on specific property categories, we considered L-5, where the command included only descriptive words related to specific property categories. For simplicity, we discuss five property categories. Deformability and Weight: In scenarios involving non-visual properties like deformability and weight, the interactive behaviors condition significantly outperformed the non-interactive one. This aligns with intuition, as visual observation alone may not suffice to disambiguate these properties. Transparency and Size: For visual properties like transparency and size, the interactive behaviors condition performed comparably to the non-interactive condition, suggesting that interaction with objects may not yield significantly more information in these scenarios. Shape: Intriguingly, for the shape property category, the interactive behaviors condition significantly outperformed the non-interactive one. This implies that interacting with objects enables the robot to observe them from various angles, enhancing its ability to predict object shape compared to merely observing from a top angle. In summary, our full MOSAIC framework demonstrated robust performance in the fetch object task, relying solely on unified representations without additional learning. These results underscore the adaptability and applicability of unified representations across diverse tasks, including those involving natural language instructions.