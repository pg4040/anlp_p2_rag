Title: VOXTLM: UNIFIED DECODER-ONLY MODELS FOR CONSOLIDATING SPEECH RECOGNITION, SYNTHESIS AND SPEECH, TEXT CONTINUATION TASKS
Authors: Soumi Maiti, Yifan Peng, Shukjae Choi, Jee-weon Jung, Xuankai Chang, Shinji Watanabe
Section: 6. REFERENCES
2 [14] Y.-A. Chung, Y. Zhang, W. Han et al., “w2v-BERT: Combining contrastive learning and masked language modeling for self-supervised speech pre-training,” in Proc. ASRU, 2021, pp. 244–250. 1 [15] N. Zeghidour, A. Luebs, A. Omran et al., “SoundStream: An endto-end neural audio codec,” IEEE/ACM TASLP, vol. 30, pp. 495–507, 2022. 1 [16] A. Défossez, J. Copet, G. Synnaeve, and Y. Adi, “High fidelity neural audio compression,” arXiv preprint arXiv:2210.13438, 2022. 1 [17] A. Bapna, Y.-a. Chung, N. Wu, A. Gulati, Y. Jia, J. H. Clark, M. Johnson, J. Riesa, A. Conneau, and Y. Zhang, “Slam: A unified encoder for speech and language modeling via speech-text joint pre-training,” arXiv preprint arXiv:2110.10329, 2021. 1 [18] Q. Dong, Z. Huang, C. Xu, Y. Zhao, K. Wang, X. Cheng, T. Ko, Q. Tian, T. Li, F. Yue et al., “Polyvoice: Language models for speech to speech translation,” arXiv e-prints, pp. arXiv–2306, 2023. 2 [19] T. Wang, L. Zhou, Z. Zhang, Y. Wu, S. Liu, Y. Gaur, Z. Chen, J. Li, and F. Wei, “Viola: Unified codec language models for speech recognition, synthesis, and translation,” arXiv preprint arXiv:2305.16107, 2023. 2 [20] D. Zhang, S. Li, X. Zhang, J. Zhan, P. Wang, Y. Zhou, and X. Qiu, “Speechgpt: Empowering large language models with intrinsic crossmodal conversational abilities,” arXiv preprint arXiv:2305.11000, 2023. 2 [21] S. Zhang, S. Roller, N. Goyal, M. Artetxe, M. Chen, S. Chen, C. Dewan, M. Diab, X. Li, X. V. Lin et al., “Opt: Open pre-trained transformer language models,” arXiv preprint arXiv:2205.01068, 2022. 2 [22] M. Hassid, T. Remez, T. A. Nguyen, I. Gat, A. Conneau, F. Kreuk, J. Copet, A. Defossez, G. Synnaeve, E. Dupoux et al., “Textually pretrained speech language models,” arXiv preprint arXiv:2305.13009, 2023. 2 [23] T. Kudo and J. Richardson, “Sentencepiece: A simple and language in- dependent subword tokenizer and detokenizer for neural text processing,” in Proc. EMNLP, 2018, pp. 66–71. 2 [24] R. Sennrich, B. Haddow, and A. Birch, “Neural machine translation of rare words with subword units,” in Proc. ACL, 2016, pp. 1715–1725. 2 [25] T. Kudo, “Subword regularization: Improving neural network translation models with multiple subword candidates,” in Proc. ACL, 2018, pp. 66–75. 2 [26] X. Chang, B. Yan, Y. Fujita, T. Maekaku, and S. Watanabe, “Exploration of efficient end-to-end asr using discretized input from selfsupervised learning,” arXiv preprint arXiv:2305.18108, 2023. 2, 4 [27] A. Vaswani, N. Shazeer, N. Parmar et al., “Attention is all you need,” in Proc. NeurIPS, 2017. 2 [28] J. Kong, J. Kim, and J. Bae, “Hifi-gan: Generative adversarial networks for