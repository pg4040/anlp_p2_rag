Title: TAPLOSS: A TEMPORAL ACOUSTIC PARAMETER LOSS FOR SPEECH ENHANCEMENT
Authors: Yunyang Zeng, Joseph Konan, Shuo Han, David Bick, Muqiao Yang, Anurag Kumar, Shinji Watanabe, Bhiksha Raj
Section: 6. REFERENCES
Hsieh, C. Yu, S.-W. Fu, X. Lu, and Y. Tsao, “Improving Perceptual Quality by Phone-Fortified Perceptual Loss Using Wasserstein Distance for Speech Enhancement,” in Proc. Interspeech, 2021, pp. 196–200. [16] M. Sambur, “Selection of acoustic features for speaker identification,” IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 23, no. 2, pp. 176–182, 1975. [17] R. Brown, “An experimental study of the relative importance of acoustic parameters for auditory speaker recognition,” Language and Speech, vol. 24, no. 4, pp. 295–310, 1981. [18] P. Tzirakis, G. Trigeorgis, M. A. Nicolaou, B. W. Schuller, and S. Zafeiriou, “End-to-end multimodal emotion recognition using deep neural networks,” IEEE Journal of selected topics in signal processing, vol. 11, no. 8, pp. 1301–1309, 2017. [19] G. d. Krom, “Some spectral correlates of pathological breathy and rough voice quality for different types of vowel fragments,” Journal of Speech, Language, and Hearing Research, vol. 38, no. 4, pp. 794– 811, 1995. [20] J. Hillenbrand, R. Cleveland, and R. Erickson, “Acoustic correlates of breathy vocal quality,” Journal of speech and hearing research, vol. 37, pp. 769–78, Sep. 1994. [21] H. Kasuya, S. Ogawa, Y. Kikuchi, and S. Ebihara, “An acoustic analysis of pathological voice and its application to the evaluation of laryngeal pathology,” Speech Communication, 1986. [22] C.-J. Peng, Y.-J. Chan, Y.-L. Shen, C. Yu, Y. Tsao, and T.-S. Chi, “Perceptual Characteristics Based Multi-objective Model for Speech Enhancement,” in Proc. Interspeech, 2022, pp. 211–215. [23] M. Yang, J. Konan, D. Bick, A. Kumar, S. Watanabe, and B. Raj, “Improving Speech Enhancement through Fine-Grained Speech Characteristics,” in Proc. Interspeech, 2022, pp. 2953–2957. [24] F. Eyben, M. Wöllmer, and B. Schuller, “Opensmile: The munich versatile and fast open-source audio feature extractor,” in Proceedings of the 18th ACM International Conference on Multimedia, ser. MM ’10, Firenze, Italy: Association for Computing Machinery, 2010, pp. 1459–1462. [25] A. Defossez, G. Synnaeve, and Y. Adi, “Real time speech enhancement in the waveform domain,” in Proc. Interspeech, 2020. [26] X. Hao, X. Su, R. Horaud, and X. Li, “Fullsubnet: A full-band and sub-band fusion model for real-time single-channel speech enhancement,” in Proc. ICASSP, 2021, pp. 6633–6637. [27] J. Pons, J. Serrà, S. Pascual, G. Cengarle, D. Arteaga, and D. Scaini, “Upsampling layers for music source separation,” arXiv preprint arXiv:2111.11773, 2021. [28] C. K. Reddy, V. Gopal, R. Cutler, E. Beyrami, R. Cheng, H. Dubey, S. Matusevych, R. Aichner, A. Aazami, S. Braun, et al., “The Interspeech 2020 deep noise suppression challenge: Datasets, subjective testing framework, and challenge results,” Proc. Interspeech, 2020. [29] J. F. Gemmeke, D.