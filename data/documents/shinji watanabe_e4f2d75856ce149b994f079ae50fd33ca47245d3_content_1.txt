Title: DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models
Authors: Yifan Peng, Yui Sudo, Shakeel Muhammad, Shinji Watanabe
Section: 7. References
and H. R. Kim, “FitHuBERT: Going Thinner and Deeper for Knowledge Distillation of Speech Self-Supervised Models,” in Proc. Interspeech, 2022. [14] T. Ashihara, T. Moriya, K. Matsuura, and T. Tanaka, “Deep versus Wide: An Analysis of Student Architectures for Task-Agnostic Knowledge Distillation of Self-Supervised Speech Models,” in Proc. Interspeech, 2022. [15] R. Reed, “Pruning algorithms-a survey,” IEEE Trans. on Neural Networks, vol. 4, no. 5, pp. 740–747, 1993. [16] Z. Liu, M. Sun, T. Zhou, G. Huang, and T. Darrell, “Rethinking the value of network pruning,” in Proc. ICLR, 2019. [17] P. Michel, O. Levy, and G. Neubig, “Are sixteen heads really better than one?” in Proc. NeurIPS, 2019. [18] A. Fan, E. Grave, and A. Joulin, “Reducing transformer depth on demand with structured dropout,” in Proc. ICLR, 2020. [19] Z. Wang, J. Wohlwend, and T. Lei, “Structured Pruning of Large Language Models,” in Proc. EMNLP, 2020. [20] M. Xia, Z. Zhong, and D. Chen, “Structured Pruning Learns Compact and Accurate Models,” in Proc. ACL, 2022. [21] P. Dong, S. Wang, W. Niu et al., “Rtmobile: Beyond real-time mobile acceleration of rnns for speech recognition,” in ACM/IEEE Design Automation Conference (DAC), 2020. [22] S. Wang, P. Lin, R. Hu et al., “Acceleration of LSTM With Structured Pruning Method on FPGA,” IEEE Access, 2019. [23] K. Tan and D. Wang, “Compressing Deep Neural Networks for Efficient Speech Enhancement,” in Proc. ICASSP, 2021. [24] C.-I. J. Lai, Y. Zhang, A. H. Liu, S. Chang, Y.-L. Liao, Y.-S. Chuang, K. Qian, S. Khurana, D. Cox, and J. Glass, “PARP: Prune, Adjust and Re-Prune for Self-Supervised Speech Recognition,” in Proc. NeurIPS, 2021. [25] Y. Peng, K. Kim, F. Wu, P. Sridhar, and S. Watanabe, “Structured Pruning of Self-Supervised Pre-trained Models for Speech Recognition and Understanding,” in Proc. ICASSP, 2023. [26] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in Proc. NeurIPS, 2017. [27] A. Pasad, J.-C. Chou, and K. Livescu, “Layer-Wise Analysis of a Self-Supervised Speech Representation Model,” in Proc. ASRU, 2021. [28] R. Wang, Q. Bai, J. Ao, L. Zhou, Z. Xiong, Z. Wei, Y. Zhang, T. Ko, and H. Li, “LightHuBERT: Lightweight and Configurable Speech Representation Learning with Once-for-All Hidden-Unit BERT,” in Proc. Interspeech, 2022. [29] H. Cai, C. Gan, T. Wang, Z. Zhang, and S. Han, “Once-for-all: Train one network and specialize it for efficient deployment,” in Proc. ICLR, 2020. [30] A. Baevski, W.-N. Hsu, Q. Xu, A. Babu, J. Gu, and M. Auli, “Data2vec: A