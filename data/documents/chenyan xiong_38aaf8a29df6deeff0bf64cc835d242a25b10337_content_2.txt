Title: Model-Generated Pretraining Signals Improves Zero-Shot Generalization of Text-to-Text Transformers
Authors: Linyuan Gong, Chenyan Xiong, Xiaodong Liu, Payal Bajaj, Yiqing Xie, Alvin Cheung, Jianfeng Gao, Xia Song
Section: A Appendix
NVIDIA V100 (16GB) GPUs. Prompt-finetuning each large++ model takes about 70 hours on 64x NVIDIA V100 (16GB) GPUs. A.7 Full Results on T0 Eval Figure 8 results of METRO-T0 versus our T0 baseline and T03B by Sanh et al. (2022) on all 9 tasks in the T0 Eval benchmark. The results shows that METRO-T0LARGE++, having only 775M parameters, 1https://github.com/facebookresearch/fairseq 2https://github.com/bigscience-workshop/promptsource 3https://huggingface.co/docs/transformers/index 4https://github.com/bigscience-workshop/t-zero consistently outperforms T03B over all tasks on the T0 Eval benchmark. A.8 Evaluation on MMLU The prompt template used to evaluate our models MMLU is the prompt template from the AI2 Reasoning Challenge (AI2-ARC) concatenated with 5 passages in MS MARCO (Nguyen et al., 2016). These 5 passages are selected via dense retrival using T5-ANCE (Ge et al., 2023; Ni et al., 2021), which maps a query to a single vector to retrieve similar passage from the corpus. Adding densely-retrieved passages to prompts is a standard approach to enhance LM’s performance on zero-shot prompting. This approach is named retrieval augmentation. All T0 and METRO-T0 results reported in Table 3 are evaluated using this prompt template with retrieval augmentation. On the other hand, all Flan-T5 results reported in Table 3 are numbers reported in their paper. For each model, we take the maximum score of the reported “direct” prompting performance and the “chain-ofthought (CoT)” prompting performance. Both prompt templates are not publicly available as of the time this paper is written. As a result, Table 3 involves comparisons across multiple prompt templates. So in Table 8, we present the performance of each model using the plain AI2-ARC prompt template without retrieval augmentation or CoT. The result in Table 8 shows that METRO-T0++ still consistently outperforms the T0 baseline and similar-sized Flan-T5 models when they are evaluated using the same prompt template. A.9 Example of the Challenge of Ill-Formed Target In our discussion about “decoding target” inSection 4, we claim that “masked tokens only” is an ill-formed target for the CLM objective in METRO-style pretraining of T5. This section shows a concrete example where such ill-formed target leads to ambiguities. In Table 9, the original sentence is “1 2 3 4 5”. Using different random samples of masked positions, we can derive two masked sequences as the input of the auxiliary model: “1 M M M 5” and “1 2 M M 5”. The difference is whether “2” is masked or not. So the target for the decoder corrective LM objective will be “2 3 4” and “3 4” respectively. After we have the masked input, the auxiliary model, which