Title: SEGMENT-LEVEL VECTORIZED BEAM SEARCH BASED ON PARTIALLY AUTOREGRESSIVE INFERENCE
Authors: Masao Someki, Nicholas Eng, Yosuke Higuchi, Shinji Watanabe
Section: 5.3.3. Segment-level Vectorized Beam Search
If a masked sequence contains multiple masks, the predicted tokens for the second or later masks may not be accurate due to the inaccuracy of the tokens predicted for the first mask. For example, in Fig. 1c, the masked sequence is se#_cuc#ber, where we have two masks at the positions of tokens e and am. Since we use the gCTC result to predict masked tokens, the decoder input for estimating the mask for the am part becomes see_cuc, which is incorrect, instead of the correct sea_cuc. This incorrect decoder input might impact the accuracy of the prediction. However, considering that even the AR decode may also have an incorrect decoder input, we believe PAR can search the same hypothesis with AR by utilizing the beam search. 6. CONCLUSION In this work, we propose a partially autoregressive framework to obtain a new trade-off balance between accuracy and latency. With our novel architecture design to compensate for the weakness inherent in each NAR and AR, PAR is a decoding method that takes advantage of the strengths of these two methods. In our experiments, we observed that the AR model can be inferred at NAR-level speeds without sacrificing accuracy. Notably, the LS-960 pre-trained model achieved a 13.75× speedup with the same WER on the test-clean set. We believe that using PAR can significantly improve the usability of the traditional hybrid CTC/Attention model. Our framework has limitations, such as memory usage issues, that restrict the scenarios where PAR can be applied. In future work, we plan to extend our framework to edge devices with limited computational resources. 7. REFERENCES [1] Geoffrey Hinton, Li Deng, Dong Yu, George E. Dahl, Abdelrahman Mohamed, Navdeep Jaitly, Andrew Senior, Vincent Vanhoucke, Patrick Nguyen, Tara N. Sainath, and Brian Kingsbury, “Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,” IEEE Signal Process. Mag., vol. 29, no. 6, pp. 82–97, 2012. [2] Alex Graves, Abdelrahman Mohamed, and Geoffrey Hinton, “Speech recognition with deep recurrent neural networks,” in Proc. ICASSP, 2013, pp. 6645–6649. [3] Rohit Prabhavalkar, Takaaki Hori, Tara N Sainath, Ralf Schlüter, and Shinji Watanabe, “End-to-end speech recognition: A survey,” arXiv preprint arXiv:2303.03329, 2023. [4] Alex Graves, “Sequence transduction with recurrent neural networks,” in Proc. ICML, 2012. [5] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang, “Conformer: Convolution-augmented transformer for speech recognition,” in Proc. Interspeech, 2020, pp. 5036–5040. [6] Kwangyoun Kim, Felix Wu, Yifan Peng, Jing Pan, Prashant