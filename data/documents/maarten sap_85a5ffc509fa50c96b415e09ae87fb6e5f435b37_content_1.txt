Title: BIASX: “Thinking Slow” in Toxic Content Moderation with Explanations of Implied Social Biases
Authors: Yiming Zhang, Sravani Nanduri, Liwei Jiang, Tongshuang Wu, Maarten Sap
Section: 4 Results and Discussion
the correctness of explanations to the accuracy of participants.7 On the hard toxic set, 60% of model explanations are accurate, which leads to 56.4% worker accuracy, a -7.7% drop from the HUMAN-EXPL condition where workers always have access to correct explanations. Figure 4b shows an example where the model explains an implicitly toxic statement as harmless and misleads content moderators (39.8% in MODEL-EXPL vs. 55.4% in NO-EXPL). On a positive note, expert-written explanations still improve moderator performance over baselines, highlighting the potential of our framework with higher quality explanations and serving as a proof-of-concept of BIASX, while motivating future work to explore methods to generate higher-quality explanations using techniques such as chain-of-thought (Camburu et al., 2018; Wei et al., 2022) and self-consistency (Wang et al., 2023) prompting.