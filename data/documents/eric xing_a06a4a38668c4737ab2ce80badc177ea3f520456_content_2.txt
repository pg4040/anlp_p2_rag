Title: CUTTLEFISH: LOW-RANK MODEL TRAINING WITHOUT ALL THE TUNING
Authors: Hongyi Wang, Saurabh Agarwal, Pongsakorn U-chupala, Yoshiki Tanaka, Eric P. Xing, Dimitris Papailiopoulos
Section: C.2 CUTTLEFISH hyperparameters
by 1 2 . CUTTLEFISH begins factorizing layers after the first embedding layer, which is simply a convolution layer, i.e., K = 1 for both DeiT-base and ResMLP-S36. Since we tune K for PUFFERFISH such that the end model sizes match CUTTLEFISH DeiT and ResMLP, PUFFERFISH starts factorizing layers from the 7th encoder block for DeiT, i.e., K = 19 (6 blocks, 3 layers in each block), and the 18th ResMLP block, i.e., K = 52 (17 blocks, 3 layers in each block). GLUE Benchmark on BERTBASE. For BERTBASE finetuning on the GLUE benchmark, the fine-tuning epochs for all downstream tasks are typically small, such as T = 3, 5. Therefore, we set E = 1 for CUTTLEFISH. Additionally, we free the fully connected layers following each multi-head attention layer when fine-tuning the factorized BERTBASE. As in LoRA, during the finetuning stage, we do not update the feed-forward network (FFN) in BERT at all; we freeze the FC1 and FC2 layers contained in the FFN (Hu et al., 2021). We perform learning rate sweeping over the learning rates Î³s for all methods during GLUE fine-tuning. The tuned learning rates are shown in Table 11. For the relatively challenging CoLA task, we fine-tune all models except for the vanilla BERTBASE for 5 epochs instead of 3 epochs. For the RTE task, we fine-tune Distill BERT for 5 epochs.