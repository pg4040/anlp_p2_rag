Title: Cross-Modal Fine-Tuning: Align then Refine
Authors: Junhong Shen, Liam Li, Lucio M. Dery, Corey Staten, Mikhail Khodak, Graham Neubig, Ameet Talwalkar
Section: 4.1. A Breadth Perspective: Can Pretrained Models Transfer Across Modalities?
Now, we evaluate the opposite approach, focusing on two tasks: DeepSEA (1D) and Spherical (2D). This evaluation is straightforward to perform by switching the model bodies, since the embedder architecture of ORCA handles all input transformations needed to obtain the sequence features. The results are shown in Table 13 in the Appendix. We see that fine-tuned RoBERTa outperforms Swin on the 1D task, possibly because the DeepSEA data (genomics sequences) are structured more like language than images with discrete units of information and general grammatical rules. More crucially, for both tasks, models with smaller final OTDDs have better fine-tuning accuracy. This suggests a way of selecting pretrained models by comparing the optimized OTDDs and picking the one with the smallest value. Apart from these three key insights, recall that one of our motivations for cross-modal fine-tuning is to help tasks with limited data, where training models from scratch is difficult. Indeed, for vanilla fine-tuning, a small amount of data may not give enough signal to update the pretrained weights, but it is possible to learn a good embedder first with ORCA, which can then make fine-tuning easier. In Figure 4 (right), we vary the dataset size and find that the performance gain of ORCA increases as the dataset size decreases. Meanwhile, using ORCA allows us to match the performance of naive fine-tuning on 3× amount of data. Thus, it can benefit model development in domains where data collection is costly. Beyond the cross-modal setting, we also verify ORCA’s efficacy for in-modality transfer in Appendix A.8.1.