Title: CONTINUAL CONTRASTIVE SPOKEN LANGUAGE UN- DERSTANDING
Authors: Umberto Cappellazzo, Enrico Fini, Muqiao Yang, Daniele Falavigna, Alessio Brutti, Bhiksha Raj
Section: A SUPPLEMENTARY MATERIAL
A.1 HYPERPARAMETERS We list the main hyperparameters used for our experiments in table 5. We also mention the number of epochs for each setting. For FSC-3, the number of epochs for each task is {40,30,30}, while for SLURP-3 we have {40,25,25}. For FSC-6 and SLURP-6 we use {40,30,30,30,30,30} and {40,25,20,20,20,20} epochs, respectively. We finally note that we set lr = 5 · 10−4 for the text encoder, the ASR decoder and the classifier, while for the audio encoder we set a smaller learning rate, lr = 5 · 10−5, because it is pre-trained. A.2 SPECAUG DETAILS In this section, we elaborate on the use of SpecAug for augmenting the audio input data. SpecAug (Park et al., 2019) is a popular augmentation technique that is applied directly on the log mel spectrogram of an audio signal, with the aim of making the model invariant to features deformation. In the original paper, they advance three different types of distortions: time warping, and time and frequency masking, where blocks of consecutive time steps and frequency channels are zero-masked, respectively. Since our audio encoders (i.e., DistilHuBERT and Wav2vec 2.0) work on the raw audio waveforms, SpecAug is not applicable by default. In order to circumvent this problem, we apply an approximated version of SpecAug directly to the raw waveform, as proposed in the SpeechBrain library (Ravanelli et al., 2021). We randomly drop chunks of the audio waveform (by zero-masking) and frequency bands (with band-drop filters). Unlike the SpeechBrain implementation, we do not apply speed perturbation. In more detail, with probability 0.5 we randomly drop up to 2 frequencies, while with probability 0.5 we randomly drop up to 3 chunks of audio whose length is sampled from a uniform distribution ∼ U(0, 0.05 · len(x)), where len(x) is the length of the considered audio waveform x. A.3 LIMITATIONS Our work comes with some limitations. First of all, the number of suitable SLU datasets for defining a CIL setting is limited since few datasets provide enough intent classes. Then, we could not use batches larger than 32 owing to computational limitations, and it is known that contrastive learning benefits from larger batches. A.4 FUTURE WORK COCONUT relies on two contrastive learning-based losses applied to the projections of audio and text encoders outputs. In principle, COCONUT could be exploited in other multi-modal settings such as audio-vision or vision-language. Therefore, it would be interesting to study whether COCONUT can be exploited in other different multi-modal scenarios. Also, since these settings usually involve a larger number of classes than