Title: ML-SUPERB: Multilingual Speech Universal PERformance Benchmark
Authors: Jiatong Shi, Dan Berrebbi, William Chen, Ho-Lam Chung, En-Pei Hu, Wei Ping Huang, Xuankai Chang, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Shinji Watanabe
Section: 5. References
[1] A. Mohamed et al., “Self-supervised speech representation learning: A review,” JSTSP, 2022. [2] S.-w. Yang et al., “SUPERB: Speech Processing Universal PERformance Benchmark,” in Proc. Interspeech, 2021, pp. 1194– 1198. [3] A. Baevski et al., “Wav2vec 2.0: A framework for selfsupervised learning of speech representations,” Proc. NeurIPS, vol. 33, pp. 12 449–12 460, 2020. [4] W.-N. Hsu et al., “HuBERT: Self-supervised speech representation learning by masked prediction of hidden units,” TASLP, vol. 29, pp. 3451–3460, 2021. [5] H.-S. Tsai et al., “SUPERB-SG: Enhanced speech processing universal performance benchmark for semantic and generative capabilities,” in Proc. ACL, 2022, pp. 8479–8492. [6] A. Babu et al., “XLS-R: Self-supervised cross-lingual speech representation learning at scale,” arXiv preprint arXiv:2111.09296, 2021. [7] A. Conneau et al., “Unsupervised cross-lingual representation learning for speech recognition,” arXiv preprint arXiv:2006.13979, 2020. [8] P.-A. Duquenne et al., “Speechmatrix: A large-scale mined corpus of multilingual speech-to-speech translations,” arXiv preprint arXiv:2211.04508, 2022. [9] J. Zhao and W.-Q. Zhang, “Improving automatic speech recognition performance for low-resource languages with selfsupervised models,” JSTSP, vol. 16, no. 6, pp. 1227–1241, 2022. [10] D. Berrebbi, J. Shi, B. Yan, et al., “Combining Spectral and SelfSupervised Features for Low Resource Speech Recognition and Translation,” in Proc. Interspeech, 2022, pp. 3533–3537. [11] A. Wu et al., “Self-supervised representations improve end-toend speech translation,” Proc. Interspeech 2020, pp. 1491–1495, 2020. [12] X. Li et al., “ASR2K: Speech Recognition for Around 2000 Languages without Audio,” in Proc. Interspeech, 2022, pp. 4885– 4889. [13] S. Evain et al., “ LeBenchmark: A Reproducible Framework for Assessing Self-Supervised Representation Learning from Speech,” in Proc. Interspeech, 2021, pp. 1439–1443. [14] T. Javed et al., “Indicsuperb: A speech processing universal performance benchmark for indian languages,” arXiv preprint arXiv:2208.11761, 2022. [15] A. Conneau et al., “XTREME-S: Evaluating Cross-lingual Speech Representations,” in Proc. Interspeech, 2022, pp. 3248– 3252. [16] V. Pratap et al., “MLS: A large-scale multilingual dataset for speech research,” Proc. Interspeech 2020, pp. 2757–2761, 2020. [17] R. Ardila et al., “Common voice: A massively-multilingual speech corpus,” in Proc. LREC, 2020, pp. 4218–4222. [18] K. MacLean, “Voxforge,” Ken MacLean.[Online]. Available: http://www. voxforge. org/home.[Accessed by 2022], 2018. [19] C. Wang et al., “VoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation,” in Proc. ACL, 2021, pp. 993–1003. [20] K. Sodimana et al., “A step-by-step process for building tts voices using open source data and framework for bangla, javanese, khmer, nepali, sinhala, and sundanese,” in Proc. SLTU, 2018, pp. 66–70. [21] O. Kjartansson et al., “Open-source high quality speech datasets for basque, catalan and galician,”