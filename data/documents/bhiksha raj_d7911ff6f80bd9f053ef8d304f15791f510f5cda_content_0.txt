Title: Completing Visual Objects via Bridging Generation and Segmentation
Authors: Xiang Li, Yinpeng Chen, Chung-Ching Lin, Hao Chen, Kai Hu, Rita Singh, Bhiksha Raj, Lijuan Wang, Zicheng Liu
Section: B. More Discussion
Image diffusion v.s. Mask denoising. During the training of the image diffusion model, Gaussian noise is introduced to the original image. A denoising U-Net is then trained to predict this noise and subsequently recover the image to its clean state during inference. Similarly, in the context of the proposed iterative mask denoising (IMD) process, we manually occlude the complete object (which can be assumed as adding noise) and train a generative model to recover the complete object. During inference, as shown in Eq. (1), we employ an iterative approach that combines the segmentation and generation model S ◦ G(·) functioning as a denoiser. This denoiser progressively denoises the partial mask to achieve a complete mask, following a similar principle to the denoising diffusion process. By drawing parallels between image diffusion and mask denoising, we establish an analogy, as depicted in Table 7. We can notice that the mask-denoising process shares the spirits of the image diffusion process and the only difference is that mask denoising does not explicitly calculate the added noise but directly predicts the denoised mask. In this way, MaskComp can be assumed as a double-loop denoising process with an inner loop for image denoising and an outer loop for mask denoising. Input Step 1 Step 2 Step 3 Step 4 Step 5 Training without complete object. In the context of image diffusion, though multiple forward steps are involved to add noise to the image, the network only learns to predict the noise added in a single step during training. Therefore, if we possess a set of noisy images generated through forward steps, the original image is not required during the training. This motivates us to explore the feasibility of training MaskComp without relying on the complete mask. Similar to image diffusion, given a partial mask, we can further occlude it and learn to predict the partial mask before further occlusion. In this way, MaskComp can be leveraged in a more generic scenario without the strict demand for complete objects. We have discussed the quantitative results in Section 4.3. Here, we visualize the IMD process with a model trained without complete objects (on OpenImage). To better visualize the object shape updating, we denote the overlapping masked area from the last step as orange. We can notice that the object shape gradually refines and converges to the complete shape as the IMD process forwards. Interestingly, the IMD process can learn to complete the object even if only a small portion of the complete object was available in the dataset