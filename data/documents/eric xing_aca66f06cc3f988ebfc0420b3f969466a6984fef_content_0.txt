Title: A SCALABLE EXPECTATION PROPAGATION APPROACH
Authors: Han Guo, Philip Greengard, Hongyi Wang, Andrew Gelman, Yoon Kim, Eric P. Xing
Section: A APPENDIX
A.1 DAMPED CLIENT AND SERVER UPDATES To simplify the notations, observe that Eq. 3 could be re-written in the following way, qnewk (θ) ∝ q̂\k(θ) q−k(θ) , where q̂\k(θ) = arg min q̂\k∈Q D ( pk(θ) q−k(θ) ‖ q̂\k(θ) ) . A partially damped client update could be carried out by, Client: qnewk (θ) ∝ ( qk(θ) )1−δ( q̂\k(θ) q−k(θ) )δ ∝ ( qk(θ) )1−δ( q̂\k(θ) qglobal(θ)/qk(θ) )δ ∝ ( qk(θ) )1−δ( qk(θ) )δ( q̂\k(θ) qglobal(θ) )δ ∝ qk(θ) ( q̂\k(θ) qglobal(θ) )δ ∝ qk(θ) ( ∆qk(θ) )δ , where we define ∆qk(θ) ∝ q̂\k(θ) qglobal(θ) . Similarly, (damped) server updates could be written as the following, Server: qnewglobal(θ) ∝ ∏ k qnewk (θ) ∝ ∏ k qk(θ) ( ∆qk(θ) )δ ∝ [ ∏ k qk(θ) ][ ∏ k ( ∆qk(θ) )δ ] ∝ qglobal(θ)∏ k ( ∆qk(θ) )δ . A.2 EXPECTATION PROPAGATION (EXTENDED) Expectation propagation (EP) Minka (2001); Vehtari et al. (2020) constructs a posterior approximation through iterating local computations that refine factors that approximate the posterior contribution from each client. In this spirit, we would ideally like to solve the following localized version of Eq. 2, where we replace one of the factors with its corresponding approximating factor, qnewk (θ) = arg min q∈Q D ( pk(θ) p−k(θ) ‖ q(θ) p−k(θ) ) , where p−k(θ) ∝ pglobal(θ) pk(θ) . Unfortunately, the right-hand side of the divergence is the intractable posterior we would like to approximate in the first place. Instead, EP solves the following problem (Eq. 3), qnewk (θ) = arg min q∈Q D ( pk(θ) q−k(θ) ‖ q(θ) q−k(θ) ) , where q−k(θ) ∝ qglobal(θ) qk(θ) . A.3 ADDITIONAL EXPERIMENTS AND DETAILS StackOverflow. Please see Fig. 5 for additional visualizations. EMNIST. Please see Fig. 7 and Table 6 for experimental results. Analysis. This section extends the experiments (the “small” setting) in Sec. 3.3. It looks at the performance as we increase the complexity (a proxy of quality) of approximate inference techniques. We vary the number of iterations in NGVI from 1 (cheap) to 10 (expensive) epochs. We can observe in Fig. 6 that as we increase NGVI’s computations, the performance improves. A.4 HYPERPARAMETERS Please see Table 7 for hyperparameter details. In Table 8, we also conduct experiments to understand their influence on the different algorithms. Algorithm 5 Approximate Inference: NGVI 1: Input: Dk,µ\k, Σ−k, TNGVI, NNGVI, βNGVI 2: Initialize s0, Σ\k,0 3: for t = 1, . . . , TNGVI do 4: F ← {} 5: for i = 1, . . . , NNGVI do