Title: Deep Speech Synthesis from MRI-Based Articulatory Representations
Authors: Peter Wu, Tingle Li, Yijing Lu, Yubin Zhang, Jiachen Lian, Alan W Black, Louis Goldstein, Shinji Watanabe, Gopala K. Anumanchipalli
Section: 9. References
in ICASSP, 2019, pp. 3617–3621. [30] J. Kong, J. Kim, and J. Bae, “Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis,” in NeurIPS, 2020. [31] M. Morrison, R. Kumar, K. Kumar, P. Seetharaman, A. Courville, and Y. Bengio, “Chunked autoregressive gan for conditional waveform synthesis,” in ICLR, 2022. [32] J. Su, Y. Wang, A. Finkelstein, and Z. Jin, “Bandwidth extension is all you need,” in ICASSP, 2021, pp. 696–700. [33] J. Su, Z. Jin, and A. Finkelstein, “Hifi-gan: High-fidelity denoising and dereverberation based on speech deep features in adversarial networks,” in Interspeech, vol. 2020, 2017, pp. 4506–4510. [34] ——, “Hifi-gan-2: Studio-quality speech enhancement via generative adversarial networks conditioned on acoustic features,” in WASPAA, 2021. [35] A. W. Black, “CMU wilderness multilingual speech dataset,” in ICASSP, 2019. [36] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, “Robust speech recognition via large-scale weak supervision,” arXiv preprint arXiv:2212.04356, 2022. [37] T. Sorensen, A. Toutios, L. Goldstein, and S. S. Narayanan, “Characterizing vocal tract dynamics across speakers using realtime mri.” in Interspeech, 2016, pp. 465–469. [38] S. R. Moisik, J. H. Esling, L. Crevier-Buchman, and P. Halimi, “Putting the larynx in the vowel space: Studying larynx state across vowel quality using mri,” in ICPhS, 2019. [39] M. I. Proctor, C. Y. Lo, and S. S. Narayanan, “Articulation of english vowels in running speech: A real-time mri study.” in ICPhS, 2015. [40] A. Alwan, S. Narayanan, and K. Haker, “Toward articulatoryacoustic models for liquid approximants based on mri and epg data. part ii. the rhotics,” The Journal of the Acoustical Society of America, vol. 101, no. 2, pp. 1078–1089, 1997.