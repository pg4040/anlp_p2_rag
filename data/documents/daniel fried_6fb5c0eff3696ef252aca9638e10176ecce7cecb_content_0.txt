Title: Generating Images with Multimodal Language Models
Authors: Jing Yu Koh, Daniel Fried
Section: F Human Annotation on PartiPrompts
In Sec. 3.3 of the main paper, we described the process of annotating PartiPrompts [65] with perexample labels to retrieve or generate. The interface shown to human annotators is shown in Fig. 9. Annotators are tasked to determine which of two anonymized images are (1) more relevant to the provided prompt, and (2) more realistic. We randomize the order of the two images as well (i.e., the output of the retrieval model shows up 50% of the time as Image A). We show each example to 5 independent human annotators. For determining whether to label a particular example as “ret” or “gen”, we take the majority vote of the 5 annotators on the image relevance question (“Is image A or image B more relevant to the above caption?”), and only keep the examples with an inter-annotator agreement of at least 4/5. This results in approximately 900 examples remaining (out of the 1,632 examples in PartiPrompts). Our annotations will be publicly released to facilitate future evaluations on this task. We conducted evaluations on the Amazon Mechanical Turk platform with human annotators located in the US and Canada. Annotators were paid at an estimated hourly rate of 15 USD per hour. In total, we spent approximately 326 USD to collect these annotations.