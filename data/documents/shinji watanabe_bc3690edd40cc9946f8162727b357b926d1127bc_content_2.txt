Title: JOINT MODELLING OF SPOKEN LANGUAGE UNDERSTANDING TASKS WITH INTEGRATED DIALOG HISTORY
Authors: Siddhant Arora, Hayato Futami, Emiru Tsunoo, Brian Yan, Shinji Watanabe
Section: 8. REFERENCES
Chiu, et al., “Conformer: Convolution-augmented transformer for speech recognition,” in Proc. Interspeech, 2020, pp. 5036–5040. [37] P. Guo, F. Boyer, X. Chang, et al., “Recent developments on espnet toolkit boosted by conformer,” in Proc. ICASSP, 2021, pp. 5874– 5878. [38] S. Chen, C. Wang, Z. Chen, et al., “WavLM: Large-scale selfsupervised pre-training for full stack speech processing,” arXiv preprint arXiv:2110.13900, 2021. [39] A. Vaswani, N. Shazeer, N. Parmar, et al., “Attention is all you need,” Proc. NeurIPS, vol. 30, pp. 5998–6008, 2017. [40] S. Karita, N. Chen, T. Hayashi, et al., “A comparative study on transformer vs RNN in speech applications,” in Proc. ASRU, 2019, pp. 449–456. [41] T. Wolf, L. Debut, V. Sanh, et al., “Transformers: State-of-the-art natural language processing,” in Proc. EMNLP, 2020, pp. 38–45. [42] J. Towns, T. Cockerill, M. Dahan, et al., “XSEDE: Accelerating scientific discovery,” Computing in Science & Engineering, vol. 16, no. 5, pp. 62–74, 2014. [43] N. A. Nystrom, M. J. Levine, R. Z. Roskies, et al., “Bridges: A uniquely flexible HPC resource for new communities and data analytics,” in Proc. XSEDE Conference: Scientific Advancements Enabled by Enhanced Cyberinfrastructure, 2015.