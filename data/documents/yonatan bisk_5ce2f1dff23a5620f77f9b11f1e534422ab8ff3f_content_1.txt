Title: Plan, Eliminate, and Track — Language Models are Good Teachers for Embodied Agents
Authors: Yue Wu, So Yeon Min, Yonatan Bisk, Ruslan Salakhutdinov, Amos Azaria, Yuanzhi Li, Tom M. Mitchell, Shrimai Prabhumoye
Section: 4.1. Experimental Details
DAgger v.s. 6 hours for Behavior Cloning). In addition, we demonstrate that our models surpass the DAgger training performance of the BUTLER (Shridhar et al., 2020b) agents trained with DAgger, even when our agent does not have the option to interact with the environment. Baselines. Our first baseline is the BUTLER::BRAIN (BUTLER) agent (Shridhar et al., 2020b), which consists of an encoder, an aggregator, and a decoder. At each time step t, the encoder takes initial observation s0, current observation st, and task string stask and generates representation rt. The recurrent aggregator combines rt with the last recurrent state ht−1 to produce ht, which is then decoded into a string at representing action. In addition, the BUTLER agent uses beam search to get out of stuck conditions in the event of a failed action. Our second baseline GPT (Micheli & Fleuret, 2021) is a fine-tuned GPT2-medium on 3553 demonstrations from the AlfWorld training set. Specifically, the GPT is fined-tuned to generate each action step word-by-word to mimic the rule-based expert using the standard maximum likelihood loss.