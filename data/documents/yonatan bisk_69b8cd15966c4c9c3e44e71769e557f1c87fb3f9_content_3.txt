Title: MOSAIC: Learning Unified Multi-Sensory Object Property Representations for Robot Learning via Interactive Perception
Authors: Gyan Tatiya, Jonathan Francis, Ho-Hsiang Wu, Yonatan Bisk, Jivko Sinapov
Section: IV. EXPERIMENTAL DESIGN
that is plastic.” Level 5 was introduced to assess the robot’s performance across various property categories. For each level, we created 20 commands for target objects and carefully selected corresponding distractor objects for each of the 5 previously explained folds. For each object (target and distractor(s)), we calculated its selection percentage, defined as S = number of times the object is selectedtotal number of commands (%). Our results are reported as the mean selection percentage across the 5 folds. We employ the approach outlined in Algorithm 2 for this task. Initially, we convert the natural language instruction into a text embedding, denoted as tc, using CLIP’s text encoder (step 1). Subsequently, the robot interacts with the presented objects, including the target object and distractors, using various available behaviors while simultaneously recording sensory signals (step 5). To simulate this step, we randomly select a trial from our dataset among 5 trials of each object. Leveraging our trained framework, we generate unified representations, denoted as ub, by processing the sensory inputs for each behavior (step 6). Next, we calculate the cosine similarity between the command embedding (tc) and the unified representation (ub) for each behavior, maintaining a cumulative similarity score (step 7). Finally, once all behaviors are considered, the object with the highest cumulative similarity score is identified as the target object, concluding the task (step 11). Baselines, Ablations, and Comparisons. We ablate our full framework (MOSAIC), featuring the multi-sensory selfattention module, against a framework that omits this component (MOSAIC-w/o-SA). We conduct these evaluations under two conditions: a non-interactive condition, where the robot solely performs the Look behavior, and an interactive condition, where the robot engages in all 9 aforementioned interactive behaviors. Notably, in the Look behavior, only visual embeddings are employed as the unified representations after passing through the self-attention layer in MOSAIC, while, in MOSAIC-w/o-SA, the Look behavior utilizes only CLIP’s vision encoder. Conversely, for interactive behaviors, all three modalities (i.e., visual, auditory, and haptic) are used to create unified representations. For the object category recognition task, we report recognition accuracy separately for the Look behavior, each of the 9 interactive behaviors individually, and the combination of all 9 interactive behaviors. The combined accuracy is calculated through a weighted combination of each behavior’s performance on the training data. We also compare our recognition accuracy with two baseline methods: Sinapov et al. [14], who trained a Support Vector Machine (SVM) classifier using handcrafted auditory, haptic features, and visual features, and Tatiya et al. [15], who applied a deep learning approach to