Title: Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation
Authors: Patrick Fernandes, Aman Madaan, Emmy Liu, António Farinhas, Pedro Henrique Martins, Amanda Bertsch, José G. C. de Souza, Shuyan Zhou, Tongshuang Wu, Graham Neubig, André F. T. Martins
Section: 5.2.1 Optimizing for Feedback Models
learning with a feedback model, finding that the latter led to a better preference rate and lower rule violation rate. The joint-feedback modeling with feedback models was explored by Korbak et al. (2023), who study pre-training an LLMs with a loss similar to Equation 6, based on feedback from a preference model trained on ranking-based feedback for toxicity. They showed that this leads to models producing less toxic generations, when compared to pretraining a model with vanilla MLE. In an approach outside these main categories, Peyrard and Gurevych (2018) use a scoring function learned from human judgments as a fitness function for a genetic algorithm to generate summaries of input texts.