Title: THE MULTIMODAL INFORMATION BASED SPEECH PROCESSING (MISP) 2023 CHALLENGE: AUDIO-VISUAL TARGET SPEAKER EXTRACTION
Authors: Shilong Wu, Chenxi Wang, Hang Chen, Yusheng Dai, Chenyue Zhang, Ruoyu Wang, Hongbo Lan, Jun Du, Chin-Hui Lee, Jingdong Chen, Shinji Watanabe, Sabato Marco Siniscalchi, Odette Scharenborg, Zhong-Qiu Wang, Jia Pan, Jianqing Gao
Section: 6. REFERENCES
[1] Felix Weninger, Hakan Erdogan, Shinji Watanabe, et al., “Speech enhancement with lstm recurrent neural networks and its application to noise-robust asr,” in Latent Variable Analysis and Signal Separation: 12th International Conference, LVA/ICA 2015, Liberec, Czech Republic, August 25-28, 2015, Proceedings 12. Springer, 2015, pp. 91–99. [2] Tom O’Malley, Arun Narayanan, Quan Wang, et al., “A conformer-based asr frontend for joint acoustic echo cancellation, speech enhancement and speech separation,” in 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2021, pp. 304–311. [3] Adelbert W Bronkhorst, “The cocktail-party problem revisited: early processing and selection of multi-talker speech,” Attention, Perception, & Psychophysics, vol. 77, no. 5, pp. 1465– 1487, 2015. [4] Katerina Zmolikova, Marc Delcroix, Tsubasa Ochiai, et al., “Neural target speech extraction: An overview,” IEEE Signal Processing Magazine, vol. 40, no. 3, pp. 8–29, 2023. [5] Marc Delcroix, Katerina Zmolikova, Keisuke Kinoshita, et al., “Single channel target speaker extraction and recognition with speaker beam,” in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018, pp. 5554–5558. [6] Jakub Janský, Jiřı́ Málek, Jaroslav Čmejla, et al., “Adaptive blind audio source extraction supervised by dominant speaker identification using x-vectors,” in ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020, pp. 676–680. [7] Marc Delcroix, Katerina Zmolikova, Tsubasa Ochiai, et al., “Compact network for speakerbeam target speaker extraction,” in ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019, pp. 6965–6969. [8] Elana Zion Golumbic, Gregory B Cogan, Charles E Schroeder, et al., “Visual input enhances selective speech envelope tracking in auditory cortex at a “cocktail party”,” Journal of Neuroscience, vol. 33, no. 4, pp. 1417–1426, 2013. [9] Hang Chen, Jun Du, Yu Hu, et al., “Correlating subword articulation with lip shapes for embedding aware audio-visual speech enhancement,” Neural Networks, vol. 143, pp. 171–182, 2021. [10] Andrew Owens and Alexei A Efros, “Audio-visual scene analysis with self-supervised multisensory features,” in Proceedings of the European conference on computer vision (ECCV), 2018, pp. 631–648. [11] Tsubasa Ochiai, Marc Delcroix, Keisuke Kinoshita, et al., “Multimodal speakerbeam: Single channel target speech extraction with audio-visual speaker clues.,” in INTERSPEECH, 2019, pp. 2718–2722. [12] Harishchandra Dubey, Ashkan Aazami, Vishak Gopal, et al., “Icassp 2023 deep noise suppression challenge,” 2023. [13] SN Graetzer, Jon Barker, Trevor J Cox, et al., “Clarity-2021 challenges: Machine learning challenges for advancing hearing aid processing,” in Proc. INTERSPEECH, 2021, pp. 686– 690. [14] Andrea Lorena Aldana Blanco, Cassia Valentini-Botinhao, Ondrej Klejch, et al., “Avse challenge: Audio-visual speech enhancement challenge,” in 2022