Title: Exploration on HuBERT with Multiple Resolutions
Authors: Jiatong Shi, Yun Tang, Hirofumi Inaguma, Hongyu Gong, Juan Pino, Shinji Watanabe
Section: 6. References
[1] A. Mohamed et al., “Self-supervised speech representation learning: A review,” IEEE Journal of Selected Topics in Signal Processing, 2022. [2] W.-N. Hsu et al., “HuBERT: Self-supervised speech representation learning by masked prediction of hidden units,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451–3460, 2021. [3] S.-w. Yang et al., “SUPERB: Speech Processing Universal PERformance Benchmark,” in Proc. Interspeech, 2021, pp. 1194– 1198. [4] H.-S. Tsai et al., “SUPERB-SG: Enhanced speech processing universal performance benchmark for semantic and generative capabilities,” in Proc. ACL, 2022, pp. 8479–8492. [5] T.-h. Feng et al., “SUPERB@ SLT 2022: Challenge on generalization and efficiency of self-supervised speech representation learning,” in Proc. SLT, 2023, pp. 1096–1103. [6] X. Chang et al., “An exploration of self-supervised pretrained representations for end-to-end speech recognition,” in Proc. ASRU, 2021, pp. 228–235. [7] D. Berrebbi et al., “Combining Spectral and Self-Supervised Features for Low Resource Speech Recognition and Translation,” in Proc. Interspeech, 2022, pp. 3533–3537. [8] S. H. Mallidi and H. Hermansky, “Novel neural network based fusion for multistream asr,” in Proc. ICASSP, 2016, pp. 5680– 5684. [9] S. H. R. Mallidi et al., “A practical and efficient multistream framework for noise robust speech recognition,” Ph.D. dissertation, Johns Hopkins University, 2018. [10] H. Hermansky, “Multistream recognition of speech: Dealing with unknown unknowns,” Proceedings of the IEEE, vol. 101, no. 5, pp. 1076–1088, 2013. [11] K. J. Han et al., “Multistream cnn for robust acoustic modeling,” in Proc. ICASSP, 2021, pp. 6873–6877. [12] J. Luo et al., “Multi-quartznet: Multi-resolution convolution for speech recognition with multi-layer feature fusion,” in Proc. SLT, 2021, pp. 82–88. [13] R. Li et al., “Multi-stream end-to-end speech recognition,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 646–655, 2019. [14] Y. Kong et al., “Multi-channel automatic speech recognition using deep complex unet,” in Proc. SLT, 2021, pp. 104–110. [15] A. Andrusenko, R. Nasretdinov, and A. Romanenko, “Uconvconformer: High reduction of input sequence length for endto-end speech recognition,” arXiv preprint arXiv:2208.07657, 2022. [16] S. Kim et al., “Squeezeformer: An efficient transformer for automatic speech recognition,” in Proc. NeurIPS. [17] W. Yao et al., “Multi-stream convolutional neural network with frequency selection for robust speaker verification,” arXiv preprint arXiv:2012.11159, 2020. [18] Z. Gao, M.-W. Mak, and W. Lin, “Unet-densenet for robust farfield speaker verification,” Proc. Interspeech 2022, pp. 3714– 3718, 2022. [19] M. Burchi and V. Vielzeuf, “Efficient conformer: Progressive downsampling and grouped attention for automatic speech recognition,” in Proc. ASRU, 2021, pp. 8–15. [20] Y. Zhang et al., “Research on speech enhancement algorithm based