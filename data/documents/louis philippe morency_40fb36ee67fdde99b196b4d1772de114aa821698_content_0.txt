Title: MULTIZOO & MULTIBENCH: A Standardized Toolkit for Multimodal Deep Learning
Authors: Paul Pu Liang, Yiwei Lyu, Xiang Fan, Arav Agarwal, Yun Cheng, Louis-Philippe Morency, Ruslan Salakhutdinov, Antti Honkela
Section: 4 Conclusion
In conclusion, we present MULTIZOO and MULTIBENCH, a large-scale open-source toolkit unifying previously disjoint efforts in multimodal research with a focus on ease of use, accessibility, and reproducibility, thereby enabling a deeper understanding of multimodal models. Through its unprecedented range of research areas, datasets, modalities, tasks, and evaluation metrics, our toolkit paves the way toward building more generalizable, lightweight, and robust multimodal models. These tools have already been used for new directions for visualizing trained models (Liang et al., 2023a), large-scale multimodal foundation models (Liang et al., 2023b), multimodal fusion methods (Huang et al., 2022; Xue and Marculescu, 2023), and other theoretical and empirical studies of multimodal learning in applications ranging from robotics (Li et al., 2022) and HCI (Wu et al., 2023) to IoT (Hou et al., 2023), remote sensing (Xiong et al., 2022), and healthcare (Suvon et al., 2022). Our toolkits are publicly available, regularly updated with new tasks and modeling paradigms, and welcome inputs from the community.