Title: GET: a foundation model of transcription across human cell types
Authors: Xi Fu, Shentong Mo, Anqi Shao, Anouchka Laurent, Alejandro Buendia, Adolfo A. Ferrando, Alberto Ciccia, Yanyan Lan, Teresa Palomero, David M. Owens, Eric P. Xing, Raul Rabadan
Section: Methods
apply a linear layer to capture the general information in the different classes of transcription factor binding sites. To learn the cis- and trans-interactions between regulatory elements and transcription factors, we apply 12 RE-wise Attention (REA) layers with a multi-head attention mechanism on the RE embeddings along the regulatory element. Suppose Nh,dv,dk denote the number of heads, the depth of values, and the depth of keys. The output from each head h is computed as Oh = So f tmax ✓ X 0 Wq(X 0Wk)Tp dk ◆ (X 0Wv) (1) where Wq,Wk 2 R(n⇥D)⇥dk ,Wv 2 R(n⇥D)⇥dv are learnable linear transformations. Then we concatenated the output from each head h for the RE-wise Attention block. The Layer Normalization (LN), Feed-forward Network (FFN), and Residual Connections are finally utilized to generate the output for each layer. Thus, the mechanism behind the RE-wise attention block is summarized as: z 0 l = MHA(LN(zl 1))+ zl 1;zl = FFN(LN(z0l))+ z 0 l (2) where z0 l ,zl 1) denote the intermediate representation in the block l and the output from the block l 1. We apply two linear layers with a GELU8 activation layer in the FFN layer. 2/8 The GET architecture is similar to the state-of-the-art model Enformer9. However, the following changes helped us improve and exceed its performance: GET uses the regulatory element (RE) embedding layer to capture the general information of regulatory elements in the different classes of transcription factor binding sites. Moreover, a masked regulatory element mechanism was utilized to learn the general cis- and trans-interactions between regulatory elements and transcription factors from different kinds of human cell types. Specifically, a random set of positions was uniformly selected to mask out M = {mi}ki=1 with a mask ratio of r = k/n. We replaced the regions in the selected positions with a [MASK] regulatory element, and the masked input regulatory element is denoted as Xmasked = (X ,M, [MASK]). where X = {xi}ni=1 is the input sample with n regulatory elements. The training goal is to predict the original values of the masked elements M. Specifically, we take masked regulatory element embeddings X masked as input to our GET, while a simple linear layer is appended as the prediction head. Therefore, the overall objective of self-supervised training is formulated as: L = E Â i2M log p(xi|Xmasked) ! (3) where xi denote the masked region to be predicted. Training scheme We conduct pre-training in the large-scale single-cell Chromatin accessibility data. Then we fine-tune the pre-trained model on the Paired