Title: EXPLORING THE INTEGRATION OF SPEECH SEPARATION AND RECOGNITION WITH SELF-SUPERVISED LEARNING REPRESENTATION
Authors: Yoshiki Masuyama, Xuankai Chang, Wangyou Zhang, Samuele Cornell, Zhong-Qiu Wang, Nobutaka Ono, Yanmin Qian, Shinji Watanabe
Section: 6. REFERENCES
[1] D. Raj and et al., “Integration of speech separation, diarization, and recognition for multi-speaker meetings: System description, comparison, and analysis,” in Proc. SLT, 2021, pp. 897–904. [2] B. Li et al., “Acoustic modeling for google home,” Proc. Interspeech, pp. 399–403, 2017. [3] Y.-J. Lu et al., “ESPnet-SE++: Speech enhancement for robust speech recognition, translation, and understanding,” in Proc. Interspeech, 2022, pp. 5458–5462. [4] J. R. Hershey et al., “Deep clustering: Discriminative embeddings for segmentation and separation,” in Proc. ICASSP, 2016, pp. 31–35. [5] D. Yu et al., “Permutation invariant training of deep models for speaker-independent multi-talker speech separation,” in Proc. ICASSP, 2017, pp. 241–245. [6] Z. Q. Wang et al., “Multi-channel deep clustering: Discriminative spectral and spatial embeddings for speakerindependent speech separation,” in Proc. ICASSP, 2018, pp. 1–5. [7] D. Wang and J. Chen, “Supervised speech separation based on deep learning: An overview,” IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 26, no. 10, pp. 1702–1726, 2018. [8] Y. Luo and N. Mesgarani, “Conv-TasNet: Surpassing ideal time–frequency magnitude masking for speech separation,” IEEE/ACM Trans. Audio Speech Lang. Process., vol. 27, no. 8, pp. 1256–1266, 2019. [9] Y. Luo et al., “Dual-path RNN: Efficient long sequence modeling for time-domain single-channel speech separation,” in Proc. ICASSP, 2020, pp. 46–50. [10] C. Subakan et al., “Attention is all you need in speech separation,” in Proc. ICASSP, 2021, pp. 21–25. [11] L. Yang et al., “TFPSNet: Time-frequency domain path scanning network for speech separation,” in Proc. ICASSP, 2022, pp. 6842–6846. [12] K. Tan et al., “Neural spectrospatial filtering,” IEEE/ACM Trans. Audio, Speech, Lang. Process., vol. 30, pp. 605–621, 2022. [13] Z. Q. Wang et al., “TF-GridNet: Integrating full- and subband modeling for speech separation,” arXiv:2211.12433, 2022. [14] M. Maciejewski et al., “WHAMR!: Noisy and reverberant single-channel speech separation,” in Proc. ICASSP, 2020, pp. 696–700. [15] M. L. Seltzer et al., “Likelihood-maximizing beamforming for robust hands-free speech recognition,” IEEE Trans. Speech, Audio process., vol. 12, no. 5, pp. 489–498, 2004. [16] B. Li et al., “Neural network adaptive beamforming for robust multichannel speech recognition,” Proc. Interspeech, pp. 1976–1980, 2016. [17] J. Heymann et al., “Beamnet: End-to-end training of a beamformer-supported multi-channel ASR system,” in Proc. ICASSP, 2017, pp. 5325–5329. [18] T. Ochiai et al., “Multichannel end-to-end speech recognition,” in Proc. ICML, 2017, pp. 2632–2641. [19] W. Minhua et al., “Frequency domain multi-channel acoustic modeling for distant speech recognition,” in Proc. ICASSP, 2019, pp. 6640–6644. [20] X. Chang et al., “MIMO-Speech: End-to-end multi-channel multi-speaker speech recognition,” in Proc. ASRU, Dec. 2019, pp. 237–244. [21] W. Zhang