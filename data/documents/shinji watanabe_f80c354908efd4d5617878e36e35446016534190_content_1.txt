Title: SEMI-AUTOREGRESSIVE STREAMING ASR WITH LABEL CONTEXT
Authors: Siddhant Arora, George Saon, Shinji Watanabe, Brian Kingsbury
Section: 4.4. Results and Discussion
is helpful. Additionally, we conduct an ablation study where, instead of using an LM subnetwork, we experiment with incorporating previously predicted labels using the multi-sequence cross-attention [46, 47] in the decoder and observe that our proposed formulation of having a LM subnetwork achieves better accuracy. Ablation study on impact of causal LM pre-training: Table 3 shows the perplexity of different pre-trained language models (LMs) and their impact on streaming SAR accuracy. Our observations reveal that, for Tedlium2, there is no significant accuracy difference associated with a stronger LM. However, in the case of Librispeech100, using a slightly superior LM results in an accuracy boost. Notably, the use of external text data further enhances accuracy, underscoring the effectiveness of our approach. 5. CONCLUSION We introduce a novel streaming SAR model that performs NAR decoding within a block while maintaining the AR property across blocks by encoding labels emitted in previous blocks using an LM subnetwork. Our approach includes a simple alignment decoding algorithm that helps to mitigate recognition errors due to block boundaries. We demonstrate the effectiveness of pre-training the LM subnetwork with external text data. We also incorporate intermediate CTC and “random block” regularization. Our experiments reveal superior accuracy compared to streaming NAR models and also competitive accuracy with streaming AR models while achieving a 2.5x reduction in latency. Future work will explore the use of external text data via advanced LM [48] or CTC-based text injection [49]. 6. ACKNOWLEDGEMENTS This work used NCSA Delta through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by NSF grants #2138259, #2138286, #2138307, #2137603, #2138296. 7. REFERENCES [1] R. Prabhavalkar et al., “End-to-end speech recognition: A survey,” arXiv preprint arXiv:2303.03329, 2023. [2] J. Li et al., “Recent advances in end-to-end automatic speech recognition,” APSIPA Transactions on Signal and Information Processing, vol. 11, no. 1, [3] S. Watanabe et al., “ESPnet: End-to-end speech processing toolkit,” in Proc. Interspeech, 2018. [4] A. Graves et al., “Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks,” in ICML 2006, vol. 148, 2006, pp. 369–376. [5] A. Graves, “Sequence transduction with recurrent neural networks,” CoRR, vol. abs/1211.3711, 2012. [6] J. Chorowski et al., “Attention-based models for speech recognition,” in Proc. NeurIPS, 2015, pp. 577–585. [7] A. Graves, A. Mohamed, and G. E. Hinton, “Speech recognition with deep recurrent neural networks,” in Proc. ICASSP, 2013. [8] G. Saon et al., “Advancing RNN transducer technology for speech recognition,” in Proc. ICASSP, 2021, pp. 5654–5658. [9] W. Chan et al., “Listen,