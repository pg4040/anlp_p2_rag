Title: SYNERGY BETWEEN HUMAN AND MACHINE APPROACHES TO SOUND/SCENE RECOGNITION AND PROCESSING: AN OVERVIEW OF ICASSP SPECIAL SESSION
Authors: Laurie M. Heller, Benjamin Elizalde, Bhiksha Raj, Soham Deshmukh
Section: 5. REFERENCES
[1] Tuomas Virtanen, Mark D Plumbley, and Dan Ellis, Computational analysis of sound scenes and events, Springer, 2018. [2] Richard F Lyon, Human and machine hearing: extract- ing meaning from sound, Cambridge University Press, 2017. [3] Jens Blauert, Spatial hearing: the psychophysics of hu- man sound localization, MIT press, 1997. [4] Bowen Zhi, Dmitry N. Zotkin, and Ramani Duraiswami, “Towards fast and convenient end-to-end hrtf personalization,” in ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 441–445. [5] Joseph Turian, Jordie Shier, Humair Raj Khan, Bhik- sha Raj, Björn W. Schuller, Christian J. Steinmetz, Colin Malloy, George Tzanetakis, Gissel Velarde, Kirk McNally, Max Henry, Nicolas Pinto, Camille Noufi, Christian Clough, Dorien Herremans, Eduardo Fonseca, Jesse Engel, Justin Salamon, Philippe Esling, Pranay Manocha, Shinji Watanabe, Zeyu Jin, and Yonatan Bisk, “HEAR: Holistic Evaluation of Audio Representations,” in Proceedings of the NeurIPS 2021 Competitions and Demonstrations Track, Douwe Kiela, Marco Ciccone, and Barbara Caputo, Eds. 06–14 Dec 2022, vol. 176 of Proceedings of Machine Learning Research, pp. 125– 145, PMLR. [6] Benjamin Elizalde, Radu Revutchi, Samarjit Das, Bhik- sha Raj, Ian Lane, and Laurie M. Heller, “Identifying actions for sound event classification,” in 2021 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), 2021, pp. 26–30. [7] Pranay Manocha, Adam Finkelstein, Richard Zhang, Nicholas J. Bryan, Gautham J. Mysore, and Zeyu Jin, “A Differentiable Perceptual Audio Metric Learned from Just Noticeable Differences,” in Proc. Interspeech 2020, 2020, pp. 2852–2856. [8] Anderson R. Avila, Hannes Gamper, Chandan Reddy, Ross Cutler, Ivan Tashev, and Johannes Gehrke, “Nonintrusive speech quality assessment using neural networks,” in ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019, pp. 631–635. [9] Jinkyu Lee and Ivan Tashev, “High-level feature rep- resentation using recurrent neural network for speech emotion recognition,” in Interspeech 2015, 2015. [10] Ishwarya Ananthabhotla, Cognitive Audio: Enabling Auditory Interfaces with an Understanding of How We Hear, Ph.D. thesis, Massachusetts Institute of Technology, 2022. [11] Shuai Wang, Yanmin Qian, and Kai Yu, “What does the speaker embedding encode?,” in Interspeech, 2017, pp. 1497–1501. [12] Neil Zeghidour, Olivier Teboul, Félix de Chau- mont Quitry, and Marco Tagliasacchi, “Leaf: A learnable frontend for audio classification,” ICLR, 2021. [13] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Is- mail, and Huaming Wang, “Clap: Learning audio concepts from natural language supervision,” arXiv preprint arXiv:2206.04769, 2022. [14] Soham Deshmukh, Benjamin Elizalde, and Huaming Wang, “Audio retrieval with wavtext5k and clap training,” arXiv preprint arXiv:2209.14275, 2022. [15] Abelino Jimenez, Benjamin Elizalde,