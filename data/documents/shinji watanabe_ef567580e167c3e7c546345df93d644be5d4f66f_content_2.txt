Title: AV-SUPERB: A MULTI-TASK EVALUATION BENCHMARK FOR AUDIO-VISUAL REPRESENTATION MODELS
Authors: Yuan Tseng, Layne Berry, Yi-Ting Chen, I-Hsiang Chiu, Hsuan-Hao Lin, Max Liu, Puyuan Peng, Yi-Jen Shih, Hung-Yu Wang, Haibin Wu, Po-Yao Huang, Chun-Mao Lai, Shang-Wen Li, David Harwath, Yu Tsao, Shinji Watanabe, Abdelrahman Mohamed, Chi-Luen Feng, Hung-yi Lee
Section: 4.4. Layer-wise Contribution Analysis
NeurIPS 2021 Competitions and Demonstrations Track. [10] Fida Mohammad Thoker et al., “How severe is benchmarksensitivity in video self-supervised learning?,” in ECCV, 2022. [11] Akash Kumar et al., “Benchmarking self-supervised video representation learning,” arXiv preprint arXiv:2306.06010, 2023. [12] Harry McGurk and John MacDonald, “Hearing lips and seeing voices,” Nature, 1976. [13] Asif A. Ghazanfar and Charles E. Schroeder, “Is neocortex essentially multisensory?,” Trends in Cognitive Sciences, 2006. [14] Yusuf Aytar et al., “Soundnet: Learning sound representations from unlabeled video,” in NeurIPS, 2016. [15] Relja Arandjelovic and Andrew Zisserman, “Look, listen and learn,” in ICCV, 2017. [16] Bruno Korbar et al., “Cooperative learning of audio and video models from self-supervised synchronization,” in NeurIPS, 2018. [17] Humam Alwassel et al., “Self-supervised learning by crossmodal audio-video clustering,” NeurIPS, 2020. [18] Mandela Patrick et al., “Space-time crop & attend: Improving cross-modal video representation learning,” in ICCV, 2021. [19] Will Kay et al., “The kinetics human action video dataset,” arXiv preprint arXiv:1705.06950, 2017. [20] Khurram Soomro et al., “Ucf101: A dataset of 101 human actions classes from videos in the wild,” arXiv preprint arXiv:1212.0402, 2012. [21] Dima Damen et al., “Rescaling egocentric vision: Collection, pipeline and challenges for epic-kitchens-100,” IJCV, 2022. [22] Martin Cooke et al., “An audio-visual corpus for speech perception and automatic speech recognition,” The Journal of the Acoustical Society of America, 2006. [23] Joon Son Chung et al., “Lip reading sentences in the wild,” in CVPR, 2017. [24] Triantafyllos Afouras et al., “Lrs3-ted: a large-scale dataset for visual speech recognition,” arXiv preprint arXiv:1809.00496, 2018. [25] A. Nagrani et al., “Voxceleb: a large-scale speaker identification dataset,” in Interspeech, 2017. [26] Joon Son Chung et al., “Voxceleb2: Deep speaker recognition,” in Interspeech, 2018. [27] Christoph Feichtenhofer et al., “A large-scale study on unsupervised spatiotemporal representation learning,” in CVPR, 2021. [28] Kristen Grauman et al., “Ego4d: Around the world in 3,000 hours of egocentric video,” in CVPR, 2022. [29] Wangchunshu Zhou et al., “VLUE: A multi-task multidimension benchmark for evaluating vision-language pretraining,” in ICML, 2022. [30] Linjie Li et al., “Value: A multi-task benchmark for videoand-language understanding evaluation,” in NeurIPS Track on Datasets and Benchmarks, 2021. [31] Paul Pu Liang et al., “Multibench: Multiscale benchmarks for multimodal representation learning,” in NeurIPS Track on Datasets and Benchmarks, 2021. [32] G. Potamianos et al., “Recent advances in the automatic recognition of audiovisual speech,” Proc. IEEE, 2003. [33] Arsha Nagrani et al., “Disentangled speech embeddings using cross-modal self-supervision,” in ICASSP, 2020. [34] Egils Avots et al., “Audiovisual emotion recognition in wild,” Machine Vision and Applications, 2019. [35] Bowen