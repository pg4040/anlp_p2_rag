Title: Grounding Language Models to Images for Multimodal Inputs and Outputs
Authors: Jing Yu Koh, Ruslan Salakhutdinov, Daniel Fried
Section: B. Further Analysis
or PaLM (540B) (Chowdhery et al., 2022). In future work, it will be interesting to test this, and trained models may learn additional interesting emergent behaviors (Wei et al., 2022). B.6. Text Generation Results In addition to the above evaluations, we also ran the zeroshot VQAv2 (Goyal et al., 2017). We apply the same normalization techniques from their GitHub repo6, and prompt the model with the prefix Q: {question} A: format. On zero-shot VQA, our model achieves a score of 28.51, which is better or comparable to prior methods: a reimplementation of Frozen achieves 25.53, while running the MAGMA pretrained model (which uses 25M image-text as training data, including the VQA training set), achieves 28.35. These results show that our approach is competitive with similar methods that use parameter efficient adaptation (Frozen, MAGMA), with the added bonus that our model can perform image retrieval interleaved with text. This allows us to handle a much wider range of tasks: for example, Frozen and MAGMA cannot produce images interleaved with generated text. Our model is also significantly more efficient (1 GPU day of training) compared to prior work (MAGMA uses 32 GPUs for 1.25 days).