Title: DOES COMPRESSING ACTIVATIONS HELP MODEL PARALLEL TRAINING?
Authors: Song Bian, Dacheng Li, Hongyi Wang, Eric P. Xing, Shivaram Venkataraman, MBZUAI Petuum
Section: 5 RELATED WORK
One direction is developed on the data parallelism setting, where workers communicate model gradients (Wang et al., 2021; Agarwal et al., 2022) during backward propagation. Common techniques to reduce the gradient communication include low-rank updates (Wang et al., 2018b), sparsification (Lin et al., 2017), and quantization (Seide et al., 2014; Bernstein et al., 2018; Dettmers, 2015). A more recent direction find that the activation produced during the forward propagation in neural networks is large, and thus compressing them is beneficial (Wang et al., 2022). In particular, they use quantization to compress the activation volume between pipeline parallelism workers. However, they focus on the geo-distributed setting where the network bandwidth is very low. In this paper, we study the effect of a rich set of popular compression techniques on tensor and pipeline parallelism, and in a typical cloud computing setting.