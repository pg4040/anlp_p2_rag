Title: The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios
Authors: Samuele Cornell, Matthew Wiesner, Shinji Watanabe, Desh Raj, Xuankai Chang, Paola Garcia, Matthew Maciejewski, Yoshiki Masuyama, Zhong-Qiu Wang, Stefano Squartini, Sanjeev Khudanpur
Section: 9. References
[22] V. Panayotov, G. Chen et al., “LibriSpeech: an ASR corpus based on public domain audio books,” in IEEE ICASSP, 2015. [23] L. Brandschain, D. Graff et al., “The mixer 6 corpus: Resources for cross-channel and text independent speaker recognition,” in LREC, 2010. [24] A. Radford, J. W. Kim et al., “Robust speech recognition via large-scale weak supervision,” ArXiv, 2022. [25] J. Li, V. Lavrukhin et al., “Jasper: An end-to-end convolutional neural acoustic model,” in InterSpeech, 2019. [26] S. Kriman, S. Beliaev et al., “Quartznet: Deep automatic speech recognition with 1d time-channel separable convolutions,” in IEEE ICASSP, 2019. [27] A. Baevski, Y. Zhou et al., “Wav2Vec 2.0: A framework for selfsupervised learning of speech representations,” in NeurIPS, 2020. [28] W.-N. Hsu, B. Bolte et al., “HuBERT: Self-supervised speech representation learning by masked prediction of hidden units,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, 2021. [29] S. Chen, C. Wang et al., “WavLM: Large-scale self-supervised pre-training for full stack speech processing,” IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, 2022. [30] S.-w. Yang, P.-H. Chi et al., “SUPERB: Speech processing universal performance benchmark,” in InterSpeech, 2021. [31] X. Chang, T. Maekaku et al., “End-to-end integration of speech recognition, speech enhancement, and self-supervised learning representation,” in InterSpeech, 2022. [32] Y. Masuyama, X. Chang et al., “End-to-end integration of speech recognition, dereverberation, beamforming, and self-supervised learning representation,” 2022. [33] E. Fonseca, X. Favory et al., “FSD50k: An open dataset of human-labeled sound events,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, 2021. [34] N. Kanda, J. Wu et al., “VarArray meets t-SOT: Advancing the state of the art of streaming distant conversational speech recognition,” ArXiv, 2022. [35] T. Yoshioka, X. Wang et al., “VarArray: Array-geometry-agnostic continuous speech separation,” in IEEE ICASSP), 2022. [36] N. Kanda, J. Wu et al., “Streaming multi-talker ASR with tokenlevel serialized output training,” in InterSpeech, 2022. [37] I. H. Toroslu and G. Üçoluk, “Incremental assignment problem,” Information Sciences, vol. 177, 2007. [38] J. G. Fiscus, J. Ajot et al., “Multiple dimension Levenshtein edit distance calculations for evaluating automatic speech recognition systems during simultaneous speech.” in LREC. Citeseer, 2006, pp. 803–808. [39] M.-W. C. Jacob Devlin and L. K. Toutanova, “BERT: Pre-training of deep bidirectional transformers for language understanding,” in NAACL-HLT, 2019. [40] A. Radford, J. W. Kim et al., “Robust speech recognition via large-scale weak supervision,” ArXiv, 2022. [41] C. Boeddeker, J. Heitkaemper et al., “Front-end processing for the CHiME-5 dinner party scenario,” in CHiME5 Workshop, 2018. [42] S. Watanabe,