Title: Model-Generated Pretraining Signals Improves Zero-Shot Generalization of Text-to-Text Transformers
Authors: Linyuan Gong, Chenyan Xiong, Xiaodong Liu, Payal Bajaj, Yiqing Xie, Alvin Cheung, Jianfeng Gao, Xia Song
Section: A Appendix
our finetuning datasets. A.4 Prompt-Finetuning Hyperparameters Once we have METRO-T5 pretrained on a natural language corpus, we discard the auxiliary model and keep the main model, which is a standard text-to-text Transformer. We finetune this model on multi-task training mixtures of NL-prompted datasets proposed by Sanh et al. (2022). Once the model parameters are initialized with pretrained METRO-T5, the finetuning procedure is standard sequence-to-sequence learning: the input sequence is fed to the encoder, and the target sequence serves as the ground truth to compute the cross-entropy loss of the decoder output. Each model is finetuned using hyperparameters listed in Table 6. Basically, we use the same hyperparameters as pretraining, except the peak learning rate is reduced to 0.1x and each target sequence is truncated to a max length of 256. We do not perform any checkpoint selection or hyperparameter selection, and simply use the last checkpoint at 125k steps of this single run for evaluation. A.5 Evaluation We evaluate zero-shot generalization on the T0 Eval benchmark (Sanh et al., 2022) and the Massive Multi-task Language Understanding (MMLU) benchmark (Hendrycks et al., 2020). T0 Eval consists of 11 held-out datasets in natural language inference, coreference, word sense disambiguation, and sentence completion, and details are shown in Table 7 MMLU includes example questions from 57 tasks such as maths, history, law, and medicine. Please refer to Hendrycks et al. (2020) for more details about MMLU. Each task in T0 Eval or MMLU is formulated as multiple-choice questions. We compute the log probability of each choice under the finetuned model and select the choice with the highest log probability as the prediction. A.6 Implementation Details Implementation We implement our T0 baseline and METRO-T0 based on fairseq1. The prompt templates to format the finetuing data are from the promptsource2 library (Bach et al., 2022). We evaluate pretrained models on the T0 Eval benchmark using transformers3 and t-zero4. Pretraining and Finetuning Costs. Pretraining METRO-T5 in the base setting takes 20.8 hours on 64x NVIDIA A100 (40GB) GPUs. The pretraining cost of METRO-T5 is T5 (our implementation) plus the auxiliary transformer, whose number of layers is 1/3 of the main transformerâ€™s encoder. Pretraining METRO-T5 in the base++ setting takes 159 hours on 128x NVIDIA A100 (40GB) GPUs. Pretraining METRO-T5 in the large++ setting takes 289 hours on 256x NVIDIA A100 (40GB) GPUs. In finetuning, we remove the auxiliary transformer and the RTD and CLM heads, so the finetuning cost of METRO-T5 and T5 are the same. Prompt-finetuning each base/base++ model takes about 22 hours on 64x