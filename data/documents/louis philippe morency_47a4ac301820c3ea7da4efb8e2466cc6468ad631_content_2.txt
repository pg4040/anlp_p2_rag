Title: SENTECON: Leveraging Lexicons to Learn Human-Interpretable Language Representations
Authors: Victoria Lin, Louis-Philippe Morency
Section: B Experimental Details
and we found 10âˆ’5 to be the best learning rate across all models. We trained for 15 epochs and selected the model with the best 5-fold cross-validation loss. All other hyperparameters were set to Trainer class defaults from the transformers library. The number of parameters for each of the deep language models used is reported in Table 9. The license names for the models are also provided. 6https://github.com/A2Zadeh/CMU-MultimodalSDK/ blob/master/LICENSE.txt 7https://catalog.ldc.upenn.edu/license/ ldc-non-members-agreement.pdf 10https://huggingface.co/ B.6 Computing resources SENTECON(+) requires only using an existing deep language model to generate embeddings and consequently is not particularly computationally demanding. Fine-tuning deep language models is more resource-intensive, but we use these only to a limited extent in our experiments, and only on small datasets. We estimate the number of GPU hours used in these experiments to be around 20. All experiments were conducted on machines with consumer-level NVIDIA graphics cards.