Title: Long-tailed Extreme Multi-label Text Classification by the Retrieval of Generated Pseudo Label Descriptions
Authors: Ruohong Zhang, Yau-Shian Wang, Yiming Yang, Donghan Yu, Tom Vu, Likun Lei
Section: A Appendix
models evaluated on the all-label prediction by the micro-averaged P@k metric is reported in table 5. Our model is compared against the SOTA sparse and dense classifiers. DEPL+c achieves the best or second best performance on all the 4 benchmark datasets, achieving comparable results to the previous best SOTA models. We argue that though our models perform significantly better on the tail label prediction, the improvement is not announced in the overall label prediction. One of the problem is on the choice of evaluation metric: the micro-averaged precision metric is averaged over instances and can be dominated by the common categories with more test instances. Therefore, the metric is incapable of reflecting the tail label performance. We want to emphasis that over 26, 545 (88.65%) labels in the Wiki10-31K dataset belong to the tail labels with less than 10 training instances, constituting a majority of the label space. The overall classification precision (P@k) only reflects a part of the success of a classification system, and the tail label evaluation is yet another part. The results also shows while our model improves on the tail label prediction, the overall label prediction comparable to the other dense and sparse SOTA models. When our model is compared on the Wiki-500K dataset, our backbone is the same as X-Transformer. DEPL achieve on par performance with Wiki-500k showing that the quality of overall ranking is similar. However, the DEPL+c achieves better performance, demonstrating the enhanced performance by combining retrieval with classification. By comparing the DEPL+c and its retrievalbased counterpart DEPL , we uncover a trade-off between the head label and tail label prediction. We observe that the DEPL outperforms the DEPL+c on the tail label prediction, but not on the all-label prediction. This shows that incorporating a classifier with label embeddings trained from supervised signal can boost performance on a high data regime. The dense classifier could learn more expressive label representation from the frequent co-occurrence of document and label pairs when the training instances are abundant, while the retrieval system is better at matching the semantic of document and label texts when data is scarce. Each of the modules captures a certain aspect of the data heuristic for text classification and a combination of them by sharing the BERT encoder yields better performance. Lastly, the sparse classifiers generally underperform the neural models and are comparable to our implement of SVM. We observe that DEPL can outperform the sparse models, which agrees with our theoretical analysis. Although the pseudo labels are extracted from the SVM