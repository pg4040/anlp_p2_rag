Title: TOWARDS PRACTICAL AND EFFICIENT IMAGE-TO-SPEECH CAPTIONING WITH VISION-LANGUAGE PRE-TRAINING AND MULTI-MODAL TOKENS
Authors: Minsu Kim, Jeongsoo Choi, Soumi Maiti, Jeong Hun Yeo, Shinji Watanabe, Yong Man Ro
Section: 3.3. Experimental Results
visual models from natural language supervision,” in ICML, PMLR, 2021, pp. 8748–8763. [21] J. Wang et al., “Git: A generative image-to-text transformer for vision and language,” Transactions on Machine Learning Research, 2022. [22] P. Esser, R. Rombach, and B. Ommer, “Taming transformers for highresolution image synthesis,” in Proc. CVPR, 2021, pp. 12 873–12 883. [23] J. Yu et al., “Vector-quantized image modeling with improved vqgan,” in ICLR, 2021. [24] T.-Y. Lin et al., “Microsoft coco: Common objects in context,” in Proc. ECCV, Springer, 2014, pp. 740–755. [25] M. Hodosh, P. Young, and J. Hockenmaier, “Framing image description as a ranking task: Data, models and evaluation metrics,” Journal of Artificial Intelligence Research, vol. 47, pp. 853–899, 2013. [26] C.-C. Lo et al., “Mosnet: Deep learning-based objective assessment for voice conversion,” Proc. Interspeech, 2019. [27] S. Maiti et al., “Speechlmscore: Evaluating speech generation using speech language model,” in ICASSP, IEEE, 2023, pp. 1–5. [28] W.-N. Hsu et al., “Text-free image-to-speech synthesis using learned segmental units,” in Proc. ACL, 2021, pp. 5284–5300. [29] J. Effendi, S. Sakti, and S. Nakamura, “End-to-end image-to-speech generation for untranscribed unknown languages,” IEEE Access, vol. 9, pp. 55 144–55 154, 2021. [30] A. Polyak et al., “Speech resynthesis from discrete disentangled selfsupervised representations,” in Proc. Interspeech, 2021. [31] J. Kong, J. Kim, and J. Bae, “Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis,” NIPS, 2020. [32] A. Dosovitskiy et al., “An image is worth 16x16 words: Transformers for image recognition at scale,” in ICLR, 2020. [33] K. Han et al., “A survey on vision transformer,” IEEE TPAMI, vol. 45, no. 1, pp. 87–110, 2022. [34] A. Mnih and K. Gregor, “Neural variational inference and learning in belief networks,” in ICML, PMLR, 2014, pp. 1791–1799. [35] S. Park et al., “Seit: Storage-efficient vision training with tokens using 1% of pixel storage,” in Proc. ICCV, 2023. [36] D. Harwath and J. Glass, “Deep multimodal semantic embeddings for speech and images,” in ASRU, IEEE, 2015, pp. 237–244. [37] A. Karpathy and L. Fei-Fei, “Deep visual-semantic alignments for generating image descriptions,” in Proc. CVPR, 2015, pp. 3128–3137. [38] A. Baevski et al., “Wav2vec 2.0: A framework for self-supervised learning of speech representations,” Advances in neural information processing systems, vol. 33, pp. 12 449–12 460, 2020. [39] K. Papineni et al., “Bleu: A method for automatic evaluation of machine translation,” in Proc. ACL, 2002, pp. 311–318. [40] M. Denkowski and A. Lavie, “Meteor universal: Language specific translation evaluation for any target language,” in Proc. workshop on statistical machine translation, 2014,