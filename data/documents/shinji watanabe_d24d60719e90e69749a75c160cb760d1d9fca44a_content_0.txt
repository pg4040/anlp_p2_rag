Title: Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff
Authors: Peter Polák, Brian Yan, Shinji Watanabe, Alex Waibel, Ondřej Bojar
Section: 6. References
[1] C. Fügen, A. Waibel, and M. Kolss, “Simultaneous translation of lectures and speeches,” Machine translation, vol. 21, pp. 209– 252, 2007. [2] A. Anastasopoulos et al., “FINDINGS OF THE IWSLT 2021 EVALUATION CAMPAIGN,” in Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021), 2021, pp. 1–29. [3] A. Anastasopoulos et al., “FINDINGS OF THE IWSLT 2022 EVALUATION CAMPAIGN,” in Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022), 2022. [4] M. Ma et al., “Stacl: Simultaneous translation with implicit anticipation and controllable latency using prefix-to-prefix framework,” in Proc. ACL, 2019, pp. 3025–3036. [5] X. Ma, J. Pino, and P. Koehn, “SimulMT to SimulST: Adapting simultaneous text translation to end-to-end simultaneous speech translation,” in Proc. ACL, 2020, pp. 582–587. [6] D. Liu, G. Spanakis, and J. Niehues, “Low-Latency Sequenceto-Sequence Speech Recognition and Translation by Partial Hypothesis Selection,” in Proc. Interspeech, 2020, pp. 3620–3624. [7] P. Polák et al., “CUNI-KIT System for Smultaneous Speech Translation Task at IWSLT 2022,” in Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022), 2022. [8] S. Watanabe et al., “Hybrid ctc/attention architecture for endto-end speech recognition,” IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240–1253, 2017. [9] X. Ma et al., “Monotonic multihead attention,” arXiv preprint arXiv:1909.12406, 2019. [10] C.-C. Chiu and C. Raffel, “Monotonic chunkwise attention,” in Proc. ICLR, 2017. [11] E. Tsunoo, Y. Kashiwagi, and S. Watanabe, “Streaming transformer asr with blockwise synchronous beam search,” in Proc. SLT, 2021, pp. 22–29. [12] K. Deng et al., “Blockwise streaming transformer for spoken language understanding and simultaneous speech translation,” arXiv preprint arXiv:2204.08920, 2022. [13] N. Moritz, T. Hori, and J. Le, “Streaming automatic speech recognition with the transformer model,” in Proc. ICASSP, 2020, pp. 6074–6078. [14] Y. Shi et al., “Emformer: Efficient memory transformer based acoustic model for low latency streaming speech recognition,” in Proc. ICASSP, 2021, pp. 6783–6787. [15] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning with neural networks,” Advances in neural information processing systems, vol. 27, 2014. [16] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly learning to align and translate,” arXiv preprint arXiv:1409.0473, 2014. [17] J. Niehues et al., “Dynamic transcription for low-latency speech translation,” in 17th Annual Conference of the International Speech Communication Association, INTERSPEECH 2016; Hyatt Regency San FranciscoSan Francisco; United States; 8 September 2016 through 16 September 2016, ser. Proceedings of the Annual Conference of the International Speech Communication Association. Ed. : N. Morgan, vol. 08-12-September-2016,