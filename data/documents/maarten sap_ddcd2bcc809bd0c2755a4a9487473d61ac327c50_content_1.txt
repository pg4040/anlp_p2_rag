Title: Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models
Authors: Natalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, Vered Shwartz
Section: 6 Summary of Findings and Insights
to be mentioned. Finally, we reassess the finding of Sap et al. (2022) that LLMs perform better on predicting the mental states of the main character vs. others (SIQA, §5.1); Sap et al. (2022) suggested that this might be due to centering theory (Grosz et al., 1995), according to which texts tends to focus on describing a single protagonist. ELIZA Effect & Anecdotal Generative vs. Automatic Large-Scale Multiple-Choice Testing The impressive anecdotal examples produced by LLMs in generative settings (e.g., observed with ChatGPT and GPT4 web-demo; Bubeck et al., 2023), tends to captivate non-expert individuals. However, it is important to recognize that these models are specifically designed to generate text that appears high-quality to human observers (Ouyang et al., 2022). This inherent bias in their design can lead to the “ELIZA effect” (Weizenbaum, 1976; Shapira et al., 2023b), i.e. the human assumption that computer behaviors are analogous to human behaviors. Thus, the illusion that a LLM has acquired human-like N-ToM often says more about the humans reading the text than about the model itself (Whang, 2023). Moreover, later models are by design trained to practice “epistemic humility” (i.e., hedge and provide multiple possible answers; Ouyang et al., 2022, p .17). This often leads them to provide rationales for each given answer without committing to actually answering the question. But humans might fall prey to confirmation bias and simply see the right answer and its rational and conclude that the model has gotten it correctly. We thus argue that in order to conclude whether a certain model possesses a certain ability, it is crucial to quantify the performance across multiple large-scale datasets, preferably using an automatic evaluation method. Using psychological tests designed for humans on LLMs In clinical psychology, tests designed for humans are carefully constructed and vetted to ensure that they have external and internal validity, i.e., that they are measuring what they aim to measure (Frank et al., 2023). While there is evidence that a person’s success in one ToM task can indicate their ToM abilities (e.g., Milligan et al., 2007), this does not necessarily transfer to models. Therefore, it is important to be cautious when drawing conclusions about ToM in models based on their performance on a few tasks (Marcus and Davis, 2023). In general, when a system succeeds on an instrument designed for humans, we can’t draw the same conclusions as we would for humans (e.g., that they have ToM). Instead, we need to consider other explanations (e.g., that they are relying on heuristics). The same holds