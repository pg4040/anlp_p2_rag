Title: SPEAKER-INDEPENDENT ACOUSTIC-TO-ARTICULATORY SPEECH INVERSION
Authors: Peter Wu, Li-Wei Chen, Cheol Jun Cho, Shinji Watanabe, Louis Goldstein, Alan W Black, Gopala K. Anumanchipalli
Section: 5.3. Resynthesis Analysis without EMA Labels
We can also compare inversion performance through resynthesizing speech from estimated features (Section 4.2) and evaluating synthesis quality. To study performance on unseen speakers this way, we resynthesize ARCTIC utterances using our synthesis model and each inversion method. Then, we compute the DTW-MCD [39] between the predicted and ground truth waveforms. Table 3 summarizes these results. Our approach outperforms the baseline for all speakers, consistent with our inversion results in Table 1 and earlier singlespeaker experiments [40]. We note that the MCD values are high since we did not optimize for synthesis quality in this work. Given the nascent nature of the multi-speaker articulatory synthesis direction, we plan to improve such models and continue validating inversion performance in this manner using MCD and more synthesis metrics [39, 41] moving forward. 6. CONCLUSION In this work, we devise an acoustic-to-articulatory inversion (AAI) approach in the context of generalizing to unseen speakers, improving state-of-the-art by 12.5% on the HPRC task [11]. We also show the interpretability of the estimated representations through directly comparing them with speech production behavior, evincing how this analysis can be done without labeled articulatory data. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels. We demonstrate the efficacy of this evaluation method using ARCTIC and observe results consistent with our inversion correlation metrics. In the future, we plan to extend our methodology to high-fidelity resynthesis and AAI for articulatory features beyond EMA. 7. ACKNOWLEDGEMENTS This research is supported by the following grants to PI Anumanchipalli — NSF award 2106928, Google Research Scholar Award, Rose Hills Foundation and Noyce Foundation. 8. REFERENCES [1] Y. Lu, C. E. Wiltshire, K. E. Watkins, et al., “Characteristics of articulatory gestures in stuttered speech: A case study using real-time magnetic resonance imaging,” Journal of Communication Disorders, vol. 97, pp. 106213, 2022. [2] H.-S. Choi, J. Lee, W. Kim, et al., “Neural analysis and synthesis: Reconstructing speech from self-supervised representations,” NeurIPS, 2021. [3] A. Polyak et al., “Speech Resynthesis from Discrete Disentangled Self-Supervised Representations,” in Interspeech, 2021. [4] G. K. Anumanchipalli, J. Chartier, and E. F. Chang, “Speech synthesis from neural decoding of spoken sentences,” Nature, vol. 568, no. 7753, pp. 493–498, 2019. [5] G. Fant, “What can basic research contribute to speech synthesis?,” Journal of Phonetics, vol. 19, no. 1, pp. 75–90, 1991. [6] P. Rubin, T. Baer, and P. Mermelstein, “An articulatory synthesizer for perceptual research,” The Journal of the Acoustical Society of America, vol. 70, no. 2, pp. 321–328, 1981. [7] C. Scully, “Articulatory synthesis,”