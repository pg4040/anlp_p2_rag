Title: AV-SUPERB: A MULTI-TASK EVALUATION BENCHMARK FOR AUDIO-VISUAL REPRESENTATION MODELS
Authors: Yuan Tseng, Layne Berry, Yi-Ting Chen, I-Hsiang Chiu, Hsuan-Hao Lin, Max Liu, Puyuan Peng, Yi-Jen Shih, Hung-Yu Wang, Haibin Wu, Po-Yao Huang, Chun-Mao Lai, Shang-Wen Li, David Harwath, Yu Tsao, Shinji Watanabe, Abdelrahman Mohamed, Chi-Luen Feng, Hung-yi Lee
Section: 4.4. Layer-wise Contribution Analysis
Shi et al., “Learning audio-visual speech representation by masked multimodal cluster prediction,” in ICLR, 2022. [36] Himangi Mittal et al., “Learning state-aware visual representations from audible interactions,” in NeurIPS, 2022. [37] Sangho Lee et al., “Parameter efficient multimodal transformers for video representation learning,” in ICLR, 2021. [38] Po-Yao Huang et al., “Mavil: Masked audio-video learners,” arXiv preprint arXiv:2212.08071, 2022. [39] Ankita Pasad et al., “Layer-wise analysis of a self-supervised speech representation model,” in ASRU, 2021. [40] Jort F. Gemmeke et al., “Audio set: An ontology and humanlabeled dataset for audio events,” in ICASSP, 2017. [41] Honglie Chen et al., “Vggsound: A large-scale audio-visual dataset,” in ICASSP, 2020. [42] Sanyuan Chen et al., “Wavlm: Large-scale self-supervised pretraining for full stack speech processing,” IEEE Journal of Selected Topics in Signal Processing, 2022. [43] Jason Phang et al., “Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks,” arXiv preprint arXiv:1811.01088, 2018. [44] Alex Wang et al., “Can you tell me how to get past sesame street? sentence-level pretraining beyond language modeling,” in ACL, 2019.