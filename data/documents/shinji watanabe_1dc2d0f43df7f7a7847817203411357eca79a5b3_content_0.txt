Title: Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute
Authors: William Chen, Xuankai Chang, Yifan Peng, Zhaoheng Ni, Soumi Maiti, Shinji Watanabe
Section: 7. References
[1] A. Vaswani et al., “Attention is all you need,” Advances in neural information processing systems, vol. 30, 2017. [2] A. Srivastava et al., “Beyond the imitation game: Quantifying and extrapolating the capabilities of language models,” arXiv preprint arXiv:2206.04615, 2022. [3] A. Wang et al., “GLUE: A multi-task benchmark and analysis platform for natural language understanding,” in Proc. EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, 2018, pp. 353–355. [4] A. Wang et al., “SuperGLUE: A stickier benchmark for generalpurpose language understanding systems,” in Proc. NeurIPS, H. Wallach et al., Eds., vol. 32, 2019. [5] S.-w. Yang et al., “SUPERB: Speech Processing Universal PERformance Benchmark,” in Proc. Interspeech, 2021, pp. 1194– 1198. [6] J. Deng et al., “ImageNet: A large-scale hierarchical image database,” in 2009 IEEE Conference on Computer Vision and Pattern Recognition, 2009, pp. 248–255. [7] X. Chang et al., “An exploration of self-supervised pretrained representations for end-to-end speech recognition,” in Proc. ASRU, 2021, pp. 228–235. [8] A. Mohamed et al., “Self-supervised speech representation learning: A review,” IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1179–1210, 2022. [9] S. Schneider et al., “wav2vec: Unsupervised Pre-Training for Speech Recognition,” in Proc. Interspeech, 2019, pp. 3465– 3469. [10] A. Baevski et al., “wav2vec 2.0: A framework for selfsupervised learning of speech representations,” in Proc. NeurIPS, H. Larochelle et al., Eds., vol. 33, 2020, pp. 12 449– 12 460. [11] W.-N. Hsu et al., “HuBERT: Self-supervised speech representation learning by masked prediction of hidden units,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451–3460, 2021. [12] S. Chen et al., “WavLM: Large-scale self-supervised pretraining for full stack speech processing,” IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505– 1518, 2022. [13] C. Wang et al., “Improving self-supervised learning for speech recognition with intermediate layer supervision,” in Proc. ICASSP, 2022, pp. 7092–7096. [14] M. Caron et al., “Deep clustering for unsupervised learning of visual features,” in Proceedings of the European conference on computer vision (ECCV), 2018, pp. 132–149. [15] J. Devlin et al., “BERT: Pre-training of deep bidirectional transformers for language understanding,” in Proc. ACL, 2019, pp. 4171–4186. [16] H.-J. Chang, S.-w. Yang, and H.-y. Lee, “DistilHuBERT: Speech representation learning by layer-wise distillation of hidden-unit BERT,” in Proc. ICASSP, 2022. [17] Y. Lee et al., “FitHuBERT: Going Thinner and Deeper for Knowledge Distillation of Speech Self-Supervised Models,” in Proc. INTERSPEECH, 2022. [18] T. Ashihara et al., “Deep versus Wide: An Analysis of Student Architectures for Task-Agnostic Knowledge Distillation