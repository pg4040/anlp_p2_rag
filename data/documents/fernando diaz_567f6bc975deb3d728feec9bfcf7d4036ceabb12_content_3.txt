Title: Fairness Through Domain Awareness: Mitigating Popularity Bias For Music Discovery
Authors: Rebecca Salganik, Fernando Diaz, Golnoosh Farnadi
Section: 5.2 RQ2: How does our individual fairness approach compare to existing methods for mitigating popularity bias?
Using our formulation of popularity we can see that the two datasets have different distributions of popularity in their training data which, in turn, helps explain fairness/performance tradeoffs. items, but also musically relevant ones for recommendation. Finally, our method’s ability to interpolate between these two perspectives of content and consumption patterns, shows that REDRESS/BOOST is able to recommend similar ratios of niche items compared to the bare features while having significantly better performance. Hyperparameter Sensitivity: While the results in Table 3 are compared among the best hyperparameter tuning that balances between utility and fairness, we also present Figure 4 where we show the balance between %LT and recall along the range of each method’s hyperparameter value. For example, xQuAD, ZeroSum, REDRESS and BOOST all have ranges that scale between (0.1, 0.9) and MACR has a value somewhere along (0, 45). As we can see, for any value of hyperparameter along the various methods, REDRESS and BOOST are able to outperform the collection of benchmarks. Given these results, we conclude that our BOOST approach is able to achieve the most effective debiasing performance while REDRESS is able to achieve the most balanced performance. Popularity Definition: As we can see in Figure 6 the definition of popularity plays a significant role in the model selection method especially in the case where user preferences encoded in the training data skew towards popular items. In particular, using a less granular Figure 6: Group By Group Analysis of Recommendations: we look at a breakdown of the recommendations for each dataset. We define visibility as the number of times an item from this group appears in the recommendations normalized by the total number of items in the recommendation lists. Bins are defined using the methodology of Section 3.3 where bin 0 has the lowest popularity. definition for popularity bins can synthetically inflate the performance of %𝐿𝑇 and 𝐿𝑇𝐶𝑣𝑔. For example, we can see that methods like xQuAD and ZeroSum are selecting a majority of their items from bins 1,2 or 3. Using a classical long tail methodology, these differences would not be as visible, masking distinctions among the baselines’ fairness.