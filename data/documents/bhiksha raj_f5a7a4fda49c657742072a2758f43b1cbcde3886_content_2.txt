Title: CONTINUAL CONTRASTIVE SPOKEN LANGUAGE UN- DERSTANDING
Authors: Umberto Cappellazzo, Enrico Fini, Muqiao Yang, Daniele Falavigna, Alessio Brutti, Bhiksha Raj
Section: 4.2 COCONUT
model. This alignment is achieved via a supervised multi-modal (i.e., audio-text) contrastive learning objective where feature representations of samples sharing the same intent token are attracted while the others are pushed away. Similar to (Kwon et al., 2022), we use the [CLS] text token (ycls) for performing the multi-modal alignment. Furthermore, following (Cha et al., 2021), we always treat the rehearsal samples as negatives, preventing them from being anchors during the learning process. This design choice is buttressed by two motivations: 1) rehearsal data have been learned by the previous model already and are preserved via the NSPT loss, and 2) we encourage the model to produce clusters for the new data that are separated from those of the rehearsal data. Formally, the MM loss takes the following form: LMM = ∑ k∈Ic −1 |P(k)| ∑ p∈P(k) [ log exp(ak · tp/τ)∑ i∈I exp(ak · ti/τ) + log exp(tk · ap/τ)∑ i∈I exp(tk · ai/τ) ] . (6) The first term of the internal loss is the audio-to-text component, whereas the second is the text-toaudio component (Zhang et al., 2022). The presence of both directions (A → T and T → A) makes the MM loss symmetric. All in all, COCONUT minimizes the following loss: L = LASR + λMMLMM + λNSPTLNSPT, (7) where lambdas are loss-specific weights. An overview of COCONUT is illustrated in Figure 1.