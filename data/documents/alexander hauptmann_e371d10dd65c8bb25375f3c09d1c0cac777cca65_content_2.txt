Title: Hyperbolic vs Euclidean Embeddings in Few-Shot Learning: Two Sides of the Same Coin
Authors: Gabriel Moreira, Manuel Marques, Alexander Hauptmann
Section: 4. Few-shot in the boundary
and kr2eff . A comparison between this metric, the ℓ2 and the ℓ22 is shown in Fig. 3. As we observe on the rightmost plot, the derivative of dPdk at α → 0 surges as we approach the boundary. As this happens, the penalty applied to the angular distance α increases. Surprisingly, the Euclidean metric ℓ2 (center plot) is not that different from the hyperbolic. In fact, it becomes a good approximation far from the boundary. In contrast, consider the squared Euclidean distance ℓ22 depicted in the leftmost plot, often employed in prototypical networks since it defines a Bregman divergence [25]. Its derivative approaches 0 as α → 0. Prior work conducted on hyperbolic prototypical learning benchmarked Euclidean space via ℓ22. Based on the previous paragraph, we propose instead fixed-radius Euclidean embeddings with the ℓ2 ∝ √ 1− cos(α) metric. Given a radius hyperparameter r = 1/ √ k (k > 0 is a spherical curvature), and the Euclidean backbone f(·; θ), the embeddings fed to the prototypical loss (13) are computed as r f(x; θ) ∥f(x; θ)∥2 . (22) As our experiments show, this Euclidean architecture is ahead of the hyperbolic few-shot state-of-the-art in most of the experiments conducted.