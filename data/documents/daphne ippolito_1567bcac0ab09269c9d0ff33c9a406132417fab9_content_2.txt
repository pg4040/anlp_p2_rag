Title: A Pretrainer’s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity
Authors: Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, Daphne Ippolito
Section: 9 Related Work
al. (2022) find its quality judgments are unaligned with factuality or literary acclaim but are instead aligned with some notion of langauge ideology more correlated with wealthier zip codes. Works in the vision domain show data filtering has important detoxification benefits but can reduce performance (Nichol et al., 2022) or introduce other biases (Nichol, 2022). In summary, pretraining data filters are ubiquitous in the development of non-toxic and high-quality models, but they are prone to reducing their abilities to serve underrepresented communities and may introduce new biases. Additional work has shown that instruction tuning (Chung et al., 2022; Longpre et al., 2023) and forms of alignment tuning (Ouyang et al., 2022; Bai et al., 2022) have both reduced unwanted toxic generation. Data & Time Natural language is known to evolve and change over time (Altmann et al., 2009; Labov, 2011; Eisenstein et al., 2014; Jaidka et al., 2018). As language’s distribution shifts, the ability of models to perform well on new test sets has also been shown to degrade, due to their static knowledge of recent events, syntactic and semantic practices (Lazaridou et al., 2021; Agarwal and Nenkova, 2022; Longpre et al., 2021). Luu et al. (2021); Lazaridou et al. (2021); Liska et al. (2022); Yao et al. (2022); Zhang and Choi (2021); Jang et al. (2022) offer evaluation sets to measure this phenomena. Proposed remedies include finetuning on more recent data (Luu et al., 2021), adaptive/continuous pretraining (Lazaridou et al., 2021; Röttger and Pierrehumbert, 2021), data augmentation (Singh and Ortega, 2022), modeling text with its timpestamps (Dhingra et al., 2022). To our knowledge, no work has thoroughly investigated the effects of temporal degradation when pretraining from scratch. Data & Domains The composition of public datasets, like C4 and the Pile, is guided mostly by licensing, which severely restricts availability. Even so, Villalobos et al. (2022); Nostalgebraist (2022); Hoffmann et al. (2022) suggest we are imminently exhausting high-quality text data on the web to train computeoptimal larger LMs, at least with existing training efficiency. This poses a challenge, given the demonstrated importance of high quality and diverse training data to strong generalization (Gao et al., 2020; Papadimitriou and Jurafsky, 2020). A great deal of literature has dedicated itself to adapting static pretrained models to new downstream domains, using domain adaptive pretraining (Gururangan et al., 2020), finding intermediate finetuning tasks (Pruksachatkun et al., 2020), dynamically balancing data sources (Wang et al., 2020), data selection (Iter and Grangier, 2021; Albalak et al., 2023), augmentation (Longpre et al., 2019), and active learning (Longpre