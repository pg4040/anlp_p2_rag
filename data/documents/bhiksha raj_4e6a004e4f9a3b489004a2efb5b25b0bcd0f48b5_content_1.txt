Title: Understanding Political Polarisation using Language Models: A dataset and method
Authors: Samiran Gode, Supreeth Bare, Bhiksha Raj, Hyungon Yoo
Section: Related Work
use of diachronic-word embedding association tests (WEAT). Other techniques that are implemented include count-based statistics dependent on a highly popular lexicon cloze test using BERT as a base model (an idea we could consider after data attention) and bias recognition using WEAT. The final model is a combination of the above three. This paper is highly relevant to our project as it uses a similar idea of our own. It uses aforementioned models to predict bias, i.e. sentiment prediction. In our project, we use data to predict political sentiment and attempt to classify certain features as being precursors to classification. (Rajani et al. 2019) tried to improve speech-based models on their ability to verbalize the reasoning that they learned during training. It uses the CAGE framework (CommonSense Auto-Generated Explanations) on the common sense explanation dataset to increase the effectiveness by 10 percent. It introduces improvements over the use of BiDAF++ (augmented with self-attention layer) in these newer models. It further uses NLE as rationale generalization within the second phase primarily as means for sentiment analysis. In this paper, Mturk (from Amazon) is used to generate explanations for the dataset. CAGE primarily uses a questionanswer format with 3 options, a label and the best explanation for that label. Furthermore, other evaluation parameters affecting performance are tested and may be used in our project either as verification models or otherwise. CAGE is certainly an interesting choice for verification given the higher accuracy it attains. A factor to be considered however is that the types of datasets and models are very different. Thus certain modifications will be made to the above framework. (Devlin et al. 2018) is the introduction paper for BERT, a model that will be used extensively. It also shows the results of fine-tuning BERT. These indirectly or directly will be used either as pre-trained constraints or as tuning methods. petroni2019language (Petroni et al. 2019) Demonstrates the ability of pretrained high-capacity models like BERT and ELMo to be used as knowledge repositories. This is mainly based on 3 observations, (1) The relational knowledge of these models is competitive to that of an NLP with access to certain oracle knowledge. (2) The effectiveness of BERT in an open domain question answer test and (3) The fact that certain facts are easily learnable. The Authors also demonstrate the usage of other models (unidirectional and bi-directional) in the study, namely ’fariseq-fconv’ and ’Transformer-XL’. They conclude by showing that BERT-Large is able to outperform other models and compete even with supervised models for the same