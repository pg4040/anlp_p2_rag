Title: TOWARD UNIVERSAL SPEECH ENHANCEMENT FOR DIVERSE INPUT CONDITIONS
Authors: Wangyou Zhang, Kohei Saijo, Zhong-Qiu Wang, Shinji Watanabe, Yanmin Qian
Section: 5. REFERENCES
[1] J. Benesty et al., Springer handbook of speech processing. Springer, 2008, vol. 1. [2] D. S. Williamson et al., “Time-frequency masking in the complex domain for speech dereverberation and denoising,” IEEE/ACM Trans. ASLP., vol. 25, no. 7, pp. 1492–1501, 2017. [3] A. Li et al., “Glance and gaze: A collaborative learning framework for single-channel speech enhancement,” Applied Acoustics, vol. 187, p. 108499, 2022. [4] S. Zhao et al., “FRCRN: Boosting feature representation using frequency recurrence for monaural speech enhancement,” in ICASSP, 2022, pp. 9281–9285. [5] Y. Xu et al., “A regression approach to speech enhancement based on deep neural networks,” IEEE/ACM Trans. ASLP., vol. 23, no. 1, pp. 7–19, 2014. [6] Z.-Q. Wang et al., “Complex spectral mapping for singleand multi-channel speech enhancement and robust ASR,” IEEE/ACM Trans. ASLP., vol. 28, pp. 1778–1787, 2020. [7] A. Li et al., “Taylor, can you hear me now? a Taylor-unfolding framework for monaural speech enhancement,” in Proc. IJCAI, 2022, pp. 4193–4200. [8] L. Liu et al., “A mask free neural network for monaural speech enhancement,” in Interspeech, 2023, pp. 2468–2472. [9] S. Pascual et al., “SEGAN: Speech enhancement generative adversarial network,” in Interspeech, 2017, pp. 3642–3646. [10] S. Maiti and M. I. Mandel, “Speech denoising by parametric resynthesis,” in ICASSP, 2019, pp. 6995–6999. [11] S.-W. Fu et al., “MetricGAN+: An improved version of MetricGAN for speech enhancement,” in Interspeech, 2021, pp. 201–205. [12] Y.-J. Lu et al., “Conditional diffusion probabilistic model for speech enhancement,” in ICASSP, 2022, pp. 7402–7406. [13] J. Serrà et al., “Universal speech enhancement with scorebased diffusion,” arXiv preprint arXiv:2206.03065, 2022. [14] R. Mira et al., “LA-VocE: Low-SNR audio-visual speech enhancement using neural vocoders,” in ICASSP, 2023, pp. 1–5. [15] Y. Luo et al., “End-to-end microphone permutation and number invariant multi-channel speech separation,” in ICASSP, 2020, pp. 6394–6398. [16] T. Yoshioka et al., “VarArray: Array-geometry-agnostic continuous speech separation,” in ICASSP, 2022, pp. 6027–6031. [17] A. Pandey et al., “Time-domain ad-hoc array speech enhancement using a triple-path network,” in Interspeech, 2022, pp. 729–733. [18] Z. Chen et al., “Continuous speech separation: Dataset and analysis,” in ICASSP, 2020, pp. 7284–7288. [19] K. Saito et al., “Sampling-frequency-independent audio source separation using convolution layer based on impulse invariant method,” in Proc. EUSIPCO, 2021, pp. 321–325. [20] J. Paulus and M. Torcoli, “Sampling frequency independent dialogue separation,” in Proc. EUSIPCO, 2022, pp. 160–164. [21] J. Yu and Y. Luo, “Efficient monaural speech enhancement with universal sample rate band-split RNN,” in ICASSP, 2023, pp. 1–5. [22] C. Valentini-Botinhao et al., “Speech enhancement for a noiserobust