Title: UNIVERSLU: UNIVERSAL SPOKEN LANGUAGE UNDERSTANDING FOR DIVERSE CLASSIFICATION AND SEQUENCE GENERATION TASKS WITH A SINGLE NETWORK
Authors: Siddhant Arora, Hayato Futami, Jee-weon Jung, Yifan Peng, Roshan Sharma, Yosuke Kashiwagi, Emiru Tsunoo, Shinji Watanabe
Section: 7. REFERENCES
https://chat.openai.com/, Accessed: 2023-07-01. [55] L. Lugosch, M. Ravanelli, P. Ignoto, V. S. Tomar, and Y. Bengio, “Speech model pre-training for end-to-end SLU,” in Proc. Interspeech, 2019. [56] A. Saade et al., “Spoken language understanding on the edge,” vol. abs/1810.12735, 2018. [57] B. Agrawal, M. Müller, M. Radfar, S. Choudhary, A. Mouchtaris, and S. Kunzmann, “Tie your embeddings down: Cross-modal latent spaces for end-to-end spoken language understanding,” arXiv preprint arXiv:2011.09044, 2020. [58] E. Bastianelli, A. Vanzo, P. Swietojanski, and V. Rieser, “SLURP: A spoken language understanding resource package,” in Proc. EMNLP, 2020. [59] P. Tomasello et al., “STOP: A dataset for Spoken Task Oriented Semantic Parsing,” in CoRR. [60] P. Warden, “Speech commands: A dataset for limited-vocabulary speech recognition,” CoRR, vol. abs/1804.03209, 2018. [61] V. Renkens, S. Janssens, B. Ons, J. F. Gemmeke, and H. V. hamme, “Acquisition of ordinal words using weakly supervised NMF,” in Proc. SLT, 2014, pp. 30–35. [62] A. Kolesau and D. Šešok, “Unsupervised pre-training for voice activation,” Applied Sciences, vol. 10, no. 23, 2020. [63] L. Benamer and O. Alkishriwo, “Database for arabic speech commands recognition,” 2020. [64] K. MacLean, Voxforge, http://www.voxforge.org/home, 2018. [65] A. Nautsch et al., “Asvspoof 2019: Spoofing countermeasures for the detection of synthesized, converted and replayed speech,” IEEE Trans. Biom. Behav. Identity Sci., vol. 3, no. 2, pp. 252–265, 2021. [66] C. Busso et al., “IEMOCAP: interactive emotional dyadic motion capture database,” Lang. Resour. Evaluation, vol. 42, no. 4, pp. 335– 359, 2008. [67] A. Ahamad, A. Anand, and P. Bhargava, “Accentdb: A database of non-native english accents to assist neural speech recognition,” in LREC, 2020, pp. 5351–5358. [68] S. Castro, D. Hazarika, V. Pérez-Rosas, R. Zimmermann, R. Mihalcea, and S. Poria, “Towards multimodal sarcasm detection (an obviously perfect paper),” in Proc. ACL, 2019, pp. 4619–4629. [69] A. Ray, S. Mishra, A. Nunna, and P. Bhattacharyya, “A multimodal corpus for emotion recognition in sarcasm,” in LREC, 2022, pp. 6992– 7003. [70] R. Vygon and N. Mikhaylovskiy, “Learning efficient representations for keyword spotting with triplet loss,” in SPECOM, ser. Lecture Notes in Computer Science, vol. 12997, 2021, pp. 773–785. [71] Y. Tian and P. J. Gorinski, “Improving end-to-end speech-to-intent classification with reptile,” in Proc. Interspeech, 2020, pp. 891–895. [72] H. Yen et al., “A study of low-resource speech commands recognition based on adversarial reprogramming,” CoRR, vol. abs/2110.03894, 2021. [73] D. Bermuth, A. Poeppel, and W. Reif, “Finstreder: Simple and fast spoken language understanding with finite state transducers using modern speech-to-text models,” CoRR, vol. abs/2206.14589, 2022. [74] J. Shor, A. Jansen, W. Han, D.