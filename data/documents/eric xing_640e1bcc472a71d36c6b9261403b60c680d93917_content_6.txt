Title: GET: a foundation model of transcription across human cell types
Authors: Xi Fu, Shentong Mo, Anqi Shao, Anouchka Laurent, Alejandro Buendia, Adolfo A. Ferrando, Alberto Ciccia, Yanyan Lan, Teresa Palomero, David M. Owens, Eric P. Xing, Raul Rabadan
Section: Methods
on the same set of elements. Model interpretation and analysis Calculation of jacobian matrix We used multiple feature attribution methods in different analysis. The gradient of the model’s output with respect to the input features, represented by the vector — f (x), tells that how much the model output (Expression) will change when we change a small amount of the input along a dimension (e.g. a certain motif in a cisregulatory region). The generalization to multiple outputs in the context of neural network feature attribution extends to the Jacobian matrix: Ji, j = ∂ fi ∂x j , (4) where fi is the i-th output, representing the transcription level on either the positive or negative strand, and x j is the j-th input feature, comprising scanned and summarized binding scores for 282 TF motif clusters, and an additional dimension for accessibility scores. For our specific analysis, the input is a region-by-feature matrix of dimensions (200,283), including 282 features for TF motif clusters and 1 for accessibility scores. Two models are considered: • With-ATAC Model: Accessibility scores are set to the normalized TPM of Tn5 insertion count in the given accessible region. • Without-ATAC Model: Accessibility scores for all regions are set to 1, focusing solely on chromatin accessibility peaks. The corresponding output is a region-by-strand matrix of dimensions (200,2), capturing the transcription levels on both positive and negative strands. This formulation enables the computation of the Jacobian matrix, vital for understanding the influence of individual features on the transcription levels. Integrated Gradients (IG) provides importance scores by approximating the integral of gradients along a path from given baselines to inputs. The mathematical formulation is: IG(x) = (x xbaseline) · Z 1 0 — f (xbaseline +a(x xbaseline))da, (5) where xbaseline is the baseline input?. DeepLIFT (Deep Learning Important FeaTures) attributes the difference in activation to each input feature, based on a reference input. These methodologies offer unique perspectives on feature importance, with choices guided by computational efficiency, granularity, and the specific modeling context. Identifying important regions and regulators We first gather inference samples across the genome by producing 200-region windows that centered around each genes promoter. Given a specific gene g on strand s 2 {0,1}, the expression value can be inferred using the General Expression 4/8 Method Conceptual Overview Gradients/Jacobians Provides a first-order approximation of feature influence on the output. Integrated Gradients12 Approximates the integral of gradients along a path, providing smoother and more detailed attribution, with baseline comparisons. DeepLIFT13 Focuses on differences in activation, running faster than Integrated