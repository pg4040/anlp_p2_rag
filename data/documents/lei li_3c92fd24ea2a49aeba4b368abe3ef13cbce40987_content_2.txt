Title: Long-tailed Extreme Multi-label Text Classification by the Retrieval of Generated Pseudo Label Descriptions
Authors: Ruohong Zhang, Yau-Shian Wang, Yiming Yang, Donghan Yu, Tom Vu, Likun Lei
Section: A Appendix
classifier, the neural retrieval model can additionally leverage the keyword semantic information and correlation of them, which is ignored in the SVM classifier. The pseudo label descriptions encode both the term importance and key semantics of labels. A.1.5 More Ablation Tests Model Pre-training We fine-tune our retrieval model on a pre-trained neural classifier (BERT) and table 6 shows that without using the pre-trained model, there is a significant drop in the precision and PSP metrics. Negative Sampling We used the top negative predictions by the SVM model as the choice of hard negative labels. By default, we use 10 hard negatives for each instance in the batch. In table 6, we observe a performance drop when no hard negatives or only 5 hard negatives are used for training. CLS vs. Mean-pooling Table 7 shows an ablation test for the design of label description encoder. We observe that using a mean-pooling over the last layer of label keyword embeddings outperforms that using the CLS embedding by a large margin. This could be because the label keywords are not natural language the optimization using CLS embedding is more difficult. A.2 Proof We include the assumptions and proofs of Theorem 3. Assumptions Similar to Luan et al. (2020), we treat neural embedding as fixed dense vector E ∈ Rd×v with each entry sampled from a random Gaussian N(0, d−1/2). ϕn(x) = Eϕt(x) is weighted average of word embeddings by the sparse vector representation of text. According to the Johnson-Lindenstrauss (JL) Lemma (Johnson and Lindenstrauss, 1984; Ben-David et al., 2002), even if the entries of E are sampled from a random normal distribution, with large probability, ⟨ϕt(x),v⟩ and ⟨Eϕt(x),Ev⟩ are close. Lemma 4. Let v be the δ-bounded keyword-selected label embedding of w. For two labels p, n, the error margins satisfy: |µ(ϕt(x),wp,wn)− µ(ϕt(x),vp,vn)| ≤ δ Proof. By the definition of δ-bounded keywords, ⟨ϕt(x),wp⟩ − δ ≤ ⟨ϕt(x),vp⟩ ≤ ⟨ϕt(x),wp⟩ (8) − ⟨ϕt(x),wn⟩ ≤ −⟨ϕt(x),vn⟩ ≤ −⟨ϕt(x),wn⟩+ δ (9) Adding equation 8 and equation 9 finishes the proof: ⟨ϕt(x),wp −wn⟩ − δ ≤ ⟨ϕt(x),vp − vn⟩ ≤ ⟨ϕt(x),wp −wn⟩+ δ (10) Lemma 5. Let ϕt(x) and ϕn(x) be the sparse and dense (dimension d) document feature, wl be the label embedding and zl be the δ-bounded keywords. Let p be a positive label and n be a negative label ranked below p be the sparse classifier. The error margin is ϵ = µ(ϕt(x),wp,wn). An error E of neural classification occurs when µ(ϕn(x), ϕn(zp), ϕn(zn)) ≤ 0. The probability P (E) ≤ 4 exp(− (ϵ−δ)