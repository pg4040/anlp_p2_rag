Title: STRUCTURED PRUNING OF SELF-SUPERVISED PRE-TRAINED MODELS FOR SPEECH RECOGNITION AND UNDERSTANDING
Authors: Yifan Peng, Kwangyoun Kim, Felix Wu, Prashant Sridhar, Shinji Watanabe
Section: 4.5. Comparison with other compression methods
D.L. Wang, “Compressing Deep Neural Networks for Efficient Speech Enhancement,” in Proc. ICASSP, 2021. [28] S. Scardapane, D. Comminiello, A. Hussain, and A. Uncini, “Group sparse regularization for deep neural networks,” Neurocomputing, vol. 241, pp. 81–89, 2017. [29] C.-I J. Lai, Y. Zhang, A. H. Liu, et al., “PARP: Prune, Ad- just and Re-Prune for Self-Supervised Speech Recognition,” in Proc. NeurIPS, 2021. [30] F. Wu, K. Kim, J. Pan, et al., “Performance-Efficiency Trade- offs in Unsupervised Pre-training for Speech Recognition,” in Proc. ICASSP, 2022. [31] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Lib- rispeech: An ASR corpus based on public domain audio books,” in Proc. ICASSP, 2015. [32] A. Vaswani, N. Shazeer, N. Parmar, et al., “Attention is all you need,” in Proc. NeurIPS, 2017. [33] D. Hendrycks and K. Gimpel, “Gaussian Error Linear Units (GELUs),” arXiv:1606.08415, 2016. [34] C. Louizos, M. Welling, and D. P. Kingma, “Learning Sparse Neural Networks through L0 Regularization,” in ICLR, 2018. [35] A. Paszke et al., “Pytorch: An imperative style, high- performance deep learning library,” Proc. NeurIPS, 2019. [36] T. Wolf et al., “Huggingface’s transformers: State-of-the-art natural language processing,” arXiv:1910.03771, 2019. [37] A. Rousseau et al., “TED-LIUM: an automatic speech recog- nition dedicated corpus.,” in Proc. LREC, 2012. [38] A. Graves, S. Fernández, F. Gomez, et al., “Connectionist tem- poral classification: labelling unsegmented sequence data with recurrent neural networks,” in Proc. ICML, 2006. [39] E. Bastianelli, A. Vanzo, P. Swietojanski, and V. Rieser, “SLURP: A Spoken Language Understanding Resource Package,” in Proc. EMNLP, 2020. [40] S. Zhang, E. Loweimi, P. Bell, and S. Renals, “On the use- fulness of self-attention for automatic speech recognition with transformers,” in Proc. SLT, 2021. [41] Y. Peng, S. Dalmia, I. Lane, and S. Watanabe, “Branch- former: Parallel MLP-attention architectures to capture local and global context for speech recognition and understanding,” in Proc. ICML, 2022. [42] T. Maekaku, Y. Fujita, Y. Peng, and S. Watanabe, “Attention Weight Smoothing Using Prior Distributions for TransformerBased End-to-End ASR,” in Proc. Interspeech, 2022. [43] F. Wu, K. Kim, S. Watanabe, et al., “Wav2seq: Pre- training speech-to-text encoder-decoder models using pseudo languages,” arXiv:2205.01086, 2022. [44] L. Lugosch, M. Ravanelli, P. Ignoto, et al., “Speech model pre-training for end-to-end spoken language understanding,” in Interspeech, 2019.