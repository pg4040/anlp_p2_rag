Title: CUTTLEFISH: LOW-RANK MODEL TRAINING WITHOUT ALL THE TUNING
Authors: Hongyi Wang, Saurabh Agarwal, Pongsakorn U-chupala, Yoshiki Tanaka, Eric P. Xing, Dimitris Papailiopoulos
Section: D.2 Additional experimental results.
BERT pre-training using CUTTLEFISH. We perform BERT pre-training on the Wikipedia and Bookcorpus datasets, adhering to the training schedule and codebase of the 24h BERTLARGE for faster training speed and due to limited computing resources (Izsak et al., 2021). The results, shown in Table 17, indicate that CUTTLEFISH enables pre-training a BERT model with only 72% of the total model parameters while achieving the same final MLM loss. Rank varying trend of other datasets. In our main paper, we presented the rank varying trend only for ResNet-18 on CIFAR10. In this section, we expand our analysis and report additional experimental results on rank varying trends. Specifically, we provide results for VGG-19 trained on CIFAR-10, as well as VGG-19 and ResNet-18 on CIFAR-100 and SVHN datasets. For ResNet-50 on ImageNet, we also show the same results. Figures 10 and 14 display the results for VGG-19 on CIFAR-10, while Figures 11, 15, 16, 13, and 17 illustrate the results for the remaining datasets. Overall, our observations indicate that the stable rank of the network layers fluctuates significantly in the early stages of training but eventually converges to a constant value. This trend holds across all the datasets we analyzed. Comparison of CUTTLEFISH to EB Train and GraSP. Furthermore, we compare CUTTLEFISH with two other state-ofthe-art approaches, namely EB Train and GraSP (as shown in Table 18). Notably, CUTTLEFISH outperforms both EB Train and GraSP in terms of accuracy while achieving smaller model sizes. ResNet-18 and VGG-19 Experiments on SVHN. The results of ResNet-18 and VGG-19 experiments on the SVHN dataset are presented in Table 19.