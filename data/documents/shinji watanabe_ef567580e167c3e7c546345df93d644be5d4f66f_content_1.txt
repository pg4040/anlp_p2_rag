Title: AV-SUPERB: A MULTI-TASK EVALUATION BENCHMARK FOR AUDIO-VISUAL REPRESENTATION MODELS
Authors: Yuan Tseng, Layne Berry, Yi-Ting Chen, I-Hsiang Chiu, Hsuan-Hao Lin, Max Liu, Puyuan Peng, Yi-Jen Shih, Hung-Yu Wang, Haibin Wu, Po-Yao Huang, Chun-Mao Lai, Shang-Wen Li, David Harwath, Yu Tsao, Shinji Watanabe, Abdelrahman Mohamed, Chi-Luen Feng, Hung-yi Lee
Section: 4.4. Layer-wise Contribution Analysis
in Table 2. For AV-HuBERT, we see that visual speech recognition on LRS3-TED is not a suitable intermediate task in general. Videoonly representations obtain small gains in generalizability, at the cost of greatly reducing audio-only and fusion performance. We posit that intermediate-task fine-tuning with (video,text) pairs shifts AV-HuBERT Transformer layers to favor video input alone, reducing usability for audio-only and audio-visual inputs. Contrarily, for audio-visual fusion with MAViL, we see that intermediate-task training on AudioSet-2M not only brings substantial improvements to all AEC and AR datasets, but also improves ASV while maintaining ASR performance. This suggests that finetuning on AudioSet-2M may be sufficiently diverse to improve speaker separability of representations without much loss of content information. 6. CONCLUSIONS We introduce AV-SUPERB, the first benchmark for assessing general-purpose capabilities of audio-visual representations. AVSUPERB includes a suite of 7 speech and audio processing datasets covering 5 audio-visual tasks. The benchmark is split into three tracks: two unimodal audio-only or video-only representations tracks, as well as a bimodal audio-visual fusion track, which enables easy comparison between unimodal and bimodal learning. Despite advances made in recent years, our experiments show that none of the models tested generalize to all tasks, leading us to conclude that further study is required to develop universal audio-visual models. As discussed in Section 3.1, although our benchmark aims to comprehensively evaluate audio-visual models, only a limited set of tasks and datasets are included in its current form. For future work, we wish to incorporate more tasks relevant to additional facets of audio-visual processing, such as cross-modal retrieval and sound/video generation, as well as improving the diversity and comprehensiveness of data sources. 7. REFERENCES [1] Alexei Baevski et al., “wav2vec 2.0: A framework for selfsupervised learning of speech representations,” in NeurIPS, 2020. [2] Wei-Ning Hsu et al., “Hubert: Self-supervised speech representation learning by masked prediction of hidden units,” TASLP, 2021. [3] Daisuke Niizumi et al., “Byol for audio: Self-supervised learning for general-purpose audio representation,” in IJCNN, 2021. [4] Po-Yao Huang et al., “Masked autoencoders that listen,” in NeurIPS, 2022. [5] Alexey Dosovitskiy et al., “An image is worth 16x16 words: Transformers for image recognition at scale,” in ICLR, 2020. [6] Kaiming He et al., “Masked autoencoders are scalable vision learners,” in CVPR, 2022. [7] Shu wen Yang et al., “SUPERB: Speech Processing Universal PERformance Benchmark,” in Interspeech, 2021. [8] Hsiang-Sheng Tsai et al., “Superb-sg: Enhanced speech processing universal performance benchmark for semantic and generative capabilities,” in ACL, 2022. [9] Joseph Turian et al., “Hear: Holistic evaluation of audio representations,” in