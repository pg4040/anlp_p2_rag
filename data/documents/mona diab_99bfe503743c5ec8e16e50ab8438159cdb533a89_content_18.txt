Title: The Troubling Emergence of Hallucination in Large Language Models – An Extensive Definition, Quantification, and Prescriptive Remediations
Authors: Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S.M Towhidul Islam Tonmoy, Aman Chadha, Amit Sheth, Amitava Das
Section: 
at the top of the interface, the actual prompt used for generating the text snippet is displayed. Directly below the prompt, the complete AI-generated text is shown. On the right-hand side, the sentence breakup is presented, with the currently selected sentence highlighted in red. Below the sentence breakup, all the relevant categories are displayed as radio buttons, allowing the annotators to easily annotate each category. This interface aims to enhance the efficiency and effectiveness of the annotation process. We have gone through a few rounds of iterations before finalizing the current version of the web interface. B.4 Selecting quality annotators on AMT It is widely acknowledged that platforms like AMT can be noisy, making the selection of high-quality annotators a critical step in ensuring accurate annotations. The in-house annotation of 2,000 data points played a significant role in achieving this goal. To identify reliable annotators, we initiated a pilot task and only selected those with an accuracy rate of over 90% based on our in-house annotated dataset. Another crucial consideration when annotating data on crowdsourcing platforms is the compensation offered to annotators. While offering too little may deter interest, excessively high wages may attract undesirable spammers. Achieving the ideal balance required several rounds of iteration to determine an appropriate compensation scheme. By carefully addressing factors such as selecting qualified annotators and establishing suitable compensation rates, we improved the quality of annotations obtained from crowdsourcing platforms. B.5 Inter-Annotator Agreement We report Fleiss’s kappa (κ) (Wikipedia_Fleiss’s_Kappa) and Krippendorff’s alpha (α) (Wikipedia_Krippendorff’s_Alpha) scores (see Tables 4 and 5) to access the reliability of agreement between the three annotators1. We compute agreement on 10% of total annotated entities and obtain substantial to almost perfect agreement in all three annotation types in both datasets, namely NYT and Politifact. We have obtained nearly or more than 80% agreement in the case of orientation and category. The agreement on degree exhibits slight variation, as it relies on the subjective assessment of the percentage of hallucination in the sentence. This interpretation of percentage tends to differ among individuals. We report both Fleiss’s kappa and Krippendorff’s alpha score because Fleiss’s kappa is a statistical measure that allows us to find agreement among multiple annotators and Krippendorff’s alpha is a statistical measure that allows us to handle various types of data like nominal (orientation and category) and ordinal (degree). 1Three graduate students C Details on chosen LLMs C.1 Criteria for choosing LLMs Beyond the primary criteria for choosing performant LLMs, our selection was meant to cover a wide gamut of