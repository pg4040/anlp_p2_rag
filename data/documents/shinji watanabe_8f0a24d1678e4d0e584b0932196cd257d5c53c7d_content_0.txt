Title: IMPROVING AUDIO CAPTIONING MODELS WITH FINE-GRAINED AUDIO FEATURES, TEXT EMBEDDING SUPERVISION, AND LLM MIX-UP AUGMENTATION
Authors: Shih-Lun Wu, Xuankai Chang, Gordon Wichern, Jee-weon Jung, François Germain, Jonathan Le Roux, Shinji Watanabe
Section: 3.4. Ablation Study
To show that every component in our AAC model is indispensable to achieve the best performance, we conduct a comprehensive ablation study that tries to remove components one at a time. Table 2 presents the results that corroborate the necessity of all of our model components—Each one of them gives at least a 2-point improvement on SPIDEr-FL,13 with BEATs audio encoder (replacing the popular PANN) being the most crucial one causing a 6-point difference. Some intriguing additional findings are: (i) hybrid reranking is required to outperform beam search (see rows 1∼4 in Table 2), suggesting that decoder and encoder reranking methods are strongly complementary and hence should be used together when possible; (ii) the ‘sampling+ reranking’ decoding approach improves performance the most when both BEATs encoder and ChatGPT mix-ups are used (see rows ‘1 vs. 4’, ‘5 vs. 6’, and ‘8 vs. 9’ in Table 2). 4. CONCLUSION AND FUTURE WORK In this work, we improved audio captioning models from multiple aspects with an extensive use of pretrained models. We employed the BEATs Transformer to extract more fine-grained audio features. We then utilized the INSTRUCTOR text embeddings for multitask learning to provide rich language-modality guidance. ChatGPT was also leveraged to generate faithful and fluent caption mix-ups which, when paired with the corresponding audio mix-ups, increased the size, diversity, and complexity of our training data. Finally, nucleus sampling and hybrid reranking were used to exploit our model’s capabilities to the fullest extent. We accomplished a state-of-the-art 32.6 SPIDEr-FL score and demonstrated via a thorough ablation study that all components are crucial to our model’s success. Future endeavors may explore audio feature extractors that are pretrained with larger amounts of data [37] or multimodal supervision [38]. More advanced reinforcement learning methods [39] can also be applied to optimize captioning metrics that correlate well with human judgment [40] without introducing disfluency issues. 13We cannot perform hybrid reranking with the ‘w/o INSTRUCTOR’ setting as the audio encoder is not trained to match the caption text embedding. Thus, we simply use beam search as it outperforms decoder-only reranking. 5. REFERENCES [1] K. Drossos, S. Adavanne and T. Virtanen, “Automated audio captioning with recurrent neural networks,” in Proc. WASPAA, 2017. [2] K. Drossos, S. Lipping and T. Virtanen, “Clotho: an audio captioning dataset,” in Proc. ICASSP, 2020. [3] C. D. Kim, B. Kim, H. Lee and G. Kim, “AudioCaps: Generating captions for audios in the wild,” in Proc. NAACL-HLT, 2019. [4] W. Yuan, Q. Han, D. Liu et al., “The DCASE 2021 challenge task 6 system: