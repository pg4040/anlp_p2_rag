Title: SPEAKER-INDEPENDENT ACOUSTIC-TO-ARTICULATORY SPEECH INVERSION
Authors: Peter Wu, Li-Wei Chen, Cheol Jun Cho, Shinji Watanabe, Louis Goldstein, Alan W Black, Gopala K. Anumanchipalli
Section: 5.3. Resynthesis Analysis without EMA Labels
“Deep speech synthesis from articulatory representations,” in Interspeech, 2022. [29] Y. Zhang and Q. Yang, “An overview of multi-task learning,” National Science Review, 2018. [30] S. wen Yang, P.-H. Chi, Y.-S. Chuang, et al., “SUPERB: Speech Processing Universal PERformance Benchmark,” in Interspeech, 2021, pp. 1194–1198. [31] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, et al., “Hubert: Selfsupervised speech representation learning by masked prediction of hidden units,” TASLP, 2021. [32] M. Ravanelli, T. Parcollet, P. Plantinga, et al., “SpeechBrain: A general-purpose speech toolkit,” 2021, arXiv:2106.04624. [33] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evaluation of gated recurrent neural networks on sequence modeling,” NeurIPS, 2014. [34] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, Learning Internal Representations by Error Propagation, MIT Press, 1986. [35] M. Morrison, R. Kumar, K. Kumar, et al., “Chunked autoregressive gan for conditional waveform synthesis,” in ICLR, April 2022. [36] J. Kong, J. Kim, and J. Bae, “HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis,” in NeurIPS, 2020. [37] C. P. Browman and L. Goldstein, “Articulatory phonology: An overview,” Phonetica, 1992. [38] M. McAuliffe et al., “Montreal forced aligner: Trainable textspeech alignment using kaldi,” in Interspeech, 2017. [39] S. Watanabe, T. Hori, S. Karita, et al., “ESPnet: End-to-end speech processing toolkit,” in Interspeech, 2018. [40] K. Richmond, Z. Ling, J. Yamagishi, et al., “On the evaluation of inversion mapping performance in the acoustic domain,” in Interspeech, 2013. [41] T. Hayashi et al., “Espnet2-tts: Extending the edge of tts research,” arXiv preprint arXiv:2110.07840, 2021.