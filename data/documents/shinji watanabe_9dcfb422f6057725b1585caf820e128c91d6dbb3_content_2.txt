Title: IMPROVING MASSIVELY MULTILINGUAL ASR WITH AUXILIARY CTC OBJECTIVES
Authors: William Chen, Brian Yan, Jiatong Shi, Yifan Peng, Soumi Maiti, Shinji Watanabe
Section: 8. REFERENCES
Speech Processing Universal PERformance Benchmark,” in Proc. Interspeech, 2021, pp. 1194–1198. [37] S. Chen, C. Wang, Z. Chen, et al., “WavLM: Large-scale self-supervised pretraining for full stack speech processing,” IEEE Journal of Selected Topics in Signal Processing, pp. 1–14, 2022. [38] D. S. Park, W. Chan, Y. Zhang, et al., “Specaugment: A simple data augmentation method for automatic speech recognition,” Proc. Interspeech, pp. 2613– 2617, 2019. [39] T. Ko, V. Peddinti, D. Povey, et al., “Audio augmentation for speech recognition,” Proc. Interspeech, 2015. [40] T. Kudo and J. Richardson, “Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,” in Proc. EMNLP 2018, 2018, pp. 66–71. [41] S. Watanabe, T. Hori, S. Karita, et al., “ESPnet: End-to-end speech processing toolkit,” in Proc. Interspeech, 2018, pp. 2207–2211. [42] A. Vaswani, N. Shazeer, N. Parmar, et al., “Attention is all you need,” in Advances in Neural Information Processing Systems (NeurIPS), 2017, pp. 5998– 6008. [43] A. Gulati, J. Qin, C.-C. Chiu, et al., “Conformer: Convolution-augmented Transformer for Speech Recognition,” in Proc. Interspeech, 2020, pp. 5036–5040. [44] P. Guo, F. Boyer, X. Chang, et al., “Recent developments on espnet toolkit boosted by conformer,” in Proc. ICASSP, 2021, pp. 5874–5878. [45] Z. Chen, A. Bapna, A. Rosenberg, et al., “Maestro-U: Leveraging joint speechtext representation learning for zero supervised speech asr,” arXiv preprint arXiv:2210.10027, 2022. [46] C. Wang, M. Riviere, A. Lee, et al., “VoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation,” in Proc. ACL, Online, Aug. 2021, pp. 993–1003. [47] V. Pratap, Q. Xu, A. Sriram, et al., “MLS: A Large-Scale Multilingual Dataset for Speech Research,” in Proc. Interspeech, 2020, pp. 2757–2761. [48] A. Radford, J. W. Kim, T. Xu, et al., “Robust speech recognition via large-scale weak supervision,” [49] N. A. Nystrom, M. J. Levine, R. Z. Roskies, et al., “Bridges: A uniquely flexible hpc resource for new communities and data analytics,” in Proc. of the 2015 XSEDE Conference: Scientific Advancements Enabled by Enhanced Cyberinfrastructure, 2015, pp. 1–8.