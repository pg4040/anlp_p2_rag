Title: Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions
Authors: Himanshu Thakur, Atishay Jain, Praneetha Vaddamanu, Paul Pu Liang, Louis-Philippe Morency
Section: A Appendix
A.1 Dataset Bias Analysis To gauge the feasibility of using a wordlist based intervention approach, we first analyze our datasets for occurrences of gender words. As shown in the word cloud 4, gender pronouns are the mostfrequent word in our datasets. Moreover, as per Figure 1, "she," "he," and "her" are the top three most frequently occurring words in our dataset. This suggests that we can definitely detect gender words in our corpus and apply our interventions. A.2 Sensitivity to Choice of Dataset To understand the effectiveness of our proposed data-interventions, we study apply our methods to two datasets under varying number of training samples (10, 50 and 100) and selection strategies (most biased first and random) as per Table 6. Our methods obtain better results on StereoSet (dev) dataset. One reason this could happen is due to the fact that StereoSet has explicit gender bias, thus it would be less likely for a sentence like "She needs a gynaecologist" to appear on it. Because our interventions perform blunt substitutions, this sentence might become incorrect due to our method - "Either he or she needs a gynaecologist". A.3 Sensitivity to Number of Training Samples and Sampling Strategy As per Figure 5, When we vary the number of training samples, we observe that the difference in performance is not huge when we transition from 10 to 100 samples, thus suggesting that our method is capable of few-shot fine-tuning. Moreover, sampling the most biased data points helps our methods achieve better performance consistently, as shown in Figure 5 and Table 6. Table ?? shows some top three most gender biased entries found in the StereoSet dataset. A.4 Ablations of interventions We study the effects of choosing different ways of replacement for name and non-name words. In addition to our three interventions proposed previously, we also experimented with a couple of others. In female-first-random-phrase-masking, we always keep the female gendered word before a male word. We wanted to understand if the order of gender words encountered by a model renders any effect on the debiasing. In Table 7, we see that it does not perform any better than random-phrase-masking. Then, we also try fixing the phrases from random-phrase-masking, thus making it fixed-phrase-masking. We obtain 4 variants of this method corresponding to the following four phrases: 1. both [1] and [2] 2. [1] and [2] 3. [1] or [2] 4. either [1] or [2] Here, [1] and [2] are substituted with opposite gender words. As we observe in Table 7, fixed-phrase-masking-3 obtains the lowest