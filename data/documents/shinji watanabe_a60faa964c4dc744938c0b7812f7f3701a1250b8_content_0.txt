Title: A SINGLE SPEECH ENHANCEMENT MODEL UNIFYING DEREVERBERATION, DENOISING, SPEAKER COUNTING, SEPARATION, AND EXTRACTION
Authors: Kohei Saijo, Wangyou Zhang, Zhong-Qiu Wang, Shinji Watanabe, Tetsunori Kobayashi, Tetsuji Ogawa, Jiao Tong
Section: 6. REFERENCES
[1] R. Haeb-Umbach et al., “Far-field automatic speech recognition,” Proceedings of the IEEE, vol. 109, no. 2, pp. 124–148, 2020. [2] Y. Luo and N. Mesgarani, “Conv-TasNet: Surpassing ideal time–frequency magnitude masking for speech separation,” IEEE Trans. Audio, Speech, Lang. Process., vol. 27, no. 8, pp. 1256–1266, 2019. [3] C. Subakan et al., “Attention is all you need in speech separation,” in Proc. ICASSP, 2021, pp. 21–25. [4] Z.-Q. Wang et al., “TF-GridNet: Making time-frequency domain models great again for monaural speaker separation,” in Proc. ICASSP, 2023, pp. 1–5. [5] J. R. Hershey et al., “Deep clustering: Discriminative embeddings for segmentation and separation,” in Proc. ICASSP, 2016, pp. 31–35. [6] D. Yu et al., “Permutation invariant training of deep models for speaker-independent multi-talker speech separation,” in Proc. ICASSP, 2017, pp. 241–245. [7] K. Žmolı́ková et al., “SpeakerBeam: Speaker aware neural network for target speaker extraction in speech mixtures,” IEEE Journal of Selected Topics in Signal Processing, vol. 13, no. 4, pp. 800–814, 2019. [8] Q. Wang et al., “VoiceFilter: Targeted voice separation by speaker-conditioned spectrogram masking,” in Proc. Interspeech, 2019, pp. 2728–2732. [9] K. Zmolikova et al., “Neural target speech extraction: An overview,” IEEE Signal Processing Magazine, vol. 40, no. 3, pp. 8–29, 2023. [10] T. Ochiai et al., “A unified framework for neural speech separation and extraction,” in Proc. ICASSP, 2019, pp. 6975–6979. [11] E. Nachmani et al., “Voice separation with an unknown number of multiple speakers,” in Proc. ICML, 2020, pp. 7164– 7175. [12] J. Zhu et al., “Multi-decoder DPRNN: Source separation for variable number of speakers,” in Proc. ICASSP, 2021, pp. 3420–3424. [13] M. Kolbæk et al., “Multitalker speech separation with utterance-level permutation invariant training of deep recurrent neural networks,” IEEE Trans. Audio, Speech, Lang. Process., vol. 25, no. 10, pp. 1901–1913, 2017. [14] Y. Luo et al., “Speaker-independent speech separation with deep attractor network,” IEEE Trans. Audio, Speech, Lang. Process., vol. 26, no. 4, pp. 787–796, 2018. [15] Y. Luo and N. Mesgarani, “Separating varying numbers of sources with auxiliary autoencoding loss,” in Proc. Interspeech, 2020, pp. 2622–2626. [16] S. Wisdom et al., “What’s all the FUSS about free universal sound separation data?” in Proc. ICASSP, 2021, pp. 186–190. [17] K. Kinoshita et al., “Listening to each speaker one by one with recurrent selective hearing networks,” in Proc. ICASSP, 2018, pp. 5064–5068. [18] N. Takahashi et al., “Recursive speech separation for unknown number of speakers,” in Proc. Interspeech, 2019, pp. 1348– 1352. [19] T. von Neumann et al., “Multi-talker ASR for an unknown number of sources: