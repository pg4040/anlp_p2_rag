Title: Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation
Authors: Yu-Kuan Fu, Liang-Hsuan Tseng, Jiatong Shi, Chen-An Li, Tsu-Yuan Hsu, Shinji Watanabe, Hung-yi Lee
Section: 5 Analysis
never appear during the training process of the model, directing translating on that induces severe domain mismatch. (III) and (IV) have outperformed (II) a lot, but they still decrease the BLEU score by about 2.6 and respectively ((III), (IV) v.s. (I)), indicating mismatch between pretraining and downstream task training is severe, and NCT has only minor improvement. Integrating text denormalization and UMT, while it did not performs better due to the mismatch between pretraining and fine-tuning, can still reduce the inference latency. To address this mismatch, a potential solution would be pretraining the model on normalized source text and unnormalized target text from scratch. Robustness of UNMT DBT has been shown to improve the performance of cascaded S2TT and S2ST systems. In this section, we investigate the robustness of UNMT. Table 4 shows the BLEU scores of translating the ground truth of CoVoST ("Clean"), and that of translating the output of UASR ("ASR"). The results indicate that for Fr-En and Es-En on "Clean", the performance drops slightly compared to that of BT, but the score drop from "Clean" to "ASR" of DBT decreased by about 1 BLEU score. For De-En, DBT even performs better than BT on "Clean", and the performance drops from "Clean" to "ASR" are the same, which means that DBT can be regraded as a new data-augmentation method to boost the performance of a UNMT model. This result from avoiding the model from directly copying the input during generating pseudo-label for back-translation. Overall, DBT increases the robustness without sacrificing its performance on clean input too much, and it even outperforms BT on "Clean" in some cases. Performance analysis of UTTS In this section, we present more analytical results of our UTTS submodule. In part (I) in Table 6, we evaluate our supervised TTS and UTTS on in-domain testing set (LJspeech) and out-domain testing set (US2ST, Frâ†’En). After we got the synthesis speech, we send the audio to whisper and then calculate the WER. We used the base model for this experiment to further accentuate the performance differences. Our results indicated that the performance drop between supervised TTS and UTTS is much lower than the error induced by the domain mismatch problem. The results also emphasized that the do- main mismatch between the training data of TTS models and the testing data is one of the main reasons for our S2ST performance drop. Leveraging the data from CVSS for UTTS training might be a solution, but it may also induce fairness concerns from our point of view. Next,