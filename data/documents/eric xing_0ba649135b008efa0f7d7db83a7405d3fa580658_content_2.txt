Title: Effective Multi-Agent Deep Reinforcement Learning Control with Relative Entropy Regularization
Authors: Chenyang Miao, Yunduan Cui, Huiyun Li, Xinyu Wu
Section: C. Cooperation in Traditional Control Task
proper torques from other agents: the agent of dimension 5 only produced effective torque near the 15-th step while the agent of dimension 1 was fixed with −1 torque during the how rollout. Due to the lack of relative entropy regularization from the algorithmic perspective, the multiple agents in MADDPG were unable to learn effective and cooperative control strategies. In the next case study, we studied the test rollouts using MADDPG and MACDPP in the ur ee position task which aims to control the end-effector of the UR5 robot arm to reach the randomly generated targets. It is observed that MACDPP quickly drove the robot to finish the task within 60 steps. The action trajectories showed proper cooperation between each joint. The base and elbow joints continuously output −180◦ and 60◦ throughout the task. At step 25, the shoulder joint and wrist 1 joint were coordinated to guide the endeffector to move forward to the target position. Around step 60, the shoulder and wrist 2 joints worked together to quickly reach the target. The action Trajectories of all dimensions were effective with minimal jitter. Compared with our method, MADDPG could not sufficiently learn the cooperative strategy over five joints. The base joint could not continuously output a certain degree, it had strong trembling between steps 40 to 80. The shoulder and wrist 1 joints failed to cooperate effectively at the beginning, resulting in redundant movements of the endeffector. After step 40, the shoulder and wrist 2 joints could not achieve seamless coordination. Both of them experienced sustained tremors which ultimately resulted in highly degraded control performance. The experimental results above revealed the advantages of MACDPP in the joint control of robot systems. The multiple agents reduced the exploration complexity in one system and resulted in faster policy convergence compared to single-agent approaches with the same amount of interactions. At the same time, the relative entropy regularization significantly avoided the mismatch between the update of multi-agent policies during the learning, promoting the effectiveness in the learning of cooperative control strategies.