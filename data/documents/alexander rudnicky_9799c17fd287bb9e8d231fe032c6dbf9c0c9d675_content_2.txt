Title: Overview of Robust and Multilingual Automatic Evaluation Metrics for Open-Domain Dialogue Systems at DSTC 11 Track 4
Authors: Mario Rodríguez-Cantelar, Chen Zhang, Chengguang Tang, Ke Shi, Sarik Ghazarian, João Sedoc, Luis Fernando D’Haro, Alexander Rudnicky
Section: 2.1 Datasets
used to collect annotations for each of the dimensions evaluated in the test data. Our annotations restricted the users to location US, >97% approval rate, >1000 HITs done, and a convenience pool of workers used for NLP evaluation tasks.10 This pool included workers from the AMT filtering pipeline (Zhang et al., 2022d) and cloudresearch. The average compensation was ∼ 15$/hr. We included text-based attention checks at the dialogue-level as well as an annotator agree- 9https://github.com/Mario-RC/dstc11_t rack4_robust_multilingual_metrics/blob/m ain/dstc11/track4-datasets-format.md 10Without the convenience pool our annotator agreement was near random. ment (both with an expert as well as between crowd workers [some from the DSTC10 dataset]) timebased filters on the turn-level. For the HCChinese data, we leveraged the power of Tencent MT11 to perform the English-to-Chinese translation of the corpus, followed by training a team of six professional Chinese annotators to annotate the dialogues. The entire annotation process spanned a month and incurred costs of approximately 6,194 US dollars, which is in line with the expenses associated with other evaluation datasets. The average cost of annotating each dialogue was 2.36 US dollars. Finally, the average correlation coefficient for Adequacy scored by six annotators is 0.79, and 0.67 for Fluency.