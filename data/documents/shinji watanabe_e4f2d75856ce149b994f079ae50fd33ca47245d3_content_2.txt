Title: DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models
Authors: Yifan Peng, Yui Sudo, Shakeel Muhammad, Shinji Watanabe
Section: 7. References
general framework for self-supervised learning in speech, vision and language,” in Proc. ICML, 2022. [31] C. Louizos, M. Welling, and D. P. Kingma, “Learning Sparse Neural Networks through L0 Regularization,” in ICLR, 2018. [32] A. Paszke et al., “Pytorch: An imperative style, high-performance deep learning library,” Proc. NeurIPS, 2019. [33] Y.-Y. Yang et al., “Torchaudio: Building blocks for audio and speech processing,” arXiv:2110.15018, 2021. [34] M. Ott et al., “fairseq: A fast, extensible toolkit for sequence modeling,” in Proc. NAACL-HLT: Demonstrations, 2019. [35] T. Wolf et al., “Huggingface’s transformers: State-of-the-art natural language processing,” arXiv preprint arXiv:1910.03771, 2019. [36] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: An ASR corpus based on public domain audio books,” in Proc. ICASSP, 2015. [37] S. Zhang, E. Loweimi, P. Bell, and S. Renals, “On the usefulness of self-attention for automatic speech recognition with transformers,” in Proc. SLT, 2021. [38] K. Shim, J. Choi, and W. Sung, “Understanding the role of self attention for efficient speech recognition,” in Proc. ICLR, 2022. [39] Y. Peng, S. Dalmia, I. Lane, and S. Watanabe, “Branchformer: Parallel MLP-attention architectures to capture local and global context for speech recognition and understanding,” in Proc. ICML, 2022. [40] T. Maekaku, Y. Fujita, Y. Peng, and S. Watanabe, “Attention Weight Smoothing Using Prior Distributions for TransformerBased End-to-End ASR,” in Proc. Interspeech, 2022. [41] T.-Q. Lin, H.-y. Lee, and H. Tang, “MelHuBERT: A simplified HuBERT on Mel spectrogram,” arXiv:2211.09944, 2022. [42] F. Wu, K. Kim, J. Pan, K. J. Han, K. Q. Weinberger, and Y. Artzi, “Performance-efficiency trade-offs in unsupervised pre-training for speech recognition,” in Proc. ICASSP, 2022.