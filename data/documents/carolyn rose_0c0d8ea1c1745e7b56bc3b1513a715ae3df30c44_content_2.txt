Title: Linguistic representations for fewer-shot relation extraction across domains
Authors: Sireesh Gururaja, Ritam Dutt, Tinglong Liao, Carolyn Rosé
Section: 6 Results and Discussion
significant in the transfer setting. There is also a significant interaction between few shot setting and case: F(2, 679) = 8.19, p < .0005. A student-t post-hoc analysis reveals that the effect is only significant for the 5- and 10-shot settings. Thus, 1 target example is too small to yield a significant effect whereas 20 or more is too many such that the representational advantage disappears. We also find a significant interaction between representation case and target dataset, but not with source dataset: F(4, 679) = 2.61, p < .05, such that the effect of representation is significant for RISeC and MSCorpus but not for EFGC. We present all of our few-shot results in Table 3. Significance testing was performed on the difference in results between the baseline and lingusitic representation cases in the transfer setting. Additionally, we investigate the impact of source domain on the utility of linguistic representations. We therefore compare results between models trained in a few-shot setting from scratch, seeing only one dataset, with the transfer model that we train on a source dataset first. We show both of these cases in table 3, with few-shot models trained from scratch denoted in the source dataset column as "From Scratch" results. How important is the choice of the source domain on the transfer performance? We see several interesting patterns in our 5- and 10-shot results when we take our few-shot models trained from scratch into account. We visualizes differences in performance between the from-scratch models and models trained with a different source domain in table 4. We find that the transfer between datasets for our text-only models is of limited utility, if not outright harmful. While we see one instance (the EFGC to RISeC transfer) in which introducing a transfer source dataset improves the baseline model’s performance on the target dataset consistently, we see more commonly that adding a transfer source dataset makes only a small difference, or even hurts the performance of the baseline model. In the cases of transfer between MSCorpus and RISeC in either direction, for instance, the baseline model in the transfer setting consistently underperforms the model trained from scratch by up to 7 F1 points, and does not close that gap even in the 50- and 100-shot settings. However, incorporating linguistic formalisms proves to be far more robust to the choice of source domain: the linguistic representations, regardless of source domain are never worse than the baseline trained on that source domain, and still frequently outperform the baseline trained from scratch, even