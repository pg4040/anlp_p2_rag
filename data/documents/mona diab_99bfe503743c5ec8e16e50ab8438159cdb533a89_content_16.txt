Title: The Troubling Emergence of Hallucination in Large Language Models – An Extensive Definition, Quantification, and Prescriptive Remediations
Authors: Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S.M Towhidul Islam Tonmoy, Aman Chadha, Amit Sheth, Amitava Das
Section: 
14 models consist of large models like GPT-3 and smaller ones like T5 and T0. ✽ Why would extrinsic hallucination be riskier? ➠ According to the “extrinsic hallucination” definition, this kind of hallucination does not have any way to verify it from the source prompt. Hence, it is likely to be more harmful than the intrinsic ones. ✽ What is the purpose of constructing Factual Mirage and Silver Lining hallucination data? ➠ We want to show that hallucinations can happen in both cases, factually correct and incorrect prompts. Hence, in this paper, we construct an exhaustive dataset called . ✽ Why do you select high-entropy points for mitigation techniques? ➠ High entropy points are more uncertain points in the context of text generation and hence, more likely places where the LLM hallucinates. Hence, our mitigation approach works by detecting and replacing such high entropy points. ✽ Why would HVI be a better hallucination evaluation metric for the LLMs (as compared to the existing ones like accuracy, precision, recall, F1, etc.)? ➠ Although the commonly used evaluation metrics like accuracy, precision, etc. can be used for downstream tasks, HVI can be more specifically used to determine the LLMs’ hallucination tendency. HVI will serve as a uniform hallucination score for all the present and future LLMs. ✽ What are the insights on using black-box vs. gray-box models for mitigation hallucinations? ➠ Both black-box and gray-box models have their own advantages and disadvantages in terms of reducing hallucinations. Therefore, the choice of the appropriate method to minimize hallucination would be LLM- and task-dependent. A Appendix This section provides supplementary material in the form of additional examples, implementation details, etc. to bolster the reader’s understanding of the concepts presented in this work. B Annotation Process, and agreement B.1 Pilot in-house annotation Crowdsourcing platforms are widely recognized for their speed and cost-effectiveness in annotation tasks. However, it is important to note that they can also introduce noise or inaccuracies in the annotations. To mitigate this, prior to utilizing crowdsourcing services, we conducted an in-house annotation process involving 2,000 samples. These samples included prompts and generated text snippets from five different LLMs. This in-house annotation process served two purposes: firstly, it allowed us to formulate comprehensive annotation guidelines, and secondly, it helped us develop an annotation interface tailored to our specific needs. By undertaking this internal annotation process, we aimed to ensure the quality and reliability of the annotations before moving on to crowdsourcing. B.2 Annotation Steps When annotating an AI-generated text snippet, we follow a