Title: ENHANCING SPEECH-TO-SPEECH TRANSLATION WITH MULTIPLE TTS TARGETS
Authors: Jiatong Shi, Yun Tang, Ann Lee, Hirofumi Inaguma, Changhan Wang, Juan Pino, Shinji Watanabe
Section: 6. REFERENCES
[1] Enrique Vidal, “Finite-state speech-to-speech translation,” in ICASSP, 1997. [2] Shigeki Matsuda, Xinhui Hu, Yoshinori Shiga, et al., “Multilingual speech-to-speech translation system: Voicetra,” in ICMDM, 2013. [3] Quoc Truong Do, Tomoki Toda, Graham Neubig, , et al., “Preserving word-level emphasis in speech-to-speech translation,” TASLP, 2016. [4] Ye Jia, Ron J Weiss, Fadi Biadsy, et al., “Direct speech-tospeech translation with a sequence-to-sequence model,” Interspeech, 2019. [5] Ye Jia, Michelle Tadmor Ramanovich, Tal Remez, , et al., “Translatotron 2: High-quality direct speech-to-speech translation with voice preservation,” in ICML, 2022. [6] Ye Jia, Yifan Ding, Ankur Bapna, et al., “Leveraging unsupervised and weakly-supervised data to improve direct speech-tospeech translation,” in Interspeech, 2022, pp. 1721–1725. [7] Ann Lee, Peng-Jen Chen, Changhan Wang, et al., “Direct speech-to-speech translation with discrete units,” in ACL, 2022. [8] Takatomo Kano, Sakriani Sakti, and Satoshi Nakamura, “Transformer-based direct speech-to-speech translation with transcoder,” in SLT, 2021. [9] Chen Zhang, Xu Tan, Yi Ren, et al., “Uwspeech: Speech to speech translation for unwritten languages,” in AAAI, 2021. [10] Xutai Ma, Hongyu Gong, Danni Liu, et al., “Direct simultaneous speech to speech translation,” arXiv preprint arXiv:2110.08250, 2021. [11] Sravya Popuri, Peng-Jen Chen, Changhan Wang, et al., “Enhanced Direct Speech-to-Speech Translation Using Selfsupervised Pre-training and Data Augmentation,” in Interspeech, 2022, pp. 5195–5199. [12] Ann Lee, Hongyu Gong, Paul-Ambroise Duquenne, et al., “Textless speech-to-speech translation on real data,” in NAACL, 2022, pp. 860–872. [13] Genichiro Kikui, Eiichiro Sumita, Toshiyuki Takezawa, et al., “Creating corpora for speech-to-speech translation,” in Eurospeech, 2003. [14] Ye Jia, Michelle Tadmor Ramanovich, et al., “CVSS corpus and massively multilingual speech-to-speech translation,” in LREC, 2022, pp. 6691–6703. [15] Pedro Jeuris and Jan Niehues, “Libris2s: A german-english speech-to-speech translation corpus,” in LREC, 2022, pp. 928– 935. [16] Jonathan Shen, Ye Jia, Mike Chrzanowski, et al., “Nonattentive Tacotron: Robust and controllable neural TTS synthesis including unsupervised duration modeling,” arXiv preprint arXiv:2010.04301, 2020. [17] Yi Ren, Chenxu Hu, Xu Tan, et al., “Fastspeech 2: Fast and high-quality end-to-end text to speech,” in ICLR, 2020. [18] Matt Post, Gaurav Kumar, Adam Lopez, et al., “Fisher and CALLHOME spanish–english speech translation,” LDC2014T23. Web Download. Philadelphia: Linguistic Data Consortium, 2014. [19] Shu-wen Yang, Po-Han Chi, Yung-Sung Chuang, et al., “SUPERB: Speech processing universal performance benchmark,” in Proc. Interspeech 2021, 2021, pp. 1194–1198. [20] Hsiang-Sheng Tsai, Heng-Jui Chang, Wen-Chin Huang, et al., “SUPERB-SG: Enhanced speech processing universal performance benchmark for semantic and generative capabilities,” in ACL, 2022, pp. 8479–8492. [21] Adam Polyak, Yossi Adi, Jade Copet, et al., “Speech resynthesis from discrete disentangled self-supervised representations,” in Interspeech, 2021.