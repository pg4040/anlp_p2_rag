Title: Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories
Authors: Suyu Ge, Chenyan Xiong, Corby Rosset, Arnold Overwijk, Jiawei Han, Paul Bennett
Section: A Appendix
we train DPR on MS MARCO (Bajaj et al., 2016) Dataset for fair comparison. Notice that this also lead to better results according to Xin et al. (2022). • ANCE constructs hard negative examples from an ANN index of the corpus. The hard negative training instances are updated in parallel during fine-tuning of the model. The model is a RoBERTa (Liu et al., 2019) model trained on MS MARCO for 600k steps. • T5-ANCE Different with default ANCE setting, we replace the backbone language model RoBERTa with T5-base. All the other model settings are the same with the original ANCE. We include this baseline because as a subroutine for MoMA, it could be viewed as an ablation without memory augmentation. We can directly observe the impact of plug-in mixture of memory by comparing T5-ANCE with MoMA. • coCondenser is a continuous pre-trained model based on BERT, with the equivalent amount of parameters to BERT-base. It enhances the representation ability of [CLS] token by changing the connections between different layers of Transformer blocks. Finetuning of coCondenser uses BM25 and self-mined negatives. • Contriever conducts unsupervised contrastive pretraining with data augmentations and momentum queues on Wikipedia and the larger CC-Net (Wenzek et al., 2020) corpora for 500k steps. • GTR initializes the dual encoders from the T5 models (Raffel et al., 2019). It is first pre-trained on Community QA6 with 2 billion question-answer pairs then fine-tuned on NQ and MS Marco dataset. In addition, they use the hard negatives released by RocketQA (Qu et al., 2021) when finetuning with MS Marco data and the hard negatives release by (Lu et al., 2021) for Natural Questions. GTRbase leverages the same T5-base model as MoMA, while GTRlarge is based on T5-large, which is not directly comparable to our method as it triples the parameters. Dense Retrieval with Generated Queries GenQ first fine-tunes a T5-base (Raffel et al., 2019) model on MS MARCO for 2 epochs and then generate 5 queries 6Unfortunately, this corpus has not been released by the authors. for each passage as additional training data for the target domain to continue to fine-tune the TAS-B (Hofstätter et al., 2021) model. Lexical Retrieval Lexical retrieval is a score function for token matching calculated between two high-dimensional sparse vectors with token weights. BM25 (Robertson et al., 2009) is the most commonly used lexical retrieval function. We use the BM25 results reported in Thakur et al. (2021b) for comparison. Late Interaction We also consider a late interaction baseline, namely ColBERT (Khattab and Zaharia, 2020b). The