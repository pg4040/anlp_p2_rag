Title: UNIVERSLU: UNIVERSAL SPOKEN LANGUAGE UNDERSTANDING FOR DIVERSE CLASSIFICATION AND SEQUENCE GENERATION TASKS WITH A SINGLE NETWORK
Authors: Siddhant Arora, Hayato Futami, Jee-weon Jung, Yifan Peng, Roshan Sharma, Yosuke Kashiwagi, Emiru Tsunoo, Shinji Watanabe
Section: 7. REFERENCES
[1] S. Chen, Y. Zhang, and Q. Yang, Multi-task learning in natural language processing: An overview, 2021. [2] R. Caruana, “Multitask learning,” Machine Learning, vol. 28, 1997. [3] Y. Zhang and Q. Yang, “A survey on multi-task learning,” CoRR, vol. abs/1707.08114, 2017. [4] Y. Zhang et al., “Google USM: scaling automatic speech recognition beyond 100 languages,” CoRR, vol. abs/2303.01037, 2023. [5] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, “Robust speech recognition via large-scale weak supervision,” CoRR, vol. abs/2212.04356, 2022. [6] K. Clark, M. Luong, U. Khandelwal, C. D. Manning, and Q. V. Le, “Bam! born-again multi-task networks for natural language understanding,” in Proc. ACL, 2019, pp. 5931–5937. [7] W. Chan, D. S. Park, C. A. Lee, Y. Zhang, Q. V. Le, and M. Norouzi, “Speechstew: Simply mix all available speech recognition data to train one large neural network,” CoRR, vol. abs/2104.02133, 2021. [8] L. Wu, Y. Rao, H. Jin, A. Nazir, and L. Sun, “Different absorption from the same sharing: Sifted multi-task learning for fake news detection,” in Proc. EMNLP, 2019, pp. 4643–4652. [9] H. Zhang, L. Xiao, Y. Wang, and Y. Jin, “A generalized recurrent neural architecture for text classification with multi-task learning,” in IJCAI, 2017. [10] S. Zhao, T. Liu, S. Zhao, and F. Wang, “A neural multi-task learning framework to jointly model medical named entity recognition and normalization,” in AAAI, 2019, pp. 817–824. [11] J. Wang et al., “Sentiment classification in customer service dialogue with topic-aware multi-task learning,” in AAAI, 2020, pp. 9177–9184. [12] J. Zhuang and Y. Liu, “Pintext: A multitask text embedding system in pinterest,” in KDD, 2019, pp. 2653–2661. [13] S. Yadav, A. Ekbal, S. Saha, and P. Bhattacharyya, “A unified multitask adversarial learning framework for pharmacovigilance mining,” in Proc. ACL, 2019, pp. 5234–5245. [14] J. Pfeiffer, I. Vulic, I. Gurevych, and S. Ruder, “MAD-X: an adapterbased framework for multi-task cross-lingual transfer,” in Proc. EMNLP, 2020, pp. 7654–7673. [15] A. C. Stickland and I. Murray, “BERT and pals: Projected attention layers for efficient adaptation in multi-task learning,” in Proc. ICML, ser. Proceedings of Machine Learning Research, vol. 97, 2019, pp. 5986–5995. [16] R. Collobert and J. Weston, “A unified architecture for natural language processing: Deep neural networks with multitask learning,” in Proc. ICML, ser. ACM International Conference Proceeding Series, vol. 307, 2008, pp. 160–167. [17] J. G. C. de Souza, M. Negri, E. Ricci, and M. Turchi, “Online multitask learning for machine translation quality estimation,” in Proc. ACL, 2015, pp. 219–228. [18] P. Gupta, H. Schütze, and