Title: A SINGLE SPEECH ENHANCEMENT MODEL UNIFYING DEREVERBERATION, DENOISING, SPEAKER COUNTING, SEPARATION, AND EXTRACTION
Authors: Kohei Saijo, Wangyou Zhang, Zhong-Qiu Wang, Shinji Watanabe, Tetsunori Kobayashi, Tetsuji Ogawa, Jiao Tong
Section: 6. REFERENCES
Joint training of source counting, separation and ASR,” in Proc. Interspeech, 2020, pp. 3097–3101. [20] J. Shi et al., “Sequence to multi-sequence learning via conditional chain mapping for mixture signals,” Advances in Neural Information Processing Systems, vol. 33, pp. 3735–3747, 2020. [21] S. R. Chetupalli and E. A. Habets, “Speaker counting and separation from single-channel noisy mixtures,” IEEE Trans. Audio, Speech, Lang. Process., 2023. [22] S. Horiguchi et al., “Encoder-decoder based attractors for endto-end neural diarization,” IEEE Trans. Audio, Speech, Lang. Process., vol. 30, pp. 1493–1507, 2022. [23] I. Kavalerov et al., “Universal sound separation,” in Proc. WASPAA, 2019, pp. 175–179. [24] J. Serrà et al., “Universal speech enhancement with scorebased diffusion,” arXiv preprint arXiv:2206.03065, 2022. [25] J. Chen et al., “Dual-path transformer network: Direct contextaware modeling for end-to-end monaural speech separation,” in Proc. Interspeech, 2020, pp. 2642–2646. [26] Y. Luo et al., “Dual-path RNN: Efficient long sequence modeling for time-domain single-channel speech separation,” in Proc. ICASSP, 2020, pp. 46–50. [27] E. Perez et al., “FiLM: Visual reasoning with a general conditioning layer,” in Proc. AAAI, vol. 32, no. 1, 2018. [28] J. S. Garofolo et al., CSR-I (WSJ0) Complete LDC93S6A, Linguistic Data Consortium, Philadelphia, 1993, web Download. [29] G. Wichern et al., “WHAM!: Extending speech separation to noisy environments,” in Proc. Interspeech, 2019, pp. 1368– 1372. [30] R. Scheibler et al., “Pyroomacoustics: A Python package for audio room simulation and array processing algorithms,” in Proc. ICASSP, 2018, pp. 351–355. [31] K. Kinoshita et al., “The REVERB challenge: A common evaluation framework for dereverberation and recognition of reverberant speech,” in Proc. WASPAA, 2013, pp. 1–4. [32] S. Watanabe et al., “ESPnet: End-to-end speech processing toolkit,” in Proc. Interspeech, 2018, pp. 2207–2211. [33] C. Li et al., “ESPnet-SE: End-to-end speech enhancement and separation toolkit designed for ASR integration,” in Proc. SLT, 2021, pp. 785–792. [34] J. Le Roux et al., “SDR–half-baked or well done?” in Proc. ICASSP, 2019, pp. 626–630. [35] P. K. Diederik and B. Jimmy, “Adam: A method for stochastic optimization,” in ICLR, 2015. [36] E. Vincent et al., “Performance measurement in blind audio source separation,” IEEE Trans. Audio, Speech, Lang. Process., vol. 14, no. 4, pp. 1462–1469, 2006. [37] R. Scheibler, “SDR — Medium rare with fast computations,” in Proc. ICASSP, 2022. [38] A. Rix et al., “Perceptual evaluation of speech quality (PESQ)a new method for speech quality assessment of telephone networks and codecs,” in Proc. ICASSP, vol. 2, 2001, pp. 749– 752. [39] A. Radford et al., “Robust speech recognition via large-scale weak supervision,” arXiv preprint arXiv:2212.04356,