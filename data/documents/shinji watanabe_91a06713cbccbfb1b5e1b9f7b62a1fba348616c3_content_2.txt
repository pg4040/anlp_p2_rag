Title: VOXTLM: UNIFIED DECODER-ONLY MODELS FOR CONSOLIDATING SPEECH RECOGNITION, SYNTHESIS AND SPEECH, TEXT CONTINUATION TASKS
Authors: Soumi Maiti, Yifan Peng, Shukjae Choi, Jee-weon Jung, Xuankai Chang, Shinji Watanabe
Section: 6. REFERENCES
efficient and high fidelity speech synthesis,” in Proc. NeurIPS, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., vol. 33. Curran Associates, Inc., 2020, pp. 17 022–17 033. [Online]. Available: https://proceedings.neurips.cc/paper_files/paper/ 2020/file/c5d736809766d46260d816d8dbc9eb44-Paper.pdf 3 [29] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur, “X-Vectors: Robust DNN embeddings for speaker recognition,” in Proc. ICASSP, 2018, pp. 5329–5333. 3 [30] T. A. Nguyen, M. de Seyssel, P. Rozé, M. Rivière, E. Kharitonov, A. Baevski, E. Dunbar, and E. Dupoux, “The zero resource speech benchmark 2021: Metrics and baselines for unsupervised spoken language modeling,” in Proc. NeuRIPS Workshop on Self-Supervised Learning for Speech and Audio Processing, 2020. 3 [31] C.-C. Lo, S.-W. Fu, W.-C. Huang et al., “MOSNet: Deep learning based objective assessment for voice conversion,” in Proc. Interspeech, 2019, pp. 1541–1545. 3 [32] E. Cooper, W.-C. Huang, T. Toda et al., “Generalization ability of mos prediction networks,” in Proc. ICASSP, 2022, pp. 8442–8446. 3 [33] J. Kahn, M. Rivière, W. Zheng et al., “Libri-light: A benchmark for asr with limited or no supervision,” in Proc. ICASSP, 2020, pp. 7669–7673. 3 [34] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librispeech: An asr corpus based on public domain audio books,” in Proc. ICASSP, 2015, pp. 5206–5210. 3 [35] V. Pratap, Q. Xu, A. Sriram, G. Synnaeve, and R. Collobert, “Mls: A large-scale multilingual dataset for speech research,” in Proc. Interspeech, 2020, pp. 2757–2761. 3 [36] H. Zen, R. Clark, R. J. Weiss, V. Dang, Y. Jia, Y. Wu, Y. Zhang, and Z. Chen, “Libritts: A corpus derived from librispeech for text-to-speech,” in Proc. Interspeech, 2019. [Online]. Available: https://arxiv.org/abs/1904.02882 3 [37] C. Veaux, J. Yamagishi, K. MacDonald et al., “Cstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit,” University of Edinburgh. The Centre for Speech Technology Research (CSTR), vol. 6, p. 15, 2017. 3 [38] D. Kingma, “Adam: a method for stochastic optimization,” in Proc. ICLR, 2014. 3 [39] J. Kim, J. Kong, and J. Son, “Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech,” in International Conference on Machine Learning. PMLR, 2021, pp. 5530–5540. 4 [40] K. Kim, F. Wu, Y. Peng, J. Pan, P. Sridhar, K. J. Han, and S. Watanabe, “E-branchformer: Branchformer with enhanced merging for speech recognition,” in Proc. SLT), 2023, pp. 84–91. 4