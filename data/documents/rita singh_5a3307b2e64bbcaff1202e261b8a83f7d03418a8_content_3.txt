Title: Rethinking Voice-Face Correlation: A Geometry View
Authors: Xiang Li, Yandong Wen, Muqiao Yang, Jinglu Wang, Rita Singh, Bhiksha Raj, Mohamed bin Zayed
Section: 3.3 Facial AM Prediction
and 𝜖𝐶𝑘 on these splits. Optional phonatory module. Inspired by linear predictive coding (LPC) [25] which leverages voice producing to learn vocal tract geometry, we aim to facilitate face geometry capture by learning characteristics of voice. We enroll a phonatory module serving as an additional constraint when predicting facial AMs. In particular, we leverage a diffusion-based [18] voice generation method to model the time-domain speech signals. As shown in Fig. 2, the diffusion model converts the noise distribution to a speech 𝑣 controlled by the voice code 𝑒 extracted from speech 𝑣 . During training speech 𝑣 ′ which shares speaker identity with 𝑣 is fed to the diffusion model as ground-truth. Please note that the phonatory module only serves as an additional training constraint and is not applied during inference. Let 𝑥0, · · · , 𝑥𝑇 be a sequence of variables with the same dimension where 𝑡 is the index for diffusion time steps. Then the diffusion process transforms 𝑥0 into a Gaussian noise 𝑥𝑇 through a chain of Markov transitions with a set of variance schedule 𝛽1, · · · , 𝛽𝑇 . Specifically, each transformation is performed according to the Markov transition probability 𝑞(𝑥𝑡 |𝑥𝑡−1, 𝑒) assumed to be independent of the style code 𝑒 as 𝑞(𝑥𝑡 |𝑥𝑡−1, 𝑒) = N(𝑥𝑡 ; √︁ 1 − 𝛽𝑡𝑥𝑡−1, 𝛽𝑡 𝐼 ). (5) Unlike the diffusion process, the denoising process aims to recover the speech signal from Gaussian noise which is defined as a conditional distribution 𝑝𝜃 (𝑥0:𝑇−1 |𝑥𝑇 , 𝑐). Through the reverse transitions 𝑝𝜃 (𝑥0:𝑇−1 |𝑥𝑇 , 𝑐), the variables are gradually restored to a speech signal with style code condition. The phonatory module actually models a distribution 𝑞(𝑥0 |𝑐). By applying the parameterization trick [21], we obtain the additional training constraint as {E∗, 𝜃∗} = arg min E,𝜃 = E𝑥0,𝜖,𝑡 ∥𝜖 − 𝜖𝜃 ( √ 𝛼𝑡𝑥0 + √ 1 − 𝛼𝑡𝜖, 𝑡, 𝑒)∥1 (6) where 𝛼𝑡 = 1 − 𝛽𝑡 and 𝛼𝑡 = ∏𝑡 𝑡 ′=1 𝛼𝑡 ′ . As shown in Fig. 2, the 𝜃 is a Net [36] with cross-attention [35]. Since the phonatory model is only utilized as an auxiliary constraint during training, we omit the inference details to obtain 𝑣 here.