Title: Why do Nearest Neighbor Language Models Work?
Authors: Frank F. Xu, Uri Alon, Graham Neubig
Section: E.6.3 Training to Optimize Interpolated Loss
In Section 4.2, we discover that using over-parameterization with standard LM training loss does not further close the gap towards kNN-LM. This suggests that some regularization term may be needed during training to make the multiple embeddings not converge to the same vector, rendering over-parameterization useless. From Table 2, we see that a better interpolated perplexity may not require a very low perplexity when measured only with the extra input representation. However, we still use a standard LM loss to only train the additional embedding matrix, that directly minimizes the perplexity using only the extra input representation. This discrepancy between training and the evaluation with interpolation suggests that training with an alternative loss function that interpolates the base LM’s output with the output using the extra input representation may be beneficial. To test the hypothesis that standard LM training loss do not emphasize the examples where base LM performs badly, we train the extra model’s parameter Wds, with interpolated loss L: L = CrossEntropy(λsoftmax(Wds · hds) + (1− λ)softmax(Wsm · hsm), y) (12) y represents the ground truth label for each context. We only learn the parameter Wds while freezing all other parameters, similar to all other experiments. We choose λ = 0.25 as it is the best hyper-parameter for kNN-LM experiments and our goal for this training is to mimic the loss of kNN-LM after interpolation. This training loss effectively assigns a higher value to the training examples where the base LM’s loss is high, suggesting the need for the extra Wds to help with these hard cases. However, for either “att” for “ffn” for hds, either V or 3V for the number of embeddings in Wds, we are unable to achieve a better perplexity than just the base LM. This suggests that, while nice on paper, the interpolated loss optimization process is not trivial.