Title: Efficient Sequence Transduction by Jointly Predicting Tokens and Durations
Authors: Hainan Xu, Fei Jia, Somshubra Majumdar, He Huang, Shinji Watanabe, Boris Ginsburg
Section: F. Robustness to Token Repetitions
We notice that RNN-T models often suffer serious performance degradation when the text sequence has repetitions of the same tokens (repetition on the subword level to be exact if using subword tokenizations). Our investigation shows that more training data will not solve this issue, and this is an intrinsic issue of RNN-Ts. Let’s demonstrate the issue with an example here: suppose we have audio with text sequence two two two two two two two five. Those words are frequent enough that they are all part of the BPE vocabulary. Let’s assume in the audio, frames 0 to frame 40 correspond to all the twos, and five starts at frame 41; let’s also assume we are at audio frame 30, and the model just emitted 5 twos during decoding. At this time, the decoder state was updated by feeding in 5 twos. At this point, there are two possibilities, Option 1. If the model emits another two between frame 30 and 40, say at frame t, this means, two = argmax ( join(enc[t], dec(<bos>, two, two, two, two, two︸ ︷︷ ︸ 5 twos ) ) (53) where join, enc, dec represent the computation of joiner, encoder, and decoder of the RNN-T model; for convenience, we assume the argmax operation directly returns the word from the distribution generated by the joiner. dec(a, b, c, d, ...) represents the final output of the decoder, after we sequentially pass a, b, c, d, ... as the decoder input. Since an LSTM decoder21 rarely has memory beyond 3-4 words, we have dec(<bos>, two, two, two, two, two︸ ︷︷ ︸ 5 twos ) ≈ dec(<bos>, two, two, two, two, two, two︸ ︷︷ ︸ 6 twos ) (54) We would like to point out that having a large number of repetitions isn’t the necessary condition for this to happen; sometimes this happens with just two repetitions of the same token. Since two is a non-blank emission, t will not get incremented, and the next decoding step operates on the same enc(t). Therefore, when we compute the output distribution of 21Or in the case of stateless decoders, if the context size is less than 5, then it should be strictly equal. the next decoding step, it’s likely that, join(enc[t], dec(<bos>, two, two, two, two, two, two︸ ︷︷ ︸ 6 twos ) ≈ join(enc[t], dec(<bos>, two, two, two, two, two︸ ︷︷ ︸ 5 twos ) (55) Note, since joiner usually has a non-linearity in its computation, this does not strictly follow; although based on what we observed this is