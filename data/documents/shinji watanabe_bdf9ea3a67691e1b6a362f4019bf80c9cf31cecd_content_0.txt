Title: FINDINGS OF THE 2023 ML-SUPERB CHALLENGE: PRE-TRAINING AND EVALUATION OVER MORE LANGUAGES AND BEYOND
Authors: Jiatong Shi, William Chen, Dan Berrebbi, Hsiu-Hsuan Wang, Wei-Ping Huang, En-Pei Hu, Ho-Lam Chuang, Xuankai Chang, Yuxun Tang, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Shinji Watanabe
Section: 8. REFERENCES
[1] Abdelrahman Mohamed, Hung-yi Lee, Lasse Borgholt, Jakob D Havtorn, Joakim Edin, Christian Igel, Katrin Kirchhoff, Shang-Wen Li, Karen Livescu, et al., “Self-supervised speech representation learning: A review,” JSTSP, 2022. [2] Shu wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I Jeff Lai, Kushal Lakhotia, Yist Y. Lin, Andy T. Liu, Jiatong Shi, et al., “SUPERB: Speech Processing Universal PERformance Benchmark,” in Proc. Interspeech, 2021, pp. 1194–1198. [3] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli, “wav2vec 2.0: A framework for self-supervised learning of speech representations,” Proc. NeurIPS, vol. 33, pp. 12449–12460, 2020. [4] Wei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed, “Hubert: Self-supervised speech representation learning by masked prediction of hidden units,” TASLP, vol. 29, pp. 3451–3460, 2021. [5] Arun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, et al., “XLS-R: Selfsupervised cross-lingual speech representation learning at scale,” arXiv preprint arXiv:2111.09296, 2021. [6] Alexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, and Michael Auli, “Unsupervised crosslingual representation learning for speech recognition,” Proc. Interspeech, 2020. [7] Paul-Ambroise Duquenne, Hongyu Gong, Ning Dong, Jingfei Du, Ann Lee, Vedanuj Goswani, Changhan Wang, Juan Pino, Benoı̂t Sagot, and Holger Schwenk, “Speechmatrix: A largescale mined corpus of multilingual speech-to-speech translations,” arXiv preprint arXiv:2211.04508, 2022. [8] Jing Zhao and Wei-Qiang Zhang, “Improving automatic speech recognition performance for low-resource languages with self-supervised models,” JSTSP, vol. 16, no. 6, pp. 1227– 1241, 2022. [9] Dan Berrebbi, Jiatong Shi, Brian Yan, Osbel López-Francisco, Jonathan Amith, and Shinji Watanabe, “Combining Spectral and Self-Supervised Features for Low Resource Speech Recognition and Translation,” in Proc. Interspeech, 2022, pp. 3533–3537. [10] Anne Wu, Changhan Wang, Juan Pino, and Jiatao Gu, “Selfsupervised representations improve end-to-end speech translation,” Proc. Interspeech, pp. 1491–1495, 2020. [11] Xinjian Li, Florian Metze, David R Mortensen, Alan W Black, and Shinji Watanabe, “ASR2K: Speech recognition for around 2000 languages without audio,” Proc. Interspeech, pp. 4885– 4889, 2022. [12] Jiatong Shi, Dan Berrebbi, William Chen, Ho-Lam Chung, EnPei Hu, Wei Ping Huang, Xuankai Chang, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, et al., “ML-SUPERB: Multilingual speech universal performance benchmark,” Proc. Interspeech, 2023. [13] Jiatong Shi, Jonathan D Amith, Xuankai Chang, Siddharth Dalmia, Brian Yan, and Shinji Watanabe, “Highland puebla nahuatl speech translation corpus for endangered language documentation,” in Proc. AmericaNLP, 2021, pp. 53–63. [14] Jiatong Shi, Jonathan D Amith, Rey Castillo Garcı́a, Esteban Guadalupe Sierra, Kevin Duh, and Shinji Watanabe, “Leveraging end-to-end asr for endangered language documentation: An empirical study on Yolóxochitl Mixtec,” in