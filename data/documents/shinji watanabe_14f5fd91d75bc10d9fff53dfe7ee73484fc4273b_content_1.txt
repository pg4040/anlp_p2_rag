Title: A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks
Authors: Yifan Peng, Kwangyoun Kim, Felix Wu, Brian Yan, Siddhant Arora, William Chen, Jiyang Tang, Suwon Shon, Prashant Sridhar, Shinji Watanabe
Section: 8. References
B. Li, T. Xiao et al., “Learning deep transformer models for machine translation,” in Proc. ACL, 2019. [23] T. Q. Nguyen and J. Salazar, “Transformers without tears: Improving the normalization of self-attention,” in Proc. IWSLT, 2019. [24] P. Ramachandran, B. Zoph, and Q. V. Le, “Searching for activation functions,” arXiv preprint arXiv:1710.05941, 2017. [25] Y. Lu et al., “Understanding and improving transformer from a multi-particle dynamic system point of view.” in Proc. ICLR Workshop on Integrat. of Deep Neural Models and Diff. Eq., 2019. [26] Z. Dai et al., “Transformer-XL: Attentive language models beyond a fixed-length context,” in Proc. ACL, 2019. [27] Y. N. Dauphin, A. Fan et al., “Language modeling with gated convolutional networks,” in Proc. ICML, 2017. [28] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network training by reducing internal covariate shift,” in Proc. ICML, 2015. [29] J. Sakuma, T. Komatsu, and R. Scheibler, “MLP-based architecture with variable length input for automatic speech recognition,” 2022. [Online]. Available: https://openreview.net/ forum?id=RA-zVvZLYIy [30] M. M. Forbes et al., “AphasiaBank: A resource for clinicians,” in Proc. Seminars in speech and language, 2012. [31] S. Kim et al., “Joint CTC-attention based end-to-end speech recognition using multi-task learning,” in Proc. ICASSP, 2017. [32] T. Hori, S. Watanabe, and J. R. Hershey, “Joint CTC/attention decoding for end-to-end speech recognition,” in Proc. ACL, 2017. [33] W. Chen, B. Yan et al., “Improving Massively Multilingual ASR With Auxiliary CTC Objectives,” in Proc. ICASSP, 2023. [34] J. Lee and S. Watanabe, “Intermediate loss regularization for ctcbased speech recognition,” in Proc. ICASSP, 2021. [35] J. Nozaki and T. Komatsu, “Relaxing the Conditional Independence Assumption of CTC-Based ASR by Conditioning on Intermediate Predictions,” in Proc. Interspeech, 2021. [36] D. S. Park, W. Chan, Y. Zhang et al., “SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition,” in Proc. Interspeech, 2019. [37] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” arXiv preprint arXiv:1412.6980, 2014. [38] “aidatatang 200zh, a free Chinese Mandarin speech corpus by Beijing DataTang Technology Co., Ltd ( www.datatang.com ).” [39] H. Bu, J. Du, X. Na, B. Wu, and H. Zheng, “AISHELL-1: An open-source Mandarin speech corpus and a speech recognition baseline,” in Proc. O-COCOSDA, 2017. [40] E. Vincent et al., “An analysis of environment, microphone and data simulation mismatches in robust speech recognition,” Computer speech & language, vol. 46, pp. 535–557, 2017. [41] M. Post et al., “Improved speech-to-text translation with the fisher and callhome Spanish-English speech translation corpus,” in Proc. IWSLT, 2013. [42] A. Conneau et al., “FLEURS: Few-Shot