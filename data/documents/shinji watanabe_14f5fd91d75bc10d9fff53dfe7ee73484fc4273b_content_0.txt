Title: A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks
Authors: Yifan Peng, Kwangyoun Kim, Felix Wu, Brian Yan, Siddhant Arora, William Chen, Jiyang Tang, Suwon Shon, Prashant Sridhar, Shinji Watanabe
Section: 8. References
[1] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, attend and spell: A neural network for large vocabulary conversational speech recognition,” in Proc. ICASSP, 2016. [2] A. Zeyer et al., “Improved Training of End-to-end Attention Models for Speech Recognition,” in Proc. Interspeech, 2018. [3] C.-C. Chiu, T. N. Sainath et al., “State-of-the-art speech recognition with sequence-to-sequence models,” in Proc. ICASSP, 2018. [4] T. N. Sainath, B. Kingsbury et al., “Improvements to Deep Convolutional Neural Networks for LVCSR,” in Proc. ASRU, 2013. [5] J. Li, V. Lavrukhin et al., “Jasper: An End-to-End Convolutional Neural Acoustic Model,” in Proc. Interspeech, 2019. [6] W. Han, Z. Zhang, Y. Zhang et al., “ContextNet: Improving Convolutional Neural Networks for Automatic Speech Recognition with Global Context,” in Proc. Interspeech, 2020. [7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones et al., “Attention is all you need,” in Proc. NeurIPS, 2017. [8] S. Karita, N. Chen, T. Hayashi et al., “A comparative study on transformer vs rnn in speech applications,” in Proc. ASRU, 2019. [9] Q. Zhang, H. Lu, H. Sak et al., “Transformer transducer: A streamable speech recognition model with transformer encoders and rnn-t loss,” in Proc. ICASSP, 2020. [10] A. Gulati, J. Qin et al., “Conformer: Convolution-augmented Transformer for Speech Recognition,” in Proc. Interspeech, 2020. [11] P. Guo, F. Boyer, X. Chang et al., “Recent developments on espnet toolkit boosted by conformer,” in Proc. ICASSP, 2021. [12] Y. Peng et al., “Branchformer: Parallel MLP-attention architectures to capture local and global context for speech recognition and understanding,” in Proc. ICML, 2022. [13] K. Kim, F. Wu et al., “E-Branchformer: Branchformer with Enhanced Merging for Speech Recognition,” in Proc. SLT, 2022. [14] V. Panayotov et al., “Librispeech: An ASR corpus based on public domain audio books,” in Proc. ICASSP, 2015. [15] A. Graves et al., “Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,” in Proc. ICML, 2006. [16] A. Graves, “Sequence transduction with recurrent neural networks,” arXiv preprint arXiv:1211.3711, 2012. [17] S. Watanabe, T. Hori, S. Karita et al., “ESPnet: End-to-End Speech Processing Toolkit,” in Proc. Interspeech, 2018. [18] H. Inaguma et al., “ESPnet-ST: All-in-one speech translation toolkit,” in Proc. ACL: System Demonstrations, 2020. [19] S. Arora et al., “ESPnet-SLU: Advancing Spoken Language Understanding Through ESPnet,” in Proc. ICASSP, 2022. [20] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proc. CVPR, 2016. [21] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” arXiv preprint arXiv:1607.06450, 2016. [22] Q. Wang,