Title: TOWARDS PRACTICAL AND EFFICIENT IMAGE-TO-SPEECH CAPTIONING WITH VISION-LANGUAGE PRE-TRAINING AND MULTI-MODAL TOKENS
Authors: Minsu Kim, Jeongsoo Choi, Soumi Maiti, Jeong Hun Yeo, Shinji Watanabe, Yong Man Ro
Section: 3.3. Experimental Results
M. Hasegawa-Johnson et al., “Image2speech: Automatically generating audio descriptions of images,” Casablanca 2017, p. 65, 2017. [2] K. Xu et al., “Show, attend and tell: Neural image caption generation with visual attention,” in ICML, PMLR, 2015, pp. 2048–2057. [3] A. Van Den Oord, O. Vinyals, et al., “Neural discrete representation learning,” NIPS, vol. 30, 2017. [4] M. Kim, J. Hong, and Y. M. Ro, “Lip-to-speech synthesis in the wild with multi-task learning,” in Proc. ICASSP, IEEE, 2023, pp. 1–5. [5] X. Wang et al., “Synthesizing spoken descriptions of images,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3242–3254, 2021. [6] K. Lakhotia et al., “On generative spoken language modeling from raw audio,” Transactions of the Association for Computational Linguistics, vol. 9, pp. 1336–1354, 2021. [7] H. Inaguma et al., “Unity: Two-pass direct speech-to-speech translation with discrete units,” in Proc. ACL, 2023. [8] S. Popuri et al., “Enhanced direct speech-to-speech translation using self-supervised pre-training and data augmentation,” in Proc. Interspeech, 2022. [9] M. Kim et al., “Many-to-many spoken language translation via unified speech and text representation learning with unit-to-unit translation,” arXiv preprint arXiv:2308.01831, 2023. [10] A. Sicherman and Y. Adi, “Analysing discrete self supervised speech representation for spoken language modeling,” in Proc. ICASSP, IEEE, 2023, pp. 1–5. [11] T. A. Nguyen et al., “Generative spoken dialogue language modeling,” Transactions of the Association for Computational Linguistics, vol. 11, pp. 250–266, 2023. [12] T. Hayashi and S. Watanabe, “Discretalk: Text-to-speech as a machine translation problem,” arXiv preprint arXiv:2005.05525, 2020. [13] J. Choi, M. Kim, and Y. M. Ro, “Intelligible lip-to-speech synthesis with speech units,” in Proc. Interspeech, 2023. [14] W.-N. Hsu et al., “Hubert: Self-supervised speech representation learning by masked prediction of hidden units,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451– 3460, 2021. [15] X. Chang et al., “Exploration of efficient end-to-end asr using discretized input from self-supervised learning,” in Proc. Interspeech, 2023. [16] M. Kim et al., “Lip reading for low-resource languages by learning and combining general speech knowledge and language-specific knowledge,” in Proc. ICCV, 2023. [17] G. Chrupała, L. Gelderloos, and A. Alishahi, “Representations of language in a model of visually grounded speech signal,” in Proc. ACL, 2017, pp. 613–622. [18] Y.-J. Shih et al., “Speechclip: Integrating speech with pre-trained vision and language model,” in SLT Workshop, IEEE, 2023. [19] J. Hong et al., “Watch or listen: Robust audio-visual speech recognition with visual corruption modeling and reliability scoring,” in Proc. CVPR, 2023, pp. 18 783–18 794. [20] A. Radford et al., “Learning transferable