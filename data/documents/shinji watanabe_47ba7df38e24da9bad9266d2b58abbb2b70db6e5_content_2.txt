Title: Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning
Authors: Xuankai Chang, Brian Yan, Yuya Fujita, Takashi Maekaku, Shinji Watanabe
Section: 5. Acknowledgements
developments on espnet toolkit boosted by conformer,” in Proc. ICASSP, 2021, pp. 5874–5878. [36] R. Sennrich, B. Haddow, and A. Birch, “Neural machine translation of rare words with subword units,” in Proc. ACL, 2016, pp. 1715–1725. [37] T. Kudo, “Subword regularization: Improving neural network translation models with multiple subword candidates,” in Proc. ACL, 2018, pp. 66–75. [38] S. Kim, T. Hori, and S. Watanabe, “Joint ctc-attention based endto-end speech recognition using multi-task learning,” in Proc. ICASSP, 2017, pp. 4835–4839. [39] D. S. Park et al., “Specaugment: A simple data augmentation method for automatic speech recognition,” Proc. Interspeech 2019, pp. 2613–2617, 2019. [40] T. DeVries and G. W. Taylor, “Improved regularization of convolutional neural networks with cutout,” arXiv preprint arXiv:1708.04552, 2017. [41] S. Watanabe et al., “ESPnet: End-to-end speech processing toolkit,” in Proc. Interspeech, 2018, pp. 2207–2211. [42] K. Kim et al., “E-branchformer: Branchformer with enhanced merging for speech recognition,” arXiv preprint arXiv:2210.00077, 2022. [43] L. Lugosch et al., “Speech model pre-training for end-toend spoken language understanding,” Proc. Interspeech 2019, pp. 814–818, 2019.