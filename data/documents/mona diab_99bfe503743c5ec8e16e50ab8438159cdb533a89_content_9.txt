Title: The Troubling Emergence of Hallucination in Large Language Models – An Extensive Definition, Quantification, and Prescriptive Remediations
Authors: Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S.M Towhidul Islam Tonmoy, Aman Chadha, Amit Sheth, Amitava Das
Section: 
Generated Golem or Acronym Ambiguity (cf. Appendix F.1). 6.1.1 Lowering Concreteness of Language It is observed in (Varshney et al., 2023) that higher uncertainty in the model’s prediction (indicated by a low probability score) suggests a higher likelihood of the model hallucinating about that particular concept. In this context, we suggest that substituting high entropy points with less concrete words can help prevent hallucinations. Concreteness (Paivio, 2013) measures how much a word embodies a tangible or perceivable concept. Concrete words are simpler to comprehend than abstract ones. The level of concreteness for each word is denoted on a 5-point scale, ranging from abstract to concrete. Concreteness ratings cover 39,954 entries, including 37,058 individual English words and 2,896 two-word expressions (Brysbaert et al., 2014), being used here. 6.2 Factuality Check of Sentences (FACTUALITYGB): A Gray-box approach We use Google Search API (Search) to search for a given prompt, which has been utilized to generate the text and retrieve the top 20 documents. Then each sentence of AI-generated text has been validated either into support, refute, or not enough information using RoBERTa Large (Liu et al., 2019), a SoTA textual entailment model trained on the SNLI (Bowman et al., 2015) (cf. Section 6.2.1). Inevitably, sentences with higher scores in the refute and not enough information categories are flagged for additional human checking. Empirically, we observe an overall alert rate of 26% on sentences generated by an LLM, implying 26% of the text required rewriting in order to mitigate. 6.2.1 FACTUALITYGB Gray-box model does require output token-level probabilities (Manakul et al., 2023). Fig. 7 shows FACTUALITYGB, representing AI-generated text (from our HILT benchmark) based on a given prompt. In this method, the prompt is sent to the Google Search API to obtain the top 20 relevant search results. Out of these 20 results, we evaluate a total of n sentences for their relevance to the prompt using a similarity measure. The top 20 sentences most similar to the prompt are selected. For each of the m sentences in the AI-generated text and the top 20 ranked sentences, we employ a textual entailment model to assess their trustworthiness individually. Based on their entailment scores, we categorize the AI-generated text into three groups: (i) support, (ii) refute, and (iii) not enough information. Performance of ENTROPYBB vs. FACTUALITYGB: Fig. 5 offers a comparative analysis of the proposed approaches. While ENTROPYBB addresses simpler hallucinations such as Acronym Ambiguity and Numeric Nuisance, FACTUALITYGB handles more complex cases. It is clear that a balanced combination of black-box and gray-box