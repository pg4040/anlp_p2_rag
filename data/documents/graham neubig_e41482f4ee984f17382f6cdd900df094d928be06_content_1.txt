Title: WEBARENA: A REALISTIC WEB ENVIRONMENT FOR BUILDING AUTONOMOUS AGENTS
Authors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig
Section: 3.2 EVALUATION ANNOTATION
the fifth example in Table 1, first obtains the URL of the latest post by examining the last state in the state sequence s. Then it navigates to the corresponding post page and obtains the post’s content by running the Javascript “document.querySelector(‘.submission__inner’).outerText”. Subsequently, we annotate keywords that need to exist within the located content. For example, the evaluation verifies if the post is correctly posted in the “nyc” subreddit by examining the URL of Function ID Intent Eval Implementation rinfo(a ∗, â) 1 Tell me the name of the customer whohas the most cancellations in the history exact_match(â, “Samantha Jones”) 2 Find the customer name andemail with phone number 8015551212 must_include(â, “Sean Miller”) must_include(â, “sean@gmail.com”) 3 Compare walking and driving time from AMC Waterfront to Randyland fuzzy_match(â, “walking: 2h58min”) fuzzy_match(â, “driving: 21min”) rprog(s) 4 Checkout merge requests assigned to me url=locate_current_url(s) exact_match(URL, “gitlab.com/merge_ requests?assignee_username=byteblaze”) 5 Post to ask “whether Ineed a car in NYC” url=locate_latest_post_url(s) body=locate_latest_post_body(s) must_include(URL, “/f/nyc”) must_include(body,“a car in NYC”) Table 1: We introduce two evaluation approaches. rinfo (top) measures the correctness of performing information-seeking tasks. It compares the predicted answer â with the annotated reference a∗ with three implementations. rprog (bottom) programmatically checks whether the intermediate states during the executions possess the anticipated properties specified by the intent. the post and if the post contains the requested content by examining the post content. We reuse the exact_match and must_include functions from information-seeking tasks for this purpose. Unachievable Tasks Due to constraints such as inadequate evidence, user permissions (§A.3), or the absence of necessary functional support on the website, humans may ask for tasks that are not possible to complete. Inspired by previous work on evaluating question-answering models on unanswerable questions (Rajpurkar et al., 2018), we design unachievable tasks in WebArena. For instance, fulfilling an intent like “Tell me the contact number of OneStopShop” is impracticable in WebArena, given that the website does not provide such contact information. We label such instances as "N/A" and expect an agent to produce an equivalent response. These examples allow us to assess an agent’s ability to avoid making unfounded claims and its adherence to factual accuracy. Annotation Process The intents were contributed by the authors following the annotation guideline in §3.1. Every author has extensive experience with web-based tasks. The reference answers to the information-seeking tasks were curated by the authors and an external annotator. To ensure consistency and accuracy, each question was annotated twice. If the two annotators disagreed, a third annotator finalized the annotation. The programs to evaluate the