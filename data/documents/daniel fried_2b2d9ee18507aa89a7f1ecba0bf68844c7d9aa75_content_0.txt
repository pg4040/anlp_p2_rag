Title: Grounding Language Models to Images for Multimodal Inputs and Outputs
Authors: Jing Yu Koh, Ruslan Salakhutdinov, Daniel Fried
Section: D. Current Limitations and Broader Impacts
Many existing large language models and large generative models are prone to certain types of unintended behavior. They sometimes make up facts, generate toxic and socially biased text outputs, propagate disinformation, or ignore user prompts (Gehman et al., 2020; Bommasani et al., 2021; Bender et al., 2021). When tasked to generate text, these large models also often exhibit failure modes such as neural text degeneration and generation of incoherent and repetitive text (Holtzman et al., 2020; Li et al., 2022b). A core component of the FROMAGe model is the frozen LLM backbone, which we ground for producing text and visual outputs. Unsurprisingly, our model also inherits some of the problems that text-only LLMs possess, such as generating repetitive outputs, not following user instructions, and other common failure modes. It is also susceptible to the limitations of these language models, including the risk of producing disinformation or toxic content. The broader issues relating to text generation for our model are also likely to be addressed and alleviated by future work on better large language models. In particular, using language models that are finetuned with human feedback (Ouyang et al., 2022) or instruction finetuned (Wei et al., 2021; Chung et al., 2022) may be one direction towards improving output quality, and for reducing the risk of producing toxic and socially biased content. As FROMAGe is modular in nature, we can easily swap out our LLM backbone for better and more robust language models released in the future, enabling us to easily improve its performance on downstream applications, and reduce the risk of generating harmful content. FROMAGe is also a model that can produce images. In this work, we produce images (interleaved within text) by retrieving from a fixed set of images from Conceptual Captions (Sharma et al., 2018). Like other image-text retrieval models (Radford et al., 2021; Jia et al., 2021), our model is susceptible to existing biases found in the training and retrieval datasets. Although image retrieval is unable to produce truly novel images from outside of the retrieval data, it also has benefits for controlling output results. Unlike image generation models which synthesize novel images from scratch, a benefit of retrieving from a fixed corpus is that it allows us to explicitly control what our model can output. Retrieval enables possible mitigation strategies such as filtering for inappropriate content, such that FROMAGe and similar models would not be able to produce particular types of objectionable images. However, for deployment of such technologies (and future research on generative multimodal dialogue models),