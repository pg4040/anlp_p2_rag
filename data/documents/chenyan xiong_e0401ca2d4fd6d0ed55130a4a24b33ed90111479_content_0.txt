Title: Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories
Authors: Suyu Ge, Chenyan Xiong, Corby Rosset, Arnold Overwijk, Jiawei Han, Paul Bennett
Section: A Appendix
A.1 Datasets Details Evaluation Datasets Target domain datasets used in our experiments are collected in the BEIR benchmark (Thakur et al., 2021b)4 and include the following domains: • Open-domain Question Answering (QA): HotpotQA (Yang et al., 2018), NQ (Kwiatkowski et al., 2019), and FiQA (Maia et al., 2018). • Bio-Medical Information Retrieval: TRECCOVID (Voorhees et al., 2021), NFCorpus (Boteva et al., 2016), and BioASQ (Tsatsaronis et al., 2015). • Argument Retrieval: Webis-Touché2020 (Bondarenko et al., 2020) and ArguAna (Wachsmuth et al., 2018). • News Retrieval: TREC-NEWS (Soboroff et al., 2018) and Robust04 (Voorhees et al., 2004). • Tweet Retrieval: Signal-1m (Suarez et al., 2018). • Duplicate Question Retrieval: Quora (Thakur et al., 2021b) and CQADupStack (Hoogeveen et al., 2015). • Entity Retrieval: DBPedia (Hasibi et al., 2017) • Citation Prediction: SCIDOCS (Cohan et al., 2020) • Fact Checking: SciFact (Wadden et al., 2020), FEVER (Thorne et al., 2018), and ClimateFEVER (Diggelmann et al., 2020) We list the statistics of the BEIR benchmark in Table 7. Augmenting Corpora Corpus size We first introduce more details on how we preprocessed the Medical Subject Headings (MeSH) Database. We select text information from the Qualifier Record Set and Descriptor Record Set. Each set contains multiple <Concept> elements, which is composed of three sub-elecments, i.e., <ConceptName>, <ScopeNote> and <TermList>. Among the sub-elecments, <ScopeNote> is the major textual information source, which is usually a short description to a medical term or phenomenon. We directly consider each <ScopeNote> as a document entry and concatenate it with corresponding <ConceptName>. We list the statistics of the augmenting corpora in Table 8. A.2 Baselines We use the baselines from the current BEIR leaderboard (Thakur et al., 2021b) and recent papers. These baselines can be divided into four groups: dense retrieval, dense retrieval with generated queries5, lexical retrieval and late interaction. 4https://github.com/beir-cellar/beir 5We separate them from dense retrieval since they usually rely on Seq2seq models to generate pseudo query-document pairs, and they train a model for each dataset independently instead of using a single model for all datasets. Dense Retrieval For dense retrieval, the baselines are the same dual-tower model as ours. We consider DPR (Karpukhin et al., 2020), ANCE (Xiong et al., 2020), T5-ANCE, coCondenser (Gao and Callan, 2022) and one recently-proposed model GTR (Ni et al., 2021) with different size configuration in this paper. • DPR uses a single BM25 retrieval example and inbatch examples as hard negative examples to train the model. Different from the original paper (Thakur et al., 2021b) that train the DPR on QA datasets,