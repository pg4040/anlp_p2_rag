Title: TOWARDS PRACTICAL AND EFFICIENT IMAGE-TO-SPEECH CAPTIONING WITH VISION-LANGUAGE PRE-TRAINING AND MULTI-MODAL TOKENS
Authors: Minsu Kim, Jeongsoo Choi, Soumi Maiti, Jeong Hun Yeo, Shinji Watanabe, Yong Man Ro
Section: 3.3. Experimental Results
pp. 376–380. [41] C.-Y. Lin, “Rouge: A package for automatic evaluation of summaries,” in Text summarization branches out, 2004, pp. 74–81. [42] R. Vedantam, C. Lawrence Zitnick, and D. Parikh, “Cider: Consensusbased image description evaluation,” in Proc. CVPR, 2015. [43] P. Anderson et al., “Spice: Semantic propositional image caption evaluation,” in Proc. ECCV, Springer, 2016, pp. 382–398. [44] J. Kim, J. Kong, and J. Son, “Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech,” in ICML, PMLR, 2021, pp. 5530–5540. [45] A. Vaswani et al., “Attention is all you need,” Advances in neural information processing systems, vol. 30, 2017. [46] K. Ito and L. Johnson, The lj speech dataset, https://keithito. com/LJ-Speech-Dataset/, 2017. [47] P. Sharma et al., “Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning,” in Proc. ACL, 2018, pp. 2556–2565. [48] V. Ordonez, G. Kulkarni, and T. Berg, “Im2text: Describing images using 1 million captioned photographs,” Advances in neural information processing systems, vol. 24, 2011. [49] A. Lee et al., “Textless speech-to-speech translation on real data,” in Proc. NAACL, 2022, pp. 860–872. [50] A. Antonios et al., “Findings of the iwslt 2022 evaluation campaign.,” in Proc. IWSLT, ACL, 2022, pp. 98–157.