Title: DECODER-ONLY ARCHITECTURE FOR SPEECH RECOGNITION WITH CTC PROMPTS AND TEXT DATA AUGMENTATION
Authors: Emiru Tsunoo, Hayato Futami, Yosuke Kashiwagi, Siddhant Arora, Shinji Watanabe
Section: 5. REFERENCES
Chen, Dalia El Badawy, Wei Han, Eugene Kharitonov, et al., “AudioPaLM: A large language model that can speak and listen,” arXiv preprint arXiv:2306.12925, 2023. [15] Yassir Fathullah, Chunyang Wu, Egor Lakomkin, Junteng Jia, Yuan Shangguan, Ke Li, Jinxi Guo, Wenhan Xiong, Jay Mahadeokar, Ozlem Kalinli, et al., “Prompting large language models with speech recognition abilities,” arXiv preprint arXiv:2307.11795, 2023. [16] Jian Wu, Yashesh Gaur, Zhuo Chen, Long Zhou, Yimeng Zhu, Tianrui Wang, Jinyu Li, Shujie Liu, Bo Ren, Linquan Liu, et al., “On decoder-only architecture for speech-totext and large language model integration,” arXiv preprint arXiv:2307.03917, 2023. [17] Siddhant Arora, Hayato Futami, Yosuke Kashiwagi, Emiru Tsunoo, Brian Yan, and Shinji Watanabe, “Integrating pretrained ASR and LM to perform sequence generation for spoken language understanding,” in Proc. INTERSPEECH 2023, 2023, pp. 720–724. [18] Marco Gaido, Mauro Cettolo, Matteo Negri, and Marco Turchi, “CTC-based compression for direct speech translation,” in Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, 2021, pp. 690–696. [19] Zhengkun Tian, Jiangyan Yi, Ye Bai, Jianhua Tao, Shuai Zhang, and Zhengqi Wen, “FSR: Accelerating the inference process of transducer-based models by applying fast-skip regularization,” in Proc. of Interspeech, 2021, pp. 4034–4038. [20] Yongqiang Wang, Zhehuai Chen, Chengjian Zheng, Yu Zhang, Wei Han, and Parisa Haghani, “Accelerating RNN-T training and inference using CTC guidance,” in Proc. of ICASSP, 2023, pp. 1–5. [21] Pengcheng Guo, Florian Boyer, Xuankai Chang, Tomoki Hayashi, Yosuke Higuchi, et al., “Recent developments on espnet toolkit boosted by conformer,” arXiv preprint arXiv:2010.13956, 2020. [22] Shinji Watanabe, Takaaki Hori, Suyoun Kim, John R. Hershey, and Tomoki Hayashi, “Hybrid CTC/attention architecture for end-to-end speech recognition,” Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240–1253, 2017. [23] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur, “LibriSpeech: an ASR corpus based on public domain audio books,” in Proc. of ICASSP, 2015, pp. 5206– 5210. [24] Alex Graves, Santiago Fernández, Faustino Gomez, and Jürgen Schmidhuber, “Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,” in Proc. of 23rd International Conference on Machine Learning, 2006, pp. 369–376. [25] Alex Graves, Abdel-Rahman Mohamed, and Geoffrey Hinton, “Speech recognition with deep recurrent neural networks,” in Proc. of ICASSP, 2013, pp. 6645–6649. [26] Kazuki Irie, Albert Zeyer, Ralf Schlüter, and Hermann Ney, “Language modeling with deep transformers,” in Proc. of Interspeech, 2019, pp. 3905–3909.