Title: FINDINGS OF THE 2023 ML-SUPERB CHALLENGE: PRE-TRAINING AND EVALUATION OVER MORE LANGUAGES AND BEYOND
Authors: Jiatong Shi, William Chen, Dan Berrebbi, Hsiu-Hsuan Wang, Wei-Ping Huang, En-Pei Hu, Ho-Lam Chuang, Xuankai Chang, Yuxun Tang, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Shinji Watanabe
Section: 8. REFERENCES
Ensembling Self-Supervised Learning Features in Robust End-to-end Speech Recognition,” in Proc. Interspeech, 2022, pp. 3058–3062. [56] Tejes Srivastava, Jiatong Shi, William Chen, and Shinji Watanabe, “EFFUSE: Efficient self-supervised feature fusion for E2E ASR in multilingual and low resource scenarios,” arXiv preprint arXiv:2310.03938, 2023. [57] Hongfei Xue, Qijie Shao, Kaixun Huang, Peikun Chen, Lei Xie, and Jie Liu, “SSHR: Leveraging self-supervised hierarchical representations for multilingual automatic speech recognition,” arXiv preprint arXiv:2309.16937, 2023. [58] Vineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, et al., “Scaling speech technology to 1,000+ languages,” arXiv preprint arXiv:2305.13516, 2023.