Title: FedNAR: Federated Optimization with Normalized Annealing Regularization
Authors: Junbo Li, Ang Li, Chong Tian, Qirong Ho, Eric P. Xing, Hongyi Wang
Section: B Supplementary proofs
∥ ≤ E∥gt,ij −∇Fi(x t,i j )∥+ E∥∇Fi(x t,i j )∥+ u0 ε E∥xt,ij ∥ ≤ σ2 + E∥∇Fi(xt,ij )∥+ u0 ε E∥xt,ij ∥. (25) Here we denote ε = min{lt} > 0. For the second term in 25, by Assumption 1, we get E∥∇Fi(xt,ij )∥ ≤ E∥∇Fi(x t,i 0 )∥+ LE∥x t,i j − x t,i 0 ∥ ≤ E∥∇Fi(xt,i0 )∥+ LjA (26) ≤ E∥∇Fi(xt,i0 )−∇F (x t,i 0 )∥+ ∥∇F (x t,i 0 )∥+ LjA ≤ M∑ i=1 E∥∇Fi(xt,i0 )−∇F (x t,i 0 )∥+ ∥∇F (x t,i 0 )∥+ LjA ≤ √ M √√√√ M∑ i=1 E∥∇Fi(xt,i0 )−∇F (x t,i 0 )∥2 + ∥∇F (x t,i 0 )∥+ LjA (27) ≤ Mσg + ∥∇F (xt,i0 )∥+ LjA (28) ≤ Mσg + ∥∇F (x0)∥+ L∥xt−1 − x0∥+ LjA ≤ Mσg + ∥∇F (x0)∥+ LλgτAt+ LτA, (29) where 26 uses similar techniques as the proof of Theorem 1, 27 uses Cauchy inequality, 28 follows from Assumption 3. For the third term in 25, we get E∥xt,ij ∥ ≤ E∥x t,i 0 ∥+ E∥x t,i j − x t,i 0 ∥ ≤ ∥∇F (x0)∥+ LλgτAt (30) Substituting 30 and 29 into 25, we get E ∥∥∥∥gt,ij + utlt xt,ij ∥∥∥∥ ≤ (1 + u0ε )LλgτA︸ ︷︷ ︸ P̃ t+Mσg + ( 1 + u0 ε ) ∥∇F (x0)∥+ LτA︸ ︷︷ ︸ Q̃ (31) Take P̃ , Q̃ as shown in 31, and set P = P̃ /A, Q = (Q̃+A)/A. Substituting 31 into 24, we have Eβt,i ≥ τ−1∑ j=0 1 Pt+Q (1− ut)τ−j lt = 1 Pt+Q (1− ut)(1− (1− ut)τ ) ut lt. This finishes the proof. Additionally, we get the following natural corollary that eliminates ut, lt. Corollary 1. Denote ε = min{lt} > 0, then there exists a constant P,Q > 0, such that for any t ∈ [T ], we have 1 M M∑ i=1 Eβt,i ≥ 1 Pt+Q (1− u0)(1− (1− u0)τ ) u0 ε, where βt,i is defined in Lemma 2, and expectation is taken with respect to random batches given xt−1. Proof. By Lemma 3, we have 1 M M∑ i=1 Eβt,i ≥ 1 Pt+Q (1− u0)(1− (1− ut)τ ) ut ε. It remains to prove that the function f(x) := 1− (1− x)τ x decreases for x ∈ (0, u0]. To show this, we take the derivative: f ′(x) = τ(1− x)τ−1x− 1 + (1− x)τ x2 . Denote g(x) to be the numerator of f ′(x), we have g′(x) = −τ(τ − 1)(1− x)τ−2x+ τ(1− x)τ−1 − τ(1− x)τ−1 = −τ(τ