Title: Structure-Aware Language Model Pretraining Improves Dense Retrieval on Structured Data
Authors: Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yu Gu, Zhiyuan Liu
Section: A Appendix
A.1 License For all datasets in our experiments, Adv and CodeSearchNet use MIT License, while ESCI uses Apache License 2.0. All of these licenses and agreements allow their data for academic use. A.2 Construction of Pretraining Data In this subsection, we show how to process the pretraining data and construct structured-unstructured data for code/product search. During pretraining, we use inbatch negatives to optimize SANTA and all data statistics are shown in Table 5. As shown in Figure 6, we show some examples to show how to construct structured-unstructured data pairs for pretraining. For code retrieval tasks, code snippets have corresponding documentation descriptions, which describe the purpose and function of these code snippets. Thus, the code documentation and its corresponding code snippet are regarded as a positive training pair. For product retrieval tasks, structured product descriptions usually have corresponding unstructured bullet points, which provide key points about the products. We randomly select one bullet point of items and use its corresponding product description to construct a positive training pair. A.3 Additional Experimental Details of Entities Identification on Structured Data We show some examples of entity identifications on structured data in Figure 7. For codes, we follow Wang et al. (2021) and regard code identifiers as entities such as variables, function names, external libraries and methods. Specifically, we use BytesIO and tree_sitter1 to identify entities in Python and other programming languages, respectively. For product descriptions, we use the NLTK tool2 to identify nouns and proper nouns that appear in both product descriptions and titles and regard them as entities. In our experiments, we replace the same entities with the same special tokens and ask SANTA to generate these masked entities (Eq. 7). These special tokens come from the predefined vocabulary of T5, such as {<extra_id_0>, <extra_id_1>, ..., <extra_id_99> }. The proportions of identified entities in pretraining data are shown in Table 5. A.4 Additional Evaluation Results of SANTA In this experiment, we follow Li et al. (2022), keep the same evaluation settings and evaluate the retrieval effectiveness of SANTA on CodeSearch dataset. The dataset consists of code retrieval tasks on six programming languages, including Ruby, Javascript, Go, Python, Java, and PHP. We show the data statistics of CodeSearch in Table 7. Since CodeT5 and CodeRetriever donâ€™t release their data processing code for pretraining. We can only refer 1https://github.com/tree-sitter/tree-sitter 2https://www.nltk.org/ to the tutorial3 to process data. When we evaluate SANTA on CodeSearch, the instances in testing and development sets are filtered out from CodeSearchNet dataset for pretraining. Some codes that can not be