Title: Why do Nearest Neighbor Language Models Work?
Authors: Frank F. Xu, Uri Alon, Graham Neubig
Section: 4.2 Increasing the Softmax Capacity
interpolated perplexity with approximate search is better than that with exact search, both with respect to the mask and the score calculation. Intrigued by this counter-intuitive result, we explore the effect of kNN search approximation. First, we plot the subsampled size of the datastore with the interpolated perplexity Figure 4, a similar plot to Figure 2, but showcasing the comparison between approximate and real masks, between approximate and real scores in both the full datastore as well as a small subsampled datastore setting. We find that using an approximate FAISS mask to find nearest neighbors is better than using the ground truth nearest neighbors and that using the approximate score returned by FAISS is better than recomputing the ground truth distances 3To calculate the real mask over a large datastore, we shard the datastore into several smaller datastores, calculate the nearest neighbors for each of the smaller datastores, and combine them back together to get the final result. between embeddings for the kNN distribution at different levels of datastore size, both at 5% or 100%. Interestingly, the gap between using an approximate score or real score given the same approximate nearest neighbors (“FAISS mask, FAISS score” vs. “FAISS mask, real score”) is larger than that between using approximate or real nearest neighbors given the same ground truth method of calculating the distance (“real mask, real score” vs. “FAISS mask, real score”), for reasons we will elucidate in the next section. 5.2 Adding Softmax Temperature to kNN Distribution Because the number of retrieved nearest neighbors, k is usually much smaller than the vocabulary size V , intuitively, the kNN distribution PkNN used for interpolation tends to be more peaky than the standard LM output distribution. When k = 1024 and V = 33000, as in our experiments, PkNN will only have a few vocabulary items with a non-zero probability. Furthermore, many of the retrieved neighbors share the same target token and thus make the kNN distribution even peakier. One way to control the entropy, or peakiness of the distribution is to add temperature to the logits that go into the softmax function (Holtzman et al., 2019). We calculate the probability of non-parametric component PkNN with the following equation where t is the softmax temperature: PkNN =Msoftmax(mask-to-k(Wds ⊗ hds)/t) (6) In general, the higher the temperature, the less “peaky” the distribution would become. We experiment with both the 5% as well as the full datastore using different temperatures ranging from 0 to 3 at 0.1 intervals. The results are shown in Figure