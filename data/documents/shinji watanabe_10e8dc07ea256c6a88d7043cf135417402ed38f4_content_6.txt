Title: Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization
Authors: Puyuan Peng, Brian Yan, Shinji Watanabe, David Harwath
Section: 2. The Whisper model
We note that the comparison in this table 5Whisper Large for En→De; LargeV2 for En→Ru and En→Fr. should only be treated as a reference. This is because even for the unsupervised and zero-shot approaches, they are particularly designed for ST, and they either leverage machine translation systems [30, 33] or multilingual sentence embedding models [34]. For Whisper, however, we simply adjust its prompt, and the goal is to probe the multilingual understanding of the model. Remarks. Although Whisper is trained with massive multilingual data, performing En→X might be harder than one expects. Because for the <|st|> task token, the model is never trained to generate non-English text; for the <|asr|> task token the model is never trained to generate text belonging to a different language than the input speech. The fact that Whisper is able to do En→X ST with a simple modification on its prompt reveals that semantically related words and phrases from different languages might be close in the model’s latent space. We also expect that we could fine-tune Whisper to boost the performance of ST on new language pairs.