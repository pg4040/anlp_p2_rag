Title: FACTORIZED CONTRASTIVE LEARNING: Going Beyond Multi-view Redundancy
Authors: Paul Pu Liang, Zihao Deng, Martin Q. Ma, James Zou, Louis-Philippe Morency, Ruslan Salakhutdinov
Section: D.3 Additional analysis and results
in Equation 13 and Equation 14, respectively. The results indicate that our Conditional InfoNCE gives estimations smaller than the true CMI, and Conditional InfoNCE-CLUB gives estimations greater than the true CMI. The performances are comparable to estimators in [54], suggesting that our method yields valid and competitive lower and upper bounds for CMI. Empirical verification on InfoMin assumption: To verify the InfoMin assumption [71] (I(Z1;Y ∣X2) = I(Z2;Y ∣X1) = 0), we use the same synthetic dataset as in Table 1 and measure I(Z1;Y ∣X2). The results are shown in Table 11: we get I(X1;X2) = 12.29 and I(Z1;Y ∣X2) = 0.4. I(Z1;Y ∣X2) is much smaller and closer to zero than I(Z1;Y ∣X2), indicating that the InfoMin assumption holds in practice. Compute resources: All experiments in this paper are run on a single NVIDIA A100 GPU. It takes about 10 to 12 GPU hours to train the model on the CIFAR10 [42] for 300 epochs, and all the other experiments can be finished within 1 GPU hour using our specified hyperparameters.