Title: A Pretrainerâ€™s Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity
Authors: Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, Daphne Ippolito
Section: 9 Related Work
et al., 2022). Another line of work demonstrates the potential of pretraining on carefully crafted synthetic data (Wu et al., 2022). Most similar to this section of our work, Xie et al. (2023a) re-balance mixtures of the Pile to achieve more performant and efficient convergence. Xie et al. (2023b) use importance sampling to select subsets of the Pile most useful for target downstream tasks, in lieu of quality filters, to achieve 2% improvement on downstream tasks. Pruksachatkun et al. (2020) systematically benchmark the effects of intermediate finetuning tasks, similar to how we benchmark different compositions of pretraining tasks. Model & Data Scaling Prior work has explored scaling model size (Kaplan et al., 2020; Tay et al., 2022; Du et al., 2022), the amount of pretraining data or the number of pretraining steps (Liu et al., 2019; Chowdhery et al., 2022; Brown et al., 2020). Chinchilla investigated and reported optimal compute scaling laws, expressing a relationship between model and data size (Nostalgebraist, 2022). Recent work has demonstrated that new abilities emerge at greater scale (Wei et al., 2022), but also that many of these benefits can be distilled or compressed into smaller models (Taori et al., 2023; Movva et al., 2022). In this work, we investigate how temporal pretraining misalignment varies on different model sizes, which to our knowledge was previously unanswered.