Title: CROSS-MODAL MULTI-TASKING FOR SPEECH-TO-TEXT TRANSLATION VIA HARD PARAMETER SHARING
Authors: Brian Yan, Xuankai Chang, Antonios Anastasopoulos, Yuya Fujita, Shinji Watanabe
Section: 7. REFERENCES
Proc. WMT, 2016. [25] Y. Liu et al., “Multilingual denoising pre-training for neural machine translation,” TACL, 2020. [26] Y. Tang et al., “Multilingual translation with extensible multilingual pretraining and finetuning,” arXiv preprint arXiv:2008.00401, 2020. [27] A. Van Den Oord, O. Vinyals, et al., “Neural discrete representation learning,” Proc. Neurips, 2017. [28] N. Zeghidour et al., “Soundstream: An end-to-end neural audio codec,” TASLP, 2021. [29] A. Défossez et al., “High fidelity neural audio compression,” arXiv preprint arXiv:2210.13438, 2022. [30] W.-N. Hsu et al., “Hubert: Self-supervised speech representation learning by masked prediction of hidden units,” TASLP, 2021. [31] A. Pasad, B. Shi, and K. Livescu, “Comparative layer-wise analysis of self-supervised speech models,” in Proc. ICASSP, 2023. [32] T. Kudo and J. Richardson, “Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,” in Proc. EMNLP, 2018. [33] I. Provilkov, D. Emelianenko, and E. Voita, “Bpe-dropout: Simple and effective subword regularization,” in Proc. ACL, 2020. [34] R. Prabhavalkar et al., “End-to-end speech recognition: A survey,” arXiv preprint arXiv:2303.03329, 2023. [35] F. Stahlberg, “Neural machine translation: A review,” Journal of Artificial Intelligence Research, 2020. [36] D. S. Park et al., “Specaugment: Simple data augmentation for automatic speech recognition,” Proc. Interspeech, 2019. [37] A. Renduchintala et al., “Multi-modal data augmentation for end-toend asr,” Proc. Interspeech, 2018. [38] S. Thomas et al., “Integrating text inputs for training and adapting rnn transducer asr models,” in Proc. ICASSP, 2022. [39] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly learning to align and translate,” in Proc. ICLR, 2015. [40] W. Chan et al., “Listen, attend and spell: A neural network for large vocabulary conversational speech recognition,” in ICASSP, 2016. [41] A. Graves et al., “Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks,” in Proc. ICML, 2006. [42] A. Graves, “Sequence Transduction with Recurrent Neural Networks,” in Proc. ICML, 2012. [43] S. Watanabe et al., “Hybrid ctc/attention architecture for end-to-end speech recognition,” JSTSP, 2017. [44] B. Yan et al., “Ctc alignments improve autoregressive translation,” in Proc. EACL, 2023. [45] H. Inaguma et al., “Espnet-st: All-in-one speech translation toolkit,” in Proc. ACL, 2020. [46] M. Gaido et al., “Ctc-based compression for direct speech translation,” in Proc. EACL, 2021. [47] B. Zhang, B. Haddow, and R. Sennrich, “Revisiting end-to-end speech-to-text translation from scratch,” in Proc. ICML, 2022. [48] B. Yan et al., “ESPnet-ST-v2: Multipurpose spoken language translation toolkit,” in Proc. ACL, 2023. [49] K. Kim et al., “E-branchformer: Branchformer with enhanced merging for speech recognition,” in Proc. SLT, 2023. [50] X.