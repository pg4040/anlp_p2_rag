Title: The Troubling Emergence of Hallucination in Large Language Models – An Extensive Definition, Quantification, and Prescriptive Remediations
Authors: Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S.M Towhidul Islam Tonmoy, Aman Chadha, Amit Sheth, Amitava Das
Section: 
to examine the effect of size on HVI, it looks like there are several other factors contributing to HVI behavior as evident in Fig. 6. 6 Hallucination Mitigation Strategies Thus far, two classes of approaches have been proposed to address the issue of hallucination: (i) preventing LLMs from hallucinating, which involves implementing strategies during the training and/or generation processes; (ii) mitigating hallucination after generation. (Manakul et al., 2023) introduced another taxonomy of classification, categorizing methods into black-box and gray-box. Factuality checks during and/or after generation without relying on external resources are known as black-box methods, while those using external resources are referred to as gray-box methods. Other hallucination mitigation techniques involve reranking the generated sample responses (Dale et al., 2022) and improving beam search (Sridhar and Visser, 2022). Some recent mitigation techniques (Li et al., 2023; Mündler et al., 2023; Pfeiffer et al., 2023; Chen et al., 2023; Zhang et al., 2023b,a; Ladhak et al., 2023a; Manakul et al., 2023; Agrawal et al., 2023) show initial attempts at reducing hallucination. Although the complete elimination of hallucination is a complex challenge, this paper explores two plausible directions for mitigation: (i) automatic and (ii) human-in-the-loop. The former is a black-box method where we identify high-entropy words in a given hallucinated text (generated by a high-HVI LLM) and replace them with predictions from another LLM (lower-HVI). The latter is a gray-box method that involves sentence-level factchecking using textual entailment techniques. This method aims to identify sentences that are deemed susceptible, urging them for human review. 6.1 High Entropy Word Spotting and Replacement (ENTROPYBB): A Black-box approach While detecting high entropy words may seem to be technically feasible, there is an inherent challenge that many modern LLMs are not open-source (their APIs are subscription-based). The feasible solution we propose here is the utilization of opensource LLMs to identify high entropy words. A lower HVI-based LLM is then used to replace the detected words (see Fig. 4). The outcomes of the detection and replacement strategies discussed earlier are presented in Table 2 for GPT-3. The results indicate that albert-large-v2 (Lan et al., 2020) performs exceptionally well in detecting high entropy words in GPT-3-generated content. On the other hand, distilroberta-base (Sanh et al., 2019) demonstrates superior performance in replacing high entropy words, which in turn, manifests as a lower hallucination. A crucial aspect of our approach is treating consecutive high-entropy words as a single unit. In such cases, these words are masked together before replacement. This strategy proves to be effective, particularly for hallucinations related to