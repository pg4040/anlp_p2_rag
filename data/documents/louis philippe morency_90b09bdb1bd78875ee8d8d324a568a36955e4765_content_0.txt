Title: Multimodal Fusion Interactions: A Study of Human and Automatic sQuantification
Authors: Paul Pu Liang, Yun Cheng, Ruslan Salakhutdinov, Louis-Philippe Morency
Section: A HUMAN ANNOTATION DETAILS
Participation in all annotations was fully voluntary and we obtained consent from all participants prior to annotations. The authors manually took anonymous notes on all results and feedback in such a manner that the identities of annotators cannot readily be ascertained directly or through identifers linked to the subjects. Participants were not the authors nor in the same research groups as the authors, but they all hold or are working towards a graduate degree in a STEM feld and have knowledge of machine learning. None of the participants knew about this project before their session and each participant only interacted with the setting they were involved in. We sample 50 datapoints from each of the 5 datasets in Table 1 and give them to a total of 18 diferent annotators: • 3 annotators for direct annotation of interactions, • 3 annotators for partial labeling of �1, �2, and �12, • 3 annotators for counterfactual, labeling �1 frst then �1+2, • 3 annotators for counterfactual, labeling �2 frst then �2+1. All annotations were performed via google spreadsheets. A.1 Annotating partial labels We asked 3 annotators to predict the partial labels in a randomized setting. For each annotator, we asked them to annotate �1 then �2 given only modality 1 or 2 respectively, and fnally � given both modalities. This completion order is designed on purpose to minimize possible memorization of the data so that the annotators can provide completely independent unimodal and multimodal predictions on the label. When annotating the visual modality of the video datasets, we explicitly require the annotators to mute the audio and predict the partial labels based only on the video frames. After that, all annotators are asked to provide a confdence score on a scale of 0 (no confdence) to 5 (high confdence) about their annotations. The confdence scale is applied to all annotation settings below. We aggregated annotator �’s �1 response, annotator �’s �2 response, and annotator �’s � response as one set of complete partial labels. Similarly, we collected �’s �1, �’s �2, and �’s � as the second set, �’s �1, �’s �2, and �’s � as the third set. Multimodal Fusion Interactions: A Study of Human and Automatic Qantification A.2 Annotating counterfactual labels We asked 6 annotators to predict the counterfactual labels in this setting. For each group of 2 annotators, we asked the frst annotator to annotate partial labels �1 given only the frst modality and provide confdence scores, then presented them with the other modality and asked for their new