Title: Overview of Robust and Multilingual Automatic Evaluation Metrics for Open-Domain Dialogue Systems at DSTC 11 Track 4
Authors: Mario Rodríguez-Cantelar, Chen Zhang, Chengguang Tang, Ke Shi, Sarik Ghazarian, João Sedoc, Luis Fernando D’Haro, Alexander Rudnicky
Section: B Appendix: Existing Benchmark Datasets
those containing inappropriate responses were filtered out. In the end, we collected 531 human-chatbot multi-turn conversations with 207 from Plato-XL, 224 from XiaoIce, and 100 dialogues from the Chinese DialoGPT. TBD-Q1-2023 Three Bot Dialog Evaluation Corpus (TBD-Q1-2023 OR TBD; Quarter 1 of 2023) from Giorgi et al. (2023) consists of dialogues with three chatbots: ChatGPT (Radford et al., 2018), GPT-3 (Brown et al., 2020), and BlenderBot3 (Shuster et al., 2022). Student participants were told to have a long conversation with the chatbots on a range of topics of their choosing. All conversations were collected between November 2022 and March 2023. They collected 21 dialogues with an average of 14.6 turns per dialogue. Conversations for BlenderBot3 were directly from the website https://blenderbot.ai/. The ChatGPT conversations were taken directly from the website https://chat.openai.com/. Finally, GPT3 used the text-davinci-003 model with the prompt Hal is a chatbot that attempts to answer questions with useful responses:. The GPT3 model parameters were temperature of 0.5, max tokens of 289, top-p of 0.3, frequency penalty of 0.5, and presence penalty of 0.