Title: Deep Speech Synthesis from MRI-Based Articulatory Representations
Authors: Peter Wu, Tingle Li, Yijing Lu, Yubin Zhang, Jiachen Lian, Alan W Black, Louis Goldstein, Shinji Watanabe, Gopala K. Anumanchipalli
Section: 9. References
P. Wu, and G. K. Anumanchipalli, “Articulation gan: Unsupervised modeling of articulatory learning,” ICASSP, 2023. [16] T. Toda, A. Black, and K. Tokuda, “Acoustic-to-articulatory inversion mapping with gaussian mixture model,” in ICSLP, 2004. [17] P. Wu, L.-W. Chen, C. J. Cho, S. Watanabe, L. Goldstein, A. W. Black, and G. K. Anumanchipalli, “Speaker-independent acoustic-to-articulatory speech inversion,” in ICASSP, 2023. [18] C. J. Cho, P. Wu, A. Mohamed, and G. K. Anumanchipalli, “Evidence of vocal tract articulation in self-supervised learning of speech,” ICASSP, 2023. [19] Y. Lim, A. Toutios, Y. Bliesener, Y. Tian, S. G. Lingala, C. Vaz, T. Sorensen, M. Oh, S. Harper, W. Chen et al., “A multispeaker dataset of raw and reconstructed speech production real-time mri video and 3d volumetric images,” Scientific data, vol. 8, no. 1, pp. 1–14, 2021. [20] J. Lian, A. W. Black, L. Goldstein, and G. K. Anumanchipalli, “Deep neural convolutive matrix factorization for articulatory representation decomposition,” in Interspeech, 2022. [21] J. Lian, A. W. Black, Y. Lu, L. Goldstein, S. Watanabe, and G. K. Anumanchipalli, “Articulatory representation learning via joint factor analysis and neural matrix factorization,” in ICASSP, 2023. [22] S. G. Lingala, B. P. Sutton, M. E. Miquel, and K. S. Nayak, “Recommendations for real-time speech mri,” Journal of Magnetic Resonance Imaging, vol. 43, no. 1, pp. 28–44, 2016. [23] S. G. Lingala, Y. Zhu, Y.-C. Kim, A. Toutios, S. Narayanan, and K. S. Nayak, “A fast and flexible mri system for the study of dynamic vocal tract shaping,” Magnetic resonance in medicine, vol. 77, no. 1, pp. 112–125, 2017. [24] E. Bresch and S. Narayanan, “Region segmentation in the frequency domain applied to upper airway real-time magnetic resonance images,” IEEE transactions on medical imaging, vol. 28, no. 3, pp. 323–338, 2008. [25] M. McAuliffe, M. Socolof, S. Mihuc, M. Wagner, and M. Sonderegger, “Montreal forced aligner: Trainable text-speech alignment using kaldi,” in Interspeech, 2017. [26] T. G. Csapó, C. Zainkó, L. Tóth, G. Gosztolya, and A. Markó, “Ultrasound-based articulatory-to-acoustic mapping with waveglow speech synthesis,” in Interspeech, 2020. [27] M.-A. Georges, P. Badin, J. Diard, L. Girin, J.-L. Schwartz, and T. Hueber, “Towards an articulatory-driven neural vocoder for speech synthesis,” in International Seminar on Speech Production, 2020. [28] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, “Hubert: Self-supervised speech representation learning by masked prediction of hidden units,” IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451–3460, 2021. [29] R. Prenger, R. Valle, and B. Catanzaro, “Waveglow: A flow-based generative network for speech synthesis,”