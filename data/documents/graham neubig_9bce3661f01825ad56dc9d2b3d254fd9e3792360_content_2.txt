Title: Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach
Authors: Masahiro Kaneko, Graham Neubig, Naoaki Okazaki
Section: 4 Evaluation Method
and decide on the final label. We compare the performance of each label before and after the dis- cussion. Here, the data for the acceptance and objection settings are half and half. Therefore, if the discussion is not properly conducted, such as by accepting all human labels or refuting all human labels, the performance will not improve. We also investigate the performance of the NLI when using argumentation prompts. We compared the performance of NLI in zero-shot, few-shot, and few-shot-discussion systems. The predicted label after “Label:” in the prompt of Figure 2 is considered as the prediction, and discussion between humans and systems is not performed. In the evaluation of NLI performance, in addition to SNLI data, we also use Adversarial NLI (ANLI) data (Nie et al., 2020). ANLI creates data by repeatedly performing adversarial annotation against NLI systems; thus, the resulting NLI examples are particularly difficult for the system to solve. There are three data sets R1, R2, and R3 with differences in the number of iterations, and the evaluation is performed using each evaluation data point.