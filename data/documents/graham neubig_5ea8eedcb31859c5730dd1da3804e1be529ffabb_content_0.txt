Title: A Gold Standard Dataset for the Reviewer Assignment Problem
Authors: Ivan Stelmakh, John Wieting, Graham Neubig, Nihar B. Shah
Section: B More details on the ACL algorithm
In this section, we provide more details on the ACL algorithm that we evaluate in this paper. Training The model is trained on a large corpus of 45,309 abstracts from the ACL anthology and is inspired by the work of Wieting et al. (2019, 2022). Specifically, the model optimizes a max-margin contrastive learning objective which is defined as follows. First, we split each abstract ai from the corpus into two disjoint segments of text uniformly at random. These segments are then uniformly at random allocated into two equally-sized groups: a (1) i ∈ A1 and a (2) i ∈ A2. Second, we construct positive and negative examples: • Positive example: For each abstract ai, pair (a (1) i , a (2) i ) constitutes a positive example. • Negative example: For each passage a(1)i ∈ A1, a counterpart ti 6= a (2) i from A2 is selected to maximize the notion of cosine similarity fθ(a (1) i , ti) = cos ( g(a (1) i , θ), g(ti, θ) ) , where g is the sentence encoder with parameters θ. Pair (a (1) i , ti) constitutes a negative example. Finally, with this procedure to build positive and negative examples, we can define the objective of the ACL algorithm: min θ ∑ i [ δ−fθ(a(1)i , a (2) i ) + fθ(a (1) i , ti)) ] + 6 Inner-working of the algorithm relies on sentencepiece embeddings7 (Kudo and Richardson, 2018) with dimension of 1,024 and vocabulary size of 20,000. The encoder, g, simply mean pools the learned sentencepiece embeddings, making for efficient encoding, even on CPU.8 In the training procedure, a batch size of 64 is used and the model is trained for 20 epochs. The margin, δ, is set to 0.4. 6We use [·]+ to denote function h : h(x) = max(x, 0). 7https://github.com/google/sentencepiece 8See Wieting et al. (2019, 2022) for more details on encoding speed. Inference At the inference stage, for a given pair of a submission and a reviewer, we define the similarity score as a combination of cosine similarities between the submission’s abstract and three most-similar abstracts from the reviewer’s profile. Specifically, let s1, s2 and s3 be the top-3 cosine similarities between the submission’s abstract and abstracts from the reviewer’s profile. The similarity score between the submission and the reviewer is then defined as follows: s = s1 + s2 2 + s3 3 . If a reviewer has less than three abstracts in the profile, we set the corresponding cosine similarity scores si