Title: SEMI-AUTOREGRESSIVE STREAMING ASR WITH LABEL CONTEXT
Authors: Siddhant Arora, George Saon, Shinji Watanabe, Brian Kingsbury
Section: 4.4. Results and Discussion
synchronous beam search,” in Proc. SLT, 2021, pp. 22–29. [30] A. Tripathi et al., “Transformer transducer: One model unifying streaming and non-streaming speech recognition,” CoRR, vol. abs/2010.03192, 2020. [31] M. Jain et al., “RNN-T for latency controlled ASR with improved beam search,” CoRR, vol. abs/1911.01629, 2019. [32] E. Battenberg et al., “Exploring neural transducers for end-to-end speech recognition,” in Proc. ASRU, 2017, pp. 206–213. [33] W. Wang, K. Hu, and T. N. Sainath, “Deliberation of streaming rnntransducer by non-autoregressive decoding,” in Proc. ICASSP, 2022, pp. 7452–7456. [34] T. Wang et al., “Streaming end-to-end ASR based on blockwise nonautoregressive models,” in Proc. Interspeech, 2021, pp. 3755–3759. [35] C. Wang, J. Zhang, and H. Chen, “Semi-autoregressive neural machine translation,” in Proc. EMNLP, 2018, pp. 479–488. [36] Y. Zhou et al., “Semi-autoregressive transformer for image captioning,” in ICCVW 2021, 2021, pp. 3132–3136. [37] L. Kürzinger et al., “Ctc-segmentation of large corpora for german end-to-end speech recognition,” in SPECOM, ser. Lecture Notes in Computer Science, vol. 12335, 2020, pp. 267–278. [38] K. Audhkhasi et al., “Forget a bit to learn better: Soft forgetting for ctc-based automatic speech recognition,” in Proc. Interspeech, 2019, pp. 2618–2622. [39] S. Horiguchi et al., “Online neural diarization of unlimited numbers of speakers using global and local attractors,” TASLP, vol. 31, pp. 706– 720, 2023. [40] Z. Yao et al., “Wenet: Production oriented streaming and nonstreaming end-to-end speech recognition toolkit,” in Proc. Interspeech, H. Hermansky et al., Eds., 2021, pp. 4054–4058. [41] Y. Sudo et al., “Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training,” in Proc. INTERSPEECH 2023, 2023, pp. 4479–4483. [42] A. Rousseau, P. Deléglise, and Y. Estève, “Enhancing the TED-LIUM corpus with selected data for language modeling and more TED talks,” in LREC, 2014, pp. 3935–3939. [43] V. Panayotov et al., “Librispeech: An ASR corpus based on public domain audio books,” in Proc. ICASSP, 2015, pp. 5206–5210. [44] J. J. Godfrey, E. Holliman, and J. McDaniel, “SWITCHBOARD: telephone speech corpus for research and development,” in Proc. ICASSP, 1992, pp. 517–520. [45] J. Lee and S. Watanabe, “Intermediate loss regularization for CTCbased speech recognition,” in Proc. ICASSP, 2021, pp. 6224–6228. [46] S. Arora et al., “Token-level sequence labeling for spoken language understanding using compositional end-to-end models,” in Findings of the EMNLP, 2022, pp. 5419–5429. [47] J. Helcl, J. Libovický, and D. Varis, “CUNI system for the WMT18 multimodal translation task,” in WMT, 2018, pp. 616–623. [48] H. Touvron et al., “Llama 2: Open foundation and fine-tuned chat models,” CoRR, vol. abs/2307.09288, 2023. [49]