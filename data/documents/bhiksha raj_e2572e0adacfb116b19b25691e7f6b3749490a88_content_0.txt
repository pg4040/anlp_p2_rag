Title: TRAINING AUDIO CAPTIONING MODELS WITHOUT AUDIO
Authors: Soham Deshmukh, Benjamin Elizalde, Dimitra Emmanouilidou, Bhiksha Raj, Rita Singh, Huaming Wang
Section: 7. REFERENCES
[1] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning with neural networks,” Advances in neural information processing systems, vol. 27, 2014. [2] Q. Kong, Y. Cao, T. Iqbal, Y. Wang, et al., “Panns: Large-scale pretrained audio neural networks for audio pattern recognition,” IEEE/ACM Trans. Audio, Speech and Lang. Proc., 2020. [3] Y. Gong, C.-I. J. Lai, Y.-A. Chung, and J. Glass, “Ssast: Self-supervised audio spectrogram transformer,” arXiv preprint arXiv:2110.09784, 2021. [4] K. Chen, X. Du, B. Zhu, Z. Ma, T. Berg-Kirkpatrick, and S. Dubnov, “Hts-at: A hierarchical token-semantic audio transformer for sound classification and detection,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022. [5] M. Lewis, Y. Liu, N. Goyal, and et. al., “BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension,” in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. ACL, July 2020. [6] Y. Koizumi, Y. Ohishi, D. Niizumi, et al., “Audio captioning using pre-trained large-scale language model guided by audio-based similar caption retrieval,” arXiv preprint arXiv:2012.07331, 2020. [7] M. Kim, K. Sung-Bin, and T.-H. Oh, “Prefix tuning for automated audio captioning,” in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2023. [8] S. Deshmukh, B. Elizalde, R. Singh, and H. Wang, “Pengi: An audio language model for audio tasks,” arXiv preprint arXiv:2305.11834, 2023. [9] I. Martin Morato and A. Mesaros, “Diversity and bias in audio captioning datasets,” 2021. [10] S. Kothinti and D. Emmanouilidou, “Investigations in audio captioning: Addressing vocabulary imbalance and evaluating suitability of language-centric performance metrics,” arXiv preprint arXiv:2211.06547, 2023. [11] I. Martı́n-Morató and A. Mesaros, “What is the ground truth? reliability of multi-annotator data for audio tagging,” in 2021 29th European Signal Processing Conference (EUSIPCO), 2021. [12] A. O. Eren and M. Sert, “Audio captioning using sound event detection,” DCASE2021 Challenge, Tech. Rep., Jun. 2021. [13] Z. Ye, H. Wang, D. Yang, and Y. Zou, “Improving the performance of automated audio captioning via integrating the acoustic and semantic information,” in Workshop on Detection and Classification of Acoustic Scenes and Events, 2021. [14] L. M. Heller, B. Elizalde, B. Raj, and S. Deshmuk, “Synergy between human and machine approaches to sound/scene recognition and processing: An overview of icassp special session,” arXiv preprint arXiv:2302.09719, 2023. [15] X. Liu, Q. Huang, X. Mei, T. Ko, H. L. Tang, M. D. Plumbley, and W. Wang, “Cl4ac: A contrastive loss for audio captioning,” Proceedings of the Detection and Classification of Acoustic Scenes and Events 2021 Workshop (DCASE 2021), 2021. [16] B. M.