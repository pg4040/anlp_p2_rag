Title: DECODER-ONLY ARCHITECTURE FOR SPEECH RECOGNITION WITH CTC PROMPTS AND TEXT DATA AUGMENTATION
Authors: Emiru Tsunoo, Hayato Futami, Yosuke Kashiwagi, Siddhant Arora, Shinji Watanabe
Section: 5. REFERENCES
[1] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al., “Deep speech: Scaling up endto-end speech recognition,” arXiv preprint arXiv:1412.5567, 2014. [2] Jan K. Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk, Kyunghyun Cho, and Yoshua Bengio, “Attention-based models for speech recognition,” in Proc. of NIPS, 2015, pp. 577– 585. [3] Zhong Meng, Sarangarajan Parthasarathy, Eric Sun, Yashesh Gaur, Naoyuki Kanda, Liang Lu, Xie Chen, Rui Zhao, Jinyu Li, and Yifan Gong, “Internal language model estimation for domain-adaptive end-to-end speech recognition,” in 2021 IEEE Spoken Language Technology Workshop (SLT), 2021, pp. 243–250. [4] Albert Zeyer, André Merboldt, Wilfried Michel, Ralf Schlüter, and Hermann Ney, “Librispeech transducer model with internal language model prior correction,” in Proc. of Interspeech, 2021, pp. 2052–2056. [5] Emiru Tsunoo, Yosuke Kashiwagi, Chaitanya Prasad Narisetty, and Shinji Watanabe, “Residual language model for end-toend speech recognition,” in Proc. of Interspeech 2022, 2022, pp. 3899–3903. [6] Adithya Renduchintala, Shuoyang Ding, Matthew Wiesner, and Shinji Watanabe, “Multi-modal data augmentation for endto-end asr,” Proc. of Interspeech 2018, pp. 2394–2398, 2018. [7] Haihua Xu, Yerbolat Khassanov, Zhiping Zeng, Eng Siong Chng, Chongjia Ni, Bin Ma, Haizhou Li, et al., “Independent language modeling architecture for end-to-end ASR,” in Proc. of ICASSP, 2020, pp. 7059–7063. [8] Peidong Wang, Tara N. Sainath, and Ron J. Weiss, “Multitask training with text data for end-to-end speech recognition,” in Proc. of Interspeech, 2021, pp. 2566–2570. [9] Zhehuai Chen, Yu Zhang, Andrew Rosenberg, Bhuvana Ram- abhadran, Pedro J. Moreno, Ankur Bapna, and Heiga Zen, “MAESTRO: Matched speech text representations through modality matching,” in Proc. of Interspeech, 2022, pp. 4093– 4097. [10] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Sub- biah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al., “Language models are few-shot learners,” Proc. of NurIPS, vol. 33, pp. 1877–1901, 2020. [11] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al., “PaLM: Scaling language modeling with pathways,” arXiv preprint arXiv:2204.02311, 2022. [12] Kai-Wei Chang, Yu-Kai Wang, Hua Shen, Iu-thing Kang, Wei- Cheng Tseng, Shang-Wen Li, and Hung-yi Lee, “Speechprompt v2: Prompt tuning for speech classification tasks,” arXiv preprint arXiv:2303.00733, 2023. [13] Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu, “SpeechGPT: Empowering large language models with intrinsic cross-modal conversational abilities,” arXiv preprint arXiv:2305.11000, 2023. [14] Paul K Rubenstein, Chulayuth Asawaroengchai, Duc Dung Nguyen, Ankur Bapna, Zalán Borsos, Félix de Chaumont Quitry, Peter