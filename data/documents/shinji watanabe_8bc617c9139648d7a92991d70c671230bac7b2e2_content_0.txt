Title: AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head
Authors: Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jiawei Huang, Jinglin Liu, Yi Ren, Zhou Zhao, Shinji Watanabe
Section: 7 Conclusion
In this work, we presented AudioGPT, which connected ChatGPT with 1) audio foundation models to handle challenging audio tasks, and 2) a modality transformation interface to enable spoken dialogue. By combining the advantages of ChatGPT and audio-modality solvers, AudioGPT presented strong capacities in processing audio information in the following four stages: modality transformation, task analysis, model assignment, and response generation. To assess the ability of multi-modal LLMs in human intention understanding and cooperation with foundation models, we outlined the design principles and processes, and evaluated AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrated the outperformed abilities of AudioGPT in solving AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, empowering humans to create rich and diverse audio content with unprecedented ease. The current manuscript mainly covers the system description, where the experiments are designed more for demonstration.