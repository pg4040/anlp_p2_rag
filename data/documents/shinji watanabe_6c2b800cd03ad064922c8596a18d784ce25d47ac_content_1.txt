Title: Integration of Frame- and Label-synchronous Beam Search for Streaming Encoder–decoder Speech Recognition
Authors: Emiru Tsunoo, Hayato Futami, Yosuke Kashiwagi, Siddhant Arora, Shinji Watanabe
Section: 6. References
Nguyen, T. N. Sainath, Z. Chen, and R. Prabhavalkar, “An analysis of incorporating an external language model into a sequence-to-sequence model,” in Proc. ICASSP, 2018, pp. 1–5828. [17] T. N. Sainath, R. Pang, D. Rybach, Y. He, R. Prabhavalkar, W. Li, M. Visontai, Q. Liang, T. Strohman, Y. Wu, et al., “Twopass end-to-end speech recognition,” in Proc. Interspeech, 2019, pp. 2773–2777. [18] W. Zhou, S. Berger, R. Schlüter, and H. Ney, “Phoneme based neural transducer for large vocabulary speech recognition,” in Proc. ICASSP, 2021, pp. 5644–5648. [19] J. Lafferty, A. McCallum, and F. C. Pereira, “Conditional random fields: Probabilistic models for segmenting and labeling sequence data,” in Proc. ICML, 2001, pp. 282–289. [20] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer, “Scheduled sampling for sequence prediction with recurrent neural networks,” in Proc. NeurIPS, vol. 28, 2015. [21] K. Murray and D. Chiang, “Correcting length bias in neural machine translation,” arXiv preprint arXiv:1808.10006, 2018. [22] E. Tsunoo, Y. Kashiwagi, T. Kumakura, and S. Watanabe, “Transformer ASR with contextual block processing,” in Proc. of ASRU Workshop, 2019, pp. 427–433. [23] N. Moritz, T. Hori, and J. Le Roux, “Triggered attention for endto-end speech recognition,” in Proc. ICASSP, 2019, pp. 5666– 5670. [24] M. Li, C. Zorilă, and R. Doddipatla, “Head-synchronous decoding for transformer-based streaming ASR,” in Proc. ICASSP, 2021, pp. 5909–5913. [25] E. Tsunoo, C. Narisetty, M. Hentschel, Y. Kashiwagi, and S. Watanabe, “Run-and-back stitch search: Novel block synchronous decoding for streaming encoder-decoder ASR,” in Proc. ICASSP, 2022, pp. 8287–8291. [26] Q. Li, C. Zhang, and P. C. Woodland, “Combining framesynchronous and label-synchronous systems for speech recognition,” arXiv preprint arXiv:2107.00764, 2021. [27] B. Yan, S. Dalmia, Y. Higuchi, G. Neubig, F. Metze, A. W. Black, and S. Watanabe, “CTC alignments improve autoregressive translation,” arXiv preprint arXiv:2210.05200, 2022. [28] L. Dong, C. Yi, J. Wang, S. Zhou, S. Xu, X. Jia, and B. Xu, “A comparison of label-synchronous and frame-synchronous end-to-end models for speech recognition,” arXiv preprint arXiv:2005.10113, 2020. [29] W. Zhou, A. Zeyer, A. Merboldt, R. Schlüter, and H. Ney, “Equivalence of segmental and neural transducer modeling: A proof of concept,” in Proc. Interspeech, 2021, pp. 2891–2895. [30] E. Tsunoo, Y. Kashiwagi, and S. Watanabe, “Streaming transformer ASR with blockwise synchronous beam search,” in Proc. SLT, 2021, pp. 22–29. [31] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen, R. Prenger, S. Satheesh, S. Sengupta, A. Coates, et al., “Deep speech: Scaling up end-to-end speech recognition,” arXiv preprint arXiv:1412.5567, 2014. [32] K. Hwang and W. Sung, “Character-level incremental