Title: The Multimodal Information Based Speech Processing (Misp) 2022 Challenge: Audio-Visual Diarization And Recognition
Authors: Zhe; Wu, Shilong ; Chen, Hang ; He, Mao-Kui ; Du, Jun ; Lee, Chin-Hui ; Chen, Jingdong ; Watanabe, Shinji ; Siniscalchi, Sabato Marco, Zhe Wang, Shilong Wu, Hang Chen, Mao-Kui He, Jun Du, Chin-Hui Lee, Jingdong Chen, Shinji Watanabe, Sabato Siniscalchi, Odette Scharenborg, Diyuan Liu, Baocai Yin, Jia Pan, Jianqing Gao, Cong Liu
Section: 7. REFERENCES
[1] Lawrence D Rosenblum, “Speech perception as a multimodal phenomenon,” Current directions in psychological science, vol. 17, no. 6, pp. 405–409, 2008. [2] Triantafyllos Afouras, Joon Son Chung, Andrew Senior, et al., “Deep audio-visual speech recognition,” IEEE transactions on pattern analysis and machine intelligence, 2018. [3] Joon Son Chung, Andrew Senior, Oriol Vinyals, et al., “Lip reading sentences in the wild,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 6447–6456. [4] Hang Chen, Hengshun Zhou, Jun Du, et al., “The first multimodal information based speech processing (misp) challenge: Data, tasks, baselines and results,” in Proc. ICASSP. IEEE, 2022, pp. 9266–9270. [5] Gaopeng Xu, Song Yang, Wei Li, et al., “Channel-wise avfusion attention for multi-channel audio-visual speech recognition,” in Proc. ICASSP. IEEE, 2022, pp. 9251–9255. [6] Wei Wang, Xun Gong, Yifei Wu, et al., “The sjtu system for multimodal information based speech processing challenge 2021,” in Proc. ICASSP. IEEE, 2022, pp. 9261–9265. [7] Ashish Arora, Desh Raj, Aswin Shanmugam Subramanian, et al., “The JHU Multi-Microphone Multi-Speaker ASR System for the CHiME-6 Challenge,” in Proc. 6th International Workshop on Speech Processing in Everyday Environments, 2020, pp. 48–54. [8] David Snyder, Daniel Garcia-Romero, Gregory Sell, et al., “Xvectors: Robust dnn embeddings for speaker recognition,” in Proc. ICASSP. IEEE, 2018, pp. 5329–5333. [9] Ivan Medennikov, Maxim Korenevsky, Tatiana Prisyach, et al., “The STC System for the CHiME-6 Challenge,” in Proc. 6th International Workshop on Speech Processing in Everyday Environments, 2020, pp. 36–41. [10] Neville Ryant, Kenneth Church, Christopher Cieri, et al., “Third dihard challenge evaluation plan,” arXiv preprint arXiv:2006.05815, 2020. [11] Hani Yehia, Philip Rubin, and Eric Vatikiotis-Bateson, “Quantitative association of vocal-tract and facial behavior,” Speech Communication, vol. 26, no. 1-2, pp. 23–43, 1998. [12] Athanasios Noulas, Gwenn Englebienne, and Ben JA Krose, “Multimodal speaker diarization,” IEEE Transactions on pattern analysis and machine intelligence, vol. 34, no. 1, pp. 79– 93, 2011. [13] Israel D Gebru, Sileye Ba, Xiaofei Li, et al., “Audio-visual speaker diarization based on spatiotemporal bayesian fusion,” IEEE transactions on pattern analysis and machine intelligence, vol. 40, no. 5, pp. 1086–1099, 2017. [14] Rehan Ahmad, Syed Zubair, Hani Alquhayz, et al., “Multimodal speaker diarization using a pre-trained audio-visual synchronization model,” Sensors, vol. 19, no. 23, pp. 5163, 2019. [15] Yifan Ding, Yong Xu, Shi-Xiong Zhang, et al., “Selfsupervised learning for audio-visual speaker diarization,” in Proc. ICASSP. IEEE, 2020, pp. 4367–4371. [16] Pingchuan Ma, Stavros Petridis, and Maja Pantic, “End-to-end audio-visual speech recognition with conformers,” in Proc. ICASSP. IEEE, 2021, pp. 7613–7617. [17] Fei Tao and