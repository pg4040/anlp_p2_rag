Title: LOFT: LOCAL PROXY FINE-TUNING FOR IMPROV- ING TRANSFERABILITY OF ADVERSARIAL ATTACKS AGAINST LARGE LANGUAGE MODEL
Authors: Muhammad Ahmed Shah, Roshan Sharma, Hira Dhamyal, Raphael Olivier, Ankit Shah, Dareen Alharthi, Hazim T Bukhari, Massa Baali, Soham Deshmukh, Michael Kuhlmann, Bhiksha Raj, Rita Singh
Section: 1 INTRODUCTION
2023)) and discovered that a sizable fraction of them do not, in fact, contain the requested harmful information, although they do bypass the alignment of the model. To remedy this we propose and conduct human evaluation of the responses, and show that responses generated by our approach using LoFT proxies yielded truly harmful content up to 43.6% of the time from ChatGPT-3.5, a 10× improvement over the baseline. In summary, this paper makes the following contributions: 1. We propose LoFT , a technique for locally fine tuning a proxy model for the purpose of generating adversarial attacks 2. We demonstrate that adversarial attacks generated on LoFT proxies successfully elicit valid responses from GPT-4 89.7% of the time – a relative improvement of 143% over prior work (Zou et al., 2023). 3. We discover that even when an adversarial attack induces the target LLM to respond to a harmful query, the response does not necessarily contain harmful content. Indicating the inadequacy of existing metrics at faithfully measuring the success rate of the attack, and consequently the safety risk. 1a valid response is one that does not contain a refusal to answer. 4. We conduct human evaluation of the adversarial attacks generated on LoFT proxies, as well as non-fine-tuned proxies, and find that LoFT proxy based attacks yielded truly harmful content up to 43.6% of the time from ChatGPT-3.5.