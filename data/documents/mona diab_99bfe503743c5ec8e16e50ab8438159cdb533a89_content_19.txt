Title: The Troubling Emergence of Hallucination in Large Language Models – An Extensive Definition, Quantification, and Prescriptive Remediations
Authors: Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S.M Towhidul Islam Tonmoy, Aman Chadha, Amit Sheth, Amitava Das
Section: 
LLMs that utilize a repertoire of recent techniques under the hood that have enabled their exceptional capabilities, namely: • FlashAttention (Dao et al., 2022) for memory-efficient exact attention. • Multi-Query Attention (Shazeer, 2019) for memory bandwidth efficiency. • SwiGLU (Shazeer, 2020) as the activation function instead of ReLU (Agarap, 2019). • ALiBi (Press et al., 2022) for larger context width. • RMSNorm (Zhang and Sennrich, 2019) for per-normalization. • RoPE (Su et al., 2022) to improve the expressivity of positional embeddings, etc. C.2 Details on Large Language Models Model details are given in Table 6. We use HuggingFace (Wolf et al., 2020) and OpenAI for implementing the large language models to generate the dataset. D - Prompt sources We used two datasets to curate HILT: (i) New York Times Tweets (NYT) for factually correct and (ii) Politifact dataset (Politifact) for factually incorrect prompts. E What is a high entropy vs. low entropy word? In the context of language modeling, a high entropy word refers to a word or token that has a high level of uncertainty or unpredictability in its occurrence. In other words, it is a word that is relatively rare or has a low probability of appearing in a given context. Entropy is often used to quantify the level of unpredictability associated with generating specific words or tokens. When a language model encounters a high entropy word, it means that the model has greater difficulty in accurately predicting or generating that word based on the context or preceding words. High entropy words are often less frequent in the training data or have complex patterns of occurrence. For example, in a language model trained on news articles, words like “pneumonoultramicroscopicsilicovolcanoconiosis” (a technical term for a lung disease) would likely have high entropy, as they are infrequent and occur in specific contexts. On the other hand, a low entropy word refers to a word or token that has a relatively high predictability or a limited range of potential next words given the context. In other words, it is a word that occurs frequently and is highly expected in a specific context. When a language model encounters a low entropy word, it means that the model has a higher confidence or accuracy in predicting or generating that word based on the context or preceding words. For example, in a language model trained on English text, common words like “the,” “and,” or “is” have low entropy because they occur frequently and are highly predictable in many contexts. These words tend to have