Title: The Troubling Emergence of Hallucination in Large Language Models – An Extensive Definition, Quantification, and Prescriptive Remediations
Authors: Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S.M Towhidul Islam Tonmoy, Aman Chadha, Amit Sheth, Amitava Das
Section: 
HVI will prove to be a useful tool for assessing the categorical impacts of these proposed mitigation techniques. 2 A Holistic View of the Hallucination Spectrum: its Types and Scales The issue of hallucination garnered research attention as early as (Maynez et al., 2020). However, with the growing size of LLMs (empirical evidence provided in Section 6), there is a corresponding increase in LLMs’ susceptibility to hallucination. Consequently, there is a growing interest within the research community to study and understand hallucination to design mitigation techniques. Researchers have loosely defined hallucinations and studied various notions of hallucinations in isolation. Early exploration of factual vs. nonfactual prompts for checking factuality of LMs is addressed in (Lee et al., 2022). A recent survey conducted by (Maynez et al., 2020), categorized hallucination into two limited classes: intrinsic and extrinsic. Another recent paper (Ladhak et al., 2023b), delved into an intriguing type of hallucination known as name-nationality category hallucination. Several other papers (Raunak et al., 2021; Maynez et al., 2020) have explored taskspecific categories of hallucination, such as summarization, question-answering, machine translation, etc. Preliminary exploration of factual versus non-factual prompts is also addressed in (Lee et al., 2022). However, we purposefully avoid confining our study to a specific task to study the nuances of hallucination. Our main contention is that hallucination can occur in any NLG task, necessitating a thorough examination based on the fundamental principles of text generation from a given prompt. The findings from this study can be applied and extended to various NLP tasks. Thus, this paper aims to offer a comprehensive categorization of hallucination, as outlined below (see Fig. 1). 2.1 Orientations of Hallucination We introduce two primary orientations of hallucination: (i) Factual Mirage (FM) and (ii) Silver Lining (SL), defined and exemplified below. 2.1.1 Factual Mirage Factual mirage (FM) is defined as the phenomenon wherein an LLM engages in hallucination or distortion of a given prompt that is factually correct. FM can be subdivided into two distinct sub-categories. MILD Prompt: Capital of France AI-generated text: ...Paris is also the world fashion capital... Fact: Paris. Intrinsic factual mirage (IFM): In the following example, the LLM is providing a correct response while adding additional supplementary facts such as “the world fashion capital,” resulting in distortion or hallucination, has also been described in (Cao et al., 2022). Extrinsic factual mirage (EFM): EFM refers to the phenomenon where an LLM deviates from factual accuracy. For example: ALARMING Prompt: Engineering effort to build Eiffel tower AI-generated text: ...Designed by Gustave Eiffel, it was inaugurated